<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>dev RSS - backend</title>
    <link>https://github.com</link>
    <description>dev backend 카테고리 RSS</description>
    <lastBuildDate>Sat, 14 Feb 2026 09:38:46 GMT</lastBuildDate>
    <item>
      <title>Scaling LLM Post-Training at Netflix</title>
      <link>https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194?source=rss----2615bd06b42e---4</guid>
      <pubDate>Fri, 13 Feb 2026 08:05:33 GMT</pubDate>
      <content:encoded>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/baolin-li-659426115/&quot;&gt;Baolin Li&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/lingyi-liu-4b866016/&quot;&gt;Lingyi Liu&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/binh-tang-3b76557b/&quot;&gt;Binh Tang&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shaojingli/&quot;&gt;Shaojing Li&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;Pre-training gives Large Language Models (LLMs) broad linguistic ability and general world knowledge, but post-training is the phase that actually aligns them to concrete intents, domain constraints, and the reliability requirements of production environments. At Netflix, we are exploring how LLMs can enable new member experiences across recommendation, personalization, and search, which requires adapting generic foundation models so they can better reflect our catalog and the nuances of member interaction histories. At Netflix scale, post-training quickly becomes an engineering problem as much as a modeling one: building and operating complex data pipelines, coordinating distributed state across multi-node GPU clusters, and orchestrating workflows that interleave training and inference. This blog describes the architecture and engineering philosophy of our internal &lt;strong&gt;Post-Training Framework&lt;/strong&gt;, built by the AI Platform team to hide infrastructure complexity so researchers and model developers can focus on model innovation — not distributed systems plumbing.&lt;/p&gt;&lt;h3&gt;A Model Developer’s Post-Training Journey&lt;/h3&gt;&lt;p&gt;Post-training often starts deceptively simply: curate proprietary domain data, load an open-weight model from Hugging Face, and iterate batches through it. At the experimentation scale, that’s a few lines of code. But when fine-tuning production-grade LLMs at scale, the gap between “running a script” and “robust post-training” becomes an abyss of engineering edge cases.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/873/1*at60qZXd0j6SphkjzdmazQ.png&quot; /&gt;&lt;figcaption&gt;Figure 1. Simple steps to post-train an open-weight model.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Getting the data right&lt;/h4&gt;&lt;p&gt;On paper, post-training is straightforward: choose a tokenizer, preprocess the dataset, and build a dataloader. In practice, data preparation is where things break. High-quality post-training — instruction following, multi-turn dialogue, Chain-of-Thought — depends on precisely controlling which tokens contribute to the loss. Hugging Face chat templates serialize conversations, but don’t specify what to train on versus ignore. The pipeline must apply explicit loss masking so only assistant tokens are optimized; otherwise the model learns from prompts and other non-target text, degrading quality.&lt;/p&gt;&lt;p&gt;Variable sequence length is another pitfall. Padding within a batch can waste compute, and uneven shapes across FSDP workers can cause GPU synchronization overhead. A more GPU-efficient approach is to pack multiple samples into fixed-length sequences and use a “document mask” to prevent cross-attention across samples, reducing padding and keeping shapes consistent.&lt;/p&gt;&lt;h4&gt;Setting up the model&lt;/h4&gt;&lt;p&gt;Loading an open-source checkpoint sounds simple until the model no longer fits on one GPU. At that point you need a sharding strategy (e.g., FSDP, TP) and must load partial weights directly onto the device mesh to avoid ever materializing the full model on a single device.&lt;/p&gt;&lt;p&gt;After loading, you still need to make the model trainable: choose full fine-tuning vs. LoRA, and apply optimizations like activation checkpointing, compilation, and correct precision settings (often subtle for RL, where rollout and policy precision must align). Large vocabularies (&amp;gt;128k) add a further memory trap: logits are&lt;em&gt; [batch, seq_len, vocab] &lt;/em&gt;and can spike peak memory. Common mitigations include dropping ignored tokens before projection and computing logits/loss in chunks along the sequence dimension.&lt;/p&gt;&lt;h4&gt;Starting the training&lt;/h4&gt;&lt;p&gt;Even with data and models ready, production training is not a simple “for loop”. The system must support everything from SFT’s forward/backward pass to on-policy RL workflows that interleave rollout generation, reward/reference inference, and policy updates.&lt;/p&gt;&lt;p&gt;At Netflix scale, training runs as a distributed job. We use Ray to orchestrate workflows via actors, decoupling modeling logic from hardware. Robust runs also require experiment tracking (model quality metrics like loss and efficiency metrics like MFU) and fault tolerance via standardized checkpoints to resume cleanly after failures.&lt;/p&gt;&lt;p&gt;These challenges motivate a post-training framework that lets developers focus on modeling rather than distributed systems and operational details.&lt;/p&gt;&lt;h3&gt;The Netflix Post-Training Framework&lt;/h3&gt;&lt;p&gt;We built Netflix’s LLM post-training framework so Netflix model developers can turn ideas like those in Figure 1 into scalable, robust training jobs. It addresses the engineering hurdles described above, and also constraints that are specific to the Netflix ecosystem. Existing tools (e.g., Thinking Machines’ &lt;a href=&quot;https://thinkingmachines.ai/tinker/&quot;&gt;Tinker&lt;/a&gt;) work well for standard chat and instruction-tuning, but their structure can limit deeper experimentation. In contrast, our internal use cases often require architectural variation (for example, customizing output projection heads for task-specific objectives), expanded or nonstandard vocabularies driven by semantic IDs or special tokens, and even transformer models pre-trained from scratch on domain-specific, non-natural-language sequences. Supporting this range requires a framework that prioritizes flexibility and extensibility over a fixed fine-tuning paradigm.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*z2IFH-4iIQ5qxARTur-wQA.png&quot; /&gt;&lt;figcaption&gt;Figure 2. The post-training library within Netflix stack&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 2 shows the end-to-end stack from infrastructure to trained models. At the base is Mako, Netflix’s internal ML compute platform, which provisions GPUs on AWS. On top of Mako, we run robust open-source components — PyTorch, Ray, and vLLM — largely out of the box. Our post-training framework sits above these foundations as a library: it provides reusable utilities and standardized training recipes for common workflows such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Reinforcement Learning (RL), and Knowledge Distillation. Users typically express jobs as configuration files that select a recipe and plug in task-specific components.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*RzZzr3wADPwism5CQP8mYw.png&quot; /&gt;&lt;figcaption&gt;Figure 3. Main components developed for the post-training framework&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 summarizes the modular components we built to reduce complexity across four dimensions. As with most ML systems, training success hinges on three pillars — &lt;strong&gt;Data&lt;/strong&gt;, &lt;strong&gt;Model&lt;/strong&gt;, and &lt;strong&gt;Compute&lt;/strong&gt; — and the rise of RL fine-tuning adds a fourth pillar: &lt;strong&gt;Workflow&lt;/strong&gt;, to support multi-stage execution patterns that don’t fit a simple training loop. Below, we detail the specific abstractions and features the framework provides for each of these dimensions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; Dataset abstractions for SFT, reward modeling, and RL; high-throughput streaming from cloud and disk for datasets that exceed local storage; and asynchronous, on-the-fly sequence packing to overlap CPU-heavy packing with GPU execution and reduce idle time.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Support for modern architectures (e.g., Qwen3, Gemma3) and Mixture-of-Experts variants (e.g., Qwen3 MoE, GPT-OSS); LoRA integrated into model definitions; and high-level sharding APIs so developers can distribute large models across device meshes without writing low-level distributed code.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Compute:&lt;/strong&gt; A unified job submission interface that scales from a single node to hundreds of GPUs; MFU (Model FLOPS Utilization) monitoring that remains accurate under custom architectures and LoRA; and comprehensive checkpointing (states of trained parameters, optimizer, dataloader, data mixer, etc.) to enable exact resumption after interruptions.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; Support for training paradigms beyond SFT, including complex online RL. In particular, we extend Single Program, Multiple Data (SPMD) style SFT workloads to run online RL with a hybrid single-controller + SPMD execution model, which we’ll describe next.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Today, this framework supports research use cases ranging from post-training large-scale foundation models to fine-tuning specialized expert models. By standardizing these workflows, we’ve lowered the barrier for teams to experiment with advanced techniques and iterate more quickly.&lt;/p&gt;&lt;h3&gt;Learnings from Building the Post-Training Framework&lt;/h3&gt;&lt;p&gt;Building a system of this scope wasn’t a linear implementation exercise. It meant tracking a fast-moving open-source ecosystem, chasing down failure modes that only appear under distributed load, and repeatedly revisiting architectural decisions as the post-training frontier shifted. Below are three engineering learnings and best practices that shaped the framework.&lt;/p&gt;&lt;h4&gt;Scaling from SFT to RL&lt;/h4&gt;&lt;p&gt;We initially designed the library around Supervised Fine-Tuning (SFT): relatively static data flow, a single training loop, and a Single Program, Multiple Data (SPMD) execution model. That assumption stopped holding in 2025. With DeepSeek-R1 and the broader adoption of efficient on-policy RL methods like GRPO, SFT became table stakes rather than the finish line. Staying close to the frontier required infrastructure that could move from “offline training loop” to “multi-stage, on-policy orchestration.”&lt;/p&gt;&lt;p&gt;SFT’s learning signal is dense and immediate: for each token position we compute logits over the full vocabulary and backpropagate a differentiable loss. Infrastructure-wise, this looks a lot like pre-training and maps cleanly to SPMD — every GPU worker runs the same step function over a different shard of data, synchronizing through Pytorch distributed primitives.&lt;/p&gt;&lt;p&gt;On-policy RL changes the shape of the system. The learning signal is typically sparse and delayed (e.g., a scalar reward at the end of an episode), and the training step depends on data generated by the current policy. Individual sub-stages — policy updates, rollout generation, reference model inference, reward model scoring — can each be implemented as SPMD workloads, but the end-to-end algorithm needs explicit coordination: you’re constantly handing off artifacts (prompts, sampled trajectories, rewards, advantages) across stages and synchronizing their lifecycle.&lt;/p&gt;&lt;p&gt;In our original SFT architecture, the driver node was intentionally “thin”: it launched N identical Ray actors, each encapsulating the full training loop, and scaling meant launching more identical workers. That model breaks down for RL. RL required us to decompose the system into distinct roles — Policy, Rollout Workers, Reward Model, Reference Model, etc. — and evolve the driver into an active controller that encodes the control plane: when to generate rollouts, how to batch and score them, when to trigger optimization, and how to manage cluster resources across phases.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*5lhd3rDmexD0KGoHy78CZQ.png&quot; /&gt;&lt;figcaption&gt;Figure 4. Architectural differences of SFT and RL framework&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 4 highlights this shift. To add RL support without reinventing distributed orchestration from scratch, we integrated the core infrastructure from the open-source &lt;a href=&quot;https://github.com/verl-project/verl&quot;&gt;&lt;strong&gt;Verl&lt;/strong&gt;&lt;/a&gt; library to manage Ray actor lifecycle and GPU resource allocation. Leveraging Verl’s backend let us focus on the “modeling surface area” — our Data/Model/Compute abstractions and internal optimizations — while keeping orchestration concerns decoupled. The result is a hybrid design: a unified user interface where developers can move between SFT and RL workflows without adopting an entirely different mental model or API set.&lt;/p&gt;&lt;h4&gt;Hugging Face-Centric Experience&lt;/h4&gt;&lt;p&gt;The Hugging Face Hub has effectively become the default distribution channel for open-weight LLMs, tokenizers, and configs. We designed the framework to stay close to that ecosystem rather than creating an isolated internal standard. Even when we use optimized internal model representations for speed, we load and save checkpoints in standard Hugging Face formats. This avoids “walled garden” friction and lets teams pull in new architectures, weights, and tokenizers quickly.&lt;/p&gt;&lt;p&gt;This philosophy also shaped our tokenizer story. Early on, we bound directly to low-level tokenization libraries (e.g., SentencePiece, tiktoken) to maximize control. In practice, that created a costly failure mode: silent training–serving skew. Our inference stack (vLLM) defaults to Hugging Face AutoTokenizer, and tiny differences in normalization, special token handling, or chat templating can yield different token boundaries — exactly the kind of mismatch that shows up later as inexplicable quality regressions. We fixed this by making Hugging Face AutoTokenizer the single source of truth. We then built a thin compatibility layer (BaseHFModelTokenizer) to handle post-training needs — setting padding tokens, injecting generation markers to support loss masking, and managing special tokens / semantic IDs — while ensuring the byte-level tokenization path matches production.&lt;/p&gt;&lt;p&gt;We do take a different approach for model implementations. Rather than training directly on transformers model classes, we maintain our own optimized, unified model definitions that can still load/save Hugging Face checkpoints. This layer is what enables framework-level optimizations — e.g., FlexAttention, memory-efficient chunked cross-entropy, consistent MFU accounting, and uniform LoRA extensibility — without re-implementing them separately for every model family. A unified module naming convention also makes it feasible to programmatically locate and swap components (Attention, MLP, output heads) across architectures, and provides a consistent surface for Tensor Parallelism and FSDP wrapping policies.&lt;/p&gt;&lt;p&gt;The trade-off is clear: supporting a new model family requires building a bridge between the Hugging Face reference implementation and our internal definition. To reduce that overhead, we use AI coding agents to automate much of the conversion work, with a strict &lt;strong&gt;logit verifier&lt;/strong&gt; as the gate: given random inputs, our internal model must match the Hugging Face logits within tolerance. Because the acceptance criterion is mechanically checkable, agents can iterate autonomously until the implementation is correct, dramatically shortening the time-to-support for new architectures.&lt;/p&gt;&lt;p&gt;Today, this design means we can only train architectures we explicitly support — an intentional constraint shared by other high-performance systems like &lt;a href=&quot;https://huggingface.co/docs/transformers/main/transformers_as_backend&quot;&gt;vLLM, SGLang&lt;/a&gt;, and &lt;a href=&quot;https://github.com/pytorch/torchtitan/pull/2048&quot;&gt;torchtitan&lt;/a&gt;. To broaden coverage, we plan to add a fallback Hugging Face backend, similar to the compatibility patterns these projects use: users will be able to run training directly on native transformers models for rapid exploration of novel architectures, with the understanding that some framework optimizations and features may not apply in that mode.&lt;/p&gt;&lt;h4&gt;Providing Differential Value&lt;/h4&gt;&lt;p&gt;A post-training framework is only worth owning if it delivers clear value beyond assembling OSS components. We build on open source for velocity, but we invest heavily where off-the-shelf tools tend to be weakest: performance tuned to our workload characteristics, and integration with Netflix-specific model and business requirements. Here are some concrete examples:&lt;/p&gt;&lt;p&gt;First, we optimize training efficiency for our real use cases. A representative example is extreme variance in sequence length. In FSDP-style training, long-tail sequences create stragglers: faster workers end up waiting at synchronization points for the slowest batch, lowering utilization. Standard bin-packing approaches help, but doing them offline at our data scale can add substantial preprocessing latency and make it harder to keep datasets fresh. Instead, we built on-the-fly sequence packing that streams samples from storage and dynamically packs them in memory. Packing runs asynchronously, overlapping CPU work with GPU compute. Figure 5 shows the impact: for our most skewed dataset, on-the-fly packing improved the effective token throughput by up to 4.7x.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Z2mdtXVFJsr764NihkguIA.png&quot; /&gt;&lt;figcaption&gt;Figure 5. Training throughput on two of our internal datasets on A100 and H200 GPUs&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We also encountered subtler performance cliffs around vocabulary expansion. Our workloads frequently add custom tokens and semantic IDs. We found that certain vocabulary sizes could cause the language model head to fall back from a highly optimized cuBLAS kernel to a much slower CUTLASS path, tripling that layer’s execution time. The framework now automatically pads vocabulary sizes to multiples of 64 so the compiler selects the fast kernel, preserving throughput without requiring developers to know these low-level constraints.&lt;/p&gt;&lt;p&gt;Second, owning the framework lets us support “non-standard” transformer use cases that generic LLM tooling rarely targets. For example, some internal models are trained on member interaction event sequences rather than natural language, and may require bespoke RL loops that integrate with highly-customized inference engines and optimize business-defined metrics. These workflows demand custom environments, reward computation, and orchestration patterns — while still needing the same underlying guarantees around performance, tracking, and fault tolerance. The framework is built to accommodate these specialized requirements without fragmenting into one-off pipelines, enabling rapid iteration.&lt;/p&gt;&lt;h3&gt;Wrap up&lt;/h3&gt;&lt;p&gt;Building the Netflix Post-Training Framework has been a continual exercise in balancing standardization with specialization. By staying anchored to the open-source ecosystem, we’ve avoided drifting into a proprietary stack that diverges from where the community is moving. At the same time, by owning the core abstractions around Data, Model, Compute, and Workflow, we’ve preserved the freedom to optimize for Netflix-scale training and Netflix-specific requirements.&lt;/p&gt;&lt;p&gt;In the process, we’ve moved post-training from a loose collection of scripts into a managed, scalable system. Whether the goal is maximizing SFT throughput, orchestrating multi-stage on-policy RL, or training transformers over member interaction sequences, the framework provides a consistent set of primitives to do so reliably and efficiently. As the field shifts toward more agentic, reasoning-heavy, and multimodal architectures, this foundation will help us translate new ideas into scalable GenAI prototypes — so experimentation is constrained by our imagination, not by operational complexity.&lt;/p&gt;&lt;h3&gt;Acknowledgements&lt;/h3&gt;&lt;p&gt;This work builds on the momentum of the broader open-source ML community. We’re especially grateful to the teams and contributors behind Torchtune, Torchtitan, and Verl, whose reference implementations and design patterns informed many of our training framework choices — particularly around scalable training recipes, distributed execution, and RL-oriented orchestration. We also thank our partner teams in Netflix AI for Member Systems for close collaboration, feedback, and shared problem-solving throughout the development and rollout of the Post-Training Framework, and the Training Platform team for providing the robust infrastructure and operational foundation that makes large-scale post-training possible.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0046f8790194&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194&quot;&gt;Scaling LLM Post-Training at Netflix&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Automating RDS Postgres to Aurora Postgres Migration</title>
      <link>https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f?source=rss----2615bd06b42e---4</guid>
      <pubDate>Thu, 12 Feb 2026 14:07:19 GMT</pubDate>
      <content:encoded>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/ramsrivatsa/&quot;&gt;Ram Srivasta Kannan,&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/wale-akintayo-30782a82/&quot;&gt;Wale Akintayo&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jay-bharadwaj-4b310ab8/&quot;&gt;Jay Bharadwaj&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/john-crimmins-39730b3a/&quot;&gt;John Crimmins&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shengwei4721/&quot;&gt;Shengwei Wang,&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/zhitao-cathy-zhu/&quot;&gt;Zhitao Zhu&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;In 2024, the Online Data Stores team at Netflix conducted a comprehensive review of the relational database technologies used across the company. This evaluation examined functionality, performance, and total cost of ownership across our database ecosystem. Based on this analysis, we decided to standardize on &lt;strong&gt;Amazon Aurora PostgreSQL as the primary relational database&lt;/strong&gt; offering for Netflix teams.&lt;/p&gt;&lt;p&gt;Several key factors influenced this decision:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;PostgreSQL already &lt;/strong&gt;underpinned&lt;strong&gt; &lt;/strong&gt;the majority of our relational workloads, which made it a natural foundation for standardization. Internal evaluations revealed that Aurora PostgreSQL had supported over 95% of the applications and workloads running on other relational databases across our internal services.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Industry momentum had continued to shift toward PostgreSQL, &lt;/strong&gt;driven by its open ecosystem, strong community support, and broad adoption across modern data platforms.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Aurora’s cloud-native, distributed architecture&lt;/strong&gt; provided clear advantages in scalability, high availability, and elasticity compared to traditional single-node PostgreSQL deployments.&lt;/li&gt;&lt;li&gt;Aurora PostgreSQL offered a &lt;strong&gt;rich feature set&lt;/strong&gt;, along with a &lt;strong&gt;strong, forward-looking roadmap&lt;/strong&gt; aligned with the needs of large-scale, globally distributed applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;A Clear Migration Path Forward&lt;/h3&gt;&lt;p&gt;As part of this strategic shift, one of our key initiatives for 2024/2025 was migrating existing users to Aurora PostgreSQL. This effort began with RDS PostgreSQL migrations and will expand to include migrations from other relational systems in subsequent phases.&lt;/p&gt;&lt;p&gt;As a data platform organization, our goal is to make this evolution predictable, well-supported, and minimally disruptive. This allows teams to adopt Aurora PostgreSQL at a pace that aligns with their product and operational roadmaps, while we move toward a unified and scalable relational data platform across the organization.&lt;/p&gt;&lt;h3&gt;Database Migration: More Than a Simple Transfer&lt;/h3&gt;&lt;p&gt;Migrating a database involves far more than copying rows from one system to another. It is a coordinated process of transitioning both data and database functionality while preserving correctness, availability, and performance. At scale, a well-designed migration must minimize disruption to applications and ensure a clean, deterministic handoff from the old system to the new one.&lt;/p&gt;&lt;p&gt;Most database migrations follow a common set of high-level steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Data Replication&lt;/strong&gt;: Data is first copied from the source database to the destination, typically using replication, so that ongoing changes are continuously captured and applied.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Quiescence&lt;/strong&gt;: Write traffic to the source database is halted, allowing the destination to fully catch up and eliminate any remaining divergence.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: The system verifies that the source and destination databases are fully synchronized and contain identical data.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Cutover&lt;/strong&gt;: Client applications are reconfigured to point to the destination database, which becomes the new primary source of truth.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Challenges&lt;/h3&gt;&lt;h4&gt;Operational Challenges&lt;/h4&gt;&lt;p&gt;Migrating to a new relational database at Netflix scale presents substantial operational challenges. With a fleet approaching 400 PostgreSQL clusters, manually migrating each one is simply not scalable for the data platform team. Such an approach would require a significant amount of time, introduce the risk of human error, and necessitate considerable hands-on engineering effort. Compounding the problem, coordinating downtime across the many interconnected services that depend on each database is extremely cumbersome at this scale.&lt;/p&gt;&lt;p&gt;To address these challenges, we designed a self-service migration workflow that enables service owners to run their own RDS PostgreSQL to Aurora PostgreSQL migrations. The workflow automatically handles orchestration, safety checks, and correctness guarantees end-to-end, resulting in lower operational overhead and a predictable, reliable migration experience.&lt;/p&gt;&lt;h3&gt;Technical challenges&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; — We must guarantee that all data from the source cluster is fully and safely migrated to the destination within a very tight window, with no possibility of data loss.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Minimal downtime — &lt;/strong&gt;Some downtime is unavoidable during migration, as applications must briefly pause write traffic while cutting over to Aurora PostgreSQL. For higher-tier services that power critical parts of the Netflix ecosystem, this window must be kept extremely short to prevent user-facing impact and maintain service reliability.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;No control over client applications&lt;/strong&gt; — As the platform team, we manage the databases, but application teams handle the read and write operations. We cannot assume that they have the ability to pause writes on demand, nor do we want to expose such controls to them, as mistakes could lead to data inconsistencies post migration. Therefore, building a self-service migration pipeline requires creative control-plane solutions to halt traffic, ensuring that no writes occur during the validation and cutover phases.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;No direct access to RDS credentials&lt;/strong&gt; — The migration automation must perform replication, quiescence, and validation without requesting database credentials from users or relying on manual authentication. Source databases are often tightly secured, allowing access only from client applications, but more importantly, requiring credential access — even if it were possible — would significantly increase operational overhead and risk. At the same time, the migration platform may operate in environments without direct access to the source database, making traditional verification or parity checks impossible.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;No Degradation in Performance&lt;/strong&gt; — The migration process must not impact the performance or stability of production databases once they are running in the Aurora PostgreSQL ecosystem.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Full Ecosystem Parity&lt;/strong&gt; — Beyond migrating the core database, associated components such as parameter groups, read replicas, and replication slots must also be migrated to ensure functional equivalence.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Minimal User Effort&lt;/strong&gt; — Since we rely on teams who are not database experts to perform migrations, the process must be simple, intuitive, and fully self-guided.&lt;/p&gt;&lt;h3&gt;AWS recommended migration techniques&lt;/h3&gt;&lt;h4&gt;Using a snapshot&lt;/h4&gt;&lt;p&gt;One of the simplest AWS-recommended approaches for migrating from RDS PostgreSQL to Aurora PostgreSQL is based on snapshots. In this model, write traffic to the source PostgreSQL database is first stopped. A manual snapshot of the RDS PostgreSQL instance is then taken and migrated to Aurora, where AWS converts it into an Aurora-compatible format.&lt;br&gt; &lt;br&gt;Once the conversion completes, a new Aurora PostgreSQL cluster is created from the snapshot. After the cluster is brought online and validated, application traffic is redirected to the Aurora endpoint, completing the migration.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.RDSPostgreSQL.Import.Console.html&quot;&gt;Reference&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Using an Aurora read replica&lt;/h4&gt;&lt;p&gt;In the read-replica–based approach, an Aurora PostgreSQL read replica is created from an existing RDS PostgreSQL instance. AWS establishes continuous, asynchronous replication from the RDS source to the Aurora replica, allowing ongoing changes to be streamed in near real time.&lt;/p&gt;&lt;p&gt;Because replication runs continuously, the Aurora replica remains closely synchronized with the source database. This enables teams to provision and validate the Aurora environment — including configuration, connectivity, and performance characteristics — while production traffic continues to flow to the source.&lt;/p&gt;&lt;p&gt;When the replication lag is sufficiently low, write traffic is briefly paused to allow the replica to fully catch up. The Aurora read replica is then promoted to a standalone Aurora PostgreSQL cluster, and application traffic is redirected to the new Aurora endpoint. This approach significantly reduces downtime compared to snapshot-based migrations and is well-suited for production systems that require minimal disruption.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/647/1*_xydKq_VWpaxyqcwCwQyuA.png&quot; /&gt;&lt;figcaption&gt;Migration Strategy Trade-Offs&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;These differences represent the key considerations when choosing a migration strategy from RDS PostgreSQL to Aurora PostgreSQL. For our automation, we opted for the Aurora Read Replica approach, trading increased implementation complexity for a significantly shorter downtime window for client applications.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/682/1*SALMQ9wXHsJY9kBTmZ1D5w.png&quot; /&gt;&lt;figcaption&gt;Netflix RDS PostgreSQL Deployment Architecture&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In Netflix’s RDS setup, a &lt;a href=&quot;https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6&quot;&gt;Data Access Layer&lt;/a&gt; (DAL) sits between applications and backend databases, acting as middleware that centralizes database connectivity, security, and traffic routing on behalf of client applications.&lt;/p&gt;&lt;p&gt;On the client side, applications connect through a forward proxy that manages mutual TLS (mTLS) authentication and establishes a secure tunnel to the Data Gateway service. The Data Gateway, acting as a reverse proxy for database servers, terminates client connections, enforces centralized authentication and authorization, and forwards traffic to the appropriate RDS PostgreSQL instance.&lt;/p&gt;&lt;p&gt;This layered design ensures that applications never handle raw database credentials, provides a consistent and secure access pattern across all datastore types, and delivers isolated, transparent connectivity to managed PostgreSQL clusters. While the primary goal of this architecture is to enforce strong security controls and standardize how applications access external AWS data stores, it also allows backend databases to be switched transparently via configuration, enabling controlled, low-downtime migrations.&lt;/p&gt;&lt;h3&gt;Migration Process&lt;/h3&gt;&lt;p&gt;The Platform team’s goal is to deliver a fully automated, self-service workflow that helps with the migration of customer RDS PostgreSQL instances to Aurora PostgreSQL clusters. This migration tool orchestrates the entire process — from preparing the source environment, initializing the Aurora read replica, and maintaining continuous synchronization, all the way through to cutover — without requiring any database credentials or manual intervention from the customer.&lt;/p&gt;&lt;p&gt;Designed for minimal downtime and seamless user experience, the workflow ensures full ecosystem parity between RDS and Aurora, preserving performance characteristics and operational behavior while enabling customers to benefit from Aurora’s improved scalability, resilience, and cost efficiency.&lt;/p&gt;&lt;h3&gt;Data Replication Phase&lt;/h3&gt;&lt;h4&gt;Enable Automated Backups&lt;/h4&gt;&lt;p&gt;Automated backups must be enabled on the source database because the Aurora &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.Configuration.html?utm_source=chatgpt.com&quot;&gt;read replica&lt;/a&gt; is initialized from a consistent snapshot of the source and then kept in sync through continuous replication. Automated backups provide the stable snapshot required to bootstrap the replica, along with the continuous streaming of write-ahead log (WAL) records needed to keep the read replica closely synchronized with the source.&lt;/p&gt;&lt;h4&gt;Port RDS parameters to an Aurora parameter group&lt;/h4&gt;&lt;p&gt;We create a dedicated Aurora parameter group for each cluster and migrate all RDS-compatible parameters from the source RDS instance. This ensures that the Aurora cluster inherits the same configuration settings — such as memory configuration, connection limits, query planner behavior, and other PostgreSQL engine parameters that have equivalents in Aurora. Parameters that are unsupported or behave differently in Aurora are either omitted or adjusted according to Aurora best practices.&lt;/p&gt;&lt;h4&gt;Create an Aurora read replica cluster and instance&lt;/h4&gt;&lt;p&gt;Creating an Aurora read replica cluster is a critical step in migrating from RDS PostgreSQL to Aurora PostgreSQL. At this stage, the Aurora cluster is created and attached to the RDS PostgreSQL primary as a replica, establishing continuous replication from the source RDS PostgreSQL instance. These Aurora read replicas stay nearly in sync with ongoing changes by streaming write-ahead logs (WAL) from the source, enabling minimal downtime during cutover. The cluster is fully operational for validation and performance testing, but it is not yet writable — RDS remains the authoritative primary.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*a3JN0YW1wuYTuez8oWw5SA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Quiescence Phase&lt;/h4&gt;&lt;p&gt;The goal of the quiescence phase is to transition client applications from the source RDS PostgreSQL instance to the Aurora PostgreSQL cluster as the new primary database, while preserving data consistency during cutover.&lt;/p&gt;&lt;p&gt;The first step in this process is to stop all write traffic to the source RDS PostgreSQL instance to guarantee consistency. To achieve this, we instruct users to halt application-level traffic, which helps prevent issues such as retry storms, queue backlogs, or unnecessary resource consumption when connectivity changes during cutover. This coordination also gives teams time to prepare operationally, for example, by suppressing alerts, notifying downstream consumers, or communicating planned maintenance to their customers.&lt;/p&gt;&lt;p&gt;However, relying solely on application-side controls is unreliable. Operational gaps, misconfigurations, or lingering connections can still modify the source database state, potentially resulting in changes that are not replicated to the destination and leading to data inconsistency or loss. To enforce a clean and deterministic cutover, we also block traffic at the infrastructure layer. This is done by detaching the RDS instance’s security groups to prevent new inbound connections, followed by a reboot of the instance. With security groups removed, no new SQL sessions can be established, and the reboot forcibly terminates any existing connections.&lt;/p&gt;&lt;p&gt;This approach intentionally avoids requiring database credentials or logging into the PostgreSQL server to manually terminate connections. While it may be slower than application- or database-level intervention, it provides a reliably automated and repeatable mechanism to fully quiesce the source RDS PostgreSQL instance before Aurora promotion, eliminating the risk of divergent writes or an inconsistent WAL state.&lt;/p&gt;&lt;h4&gt;Validation Phase&lt;/h4&gt;&lt;p&gt;To determine whether the Aurora read replica has fully caught up with the source RDS PostgreSQL instance, we track replication progress using Aurora’s OldestReplicationSlotLag metric. This metric represents how far the Aurora replica is behind the source in applying write-ahead log (WAL) records.&lt;/p&gt;&lt;p&gt;Once client traffic is halted during quiescence, the source RDS PostgreSQL instance stops producing meaningful WAL entries. At that point, the replication lag should converge to zero, indicating that all WAL records corresponding to real writes have been fully replayed on Aurora.&lt;/p&gt;&lt;p&gt;However, in practice, our experiments show that the metric never settles at a steady zero. Instead, it briefly drops to &lt;strong&gt;0&lt;/strong&gt;, then quickly returns to &lt;strong&gt;64 MB&lt;/strong&gt;, repeating this pattern every few minutes as shown in the figure below.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/674/1*0pgUo0X6RiwoeATejg6NfQ.png&quot; /&gt;&lt;figcaption&gt;OldestReplicationSlotLag&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This behavior stems from how OldestReplicationSlotLag is calculated. Internally, the lag is derived using the following query:&lt;/p&gt;&lt;pre&gt;SELECT&lt;br&gt;  slot_name,&lt;br&gt;  pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS slot_lag_bytes&lt;br&gt;FROM pg_replication_slots;&lt;/pre&gt;&lt;p&gt;Conceptually, this translates to:&lt;/p&gt;&lt;pre&gt;OldestReplicationSlotLag = current_WAL_position_on_RDS &lt;br&gt;                           – restart_lsn &lt;/pre&gt;&lt;p&gt;See AWS references &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.Monitor.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://repost.aws/knowledge-center/rds-postgresql-use-logical-replication&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The &lt;a href=&quot;https://www.morling.dev/blog/postgres-replication-slots-confirmed-flush-lsn-vs-restart-lsn/#:~:text=And%20this%20is%20exactly%20the,has%20a%20few%20important%20implications:&quot;&gt;&lt;em&gt;restart_lsn&lt;/em&gt;&lt;/a&gt; represents the oldest write-ahead log (WAL) record that PostgreSQL must retain to ensure a replication consumer can safely resume replication.&lt;/p&gt;&lt;p&gt;When PostgreSQL performs a WAL segment switch, Aurora typically catches up almost immediately. At that moment, the restart_lsn briefly matches the source’s current WAL position, causing the reported lag to drop to 0. During idle periods, PostgreSQL performs an empty WAL segment rotation approximately every five minutes, driven by the archive_timeout = 300s setting in the database parameter group.&lt;/p&gt;&lt;p&gt;Immediately afterward, PostgreSQL begins writing to the new WAL segment. Since this new segment has not yet been fully flushed or consumed by Aurora, the WAL position in source RDS PostgreSQL advances ahead of the restart_lsn of Aurora PostgreSQL by exactly one segment. As a result, OldestReplicationSlotLag jumps to 64 MB, which corresponds to the configured WAL segment size at database initialization, and remains there until the next segment switch occurs.&lt;/p&gt;&lt;p&gt;Because idle PostgreSQL performs an empty WAL rotation approximately every five minutes, this zero-then-64 MB oscillation is expected. Importantly, the moment when the lag drops to 0 indicates that all meaningful WAL records have been fully replicated, and the Aurora read replica is fully caught up with the source.&lt;/p&gt;&lt;h4&gt;Cutover Phase&lt;/h4&gt;&lt;p&gt;Once the Aurora read replica has fully caught up with the source RDS PostgreSQL instance — as confirmed through replication lag analysis — the final step is to promote the replica and redirect application traffic. Promoting the Aurora read replica converts it into an independent, writable Aurora PostgreSQL cluster with its own writer and reader endpoints. At this point, the source RDS PostgreSQL instance is no longer the authoritative primary and is made inaccessible.&lt;/p&gt;&lt;p&gt;Because Netflix’s RDS ecosystem is fronted by a Data Access Layer (DAL), consisting of client-side forward proxies and a centralized Data Gateway, switching databases does not require application code changes or database credential access. Instead, traffic redirection is handled entirely through configuration updates in the reverse-proxy layer. Specifically, we update the runtime configuration of the Envoy-based Data Gateway to route traffic to the newly promoted Aurora cluster. Once this configuration change propagates, all client-initiated database connections are transparently routed through the DAL to the Aurora writer endpoint, completing the migration without requiring any application changes.&lt;/p&gt;&lt;p&gt;This proxy-level cutover, combined with Aurora promotion, enables a seamless transition for service owners, minimizes downtime, and preserves data consistency throughout the migration process.&lt;/p&gt;&lt;h3&gt;Customer Experience: Migrating a Business-Critical Partner Platform&lt;/h3&gt;&lt;p&gt;One of the critical teams to adopt the RDS PostgreSQL to Aurora PostgreSQL migration workflow was the Enablement Applications team. This team owns a set of databases that model Netflix’s entire ecosystem of partner integrations, including device manufacturers, discovery platforms, and distribution partners. These databases power a suite of enterprise applications that partners worldwide rely on to build, test, certify, and launch Netflix experiences on their devices and services.&lt;/p&gt;&lt;p&gt;Because these databases sit at the center of Netflix’s partner enablement and certification workflows, they are consumed by a diverse set of client applications across both internal and external organizations. &lt;strong&gt;Internally&lt;/strong&gt;, reliability teams use this data to identify streaming failures for specific devices and configurations, supporting quality improvements across the device ecosystem. At the same time, these databases directly serve &lt;strong&gt;external&lt;/strong&gt; partners operating across many regions. Device manufacturers rely on them to configure, test, and certify new hardware, while payment partners use them to set up and launch bundled offerings with Netflix.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*DJHayui83MkdSMYYd4TIQw.png&quot; /&gt;&lt;figcaption&gt;Simplified Enablement Applications Overview&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Device Lifecycle Management&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Netflix works with a wide range of device partners to ensure Netflix streams seamlessly across a diverse ecosystem of consumer devices. A core responsibility of Device Lifecycle Management is to provide tools and workflows that allow partners to develop, test, and certify Netflix integrations on their devices.&lt;/p&gt;&lt;p&gt;As part of the device lifecycle, partners run Netflix-provided test suites against their NRDP implementation. We store &lt;strong&gt;signals that represent the current stage for each device in the certification process&lt;/strong&gt;. This certification data forms the backbone of Netflix’s device enablement program, ensuring that only validated devices can launch Netflix experiences.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Partner Billed Integrations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In addition to device enablement, the same partner metadata is also consumed by Netflix’s Partner Billed Integrations organization. This group enables external partners to offer Netflix as part of bundled subscription and billing experiences.&lt;/p&gt;&lt;p&gt;Any disruption in these databases affects partner integration workflows. If the database is unavailable, partners may be unable to configure or launch service bundles with Netflix. Maintaining high availability and data correctness is essential to preserving smooth integration operations.&lt;/p&gt;&lt;p&gt;The global nature of these workflows makes it difficult to schedule downtime windows. Any disruption would impact partner productivity and risk eroding trust in Netflix’s integration and certification processes.&lt;/p&gt;&lt;h3&gt;Preparation&lt;/h3&gt;&lt;p&gt;Given the criticality of the Enablement Applications databases, thorough preparation was essential before initiating the migration. The team invested significant effort upfront to understand traffic patterns, identify all consumers, and establish clear communication channels.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Understand Client Fan-Out and Traffic Patterns&lt;br&gt;&lt;/strong&gt;The first step was to gain a complete view of how the databases were being used in production. Using observability tools like CloudWatch metrics, the team analyzed PostgreSQL connection counts, read and write patterns, and overall load characteristics. This helped establish a baseline for normal behavior and ensured there were no unexpected traffic spikes or hidden dependencies that could complicate the migration.&lt;/p&gt;&lt;p&gt;Just as importantly, this baseline gave the Enablement Applications team a rough idea of the post-migration behavior on Aurora. For example, they expected to see a similar number of active database connections and comparable traffic patterns after cutover, making it easier to validate that the migration had preserved operational characteristics.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Identify and Enumerate All Database Consumers&lt;br&gt;&lt;/strong&gt;Unlike most databases, where the set of consumers is well known to the owning team, these databases were accessed by a wide range of internal services and external-facing systems that were not fully enumerated upfront. To address this, we leveraged a tool called flowlogs, an eBPF-based network attribution tooling was used to capture TCP flow data to identify the services and applications establishing connections to the database(&lt;a href=&quot;https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc&quot;&gt;link&lt;/a&gt;).&lt;br&gt; &lt;br&gt;This approach allowed the team to enumerate active consumers, including those that were not previously documented, ensuring no clients were missed during migration planning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Establish Dedicated Communication Channels&lt;br&gt;&lt;/strong&gt;Once all consumers were identified, a dedicated communication channel was created to provide continuous updates throughout the migration process. This channel was used to share timelines, readiness checks, status updates, and cutover notifications, ensuring that all stakeholders remained aligned and could respond quickly if issues arose.&lt;/p&gt;&lt;h3&gt;Migration Process&lt;/h3&gt;&lt;p&gt;After completing application-side preparation, the Enablement Applications team initiated the data replication phase of the migration workflow. The automation successfully provisioned the Aurora read replica cluster and ported the RDS PostgreSQL parameter group to a corresponding Aurora parameter group, bringing the destination environment up with equivalent configuration.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Unexpected Replication Slot Behavior&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;However, shortly after replication began, we observed that the OldestReplicationSlotLag metric was unexpectedly high. This was counterintuitive, as Aurora read replicas are designed to remain closely synchronized with the source database by continuously streaming write-ahead logs (WAL).&lt;/p&gt;&lt;p&gt;Further investigation revealed the presence of an inactive logical replication slot on the source RDS PostgreSQL instance. An inactive replication slot can cause elevated OldestReplicationSlotLag because PostgreSQL must retain all WAL records required by the slot’s last known position (restart_lsn), even if no client is actively consuming data from it. Replication slots are intentionally designed to prevent data loss by ensuring that a consumer can resume replication from where it left off. As a result, PostgreSQL will not recycle or delete WAL segments needed by a replication slot until the slot advances. When a slot becomes inactive — such as when a client migration task is stopped or abandoned — the slot’s position no longer moves forward. Meanwhile, the database continues to generate WAL, forcing PostgreSQL to retain increasingly older WAL files. This growing gap between the current WAL position and the slot’s restart_lsn manifests as a high OldestReplicationSlotLag.&lt;/p&gt;&lt;p&gt;Identifying and addressing these inactive replication slots was a critical prerequisite to proceeding safely with the migration and ensuring accurate replication state during cutover.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Successful Migration After Remediation&lt;br&gt; &lt;/strong&gt;After identifying the inactive logical replication slot, the team safely cleaned it up on the source RDS PostgreSQL instance and resumed the migration workflow. With the stale slot removed, replication progressed as expected, and the Aurora read replica quickly converged with the source. The migration then proceeded smoothly through the quiescence phase, with no unexpected behavior or replication anomalies observed.&lt;/p&gt;&lt;p&gt;Following promotion, application traffic transitioned seamlessly to the newly writable Aurora PostgreSQL cluster. Through the Data Access Layer, new client connections were automatically routed to Aurora, and observability metrics confirmed healthy behavior — connection counts, read/write patterns, and overall load closely matched pre-migration baselines. From the application and partner perspective, the cutover was transparent, validating both the correctness of the migration workflow and the effectiveness of the preparation steps.&lt;/p&gt;&lt;h3&gt;Open questions&lt;/h3&gt;&lt;h4&gt;How do we select target Aurora PostgreSQL instance types based on the existing production RDS PostgreSQL instance?&lt;/h4&gt;&lt;p&gt;When selecting the target Aurora PostgreSQL instance type for a production migration, our guidance is intentionally conservative. We prioritize stability and performance first, and optimize for cost only after observing real workload behavior on Aurora.&lt;/p&gt;&lt;p&gt;In practice, the recommended approach is to adopt Graviton2-based instances (particularly the &lt;em&gt;r6g&lt;/em&gt; family) whenever possible, maintain the same instance family and size where feasible, and — at minimum — preserve the memory footprint of the existing RDS instance.&lt;/p&gt;&lt;p&gt;Unlike RDS PostgreSQL, Aurora does not support the &lt;em&gt;m&lt;/em&gt;-series, making a direct family match impossible for those instances. In such cases, simply keeping the same “size” (e.g., 2xlarge → 2xlarge) is not meaningful because the memory profiles differ across families. Instead, we map instances by memory equivalence. For example, an Aurora &lt;em&gt;r6g.xlarge&lt;/em&gt; provides a memory footprint comparable to an RDS &lt;em&gt;m5.2xlarge&lt;/em&gt;, making it a practical replacement. This memory-aligned strategy offers a safer and more predictable baseline for production migrations.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Downtime During RDS → Aurora Cutover?&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;To achieve minimal downtime during an RDS PostgreSQL → Aurora PostgreSQL migration, we front-load as much work as possible into the preparation phase. By the time we reach cutover, the Aurora read replica is already provisioned and continuously replicating WAL from the source RDS instance. Before initiating downtime, we ensure that the replication lag between Aurora and RDS has stabilized within an acceptable threshold. If the lag is large or fluctuating significantly, forcing a cutover will only inflate downtime.&lt;/p&gt;&lt;p&gt;Downtime begins the moment we remove the security groups from the source RDS instance, blocking all inbound traffic. We then reboot the instance to forcibly terminate existing connections, which typically takes up to a minute. From this point forward, no writes can be performed.&lt;/p&gt;&lt;p&gt;After traffic is halted, the next objective is to verify that Aurora has fully replayed all meaningful WAL records from RDS. We track this using &lt;strong&gt;OldestReplicationSlotLag&lt;/strong&gt;. We first wait for the metric to drop to &lt;strong&gt;0&lt;/strong&gt;, indicating that Aurora has consumed all WAL with real writes. Under normal idle behavior, PostgreSQL triggers an empty WAL switch every five minutes. After observing one data point at 0, we wait for an additional idle WAL rotation and confirm that the lag oscillates within the expected &lt;strong&gt;0 → 64 MB&lt;/strong&gt; pattern — signifying that the only remaining WAL segments are empty ones produced during idle time. At this point, we know the Aurora replica is fully caught up and can be safely promoted.&lt;/p&gt;&lt;p&gt;While these validation steps run, we perform the configuration updates on the Envoy reverse proxy in parallel. Once promotion completes and Envoy is restarted with the new runtime configuration, all client-initiated connections begin routing to the Aurora cluster. In practice, the total write-downtime observed across services averages &lt;strong&gt;around 10 minutes&lt;/strong&gt;, dominated largely by the RDS reboot and the idle WAL switch interval.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Optimization: Reducing Idle-Time Wait&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For services requiring stricter downtime budgets, waiting the full five minutes for an idle WAL switch can be prohibitively expensive. In such cases, we can force a WAL rotation immediately after traffic is cut off by issuing:&lt;/p&gt;&lt;p&gt;SELECT pg_switch_wal();&lt;/p&gt;&lt;p&gt;Once the switch occurs, OldestReplicationSlotLag will drop to 0 again as Aurora consumes the new (empty) WAL segment. This approach eliminates the need to wait for the default archive_timeout interval, which can significantly reduce overall downtime.&lt;/p&gt;&lt;h4&gt;How do we migrate CDC consumers?&lt;/h4&gt;&lt;p&gt;As part of the data platform organization in Netflix, we provide a managed Change Data Capture (CDC) service across a variety of datastores. For PostgreSQL, logical replication slots is the way of implementing change data capture. At Netflix, we build a managed abstraction on top of these replication slots called &lt;strong&gt;datamesh&lt;/strong&gt; to manage customers who are leveraging them (&lt;a href=&quot;https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873&quot;&gt;link&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Each logical replication slot tracks a consumer’s position in the write-ahead log (WAL), ensuring that WAL records are retained until the consumer has successfully processed them. This guarantees ordered and reliable delivery of row-level changes to downstream systems. At the same time, it tightly couples the lifecycle of replication slots to database operations, making their management a critical consideration during database migrations.&lt;/p&gt;&lt;p&gt;A key challenge in migrating from RDS PostgreSQL to Aurora PostgreSQL is transitioning these CDC consumers safely — without data loss, stalled replication, or extended downtime — while ensuring that replication slots are correctly managed throughout the cutover process.&lt;/p&gt;&lt;p&gt;Each row-level change in PostgreSQL is emitted as a CDC event with an operation type of INSERT, UPDATE, DELETE, or REFRESH. REFRESH events are generated during backfills by querying the database directly and emitting the current state of rows in chunks. Downstream consumers are designed to be idempotent and eventually consistent, allowing them to safely process retries, replays, and backfills.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Handling Replication Slots During Migration&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Before initiating database cutover, we temporarily pause CDC consumption by stopping the infrastructure responsible for consuming from PostgreSQL replication slots and writing into datamesh source. This also drops the replication slot from the database and cleans up our internal state around replication slot offsets. This essentially resets the state of the connector to one of a brand new one.&lt;/p&gt;&lt;p&gt;This step is critical for two reasons. First, it prevents replication slots from blocking WAL recycling during migration. Second, it ensures that no CDC consumers are left pointing at the source database once traffic is quiesced and cutover begins. While CDC consumers are paused, downstream systems temporarily stop receiving new change events, but remain stable. Once CDC consumers are paused, we proceed with stopping other client traffic and executing the RDS-to-Aurora cutover.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reinitializing CDC After Cutover&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After the Aurora PostgreSQL cluster has been promoted and traffic has been redirected, CDC consumers are reconfigured to point to the Aurora endpoint and restarted. Because their previous state was intentionally cleared, consumers initialize as if they are starting fresh.&lt;/p&gt;&lt;p&gt;On startup, new logical replication slots are created on Aurora, and a full backfill is performed by querying the database and emitting REFRESH events for all existing rows. These events let the consumer know that a manual refresh was done from Aurora and to treat this as an upsert operation. This establishes a clean and consistent baseline from which ongoing CDC can resume. Consumers are expected to handle these refresh events correctly as part of normal operation.&lt;/p&gt;&lt;p&gt;By explicitly managing PostgreSQL replication slots as part of the migration workflow, we are able to migrate CDC consumers safely and predictably, without leaving behind stalled slots, retained WAL, or consumers pointing to the wrong database. This approach allows CDC pipelines to be cleanly re-established on Aurora while preserving correctness and operational simplicity.&lt;/p&gt;&lt;h4&gt;How do we roll back in the middle of the process?&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Pre-quiescence&lt;br&gt;&lt;/strong&gt;Rolling back before the pre-quienscence phase is quite easy. Your primary RDS database is still the source. Rolling back before the quiescence phase is straightforward. At this stage, the primary RDS PostgreSQL instance continues to serve as the sole source of truth, and no client traffic has been redirected.&lt;/p&gt;&lt;p&gt;If a rollback is required, the migration can be safely aborted by deleting the newly created Aurora PostgreSQL cluster along with its associated parameter groups. No changes are needed on the application side, and normal operations on RDS PostgreSQL can continue without impact.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;During-quiescence&lt;br&gt;&lt;/strong&gt;Rolling back during the quiescence phase is more involved. At this point, client traffic to the source RDS PostgreSQL instance has already been stopped by detaching its security groups. To roll back safely, access must first be restored by reattaching the original security groups to the RDS instance, allowing client connections to resume. In addition, any logical replication slots removed during the migration must be recreated so that CDC consumers can continue processing changes from the source database.&lt;/p&gt;&lt;p&gt;Once connectivity and replication slots are restored, the RDS PostgreSQL instance can safely resume its role as the primary source of truth.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Post-quiescence &lt;br&gt;&lt;/strong&gt;Rolling back after cutover, once the Aurora PostgreSQL cluster is serving production traffic, is significantly more complex. At this stage, Aurora has become the primary source of truth, and client applications may already have written new data to it.&lt;/p&gt;&lt;p&gt;In this scenario, rollback requires setting up replication in the opposite direction, with Aurora as the source and RDS PostgreSQL as the destination. This can be achieved using a service such as AWS Database Migration Service (DMS). AWS provides detailed guidance for setting up this reverse replication flow, which can be followed to migrate data back to RDS if necessary.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Standardizing and reducing the surface area of data technologies is crucial for any large-scale platform. For the Netflix platform team, this strategy allows us to concentrate engineering effort, deliver deeper value on a smaller set of well-understood systems, and significantly cut the operational overhead of running multiple database technologies that serve similar purposes. Within the relational database ecosystem, Aurora PostgreSQL has become the paved-path datastore — offering strong scalability, resilience, and consistent operational patterns across the fleet.&lt;/p&gt;&lt;p&gt;Migrations of this scale demand solutions that are reliable, low-touch, and minimally disruptive for service owners. Our automated RDS PostgreSQL → Aurora PostgreSQL workflow represents a major step forward, providing predictable cutovers, strong correctness guarantees, and a migration experience that works uniformly across diverse workloads.&lt;/p&gt;&lt;p&gt;As we continue this journey, the Relational Data Platform team is building higher-level abstractions and capabilities on top of Aurora, enabling service owners to focus less on the complexities of database internals and more on delivering product value. More to come — stay tuned.&lt;/p&gt;&lt;h3&gt;Acknowledgements&lt;/h3&gt;&lt;p&gt;Special thanks to our other stunning colleagues/customers who contributed to the success of the RDS PostgreSQL to Aurora PostgreSQL migration. &lt;a href=&quot;https://www.linkedin.com/in/sumanth-pasupuleti&quot;&gt;Sumanth Pasupuleti&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/coleaperez&quot;&gt;Cole Perez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/akhaku&quot;&gt;Ammar Khaku&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=261ca045447f&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f&quot;&gt;Automating RDS Postgres to Aurora Postgres Migration&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>무신사의 AI 코드 리뷰 프로세스 구축기</title>
      <link>https://techblog.musinsa.com/%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-ai-%EC%BD%94%EB%93%9C-%EB%A6%AC%EB%B7%B0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B5%AC%EC%B6%95%EA%B8%B0-3ddb3c674e56?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-ai-%EC%BD%94%EB%93%9C-%EB%A6%AC%EB%B7%B0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B5%AC%EC%B6%95%EA%B8%B0-3ddb3c674e56?source=rss----f107b03c406e---4</guid>
      <pubDate>Mon, 09 Feb 2026 22:01:01 GMT</pubDate>
      <content:encoded>&lt;p&gt;안녕하세요. 무신사 Core Engineering/Personalization 팀에서 프론트엔드 개발을 맡고 있는 김의중입니다.&lt;br&gt;이 글은 LLM 기반 코드 리뷰를 무신사에 도입하고, 운영 가능한 수준의 인프라와 표준화된 프로세스로 구축해온 과정을 담고 있습니다. 단순히 “AI를 도입했습니다”가 아니라, &lt;strong&gt;도입 과정에서 마주한 시행착오와 이를 해결해 나간 방법&lt;/strong&gt;을 공유합니다. 무신사의 여러 엔지니어들이 함께 아이디어를 모으고, 각자의 경험과 관찰을 공유하며 어떻게 AI 인프라로 발전시켰는지 그 과정의 기록입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*YGFe1UTiYSTgwDOB71g-2w.png&quot; /&gt;&lt;figcaption&gt;출처: ChatGPT&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;1. 왜 지금 AI 코드 리뷰인가?&lt;/h3&gt;&lt;h4&gt;과거의 한계 — “높은 진입장벽”&lt;/h4&gt;&lt;p&gt;AI를 활용한 코드 리뷰 는 새로운 아이디어가 아닙니다. 다만 과거에는 실무에 도입하기에는 ‘구축 비용’ 이라는 장벽이 다소 높았습니다. EKS(Kubernetes)에 Ollama 같은 LLM 서버를 직접 띄워서 운영 한다던지, PR이 생성될 때마다 diff를 추출해, 전송하고, 응답을 파싱해서 다시 코멘트로 작성하는 전체 파이프라인을 직접 설계 운영 해야 했습니다.&lt;/p&gt;&lt;h4&gt;전환점: anthropics/claude-code-action@v1 정식 출시&lt;/h4&gt;&lt;p&gt;Anthropic에서 GitHub Actions을 공식 지원하면서 이러한 문제들이 해소되었습니다. 이제 복잡한 인프라 구축 없이 몇 줄의 YAML만으로 고품질 코드 리뷰를 도입할 수 있게 되었습니다. 기술적 진입 장벽이 허물어지면서, &lt;strong&gt;‘구축’이 아닌 ‘활용’&lt;/strong&gt;에 집중할 수 있는 시점을 맞이했습니다.&lt;/p&gt;&lt;h4&gt;AI 코드 리뷰의 가치 “워크플로우 증강하는 AI”&lt;/h4&gt;&lt;p&gt;AI를 단순히 코드를 생성하는 도구에 머무르기보다, &lt;strong&gt;협업 과정 전체를 매끄럽게 만드는 워크플로우 증강 도구&lt;/strong&gt;로 활용하는 것이 중요합니다. 복잡한 변경 사항을 자동으로 요약해 리뷰어가 빠르게 맥락을 파악하도록 도울 뿐만 아니라, &lt;strong&gt;코드 라인 단위의 인라인 코멘트로 개선 방향을 구체적으로 제안하고, 누락된 예외 처리나 잠재적 버그까지 짚어주는 역할&lt;/strong&gt;도 수행합니다.&lt;br&gt;이러한 기능들은 자연스럽게 리뷰 병목을 줄여주고, 팀 전체의 품질 기준을 일정 수준 이상으로 유지하는 데 큰 도움을 줍니다. 그 결과, 개인 개발자의 작업 속도만 빨라지는 것이 아니라 &lt;strong&gt;팀 전체의 개발 사이클이 더 유연하게 연결되는 구조적 효율성&lt;/strong&gt;이 생깁니다.&lt;br&gt;결국 지금 LLM 코드 리뷰가 중요한 이유는 “코드를 대신 짓는 AI”가 등장해서가 아니라, &lt;strong&gt;AI가 팀의 워크플로우를 근본적으로 재설계하며 조직의 생산성을 실질적으로 끌어올리는 핵심 기술로 자리 잡았기 때문&lt;/strong&gt;입니다.&lt;/p&gt;&lt;h3&gt;2. AI 코드 리뷰가 효과적인 이유&lt;/h3&gt;&lt;h4&gt;AI가 코드 리뷰를 잘하는 이유: 규칙과 패턴을 ‘꾸준히’ 잡아낸다&lt;/h4&gt;&lt;p&gt;코드는 자연어보다 훨씬 &lt;strong&gt;규칙적&lt;/strong&gt;이고, 좋은 코드에는 반복되는 &lt;strong&gt;베스트 프랙티스 패턴&lt;/strong&gt;이 있습니다. AI는 이런 패턴을 대량으로 학습해 두었기 때문에, 사람이 피곤하거나 바쁜 상황에서 놓치기 쉬운 것들을 &lt;strong&gt;일관된 기준으로&lt;/strong&gt; 찾아냅니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;사소한 오타/휴먼 에러&lt;/li&gt;&lt;li&gt;흔한 성능 실수(불필요한 렌더/루프/쿼리 등)&lt;/li&gt;&lt;li&gt;스타일/컨벤션 불일치&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;즉, AI에게 “고차원 의사결정”을 기대하기보다, &lt;strong&gt;반복적으로 발생하는 실수와 패턴 기반 이슈를 먼저 걸러서 리뷰어의 시간을 아끼는 것&lt;/strong&gt;이 가장 큰 가치입니다.&lt;/p&gt;&lt;h4&gt;인라인 코멘트: 사람이 받기 좋은 형태로 피드백하기&lt;/h4&gt;&lt;p&gt;코드 리뷰에서 중요한 건 “요약”이 아니라 &lt;strong&gt;어디를 왜 고쳐야 하는지&lt;/strong&gt;입니다.&lt;br&gt;anthropics/claude-code-action@v1은 인라인 코멘트를 공식 지원해서, 문제가 있는 라인에 바로 코멘트를 남기고:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;버그/성능/구조/스타일 같은 &lt;strong&gt;카테고리&lt;/strong&gt;를 명확히 하고&lt;/li&gt;&lt;li&gt;“왜 문제인지 + 어떻게 바꾸면 좋은지”를 함께 제시합니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;그래서 작성자는 해당 라인에서 바로 맥락을 잡고, “AI가 뭘 근거로 말하는지”도 빠르게 확인할 수 있습니다.&lt;/p&gt;&lt;h4&gt;커밋 Suggestion: 리뷰를 넘어 실제 ‘패치’까지 자동 제안&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*OZVyUyHmtzG0az9912UQ0w.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;단순히 의견을 말하는 수준을 넘어서 &lt;strong&gt;GitHub의 Suggestion 블록을 활용해 실제 적용 가능한 수정안(diff)를 생성합니다&lt;/strong&gt;. 즉, 리뷰 중 발견한 문제를 &lt;strong&gt;바로 적용 가능한 패치 형태로 만들어줍니다.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;이런 제안을 클릭 한 번으로 커밋에 반영할 수 있어, 사실상 &lt;em&gt;AI가 자동으로 작은 리팩토링과 안전한 수정 작업을 PR 과정에서 도와주는 흐름&lt;/em&gt;이 만들어집니다. 결국 &lt;strong&gt;팀 전체의 리뷰 속도와 코드 품질이 자연스럽게 올라가는 구조&lt;/strong&gt;가 됩니다&lt;/p&gt;&lt;h3&gt;3. “몇 줄의 YAML”로 끝나는 AI 코드 리뷰 도입&lt;/h3&gt;&lt;h4&gt;도입은 3단계면 끝납니다.&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;1단계: 토큰 발급&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;claude setup-token&lt;/pre&gt;&lt;p&gt;Claude Pro/Max 구독자라면 터미널에서 위 명령어 한줄로 인증 토큰을 발급받을 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2단계: GitHub Secrets 등록&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;발급받은 토큰을 Organization의 Secrets에 CLAUDE_CODE_OAUTH_TOKEN으로 등록합니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3단계: Workflow 파일 추가&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;.github/workflows/claude-review.yml 파일을 생성하고 설정을 붙여 넣습니다.&lt;/p&gt;&lt;h4&gt;자주 묻는 질문: 토큰 비용&lt;/h4&gt;&lt;blockquote&gt;&lt;strong&gt;&lt;em&gt;Q. 토큰 발급과 사용은 유료인가요?&lt;/em&gt;&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;많은 분들이 오해하시는 부분으로 Pro/Max 플랜에 한하여 추가 API 비용은 없습니다. claude setup-token으로 생성하는 것은 &lt;strong&gt;API 키가 아닌 Claude 구독 인증 토큰&lt;/strong&gt;입니다. 따라서 플랜의 사용량 &lt;strong&gt;한도 내에서 무료로 사용 가능&lt;/strong&gt;합니다. 이 부분 때문에 도입을 망설이셨다면 안심하고 시작하셔도 됩니다.&lt;/p&gt;&lt;h3&gt;4. 함께 다듬다: 전사 프로세스 시스템 구축&lt;/h3&gt;&lt;h4&gt;공유 하나가 시작된 연쇄 반응&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;AI 코드 리뷰 가이드&lt;/strong&gt;를 만들어 전사에 공유하자, 슬랙 스레드에는 &lt;strong&gt;43개의 댓글&lt;/strong&gt;이 달렸습니다. 각 팀에서 자발적으로 “우리 팀은 이렇게 쓰고 있다”, “이런 문제가 있었는데 이렇게 해결했다”, “이건 이렇게 바꾸면 더 좋지 않을까”라는 논의가 이어졌습니다. 이 논의들은 &lt;strong&gt;LLM 코드 리뷰가 단순 스크립트에서 견고한 사내 인프라로 발전&lt;/strong&gt;하는데 중요한 계기가 되었습니다.&lt;/p&gt;&lt;h4&gt;사례1: 개인 토큰에서 전사 공용 토큰으로(확장성)&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Mcp4Ee05YuX6-JW0MSgljA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;처음에는 각자 개인 Claude 토큰을 발급받아 사용했습니다. 하지만 &lt;strong&gt;송정훈&lt;/strong&gt;(&lt;strong&gt;29CM Engineering)님&lt;/strong&gt; 은 &lt;strong&gt;모든 개발 조직이 더 안정적으로 사용할 수 있는 구조를 고민해&lt;/strong&gt; “개인 토큰과 리뷰용 계정을 분리하여 관리하자”는 제안을 주셨습니다. 이 제안 덕분에 코드 리뷰 전용 전사 Claude 토큰을 발급받았고, 이를 GitHub Organization Secret(secrets.CLAUDE_CODE_TOKEN)으로 등록했습니다. 덕분에 무신사의 모든 레포지토리에서는 개별적인 키 발급 없이 안전하고 편리하게 AI 리뷰를 적용할 수 있는 기반이 만들어졌습니다.&lt;/p&gt;&lt;h4&gt;사례2: 기록은 남기고, 노이즈만 지운다(사용성)&lt;/h4&gt;&lt;p&gt;LLM 코드 리뷰를 PR에 붙이면 흔히 겪는 문제가 있습니다. &lt;strong&gt;커밋할 때마다 코멘트가 쌓여 PR이 지저분해지는 현상&lt;/strong&gt;입니다.&lt;br&gt;처음에는 “그냥 전부 삭제하면 되지 않나?”라고 생각하기 쉽지만, 그렇게 하면 사람이 남긴 중요한 논의까지 같이 사라져 버립니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*TXM_XTunFttKi1uaiQtdRw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;임영태(MSS Engagement Frontend)&lt;/strong&gt;님은 이 문제를 “무조건 삭제”가 아니라 ‘가치 있는 기록은 남기고, 봇이 만든 노이즈만 정리’하는 방식으로 풀었습니다. 핵심은 상태 기반 정책입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;보존:&lt;/strong&gt; 사람이 답글을 달았거나, 이미 &lt;strong&gt;Resolved&lt;/strong&gt; 처리된 스레드 → 팀이 의미 있게 소비한 기록&lt;/li&gt;&lt;li&gt;&lt;strong&gt;삭제:&lt;/strong&gt; 아무 상호작용 없이 방치된 미해결 봇 코멘트 → 반복 생성되는 노이즈&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 방식 덕분에 PR 타임라인은 사람이 남긴 의미 있는 논의 중심으로 깔끔하게 유지되면서도, AI의 피드백은 적시에 필요한 만큼만 제공되는 쾌적한 리뷰 환경이 완성되었습니다.&lt;/p&gt;&lt;h4&gt;사례3: “Composite Action”으로 사내 플랫폼 구축&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*oMvz_DiEgMXvzJeqET_bXw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;여러 레포지토리에 스크립트를 복사해서 쓰다 보니, 프롬프트 한 줄을 고치려 해도 모든 저장소를 수정해야 하는 ‘파편화 문제’가 발생했습니다. &lt;strong&gt;최용한(MSSnE Backend/Campaign) &lt;/strong&gt;님은 이 비효율을 개선하기 위해, 무신사 공용 워크플로우 저장소(musinsa/workflows)에 액션을 추가했습니다. 그리고 &lt;strong&gt;홍일선(MSS Engineering/MSSnE Backend)&lt;/strong&gt; 님의 제언이 더해져, 단순 공유를 넘어선 &lt;strong&gt;Composite Action&lt;/strong&gt; 기반의 표준화된 구조로 고도화되었습니다. &lt;strong&gt;복잡한 로직은 중앙 저장소에 캡슐화하고, 각 서비스 레포에서는 몇 줄의 설정만 가져다 사용하는 구조&lt;/strong&gt;입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;복잡함은 숨기고, 편의성은 높이다&lt;/strong&gt; &lt;br&gt;리뷰 실행, 이전 코멘트 정리 같은 로직을 공용 레포지토리에 &lt;strong&gt;Composite Action&lt;/strong&gt; 형태로 패키징했습니다. 덕분에 각 서비스 레포지토리에서는 내부 구현을 알 필요 없이, 마치 오픈소스 액션을 쓰듯, 단 몇 줄의 YAML로 모든 기능을 호출할 수 있습니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;“전사 표준”과 “팀별 커스텀”의 유연한 분리 &lt;/strong&gt;&lt;br&gt;“다 똑같이 해”가 아니라 &lt;strong&gt;공통 규칙은 지키되, 팀별 특성은 남길 수 있게&lt;/strong&gt;. Composite Action의 inputs 기능을 활용해 인터페이스를 설계했습니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;고정 영역 (전사 표준):&lt;/strong&gt; 이전 리뷰 자동 정리(Resolved 보존), 중복 코멘트 제거 로직, 기본 프롬프트 골격과 출력 포맷등은 중앙에서 관리하여 조직 전체 품질 표준을 유지합니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;가변 영역:&lt;/strong&gt; review_language, project_context_path(CLAUDE.md 등), extra_prompt 등 팀별로 특화가 필요한 부분만 파라미터(with)로 열어두었습니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 구조 덕분에 “전사 공통 규칙”을 안전하게 지키면서도, 도메인 특성에 맞는 “팀별 유연성”을 확보했습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;인프라 버전 관리 투트랙 전략 (@main vs @v1.0)&lt;/strong&gt;&lt;br&gt;‘지속적으로 진화하는 사내 제품’ 을 위한 두 가지 버전 관리 전략을 사용합니다. LLM의 응답 품질은 프롬프트 엔지니어링에 달려 있기 때문입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;@main: 매 순간 업데이트되는 고도화된 프롬프트와 기능을 실시간으로 사용합니다.&lt;/li&gt;&lt;li&gt;@v1.x: 안정적인 운영이 중요한 서비스 팀이 사용하여, 프롬프트 변경에 따른 예기치 않은 부작용을 방지 합니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;결과적으로 사내 플랫폼 구축으로, 단순 스크립트 실행을 넘어 버전 관리와 배포가 가능한 하나의 &lt;strong&gt;‘사내 인프라 플랫폼’&lt;/strong&gt;으로 자리 잡게 되었습니다. 이제 무신사 전체 레포지토리에서 단 몇 줄의 YAML 코드만으로 AI 리뷰 환경을 즉시 구축할 수 있습니다.&lt;/p&gt;&lt;h3&gt;5. 팀 단위 도입: Personalization 팀 전체로의 확장&lt;/h3&gt;&lt;h4&gt;다양한 직군, 하나의 협업 방식&lt;/h4&gt;&lt;p&gt;무신사 Personalization 팀은 Frontend, Backend, ML Engineer가 한곳에 모여 일하는 목적 조직(Cross-functional Team)입니다. 사용하는 기술 스택과 역할은 다르지만, 모두 GitHub PR 을 사용합니다. 그래서 AI 코드 리뷰는 특정 직군만의 도구가 아니라, 팀 전체가 같은 지점에서 효용을 체감할 수 있는 개선이었습니다. 아래는 팀 구성원이 남긴 실제 피드백 입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*7DtPVT22-TJt_HMo43HaCA.png&quot; /&gt;&lt;figcaption&gt;ML 엔지니어 김윤태 님&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*E2Vz-kDAOLU9EDk8TIqxHg.png&quot; /&gt;&lt;figcaption&gt;Backend 엔지니어 정재호 님&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;자연스러운 확장&lt;/h4&gt;&lt;p&gt;이처럼 각 직군 모두가 명확한 개선 효과를 경험하면서, 팀 전체 레포지토리로 범위를 확장하는 것이 자연스럽게 결정되었습니다. 무신사가 추구하는 “AI First”와 “자동화된 생산성 향상” 가치에 따라 Personalization 팀은 &lt;strong&gt;관리하는 모든 레포지토리에 AI 코드 리뷰를 기본 탑재&lt;/strong&gt;하게 되었습니다.&lt;/p&gt;&lt;h3&gt;6. 최적화 전략 “PR은 깔끔하게, 프롬프트는 가볍게”&lt;/h3&gt;&lt;p&gt;다음은 팀에서 실제 사용 중인 설정입니다. 임영태 님이 제안주신 &lt;strong&gt;이전 코멘트 자동 정리&lt;/strong&gt;와 최신 LLM의 추론 능력을 극대화 하기 위한 &lt;strong&gt;‘Minimalist Prompting’&lt;/strong&gt; 전략 입니다.&lt;/p&gt;&lt;h4&gt;전략 1. 스마트 클린업: “봇의 노이즈는 지우되, 사람의 대화는 지킨다”&lt;/h4&gt;&lt;p&gt;PR에 커밋을 추가할 때마다 이전 코멘트와 새 코멘트가 뒤섞이는문제를 해결하기 위해 GraphQL을 활용한 &lt;strong&gt;맥락 보존 삭제 (Context-Aware Deletion) 규칙&lt;/strong&gt;을 적용했습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;보존 규칙 (Whitelist):&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Resolved(해결됨):&lt;/strong&gt; 사용자가 ‘Resolve conversation’을 체크한 스레드는 이미 수정이 완료된 유의미한 기록이므로 보존합니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Conversation(대화 중):&lt;/strong&gt; 봇의 지적에 사람이 답글(Reply)을 달았다면, 이는 진행 중인 토론이므로 맥락 유지를 위해 보존합니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;삭제 대상:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;결국 “미해결(Unresolved) 상태”이면서 + &lt;strong&gt;“사람의 개입 없이 봇 혼자 떠들고 있는”&lt;/strong&gt; 고립된 코멘트만 노이즈로 간주하여 제거합니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이로써 개발자의 코멘트와 히스토리는 보호하면서, 더 이상 유효하지 않은 봇의 리뷰만 제거합니다.&lt;/p&gt;&lt;h4&gt;전략 2. Minimalist Prompting: “최고의 프롬프트는 가장 짧은 프롬프트”&lt;/h4&gt;&lt;p&gt;prompt 섹션은 놀라울 정도로 짧습니다. 이는 &lt;strong&gt;최신 파운데이션 모델의 제로샷(Zero-shot)능력을 극대화 하기 위한 선택입니다.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;왜 짧은 프롬프트가 더 강력한가? “Less is More”&lt;/strong&gt;&lt;br&gt;흔히들 프롬프팅이 길고 정교할수록 잘 짜여진 프롬프트라고 생각하기 쉽습니다. 하지만 모델의 성능이 비약적으로 향상된 지금, 핵심 목표만 명확히 주고 나머지는 모델이 자연스럽게 판단하도록 두는 편이 오히려 안정적인 경우가 많습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;신호 대 잡음비(Signal-to-Noise) 악화:&lt;/strong&gt; 프롬프트가 길어질수록 핵심 지시(Signal)가 부가적인 규칙(Noise)에 묻혀, 모델이 정작 중요한 요청을 놓칠 확률이 높아집니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;추론 능력 제한:&lt;/strong&gt; 구체적인 예시를 강제하면, 모델은 자신이 가진 방대한 지식과 추론 능력을 사용하는 대신 좁은 예시를 단순히 흉내 내는 데 그칠 수 있습니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;따라서 최소한의 프롬프트로 시작하고, 반복적으로 발생하는 이슈만 점진적으로 추가하는 방식으로 운영합니다.&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt;name: &amp;quot;Claude Code Review&amp;quot;&lt;br&gt;on:&lt;br&gt;  pull_request:&lt;br&gt;    types: [ opened, reopened, synchronize ]&lt;br&gt;jobs:&lt;br&gt;  review:&lt;br&gt;    runs-on: ubuntu-latest&lt;br&gt;    permissions:&lt;br&gt;      contents: read&lt;br&gt;      pull-requests: write&lt;br&gt;    steps:&lt;br&gt;      - name: Checkout Code&lt;br&gt;        uses: actions/checkout@v4&lt;br&gt;      - name: Delete Previous Claude Reviews&lt;br&gt;        if: github.event.action == &amp;#39;synchronize&amp;#39;&lt;br&gt;        env:&lt;br&gt;          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}&lt;br&gt;        run: |&lt;br&gt;          set +e  # Continue on error&lt;br&gt;          PR_NUMBER=${{ github.event.pull_request.number }}&lt;br&gt;          REPO=${{ github.repository }}&lt;br&gt;          echo &amp;quot;🔍 이전 Claude 리뷰 검색 중...&amp;quot;&lt;br&gt;          # 1. PR 코멘트(요약) 삭제 - github-actions[bot]이 작성한 코멘트&lt;br&gt;          gh api &amp;quot;repos/$REPO/issues/$PR_NUMBER/comments&amp;quot; \&lt;br&gt;            --jq &amp;#39;.[] | select(.user.login == &amp;quot;github-actions[bot]&amp;quot;) | .id&amp;#39; \&lt;br&gt;            | while read comment_id; do&lt;br&gt;                if [ -n &amp;quot;$comment_id&amp;quot; ]; then&lt;br&gt;                  echo &amp;quot;🗑️  PR 코멘트 삭제: $comment_id&amp;quot;&lt;br&gt;                  gh api -X DELETE &amp;quot;repos/$REPO/issues/comments/$comment_id&amp;quot; 2&amp;gt;/dev/null || echo &amp;quot;⚠️  삭제 실패 (무시)&amp;quot;&lt;br&gt;                  sleep 0.3&lt;br&gt;                fi&lt;br&gt;              done&lt;br&gt;          # 2. 인라인 리뷰 코멘트 삭제&lt;br&gt;          # - 미해결 스레드 중 bot 코멘트만 있는 스레드만 삭제&lt;br&gt;          # - 사용자 답글이 있는 스레드는 삭제하지 않음 (코드만 덩그러니 남는 문제 방지)&lt;br&gt;          OWNER=${REPO%/*}&lt;br&gt;          NAME=${REPO#*/}&lt;br&gt;          gh api graphql -F owner=&amp;quot;$OWNER&amp;quot; -F name=&amp;quot;$NAME&amp;quot; -F number=$PR_NUMBER -f query=&amp;#39;&lt;br&gt;            query($owner: String!, $name: String!, $number: Int!) {&lt;br&gt;              repository(owner: $owner, name: $name) {&lt;br&gt;                pullRequest(number: $number) {&lt;br&gt;                  reviewThreads(last: 100) {&lt;br&gt;                    nodes {&lt;br&gt;                      isResolved&lt;br&gt;                      comments(first: 50) {&lt;br&gt;                        nodes {&lt;br&gt;                          databaseId&lt;br&gt;                          author {&lt;br&gt;                            login&lt;br&gt;                          }&lt;br&gt;                        }&lt;br&gt;                      }&lt;br&gt;                    }&lt;br&gt;                  }&lt;br&gt;                }&lt;br&gt;              }&lt;br&gt;            }&amp;#39; \&lt;br&gt;            --jq &amp;#39;.data.repository.pullRequest.reviewThreads.nodes[] | select(.isResolved == false) | select([.comments.nodes[].author.login] | all(. == &amp;quot;github-actions&amp;quot;)) | .comments.nodes[].databaseId&amp;#39; \&lt;br&gt;            | while read comment_id; do&lt;br&gt;                if [ -n &amp;quot;$comment_id&amp;quot; ]; then&lt;br&gt;                  echo &amp;quot;🗑️  인라인 코멘트 삭제: $comment_id&amp;quot;&lt;br&gt;                  gh api -X DELETE &amp;quot;repos/$REPO/pulls/comments/$comment_id&amp;quot; 2&amp;gt;/dev/null || echo &amp;quot;⚠️  삭제 실패 (무시)&amp;quot;&lt;br&gt;                  sleep 0.3&lt;br&gt;                fi&lt;br&gt;              done&lt;br&gt;          echo &amp;quot;✅ 이전 리뷰 삭제 완료&amp;quot;&lt;br&gt;      - name: Run Claude Code Review&lt;br&gt;        uses: anthropics/claude-code-action@v1&lt;br&gt;        with:&lt;br&gt;          show_full_output: true&lt;br&gt;          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}&lt;br&gt;          github_token: ${{ secrets.GITHUB_TOKEN }}&lt;br&gt;          prompt: |&lt;br&gt;            REPO: ${{ github.repository }}&lt;br&gt;            PR NUMBER: ${{ github.event.pull_request.number }}&lt;br&gt;            이 PR을 리뷰하고 코멘트로 작성해주세요.&lt;br&gt;          claude_args: |&lt;br&gt;            --allowedTools &amp;quot;mcp__github_inline_comment__create_inline_comment,Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*)&amp;quot;&lt;/pre&gt;&lt;h3&gt;7. 마무리 — 함께 만들어가는 AI First&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*9pvOK3sHwjjvv4GEG03irQ.png&quot; /&gt;&lt;figcaption&gt;출처: ChatGPT&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;무신사 테크 조직은 ‘AI First’를 슬로건이 아닌 실천으로 만들어가고 있습니다. 실제 업무 속에서 &lt;strong&gt;반복되는 비효율을 줄이고 더 높은 가치&lt;/strong&gt;를 만드는 데 집중하고 있습니다. 중요한건 ‘AI를 붙였다’가 아니라, &lt;strong&gt;운영 가능한 형태로 만들기 위해 무엇을 고민했고 어떻게 다듬었는가&lt;/strong&gt; 였습니다.&lt;/p&gt;&lt;h4&gt;각자의 자리에서, 같은 방향으로&lt;/h4&gt;&lt;p&gt;엔지니어들은 각자의 자리에서 최적화를 고민했습니다. 그리고 해결책을 제안하고, 동료들과 논의하며 개선해 나갔습니다. 이렇게 서로 다른 관점의 개선이 쌓이면서 &lt;strong&gt;개인의 노하우는 곧 팀의 규칙이 되고, 팀의 규칙은 다시 전사적인 프로세스&lt;/strong&gt;로 확장되었습니다.&lt;/p&gt;&lt;h4&gt;작은 실천이 프로세스가 되고, 프로세스가 문화가 된다&lt;/h4&gt;&lt;p&gt;이런 노력들이 모이자, 자연스럽게 &lt;strong&gt;시스템화가&lt;/strong&gt; 이루어졌습니다. 흩어져 있던 스크립트는 Composite Action으로 표준화되었고, 개인의 토큰은 전사 인프라가 되었습니다. 이제 무신사의 어떤 팀이든 몇 줄의 YAML만으로 AI 코드 리뷰를 도입할 수 있으며, 누군가 더 좋은 방법을 찾아 기여하면 그 혜택이 전사로 전파되는 &lt;strong&gt;선순환 플랫폼&lt;/strong&gt;이 구축되었습니다.&lt;/p&gt;&lt;h4&gt;앞으로의 격차: ‘과정의 밀도’&lt;/h4&gt;&lt;p&gt;향후 기술 조직 간의 격차는 단순히 “AI를 도입했는가”가 아니라, “AI를 내재화하는 과정을 어떻게 겪어냈는가”에서 벌어질 것입니다. 기술의 맹목적 수용이 아닌, 개발 프로세스에 깊숙이 녹여 자동화하고 시스템으로 정착시키는 ‘과정의 밀도’가 중요하기 때문입니다.&lt;br&gt;무신사는 오늘도 각자의 작은 실천을 집단 지성으로 모아, &lt;strong&gt;조직 전체의 AI 활용 능력&lt;/strong&gt;을 체계적으로 성장시켜 나가고 있습니다. 이 글이 비슷한 고민을 하고 계신 분들께 의미 있는 참고가 되기를 바랍니다.&lt;/p&gt;&lt;h3&gt;TEAM MUSINSA CAREER&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사는 2001년 온라인 커뮤니티로 시작해 2005년 무신사 매거진, 2009년 무신사 스토어를 오픈하며 빠르게 성장하고 있는 국내 대표 온라인 패션 스토어입니다. ‘입점 브랜드와 동반성장’이라는 경영 철학을 바탕으로 브랜드가 안정적으로 사업을 전개할 수 있도록 무신사가 보유한 노하우와 인프라를 지원합니다. 고객에게는 풍성한 패션 콘텐츠와 패션에 특화된 차별화된 서비스로 최상의 온라인 쇼핑 경험을 제공하고 있습니다. 글로벌 №1 패션 기업으로 성장할 무신사와 함께 새로운 도전과 혁신을 만들 인재를 기다립니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;29CM는 ‘고객의 더 나은 선택을 돕는다’라는 미션으로 출발했습니다. 우리는 우리만의 방식으로 콘텐츠를 제공하며, 브랜드와 고객 모두에게 대체 불가능한 커머스 플랫폼을 만들어가고 있습니다. 이 미션을 이루기 위해 우리는 흥미로우면서도 복잡한 문제들을 해결하고 있습니다. 만약 우리와 함께 이 문제들을 해결해 보고 싶다면, 주저하지 말고 29CM에 합류하세요!&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3ddb3c674e56&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-ai-%EC%BD%94%EB%93%9C-%EB%A6%AC%EB%B7%B0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B5%AC%EC%B6%95%EA%B8%B0-3ddb3c674e56&quot;&gt;무신사의 AI 코드 리뷰 프로세스 구축기&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>질풍노도의 AI(Claude)에게 엄격한 선생님 장착하기</title>
      <link>https://techblog.musinsa.com/%EC%A7%88%ED%92%8D%EB%85%B8%EB%8F%84%EC%9D%98-ai-claude-%EC%97%90%EA%B2%8C-%EC%97%84%EA%B2%A9%ED%95%9C-%EC%84%A0%EC%83%9D%EB%8B%98-%EC%9E%A5%EC%B0%A9%ED%95%98%EA%B8%B0-61c3d533fc40?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%A7%88%ED%92%8D%EB%85%B8%EB%8F%84%EC%9D%98-ai-claude-%EC%97%90%EA%B2%8C-%EC%97%84%EA%B2%A9%ED%95%9C-%EC%84%A0%EC%83%9D%EB%8B%98-%EC%9E%A5%EC%B0%A9%ED%95%98%EA%B8%B0-61c3d533fc40?source=rss----f107b03c406e---4</guid>
      <pubDate>Mon, 09 Feb 2026 08:01:02 GMT</pubDate>
      <content:encoded>&lt;h3&gt;질풍노도의 AI(Claude)에게 엄격한 선생님 장착하기 — ArchUnit으로 아키텍처 규칙 자동 검증&lt;/h3&gt;&lt;h3&gt;들어가며&lt;/h3&gt;&lt;p&gt;무신사 물류플랫폼팀에서 백엔드 개발을 담당하고 있는 이상호입니다.&lt;/p&gt;&lt;p&gt;어느 날 코드 리뷰를 하다가 익숙한 장면을 마주했습니다. Claude가 생성한 코드에서 Repository 클래스가 Controller를 직접 참조하고 있었습니다. 레이어 의존성 위반. 리뷰 코멘트를 남겼습니다. 다음 PR에서도, 그다음 PR에서도 비슷한 위반이 보였습니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;“AI한테 아키텍처 규칙을 매번 알려줬는데, 왜 자꾸 어기는 걸까?”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Claude Skill에 규칙을 정리해두기도 하고, 컨텍스트로 아키텍처 문서를 넘겨보기도 했습니다. 하지만 hit률은 기대에 한참 못 미쳤습니다. AI는 “알겠습니다”라고 대답하면서도 종종 규칙을 빠뜨렸습니다.&lt;/p&gt;&lt;p&gt;사실 이건 AI만의 문제가 아닙니다 사람도 마찬가지입니다. 아키텍처 규칙이라는 것은 한두 번 읽어서 되는 게 아니라, 매번 일관되게 지켜야 하는 규약입니다. 읽은 것과 지키는 것은 전혀 다른 이야기입니다.&lt;/p&gt;&lt;p&gt;그렇다면 접근을 바꿔야 했습니다. AI에게 규칙을 “알려주는” 것이 아니라, 규칙을 “어기면 안 되게 만드는” 방법은 없을까?&lt;/p&gt;&lt;h3&gt;기존 방식의 한계: 부탁은 강제가 아니다&lt;/h3&gt;&lt;h3&gt;첫 번째 시도 — 코드 리뷰에서 잡자&lt;/h3&gt;&lt;p&gt;모든 PR에서 리뷰어가 아키텍처 규칙 준수 여부를 눈으로 확인했습니다. 레이어 간 의존성 방향이 올바른지, 패키지 구조가 컨벤션에 맞는지를 한 줄 한 줄 살폈습니다.&lt;/p&gt;&lt;p&gt;문제는 AI의 코드 생성량이 늘수록 리뷰어의 부담도 비례해서 커진다는 점이었습니다. 비즈니스 로직을 검토해야 하는데, 아키텍처 위반을 찾느라 시간을 쓰고 있었습니다. 게다가 사람인 이상 누락은 불가피했습니다.&lt;/p&gt;&lt;h3&gt;두 번째 시도 — AI에게 교과서를 읽어주자&lt;/h3&gt;&lt;p&gt;Claude Skill과 컨텍스트에 아키텍처 규칙을 상세히 기술했습니다. “이 프로젝트는 Controller → Service → Repository 방향으로만 의존합니다. 역방향 의존은 금지입니다.”&lt;/p&gt;&lt;p&gt;AI는 이 내용을 잘 이해했습니다. 대부분의 경우 규칙을 따랐습니다. 하지만 대부분이 문제였습니다. 복잡한 기능을 구현할 때, 여러 파일을 동시에 생성할 때, 간혹 규칙이 빠졌습니다. 컨텍스트 기반의 접근은 “참고 자료”일 뿐, 강제력이 없었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*xLn6x_FtIuaZbsMppb3gmg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;두 방식의 공통점이 보였습니다. 결국 누군가가 나중에 확인해야 한다는 것. 위반을 사전에 차단하는 것이 아니라, 사후에 발견하는 구조였습니다.&lt;/p&gt;&lt;p&gt;저희가 원한 것은 이것이었습니다:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;아키텍처 규칙 위반을 빌드 단계에서 자동 감지&lt;/li&gt;&lt;li&gt;AI가 스스로 위반을 인지하고 자동 수정&lt;/li&gt;&lt;li&gt;코드 리뷰에서 아키텍처 검증에 쏟는 시간을 0으로 만들기&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;교과서의 한계, 시험지의 가능성&lt;/h3&gt;&lt;p&gt;여기서 한 발 물러나 생각해보았습니다. 왜 교과서(컨텍스트)는 한계가 있을까?&lt;/p&gt;&lt;p&gt;교과서는 학습을 위한 도구입니다. AI는 교과서를 읽고 이해합니다. 하지만 이해한 것과 매번 정확하게 수행하는 것은 별개의 문제입니다. 사람도 교과서를 읽었다고 해서 시험에서 만점을 받지는 못합니다. AI도 마찬가지입니다. 교과서는 “이렇게 하면 좋겠다”는 방향을 제시할 뿐, 지금 이 코드가 규칙을 지키고 있는지 판단해주지 않습니다.&lt;/p&gt;&lt;p&gt;반면 시험지는 다릅니다. 시험지는 옳고 그름을 직접 판단합니다. “이 답은 맞다”, “이 답은 틀렸다” — 이 명확한 피드백이 핵심입니다. AI에게 “네가 방금 생성한 코드, 여기가 틀렸어”라고 구체적으로 알려줄 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*5srhGu55DIGL2OQncxNT9w.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;결국 필요한 것은 “더 좋은 교과서”가 아니라 “시험지”였습니다. AI가 생성한 코드를 즉시 검증하고, 위반이 있으면 명시적으로 “틀렸다”고 알려주는 자동화된 장치. 이 관점에서 도구를 찾기 시작했습니다.&lt;/p&gt;&lt;h3&gt;후보 비교와 선택: 선생님을 찾아서&lt;/h3&gt;&lt;h3&gt;먼저 떠오른 것: Java Lint&lt;/h3&gt;&lt;p&gt;“Java에도 Lint 도구가 있지 않나? Checkstyle이나 PMD로 규칙을 검증하면 되는 거 아닌가?”&lt;/p&gt;&lt;p&gt;결론부터 말하면, 기존의 Java Lint 도구로는 아키텍처 규칙을 검증하는 데 근본적인 한계가 있습니다.&lt;/p&gt;&lt;p&gt;Checkstyle이나 PMD 같은 Lint 도구는 단일 파일 수준에서 동작합니다. 변수 네이밍이 카멜케이스를 따르는지, 메서드 길이가 적정한지, 사용하지 않는 import가 있는지 — 이런 것들은 파일 하나만 보고 판단할 수 있습니다.&lt;/p&gt;&lt;p&gt;하지만 아키텍처 규칙은 다릅니다. “Repository가 Controller를 참조하면 안 된다”는 규칙을 생각해보겠습니다. 이 규칙을 검증하려면 OrderRepository.java 파일 하나만 봐서는 알 수 없습니다. 이 클래스가 어떤 패키지에 있는지, import하는 클래스가 어떤 레이어에 속하는지, 프로젝트 전체의 패키지 구조가 어떻게 되어 있는지를 함께 알아야 합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*5yly8cVwowGWs4CusHgrlA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;비유하자면, Lint는 글자 맞춤법 검사기입니다. 각 문장의 띄어쓰기와 문법은 잡아주지만, “3장의 내용이 1장의 결론과 모순된다”는 것은 발견하지 못합니다. 아키텍처 검증은 글 전체의 구조적 일관성을 보는 일이고, 이를 위해서는 프로젝트 전체를 하나의 단위로 분석할 수 있는 도구가 필요했습니다.&lt;/p&gt;&lt;h3&gt;아키텍처 검증 도구 3가지 비교&lt;/h3&gt;&lt;p&gt;Lint의 한계를 확인한 후, 아키텍처 규칙을 코드 레벨에서 자동 검증할 수 있는 도구 3가지를 검토했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*byYGoHg7QeLqla9zaGbYwQ.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;최종 선택: ArchUnit&lt;/h3&gt;&lt;p&gt;세 가지 후보 중 ArchUnit을 선택한 결정적 이유는 AI 연동 친화성이었습니다.&lt;/p&gt;&lt;p&gt;ArchUnit은 일반 JUnit 테스트처럼 동작합니다. 아키텍처 위반이 있으면 테스트가 실패하고, 무엇이 왜 잘못되었는지 명확한 에러 메시지를 출력합니다. 여기서 핵심적인 발견이 있었습니다. Claude는 테스트 실패 메시지를 매우 잘 이해합니다. 실패 원인을 읽고, 위반을 파악하고, 스스로 코드를 수정합니다.&lt;/p&gt;&lt;p&gt;즉, ArchUnit은 AI에게 “엄격한 선생님” 역할을 할 수 있었습니다.&lt;/p&gt;&lt;p&gt;교과서를 건네주며 “이렇게 해주세요”라고 부탁하는 것과, 시험을 보게 하며 “이것을 어기면 탈락입니다”라고 알려주는 것. 이 두 가지 사이에는 근본적인 차이가 있습니다.&lt;/p&gt;&lt;h3&gt;도입 과정: 선생님 세팅하기&lt;/h3&gt;&lt;h3&gt;1. ArchUnit 프로젝트 세팅&lt;/h3&gt;&lt;p&gt;첫 번째로 놀란 점은 도입이 너무 간단하다는 것이었습니다. Gradle 의존성 하나만 추가하면 끝입니다.&lt;/p&gt;&lt;pre&gt;dependencies {&lt;br&gt;    testImplementation(&amp;quot;com.tngtech.archunit:archunit-junit5:1.3.0&amp;quot;)&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;별도의 서버를 구축하거나 인프라를 변경할 필요가 전혀 없었습니다. 기존 테스트 프레임워크 위에서 바로 동작했습니다.&lt;/p&gt;&lt;h3&gt;2. 시험지도 AI가 만든다&lt;/h3&gt;&lt;p&gt;여기서 재미있는 장면이 연출됩니다. ArchUnit 테스트 코드를 누가 작성할까요? 사람이 ArchUnit API를 공부해서 하나하나 작성할 수도 있습니다. 하지만 저희는 이 과정도 AI에게 맡겼습니다.&lt;/p&gt;&lt;p&gt;방법은 간단합니다. 사람은 자연어로 규칙을 설명하면 됩니다.&lt;/p&gt;&lt;blockquote&gt;“Repository 레이어는 Controller 레이어에 의존하면 안 됩니다” “Service 클래스는 반드시 service 패키지 안에 위치해야 합니다” “infrastructure 패키지는 domain 패키지에 접근할 수 있지만, 그 반대는 안 됩니다”&lt;/blockquote&gt;&lt;p&gt;Claude에게 이런 규칙을 자연어로 전달하면, AI가 해당 규칙에 대응하는 ArchUnit 테스트 코드를 생성합니다. 아키텍처에 대한 도메인 지식은 사람이, 그것을 코드로 옮기는 작업은 AI가 담당하는 셈입니다.&lt;/p&gt;&lt;p&gt;아이러니하게도, AI가 스스로 응시할 시험지를 AI가 만드는 구조입니다. 하지만 이것이 정확히 의도한 바입니다. 시험지를 만드는 것과 시험을 통과하는 것은 전혀 다른 일입니다. 시험지는 한 번만 만들면 되지만, 시험은 코드를 생성할 때마다 매번 치러야 합니다. 한 번 잘 만들어진 ArchUnit 테스트는 이후 생성되는 모든 코드에 대해 일관된 검증을 수행합니다.&lt;/p&gt;&lt;h3&gt;3. AI와의 연동: 질풍노도에서 절도 있는 행군으로&lt;/h3&gt;&lt;p&gt;선생님 세팅이 끝났습니다. 이제 실제로 AI와 어떻게 동작하는지가 중요합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lJuIaooI3-0ncUcMiqr-Aw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;이전에는 이런 흐름이었습니다:&lt;/p&gt;&lt;blockquote&gt;개발자: “레이어 의존성을 지켜서 코드를 작성해줘” Claude: “네, 알겠습니다” (대부분 지키지만, 가끔 빠뜨림) 개발자: (코드 리뷰에서 발견) “여기 의존성 방향이 반대야” Claude: “죄송합니다, 수정하겠습니다”&lt;/blockquote&gt;&lt;p&gt;ArchUnit 도입 후에는 이렇게 바뀌었습니다:&lt;/p&gt;&lt;blockquote&gt;Claude가 코드를 생성합니다. 테스트를 실행합니다. ArchUnit: “Architecture Violation — Repository is not allowed to access Controller” Claude: (실패 메시지를 읽고) “레이어 의존성 위반이 있습니다. Service를 통해 접근하도록 수정하겠습니다.” 테스트를 재실행합니다. 통과.&lt;/blockquote&gt;&lt;p&gt;사람이 개입할 틈이 없습니다. AI가 코드를 생성하고, 테스트가 위반을 잡고, AI가 스스로 수정합니다. &lt;strong&gt;부탁에서 강제로, 사후 검증에서 사전 차단으로&lt;/strong&gt; 전환된 것입니다.&lt;/p&gt;&lt;p&gt;흥미로운 점은 Claude가 ArchUnit 실패 메시지에 대응하는 방식이었습니다. 단순히 에러를 회피하는 것이 아니라, 위반 원인을 이해하고 올바른 아키텍처 패턴으로 코드를 재구성했습니다. 교과서를 읽었을 때는 가끔 빠뜨리던 AI가, 시험을 보게 하니 틀린 문제를 스스로 교정하기 시작한 셈입니다.&lt;/p&gt;&lt;h3&gt;적용한 규칙들: 선생님의 훈련 과목&lt;/h3&gt;&lt;h3&gt;1. 레이어 의존성 규칙 정의&lt;/h3&gt;&lt;p&gt;가장 먼저 잡고 싶었던 규칙은 레이어 간 의존성 방향이었습니다. Controller → Service → Repository 방향으로만 의존해야 하며, 역방향 의존은 허용하지 않습니다. 처음 코드 리뷰에서 발견했던 바로 그 문제입니다.&lt;/p&gt;&lt;p&gt;자연어로 설명한 규칙이 ArchUnit 코드로 변환되면 다음과 같은 모습이 됩니다:&lt;/p&gt;&lt;pre&gt;@ArchTest&lt;br&gt;val `레이어 의존성 규칙` = layeredArchitecture()&lt;br&gt;    .consideringAllDependencies()&lt;br&gt;    .layer(&amp;quot;Controller&amp;quot;).definedBy(&amp;quot;..controller..&amp;quot;)&lt;br&gt;    .layer(&amp;quot;Service&amp;quot;).definedBy(&amp;quot;..service..&amp;quot;)&lt;br&gt;    .layer(&amp;quot;Repository&amp;quot;).definedBy(&amp;quot;..repository..&amp;quot;)&lt;br&gt;    .layer(&amp;quot;Domain&amp;quot;).definedBy(&amp;quot;..domain..&amp;quot;)&lt;br&gt;    .whereLayer(&amp;quot;Controller&amp;quot;).mayOnlyBeAccessedByLayers(&amp;quot;Controller&amp;quot;)&lt;br&gt;    .whereLayer(&amp;quot;Service&amp;quot;).mayOnlyBeAccessedByLayers(&amp;quot;Controller&amp;quot;, &amp;quot;Service&amp;quot;)&lt;br&gt;    .whereLayer(&amp;quot;Repository&amp;quot;).mayOnlyBeAccessedByLayers(&amp;quot;Service&amp;quot;)&lt;br&gt;    .whereLayer(&amp;quot;Domain&amp;quot;).mayOnlyBeAccessedByLayers(&amp;quot;Service&amp;quot;, &amp;quot;Repository&amp;quot;, &amp;quot;Controller&amp;quot;)&lt;/pre&gt;&lt;p&gt;이 테스트가 실패하면 다음과 같은 메시지가 출력됩니다:&lt;/p&gt;&lt;pre&gt;Architecture Violation [Rule &amp;#39;Layered architecture ...&amp;#39;] -&lt;br&gt;Layer &amp;#39;Repository&amp;#39; is not allowed to access layer &amp;#39;Controller&amp;#39;,&lt;br&gt;but class com.musinsa.warehouse.repository.OrderRepository&lt;br&gt;accesses com.musinsa.warehouse.controller.OrderController&lt;/pre&gt;&lt;p&gt;메시지가 구체적입니다. 어떤 클래스가 어떤 레이어를 잘못 참조했는지 정확히 알려줍니다. 사람이 읽어도, AI가 읽어도 즉시 원인을 파악할 수 있습니다.&lt;/p&gt;&lt;p&gt;실제 프로젝트에서는 멀티 모듈 구조에 맞춰 모듈별로 레이어 규칙을 정의합니다. 예를 들어, API 모듈이 다른 모듈의 내부 구현에 직접 접근하는 것을 차단하는 규칙은 이렇게 작성됩니다&lt;/p&gt;&lt;pre&gt;@Test&lt;br&gt;@DisplayName(&amp;quot;API 패키지만 접근해야 한다&amp;quot;)&lt;br&gt;void adminShouldOnlyAccessAllowedApiPackages() {&lt;br&gt;    ArchRule rule = noClasses()&lt;br&gt;            .that()&lt;br&gt;            .resideInAPackage(&amp;quot;com.musinsalogistics.sampleproject.admin..&amp;quot;)&lt;br&gt;            .should()&lt;br&gt;            .accessClassesThat(resideInAPackage(&amp;quot;com.musinsalogistics.sampleproject..&amp;quot;)&lt;br&gt;                    .and(resideOutsideOfPackages(&lt;br&gt;                            &amp;quot;com.musinsalogistics.sampleproject.admin..&amp;quot;,&lt;br&gt;                            &amp;quot;com.musinsalogistics.sampleproject..api..&amp;quot;,&lt;br&gt;                            &amp;quot;com.musinsalogistics.sampleproject..exception..&amp;quot;)))&lt;br&gt;            .allowEmptyShould(true)&lt;br&gt;            .because(&amp;quot;Admin은 다른 모듈의 api 패키지와 exception 패키지만 접근할 수 있습니다&amp;quot;); &lt;br&gt;    rule.check(classes);&lt;br&gt;}&lt;br&gt;&lt;/pre&gt;&lt;blockquote&gt;“다른 모듈의 api 패키지와 exception 패키지만 접근 가능&amp;quot; — 이 한 줄의 비즈니스 규칙이 코드로 표현되면, AI가 모듈 경계를 넘어 내부 구현에 직접 의존하는 코드를 생성했을 때 즉시 테스트가 실패합니다.&lt;/blockquote&gt;&lt;h3&gt;2. 패키지 구조 규칙 정의&lt;/h3&gt;&lt;p&gt;레이어 의존성 외에도 팀의 패키지 네이밍 컨벤션을 강제했습니다. “Controller 클래스는 controller 패키지에”, “Service 클래스는 service 패키지에” — 단순하지만 AI가 종종 빠뜨리는 규칙이었습니다.&lt;/p&gt;&lt;pre&gt;@ArchTest&lt;br&gt;val `패키지 구조 규칙 - Controller는 controller 패키지에 위치` = classes()&lt;br&gt;    .that().haveSimpleNameEndingWith(&amp;quot;Controller&amp;quot;)&lt;br&gt;    .should().resideInAPackage(&amp;quot;..controller..&amp;quot;)&lt;br&gt;@ArchTest&lt;br&gt;val `패키지 구조 규칙 - Service는 service 패키지에 위치` = classes()&lt;br&gt;    .that().haveSimpleNameEndingWith(&amp;quot;Service&amp;quot;)&lt;br&gt;    .should().resideInAPackage(&amp;quot;..service..&amp;quot;)&lt;br&gt;@ArchTest&lt;br&gt;val `패키지 구조 규칙 - Repository는 repository 패키지에 위치` = classes()&lt;br&gt;    .that().haveSimpleNameEndingWith(&amp;quot;Repository&amp;quot;)&lt;br&gt;    .should().resideInAPackage(&amp;quot;..repository..&amp;quot;)&lt;/pre&gt;&lt;h3&gt;3. 순환 의존성 감지 규칙&lt;/h3&gt;&lt;p&gt;레이어 의존성과 패키지 구조 외에도, AI가 복잡한 기능을 구현하다 보면 의도치 않게 &lt;strong&gt;패키지 간 순환 참조&lt;/strong&gt;를 만들어내는 경우가 있었습니다. A 패키지가 B를 참조하고, B가 다시 A를 참조하는 구조. 사람이 코드 리뷰에서 이런 순환을 발견하기란 쉽지 않습니다. 하지만 ArchUnit에게는 간단한 일입니다.&lt;/p&gt;&lt;blockquote&gt;“패키지 간 순환 의존이 발생하면 안 됩니다”&lt;/blockquote&gt;&lt;p&gt;이 한 줄의 자연어가 다음 테스트 코드가 됩니다:&lt;/p&gt;&lt;pre&gt;@ArchTest&lt;br&gt;val `패키지 간 순환 의존 금지` = slices()&lt;br&gt;    .matching(&amp;quot;com.musinsa.warehouse.(*)..&amp;quot;)&lt;br&gt;    .should().beFreeOfCycles()&lt;/pre&gt;&lt;p&gt;단 세 줄로, 프로젝트 전체의 패키지 순환을 감지합니다. 앞서 설명한 Lint가 절대 할 수 없는 영역입니다. 파일 하나만 봐서는 순환인지 알 수 없고, 전체 의존 그래프를 그려봐야 비로소 드러나는 문제이기 때문입니다.&lt;/p&gt;&lt;p&gt;ArchUnit의 검증 범위는 클래스 구조에 그치지 않습니다. 메서드 레벨의 규칙도 작성할 수 있습니다. 예를 들어, API URL이 팀에서 정한 형식을 따르는지 검증하는 규칙입니다:&lt;/p&gt;&lt;pre&gt;@Test&lt;br&gt;@DisplayName(&amp;quot;Controller의 API는 /api/sampleproject/public/v{버전}/{비즈니스}... 형식을 준수해야 한다&amp;quot;)&lt;br&gt;void controllerApiUrlShouldFollowPublicApiFormat() {&lt;br&gt;    ArchRule rule = ArchRuleDefinition.methods()&lt;br&gt;            .that().areDeclaredInClassesThat().resideInAPackage(&amp;quot;..publicapi.controller..&amp;quot;)&lt;br&gt;            .and().areDeclaredInClassesThat().haveSimpleNameEndingWith(&amp;quot;Controller&amp;quot;)&lt;br&gt;            .and().areAnnotatedWith(isHttpMappingAnnotation())&lt;br&gt;            .should(haveValidPublicApiUrlFormat())&lt;br&gt;            .allowEmptyShould(true)&lt;br&gt;            .because(&amp;quot;Public API는 /api/sampleproject/public/v{버전번호}/{비즈니스}... 형식을 준수해야 합니다&amp;quot;);&lt;/pre&gt;&lt;p&gt;AI가 새 API 엔드포인트를 생성할 때 URL 형식을 자유롭게 만들어버리는 경우가 있습니다. 이 규칙이 있으면 /api/sampleproject/public/v1/... 형식에 맞지 않는 URL은 즉시 감지됩니다. 사소해 보이지만, API URL의 일관성은 외부 시스템 연동에서 매우 중요합니다.&lt;/p&gt;&lt;h3&gt;4. 도메인 순수성 보호 규칙&lt;/h3&gt;&lt;p&gt;저희 팀이 특히 신경 쓰는 규칙이 하나 더 있습니다. 도메인 레이어는 외부 프레임워크에 의존하지 않아야 한다는 것입니다. Spring의 @Autowired, JPA의 @Entity 같은 프레임워크 어노테이션이 도메인 모델에 침투하면, 도메인이 특정 기술에 종속되어 버립니다.&lt;/p&gt;&lt;blockquote&gt;“domain 패키지의 클래스는 Spring이나 JPA 프레임워크에 의존하면 안 됩니다”&lt;/blockquote&gt;&lt;pre&gt;@ArchTest&lt;br&gt;val `도메인은 프레임워크에 의존하지 않는다` = classes()&lt;br&gt;    .that().resideInAPackage(&amp;quot;..domain..&amp;quot;)&lt;br&gt;    .should().onlyDependOnClassesThat()&lt;br&gt;    .resideInAnyPackage(&lt;br&gt;        &amp;quot;..domain..&amp;quot;,&lt;br&gt;        &amp;quot;java..&amp;quot;,&lt;br&gt;        &amp;quot;kotlin..&amp;quot;,&lt;br&gt;        &amp;quot;org.slf4j..&amp;quot;&lt;br&gt;    )&lt;/pre&gt;&lt;p&gt;이 규칙이 없을 때, AI는 가끔 도메인 클래스에 @Component를 붙이거나 JpaRepository를 직접 주입하는 코드를 생성했습니다. 테스트가 이를 잡아내면서, AI는 &amp;quot;도메인은 순수해야 한다&amp;quot;는 원칙을 코드 레벨에서 학습하게 되었습니다.&lt;/p&gt;&lt;p&gt;의존성 역전 원칙(DIP)도 ArchUnit으로 강제할 수 있습니다. 예를 들어, Security 모듈에서 provider 패키지가 component 패키지의 구체 클래스에 직접 의존하지 않도록 하는 규칙입니다.&lt;/p&gt;&lt;p&gt;먼저 DIP 위반과 준수가 어떤 구조인지 살펴보겠습니다:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*UOv8AaB-s-Q9XK51NFofoQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Provider가 Component의 구체 클래스를 직접 참조하면 두 패키지가 강하게 결합됩니다. 대신 security.api 패키지의 인터페이스를 통해 소통하면, Component의 내부 구현이 바뀌어도 Provider는 영향을 받지 않습니다. 이 설계 원칙을 ArchUnit으로 강제하면:&lt;/p&gt;&lt;pre&gt;@Test&lt;br&gt;void securityProviderShouldNotDependOnComponentClasses() {&lt;br&gt;    ArchRule rule = noClasses()&lt;br&gt;            .that().resideInAnyPackage(&amp;quot;..security.internal.provider..&amp;quot;)&lt;br&gt;            .should().dependOnClassesThat()&lt;br&gt;            .resideInAnyPackage(&amp;quot;..security.internal.component..&amp;quot;)&lt;br&gt;            .because(&amp;quot;의존성 역전 원칙(DIP)을 준수하여 추상화(인터페이스)에 의존해야 합니다&amp;quot;);&lt;br&gt;    rule.check(securityModuleClasses);&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;AI가 Provider 클래스에서 Component의 구체 클래스를 직접 import하면 즉시 테스트가 실패합니다.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;“구체 클래스가 아닌 인터페이스에 의존하라”&lt;/strong&gt;는 SOLID 원칙이 코드 레벨에서 자동 검증되는 셈입니다.&lt;/blockquote&gt;&lt;h3&gt;5. 어노테이션과 클래스 네이밍 일관성 규칙&lt;/h3&gt;&lt;p&gt;AI가 은근히 자주 저지르는 실수가 하나 더 있습니다. @Service 어노테이션을 Service가 아닌 클래스에 붙이는 것입니다. 예를 들어 OrderHelper나 PriceCalculator 같은 유틸리티 클래스에 습관적으로 @Service를 달아버립니다. 컴파일도 되고 동작도 합니다. 하지만 팀 컨벤션에서는 @Service는 ~Service로 끝나는 클래스에만 사용해야 합니다. 어노테이션과 클래스 이름이 일치하지 않으면, 코드를 읽는 사람이 역할을 오해하게 됩니다.&lt;/p&gt;&lt;blockquote&gt;“@Service 어노테이션은 이름이 Service로 끝나는 클래스에만 붙여야 합니다”&lt;/blockquote&gt;&lt;pre&gt;@ArchTest&lt;br&gt;val `@Service는 Service 클래스에만 사용` = classes()&lt;br&gt;    .that().areAnnotatedWith(Service::class.java)&lt;br&gt;    .should().haveSimpleNameEndingWith(&amp;quot;Service&amp;quot;)&lt;br&gt;@ArchTest&lt;br&gt;val `@Repository는 Repository 클래스에만 사용` = classes()&lt;br&gt;    .that().areAnnotatedWith(Repository::class.java)&lt;br&gt;    .should().haveSimpleNameEndingWith(&amp;quot;Repository&amp;quot;)&lt;br&gt;@ArchTest&lt;br&gt;val `@Controller는 Controller 클래스에만 사용` = classes()&lt;br&gt;    .that().areAnnotatedWith(RestController::class.java)&lt;br&gt;    .should().haveSimpleNameEndingWith(&amp;quot;Controller&amp;quot;)&lt;/pre&gt;&lt;p&gt;이 규칙의 효과는 생각보다 컸습니다. AI는 “빈으로 등록해야 하니까 @Service를 붙이자&amp;quot;라는 판단을 자주 합니다. 기능적으로는 맞지만, 팀의 네이밍 컨벤션과는 어긋납니다. 테스트가 실패하면 AI는 @Service 대신 @Component를 사용하거나, 클래스 이름을 컨벤션에 맞게 변경합니다.&lt;/p&gt;&lt;p&gt;어노테이션 규칙은 여기서 한 단계 더 깊어질 수 있습니다. 저희 팀에서는 @Transactional의 readOnly 속성에 따라 Service 클래스의 네이밍까지 강제합니다&lt;/p&gt;&lt;pre&gt;@ArchTest&lt;br&gt;void Transactional_readOnly_false인_클래스는_FacadeService_또는_WriteService_이름을_가져야_한다(JavaClasses classes) {&lt;br&gt;    classes()&lt;br&gt;            .that().resideInAPackage(&amp;quot;..domain..&amp;quot;)&lt;br&gt;            .and().areAnnotatedWith(Transactional.class)&lt;br&gt;            .should(haveWriteServiceNaming())&lt;br&gt;            .allowEmptyShould(true)&lt;br&gt;            .because(&amp;quot;쓰기 트랜잭션 서비스는 FacadeService 또는 WriteService 네이밍을 따라야 합니다&amp;quot;)&lt;br&gt;            .check(classes);&lt;br&gt;}&lt;br&gt;&lt;br&gt;@ArchTest&lt;br&gt;void Transactional_readOnly_true인_클래스는_ReadService_이름을_가져야_한다(JavaClasses classes) {&lt;br&gt;    classes()&lt;br&gt;            .that().resideInAPackage(&amp;quot;..domain..&amp;quot;)&lt;br&gt;            .and().areAnnotatedWith(Transactional.class)&lt;br&gt;            .should(haveReadServiceNaming())&lt;br&gt;            .allowEmptyShould(true)&lt;br&gt;            .because(&amp;quot;읽기 전용 서비스는 ReadService 또는 SearchService 네이밍을 따라야 합니다&amp;quot;)&lt;br&gt;            .check(classes);&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;@Transactional(readOnly = true)인데 WriteService라는 이름을 가진 클래스가 있다면? 테스트가 실패합니다. 트랜잭션 속성과 클래스 이름이 일치해야 한다는 규칙은, 코드를 읽는 사람이 &amp;quot;이 서비스가 읽기 전용인지, 쓰기도 하는지&amp;quot;를 이름만으로 즉시 판단할 수 있게 해줍니다.&lt;/p&gt;&lt;h3&gt;왜 사람이 읽을 수 있는 코드여야 하는가&lt;/h3&gt;&lt;p&gt;“기능만 돌아가면 되는 거 아닌가?”라고 생각할 수도 있습니다. 하지만 여기서 한 가지 잊지 말아야 할 것이 있습니다. 코드를 생성하는 것은 AI지만, 그 코드를 감독하고, 유지보수하고, 장애에 책임지는 것은 사람입니다.&lt;/p&gt;&lt;p&gt;AI가 만든 코드는 결국 사람이 읽어야 합니다. 새로운 팀원이 합류했을 때, 장애가 터져서 새벽에 코드를 열었을 때, 반년 뒤 요구사항이 바뀌어 로직을 수정해야 할 때 — 그 코드를 마주하는 것은 사람입니다. OrderHelper에 @Service가 붙어 있으면, 사람은 &amp;quot;이게 비즈니스 서비스인가?&amp;quot;라고 오해합니다. 작은 혼란이 쌓이면 코드베이스 전체의 가독성이 무너집니다.&lt;/p&gt;&lt;p&gt;어노테이션과 클래스 이름의 일관성은 사소해 보이지만, &lt;strong&gt;사람이 코드를 빠르고 정확하게 이해하기 위한 최소한의 약속&lt;/strong&gt;입니다.&lt;/p&gt;&lt;blockquote&gt;AI 시대에도 코드의 최종 독자는 여전히 사람이기 때문입니다.&lt;/blockquote&gt;&lt;h3&gt;도입 후 변화&lt;/h3&gt;&lt;h3&gt;정량적 변화&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*B_1Hgo-TwOUMC6cCc0JGpA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;가장 체감이 컸던 변화는 코드 리뷰의 질이었습니다. 아키텍처 위반을 더 이상 눈으로 찾을 필요가 없으니, 리뷰어는 비즈니스 로직의 정확성과 엣지 케이스 처리에 온전히 집중할 수 있게 되었습니다.&lt;/p&gt;&lt;h3&gt;팀 피드백&lt;/h3&gt;&lt;p&gt;팀원들의 반응은 긍정적이었습니다. 특히 두 가지 목소리가 인상적이었습니다:&lt;/p&gt;&lt;blockquote&gt;“이제 아키텍처 위반은 ArchUnit이 잡아주니, 리뷰에서 비즈니스 로직에만 집중할 수 있어서 좋습니다”&lt;/blockquote&gt;&lt;blockquote&gt;“AI가 생성한 코드도 테스트를 통과해야 하니까, 예전보다 안심하고 AI를 활용할 수 있게 됐습니다”&lt;/blockquote&gt;&lt;p&gt;AI 활용에 대한 심리적 안전망이 생긴 것도 큰 변화였습니다. “AI가 혹시 규칙을 어기면 어쩌지?”라는 불안감이, “어겨도 테스트에서 잡히니까 괜찮아”로 바뀌었습니다.&lt;/p&gt;&lt;h3&gt;마무리&lt;/h3&gt;&lt;h3&gt;AI에게 부탁하지 말고, 시험을 보게 하라&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Kk0_qwVuPzw3VXqplJW56w.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;이번 도입 과정에서 가장 크게 깨달은 것은 AI에게 규칙을 전달하는 방식의 차이입니다.&lt;/p&gt;&lt;blockquote&gt;컨텍스트는 가이드라인입니다. 테스트는 제약 조건입니다. AI는 “이렇게 해주세요”라는 부탁보다 &lt;strong&gt;“이것을 어기면 빌드가 실패합니다”&lt;/strong&gt;라는 명확한 피드백에 훨씬 잘 반응합니다.&lt;/blockquote&gt;&lt;blockquote&gt;이 관점은 아키텍처 규칙에만 국한되지 않습니다. 코딩 컨벤션, 네이밍 규칙, 모듈 경계 — &lt;strong&gt;팀이 일관되게 지키고 싶은 모든 규약&lt;/strong&gt;을 “테스트로 검증 가능한 형태”로 만들 수 있다면, AI와의 협업 품질은 한 단계 올라갈 수 있습니다.&lt;/blockquote&gt;&lt;h3&gt;향후 과제&lt;/h3&gt;&lt;p&gt;현재는 레이어 의존성과 패키지 구조라는 가장 기본적인 규칙만 적용한 상태입니다. 앞으로 두 가지 방향으로 확장을 계획하고 있습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;규칙 확대: 도메인 이벤트 발행 규칙, DTO 변환 위치 규칙, 외부 라이브러리 사용 범위 제한 등 더 세밀한 아키텍처 규칙을 추가할 예정입니다. 선생님의 훈련 과목을 늘리는 셈입니다.&lt;/li&gt;&lt;li&gt;다른 프로젝트 확산: 물류플랫폼팀 내 다른 프로젝트에도 동일한 ArchUnit 규칙 세트를 적용하여, 프로젝트가 달라져도 아키텍처의 일관성을 유지하려 합니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;AI 코드 생성 도구를 적극 활용하면서도 아키텍처 품질에 대한 고민이 있으신 분들에게, ArchUnit이 좋은 출발점이 될 수 있을 것입니다.&lt;/p&gt;&lt;p&gt;질풍노도의 AI에게 필요한 것은 더 많은 교과서가 아니라,&lt;/p&gt;&lt;blockquote&gt;시험지 한 장이었습니다.&lt;/blockquote&gt;&lt;h3&gt;Platform Business Operation 조직 및 팀 소개&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사 플랫폼 비즈니스 오퍼레이션 조직은 국내외 물류 서비스, 재고 관리, 스토어 운영을 위한 물류 프로덕트를 구축하고 다양한 오프라인 비즈니스 모델에 맞춘 스토어 관리 시스템을 개발·고도화하고 있습니다.&lt;br&gt;또한, 무배당발 서비스를 포함한 무신사의 차별화된 고객 경험을 브랜딩하고 확장할 수 있는 멤버십 구조를 설계하며, 온·오프라인을 넘나드는 통합 커머스 경험을 기술로 실현하고 있습니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;저희 팀은 OMS(주문관리시스템)를 기반으로 온라인 주문부터 재고·출고·배송·정산에 이르는 전 과정을 유기적으로 연결하고, 무신사의 다양한 오프라인 스토어를 효과적으로 운영할 수 있는 관리 시스템을 구축하여 고객이 온라인(무신사 스토어, 29CM 등)과 오프라인(무신사 스탠다드, 편집숍 등)에서 끊김 없는 쇼핑 경험을 누릴 수 있도록 지원합니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/pbo&quot;&gt;&lt;em&gt;🚀 Platform Business Operation 한걸음 더 알아보기&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=61c3d533fc40&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%A7%88%ED%92%8D%EB%85%B8%EB%8F%84%EC%9D%98-ai-claude-%EC%97%90%EA%B2%8C-%EC%97%84%EA%B2%A9%ED%95%9C-%EC%84%A0%EC%83%9D%EB%8B%98-%EC%9E%A5%EC%B0%A9%ED%95%98%EA%B8%B0-61c3d533fc40&quot;&gt;질풍노도의 AI(Claude)에게 엄격한 선생님 장착하기&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>“스케줄이 또 안 돌았어요” — 우리가 Temporal을 선택한 이유</title>
      <link>https://techblog.musinsa.com/%EC%8A%A4%EC%BC%80%EC%A4%84%EC%9D%B4-%EB%98%90-%EC%95%88-%EB%8F%8C%EC%95%98%EC%96%B4%EC%9A%94-%EC%9A%B0%EB%A6%AC%EA%B0%80-temporal%EC%9D%84-%EC%84%A0%ED%83%9D%ED%95%9C-%EC%9D%B4%EC%9C%A0-f491e79a0f8f?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%8A%A4%EC%BC%80%EC%A4%84%EC%9D%B4-%EB%98%90-%EC%95%88-%EB%8F%8C%EC%95%98%EC%96%B4%EC%9A%94-%EC%9A%B0%EB%A6%AC%EA%B0%80-temporal%EC%9D%84-%EC%84%A0%ED%83%9D%ED%95%9C-%EC%9D%B4%EC%9C%A0-f491e79a0f8f?source=rss----f107b03c406e---4</guid>
      <pubDate>Thu, 05 Feb 2026 02:17:35 GMT</pubDate>
      <content:encoded>&lt;h3&gt;“스케줄이 또 안 돌았어요” — 우리가 Temporal을 선택한 이유&lt;/h3&gt;&lt;p&gt;어느 날 아침, 슬랙 알림이 울렸습니다. &lt;br&gt;“출고지시 스케줄이 실행 안 된 것 같은데 확인 부탁드려요.”&lt;/p&gt;&lt;p&gt;Jenkins 콘솔을 열어보니 job이 멈춰있었고, 모니터링 job마저 함께 멈춰있었습니다. 급하게 수동으로 출고지시를 트리거하고 나서야 물류센터 작업이 시작될 수 있었습니다. 이런 일이 반복되면서 우리는 고민하게 되었습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“스케줄 하나 도는 게 왜 이렇게 불안할까?”&lt;/strong&gt;&lt;/p&gt;&lt;h3&gt;출고지시, 그리고 우리가 마주한 문제들&lt;/h3&gt;&lt;p&gt;무신사 풀필먼트의 OMS는 주문부터 출고까지 물류 전반을 책임지는 시스템입니다. 그 중에서도 &lt;strong&gt;출고지시는 물류센터의 하루를 시작하는 신호&lt;/strong&gt;와도 같습니다. 정해진 시간에 출고지시가 생성되지 않으면 물류센터 작업이 지연되고, 그것은 곧 배송 지연으로 이어집니다.&lt;/p&gt;&lt;p&gt;초기에는 Jenkins crontab으로 충분했습니다. 정해진 시간에 실행하기만 하면 됐습니다. 하지만 화주사가 늘고 출고 물량이 커지면서, 이 구조의 한계가 하나둘 보이기 시작했습니다.&lt;/p&gt;&lt;h3&gt;1. 실패를 놓치는 순간들&lt;/h3&gt;&lt;p&gt;Jenkins 기반 스케줄은 job이 멈춰도 알려주지 않았습니다. 그래서 우리는 별도의 모니터링 job을 만들어 실행했습니다. &lt;strong&gt;“스케줄이 실행됐는지 확인하는 스케줄”&lt;/strong&gt;을 또 만드는 거죠.&lt;/p&gt;&lt;p&gt;문제는 이 모니터링 job도 언제든 멈출 수 있다는 것이었습니다. 실제로 Jenkins 자체에 문제가 생기면 스케줄과 모니터링이 함께 멈췄고, 우리는 뒤늦게야 알게 되곤 했습니다. 모니터링을 위한 모니터링을 또 만들 수는 없었고, 이 구조 자체가 근본적인 한계를 가지고 있다는 걸 깨달았습니다.&lt;/p&gt;&lt;h3&gt;2. 로그 속에서 원인 찾기&lt;/h3&gt;&lt;p&gt;출고지시에 문제가 생기면, 저희는 Jenkins 콘솔 로그를 시작으로 애플리케이션 로그, DB 이력을 차례로 확인해야 했습니다.&lt;/p&gt;&lt;p&gt;“이번엔 어디서 실패한 거지?” &lt;br&gt;“입력값은 뭐였지?” &lt;br&gt;“결과는 어떻게 됐지?”&lt;/p&gt;&lt;p&gt;실행 이력을 한눈에 볼 수 있는 방법이 없었고, &lt;strong&gt;문제를 분석하는 것보다 로그를 따라가는 데 더 많은 시간&lt;/strong&gt;이 들었습니다. 빠른 대응이 필요한 순간일수록, 이런 가시성 부족은 운영 리스크를 키우는 요인이 되었습니다.&lt;/p&gt;&lt;h3&gt;3. “다시 눌러주세요”&lt;/h3&gt;&lt;p&gt;스케줄이 실패하면 저희가 직접 재실행해야 했습니다. 단순히 일시적인 네트워크 오류로 실패한 경우에도, 원인을 확인하고 수동으로 재실행하는 과정에서 시간이 소요되었습니다.&lt;/p&gt;&lt;p&gt;대응이 조금만 늦어도 출고 SLA에 영향을 주는 경우가 생겼고, 담당자의 부담은 자연스럽게 커질 수밖에 없었습니다. 실제로는 단순 재시도만으로 해결될 수 있는 케이스도 많았지만, 원인을 확인하는 절차 자체가 출고 지연으로 이어지는 경우가 반복되었습니다.&lt;/p&gt;&lt;p&gt;결국 이러한 구조에서는 안정적인 운영을 기대하기 어려워졌습니다.&lt;/p&gt;&lt;h3&gt;4. 이벤트 기반으로의 확장&lt;/h3&gt;&lt;p&gt;출고 도메인에서는 정해진 스케줄 외에도, 특정 이벤트를 기점으로 Workflow를 실행해야 하는 요구가 늘어나고 있었습니다. 하지만 Jenkins는 cron 기반 실행에 최적화되어 있었고, 이벤트 기반 트리거를 자연스럽게 처리하기에는 구조적인 한계가 있었습니다.&lt;/p&gt;&lt;p&gt;결국 저희는 깨달았습니다. &lt;br&gt;&lt;strong&gt;“출고 물량과 요구사항이 늘어날수록, 기존 구조로는 안정적인 운영을 기대하기 어렵다.”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;구조 자체를 다시 고민해야 할 시점이었습니다.&lt;/p&gt;&lt;h3&gt;대안을 찾아서&lt;/h3&gt;&lt;p&gt;저희는 여러가지 가능성을 열어두고 대안을 비교해보기 시작했습니다.&lt;/p&gt;&lt;h4&gt;Jenkins + 모니터링 Job 개선&lt;/h4&gt;&lt;p&gt;이미 운영 중인 구조라 리스크는 적었지만, 근본적인 한계를 해결하기는 어려웠습니다. 모니터링을 아무리 촘촘하게 만들어도, Jenkins 자체의 고가용성 문제는 해결되지 않았습니다.&lt;/p&gt;&lt;h4&gt;Spring Batch + Quartz&lt;/h4&gt;&lt;p&gt;Spring Batch + Quartz는 배치 처리에 최적화된 구조였고, 팀에서도 익숙한 스택이었습니다. 하지만 Batch Job 실행 이력은 확인할 수 있어도, “주문이 어디서 멈췄고, 왜 실패했는지”와 같은 비즈니스 흐름은 보이지 않았습니다. 재시도 로직 구현은 가능했지만, “이 조건이면 재시도, 저 조건이면 스킵” 같은 의사결정 로직이 코드 곳곳에 흩어지면서 전체 워크플로우를 파악하기 어려워졌습니다. 결국 “배치 Job”이 아닌 “비즈니스 워크플로우” 단위로 실행과 상태를 추적하고 싶었습니다.&lt;/p&gt;&lt;h4&gt;Temporal&lt;/h4&gt;&lt;p&gt;Temporal은 장기 실행되는 비즈니스 프로세스를 코드로 표현하고, 실패·재시도·상태 관리를 플랫폼 레벨에서 보장해주는 Workflow Engine입니다. Uber에서 만든 Cadence를 기반으로 개발되었으며, 서버 장애가 발생해도 중단된 지점부터 자동 복구됩니다.&lt;/p&gt;&lt;p&gt;생소한 이름이었습니다. 러닝 커브가 높다는 것도 부담이었습니다. 하지만 회의실에서 Temporal 문서를 보며 이야기를 나누다 보니, 저희가 원하던 것들이 하나하나 눈에 들어왔습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;스케줄 기반 실행&lt;/li&gt;&lt;li&gt;Workflow 전체 흐름에 대한 가시성&lt;/li&gt;&lt;li&gt;자동 재시도와 복구&lt;/li&gt;&lt;li&gt;이벤트 기반 트리거&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;어느 순간 팀 모두가 적합해 보인다고 생각했고, &lt;strong&gt;단순히 스케줄을 실행하는 기능을 넘어, 출고 도메인 전반의 흐름을 안정적으로 오케스트레이션할 수 있는 플랫폼&lt;/strong&gt;이 될 수 있겠다고 판단했습니다.&lt;/p&gt;&lt;h3&gt;설계하면서 고민한 것들&lt;/h3&gt;&lt;h4&gt;무엇을 목표로 설계하는가&lt;/h4&gt;&lt;p&gt;기존 Jenkins crontab 기반 스케줄링 방식은 운영 안정성, 실행 가시성, 복구 가능성 측면에서 여러 한계를 갖고 있었습니다.&lt;/p&gt;&lt;p&gt;이에 따라 스케줄 실행 방식을 보다 안정적이고 자동화된 구조로 전환하고, 운영자가 수행하던 수동 확인과 재실행 작업을 최소화하는 것을 목표로 했습니다.&lt;/p&gt;&lt;h4&gt;Workflow와 Activity, 어떻게 나눌 것인가&lt;/h4&gt;&lt;p&gt;Temporal 공식 문서는 이렇게 말합니다.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;Workflow는 전체 비즈니스 프로세스의 오케스트레이션을 담당하고, Activity는 외부 시스템과의 상호작용이나 단위 작업을 수행한다.&lt;/em&gt;&lt;/blockquote&gt;&lt;p&gt;Workflow와 Activity의 경계를 정하는 데 가장 어려웠던 점은, 단순히 “외부 시스템을 호출하는가”가 아니라 “이 분기가 비즈니스 정책인가, 실행 세부사항인가”를 판단하는 일이었습니다. 예를 들어 “중복 주문이면 스킵한다”는 비즈니스 정책이므로 Workflow에서 분기하고, “DB에서 중복 여부를 조회한다”는 실행 세부사항이므로 Activity로 분리했습니다.&lt;/p&gt;&lt;p&gt;논의 끝에 저희가 세운 기준은 이렇습니다:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Workflow: 비즈니스 정책에 따른 분기와 흐름 제어&lt;/li&gt;&lt;li&gt;Activity: 멱등성이 보장되는 단위 작업, 외부 시스템 호출&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;출고지시의 경우, Workflow는 “정책 조회 → 중복 체크 → 차수 생성 → 파이프라인 트리거” 흐름을 조율하고, 각 단계의 실제 작업은 Activity가 수행합니다. 이 구분이 명확할수록 향후 출고 파이프라인 전체를 Workflow로 확장할 때도 일관성 있게 설계할 수 있을 거라 생각했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iO5YSi38Zj2O5Ue3f43rbQ.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;앞서 정의한 기준에 따라, Workflow는 출고지시 트리거의 전체 흐름과 분기·의사결정을 조율하고 Activity는 정책 조회, 중복 체크, 차수 생성 등 실제 작업을 수행합니다.&lt;/li&gt;&lt;li&gt;Temporal 내 정의된 스케줄을 기반으로 Workflow가 실행되며, 각 단계는 Activity로 위임되어 상태를 명확히 관리하고 출고지시 흐름을 조율합니다.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;출고지시 Workflow 구현&lt;/h3&gt;&lt;p&gt;최종적으로 &lt;strong&gt;1개의 Workflow&lt;/strong&gt;와 &lt;strong&gt;5개의 Activity&lt;/strong&gt;로 구성했습니다. (아래는 간략화된 코드입니다.)&lt;/p&gt;&lt;pre&gt;@WorkflowImplement&lt;br&gt;class CutOffTriggerWorkflowImpl : CutOffTriggerWorkflow {&lt;br&gt;    override fun run() {&lt;br&gt;      &lt;br&gt;        // 출고지시 정책 조회&lt;br&gt;        val policies = getCutOffPolicies.get(now)&lt;br&gt;&lt;br&gt;        policies.forEach { policy -&amp;gt;&lt;br&gt;            // 출고지시 차수 생성&lt;br&gt;            val cutSequence = createCutSequence.create(policy)&lt;br&gt;&lt;br&gt;            // 파이프라인 트리거&lt;br&gt;            triggerPipeline.trigger(cutSequence, now)&lt;br&gt;        }&lt;br&gt;    }&lt;br&gt;}&lt;/pre&gt;&lt;h3&gt;29CM 주문수집도 자동화하다&lt;/h3&gt;&lt;p&gt;출고지시 트리거를 Temporal로 전환하고 나니, 또 다른 수동 작업이 눈에 들어왔습니다. &lt;strong&gt;29CM 주문수집&lt;/strong&gt;이었습니다.&lt;/p&gt;&lt;p&gt;담당자가 MOMS 화면에서 버튼을 눌러 수동으로 주문을 수집하고 있었는데, 물량이 늘면서 이 방식도 한계가 드러나기 시작했습니다. 버튼 클릭을 누락하는 경우도 있었고, 담당자 부재 시 주문 수집이 지연되기도 했습니다.&lt;/p&gt;&lt;p&gt;“출고지시와 비슷한 문제잖아?”&lt;/p&gt;&lt;p&gt;팀 내에서 자연스럽게 이러한 논의가 이어졌고, 29CM 주문수집도 Temporal Workflow로 자동화하기로 했습니다.&lt;/p&gt;&lt;h3&gt;개인정보 보호를 고려한 설계&lt;/h3&gt;&lt;p&gt;하지만 여기서 중요한 고민이 하나 있었습니다. 우리는 Temporal Cloud를 사용할 계획이므로 무신사 고객의 개인정보를 Temporal에 전달하는 것은 부적절했습니다.&lt;/p&gt;&lt;p&gt;회의실에서 여러 방안을 논의했고, 결국 다음과 같이 설계했습니다:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;29CM에서 받은 API Payload를 MOMS DB에 저장&lt;/li&gt;&lt;li&gt;Payload ID만 Temporal Workflow에 전달&lt;/li&gt;&lt;li&gt;Activity에서 Payload ID로 DB 조회 후 처리&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;즉, 개인정보는 우리 인프라 안에 머물고, Temporal에는 ID만 전달되는 구조&lt;/strong&gt;입니다.&lt;/p&gt;&lt;h3&gt;주문수집 Workflow 시나리오&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*3ZllJ7Z8IUJJ-n4sSJFbYQ.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;29CM API에서 주문을 조회해서 DB에 저장(ORDER_COLLECTED)합니다.&lt;/li&gt;&lt;li&gt;성공한 건에 대해 29CM에 상품준비중 상태 변경을 요청한 뒤, 상태를 ORDER_ACCEPTED로 변경하는 플로우입니다.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;주문수집 Workflow 구현&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;1개의 Workflow&lt;/strong&gt;와 &lt;strong&gt;4개의 Activity&lt;/strong&gt;로 구성했습니다.&lt;/p&gt;&lt;pre&gt;@WorkflowImplement&lt;br&gt;class AddStandardOrderWorkflowImpl : AddStandardOrderWorkflow {&lt;br&gt;    override fun addOrders(payloadId: Long) {&lt;br&gt;        // 1. 주문 수집&lt;br&gt;        val upsertResults = addStandardOrderActivity.addOrders(payloadId)&lt;br&gt;        val successOrders = upsertResults.filter { it.success }&lt;br&gt;        if (successOrders.isEmpty()) return&lt;br&gt;&lt;br&gt;        // 2. 상품 준비중 API 요청&lt;br&gt;        val shippingResults = requestPrepareShippingActivity.request(successOrders)&lt;br&gt;&lt;br&gt;        // 3. 주문 상태를 ORDER_ACCEPTED로 변경&lt;br&gt;        if (shippingResults.items.isNotEmpty()) {&lt;br&gt;            val request = convertActivity.convert(shippingResults)&lt;br&gt;            changeStatusActivity.updateToAccepted(request)&lt;br&gt;        }&lt;br&gt;    }&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;주문 수집 → 29CM API 호출 → 상태 변경까지의 흐름이 하나의 Workflow로 표현됩니다. 출고지시 Workflow와 비슷한 구조지만, 도메인의 특성에 맞게 Activity를 구성했습니다.&lt;/p&gt;&lt;h3&gt;운영하면서 느낀 것들&lt;/h3&gt;&lt;h4&gt;좋았던 점들&lt;/h4&gt;&lt;h4&gt;1. 장애 대응이 ‘케이스 바이 케이스’에서 ‘시스템’으로&lt;/h4&gt;&lt;p&gt;가장 먼저 체감한 변화는 &lt;strong&gt;자동 재시도&lt;/strong&gt;였습니다. Activity에 정의한 Retry/Backoff/Timeout 규칙에 따라 자동으로 재시도되는 걸 보면서, “아, 이제 Spring Batch에 복잡하게 구현하지 않아도 되겠구나”라는 생각이 들었습니다.&lt;/p&gt;&lt;p&gt;실제로 운영 중 일시적인 네트워크 오류로 출고지시가 실패한 적이 있었는데, Temporal UI를 보니 자동으로 3번 재시도하고 성공한 걸 확인할 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*xRR-OkSmY4zf8tIl_XvT1A.png&quot; /&gt;&lt;figcaption&gt;EventID 12 : 재시도 후 성공한 케이스&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;2. Workflow 단위로 보이는 실행 이력&lt;/h4&gt;&lt;p&gt;이전에는 Jenkins 로그, 애플리케이션 로그를 모두 열어봐야 했지만, &lt;strong&gt;Temporal UI에서 타임라인을 한눈에 확인&lt;/strong&gt;할 수 있게 되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*GbkPp4FK89-3N4aff2Xllg.png&quot; /&gt;&lt;figcaption&gt;하단부터 시간순 타임라인&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;특히 Activity 단위로 input, output을 빠르게 확인할 수 있어 원인 파악이 훨씬 쉬워졌습니다. “이 시점에 어떤 정책이 들어왔고, 어떤 결과가 나왔는지”를 클릭 몇 번으로 확인할 수 있게 되니, 장애 대응 시간이 크게 줄었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*nynJLox02b6lW5mbMBDJvg.png&quot; /&gt;&lt;figcaption&gt;특정 Activity의 Input, Output&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;3. 부분 도입만으로도 체감된 변화&lt;/h4&gt;&lt;p&gt;출고지시 전체 파이프라인이 아닌 &lt;strong&gt;“트리거 + 특정 화주 주문수집”만 Temporal로 옮겼는데도&lt;/strong&gt;, 해당 구간의 장애 대응과 모니터링 편의성은 즉시 체감할 수 있었습니다.&lt;/p&gt;&lt;p&gt;또한, 동일 도메인을 Spring Batch + Jenkins와 Temporal 두 체계에서 비교해보면서, 어떤 유형의 작업이 Temporal에 더 적합한지 기준을 잡는 데 도움이 되었습니다.&lt;/p&gt;&lt;h4&gt;고려해봐야 할 것들&lt;/h4&gt;&lt;h4&gt;1. Workflow·Activity 경계 설정의 어려움&lt;/h4&gt;&lt;p&gt;“무엇을 하나의 Workflow로 보고, 어느 단위를 Activity로 분리할 것인가”에 대한 기준을 잡는 게 쉽지 않았습니다. 팀 내에서도 의견이 엇갈릴 때가 많았고, 결국 “실제로 만들어보고 조정하자”는 방식으로 진행했습니다.&lt;/p&gt;&lt;p&gt;이번에는 일부에만 적용했지만, 향후 출고 전체 파이프라인으로 확장하려면 &lt;strong&gt;더 명확한 설계 원칙&lt;/strong&gt;이 필요할 것 같습니다.&lt;/p&gt;&lt;h4&gt;2. 디버깅 포인트의 증가&lt;/h4&gt;&lt;p&gt;Temporal을 도입하면서 애플리케이션 로그 외에 Temporal History, Worker 메트릭까지 함께 봐야 해서 디버깅 지점이 늘어난 것은 사실입니다. 하지만 이는 단순히 복잡도가 증가했다기보다, 실행 상태를 구조적으로 관측할 수 있게 된 결과라고 느끼고 있습니다.&lt;/p&gt;&lt;p&gt;다만 이 장점을 살리기 위해서는, 로그·메트릭·Workflow History를 어떤 순서로 확인할지에 대한 팀 차원의 디버깅 가이드가 반드시 필요하다는 점도 함께 깨달았습니다. 현재는 이슈 대응 시 “애플리케이션 로그 → Temporal History → Worker 메트릭” 순으로 확인하는 기준을 정리 중이며, Datadog/Slack 알림 임계값도 함께 다듬어가고 있습니다.&lt;/p&gt;&lt;h4&gt;3. Batch와 Temporal의 공존&lt;/h4&gt;&lt;p&gt;Batch와 Temporal이 공존하는 현재 상태는 운영 부담이 분명 존재합니다. “이 배치는 Jenkins에서 보고, 저 워크플로우는 Temporal에서 본다”는 식으로 운영하다 보니 온콜 대응 가이드도 두 체계로 작성해야 했습니다.&lt;/p&gt;&lt;p&gt;하지만 이 공존은 모든 파이프라인을 한 번에 전환하지 않기 위한 의도적인 과도기이기도 합니다. 이 기간 동안 동일 도메인을 두 체계에서 비교해보면서, “장기 실행·재시도·이력 추적이 중요한 작업”은 Temporal에, “단순 반복 처리”는 Batch에 적합하다는 기준을 점점 명확히 하고 있습니다. 이러한 기준이 쌓이면 전환 시점에 대한 판단도 훨씬 명확해질 것으로 기대하고 있습니다.&lt;/p&gt;&lt;h3&gt;다음 여정: 출고지시 파이프라인 전체를 Temporal로&lt;/h3&gt;&lt;p&gt;이제 저희는 더 큰 그림을 그리고 있습니다. 출고지시 트리거와 주문수집을 Temporal로 전환하면서, 다음 단계가 분명해졌습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;출고지시 파이프라인 전체를 Workflow로 전환하는 것.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;현재 출고지시는 대용량 처리 시 약 30분이 소요됩니다 (2만 건 기준). 이를 Workflow 기반 병렬 처리 구조로 전환하면 &lt;strong&gt;평균 90% 이상 단축&lt;/strong&gt;할 수 있을 것으로 예상하고 있습니다.&lt;/p&gt;&lt;p&gt;또한 출고 전 구간의 상태, 처리 속도, 실패 지점을 실시간으로 가시화하여 문제의 원인과 영향 범위를 신속하게 파악할 수 있는 Observability 체계를 구축하려 합니다.&lt;/p&gt;&lt;p&gt;지금은 Child Workflow, Signal 기반 접근 방식 등 다양한 방법을 테스트하고 있습니다. 만만치 않은 도전이겠지만, 트리거와 주문수집을 통해 얻은 경험이 큰 자산이 되고 있습니다.&lt;/p&gt;&lt;h4&gt;AS-IS&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*UWukKCCNJBhQrh_sCo_aMw.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;TO-BE&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*5BuNX8-EiwvfJ1Pn_BVhsw.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*1q4EV4qdgzZjN1HD2ClhOg.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;마치며&lt;/h3&gt;&lt;p&gt;“스케줄이 또 안 돌았어요”라는 알림에서 시작된 이 여정은, 단순히 기술 스택을 바꾸는 것 이상의 의미를 가졌습니다.&lt;/p&gt;&lt;p&gt;저희는 &lt;strong&gt;“실패하지 않는 시스템”을 만드는 게 아니라, “실패해도 스스로 복구되는 시스템”&lt;/strong&gt;을 만들고 있습니다. Temporal을 도입하면서 가장 크게 느낀 건, 기술 선택이 단순히 “어떤 라이브러리를 쓸 것인가”가 아니라 &lt;strong&gt;“우리가 어떻게 운영할 것인가”&lt;/strong&gt;를 결정한다는 점이었습니다.&lt;/p&gt;&lt;p&gt;물론 아직 갈 길이 멉니다. Workflow 설계 기준도 계속 다듬어야 하고, 모니터링 체계도 개선해야 합니다. 출고 파이프라인 전체를 Workflow로 전환하는 것도 쉽지 않은 도전이 될 것입니다.&lt;/p&gt;&lt;p&gt;하지만 새벽에 출고지시 알림을 받고 급하게 수동 실행하던 날들이 조금씩 줄어들고 있습니다. 그리고 그 시간에 저희는 더 중요한 문제를 고민할 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;긴 글 읽어주셔서 감사합니다. &lt;br&gt;저희의 경험이 비슷한 고민을 하고 계신 분들께 조금이나마 도움이 되었으면 좋겠습니다.&lt;/p&gt;&lt;h3&gt;Platform Business Operation 조직 및 팀 소개&lt;/h3&gt;&lt;blockquote&gt;무신사 플랫폼 비즈니스 오퍼레이션 조직은 국내외 물류 서비스, 재고 관리, 스토어 운영을 위한 물류 프로덕트를 구축하고 다양한 오프라인 비즈니스 모델에 맞춘 스토어 관리 시스템을 개발·고도화하고 있습니다.&lt;br&gt;또한, 무배당발 서비스를 포함한 무신사의 차별화된 고객 경험을 브랜딩하고 확장할 수 있는 멤버십 구조를 설계하며, 온·오프라인을 넘나드는 통합 커머스 경험을 기술로 실현하고 있습니다.&lt;/blockquote&gt;&lt;blockquote&gt;저희 팀은 OMS(주문관리시스템)를 기반으로 온라인 주문부터 재고·출고·배송·정산에 이르는 전 과정을 유기적으로 연결하고, 무신사의 다양한 오프라인 스토어를 효과적으로 운영할 수 있는 관리 시스템을 구축하여 고객이 온라인(무신사 스토어, 29CM 등)과 오프라인(무신사 스탠다드, 편집숍 등)에서 끊김 없는 쇼핑 경험을 누릴 수 있도록 지원합니다.&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/pbo&quot;&gt;🚀 Platform Business Operation 한걸음 더 알아보기&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;🚀 팀 무신사 채용 페이지&lt;/a&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/blockquote&gt;&lt;blockquote&gt;🚀 &lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;🚀 &lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;팀 무신사 뉴스룸&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f491e79a0f8f&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%8A%A4%EC%BC%80%EC%A4%84%EC%9D%B4-%EB%98%90-%EC%95%88-%EB%8F%8C%EC%95%98%EC%96%B4%EC%9A%94-%EC%9A%B0%EB%A6%AC%EA%B0%80-temporal%EC%9D%84-%EC%84%A0%ED%83%9D%ED%95%9C-%EC%9D%B4%EC%9C%A0-f491e79a0f8f&quot;&gt;“스케줄이 또 안 돌았어요” — 우리가 Temporal을 선택한 이유&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>2025 프론트엔드 뉴스 한 방에 몰아 보기</title>
      <link>https://meetup.nhncloud.com/posts/407</link>
      <guid>https://meetup.nhncloud.com/posts/407</guid>
      <pubDate>Wed, 04 Feb 2026 07:54:51 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner_frontendnews2025_202601_1400.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannerfrontendnews20252026011400%282%29.png)](https://www.nhncloud.com/?utm_source=meetup&amp;amp;utm_medium=post&amp;amp;utm_campaign=branding&amp;amp;utm_content=0130_frontend&amp;amp;utm_term=top)&amp;#xD;
        &amp;#xD;
        안녕하세요. NHN Cloud NCUI개발팀 이진우입니다.&amp;#xD;
        &amp;#xD;
        저는 매주 30여 개의 Web, Frontend, Design, Design System 등의 기술 뉴스레터를 살피고 동료들에게 공유해 왔는데요. 작년에 이어 올해도 개인적으로 인상 깊었던 아티클을 주제별로 공유드리고자 합니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; 작년에 공유했던 2024년의 프론트엔드 뉴스 모음은 아래 경로에서 확인하실 수 있습니다.&amp;#xD;
        &amp;gt;  [2024 프론트엔드 뉴스 한 방에 몰아 보기](https://meetup.nhncloud.com/posts/390)&amp;#xD;
        &amp;#xD;
        관심사와 조직 내 영향력이 큰 기술 스택을 중심으로 정리한 내용이기에 프론트엔드 생태계 전체를 대변한다고 보기는 어렵지만, 흥미가 가는 주제가 있다면 함께 첨부한 링크들이 출발점으로서 작은 도움이 되기를 바랍니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; 본문의 링크들은 과거 시점 기준의 자료이므로 변경된 내용이 있을 수 있습니다.  &amp;#xD;
        &amp;#xD;
        ## Node.js의 TypeScript 지원&amp;#xD;
        &amp;#xD;
        2025년 초 Node.js는 Bun, Deno와 같은 경쟁자들에게 대적하기 위해 TypeScript 파일 실행 지원을 추가했습니다. 개발자 경험을 향상시키는 것이 주 목적이었기에 타입 체크, 컴파일이 아닌 타입을 걷어내는 것(Type Stripping)으로 방향을 잡았는데요. 이를 위해 SWC를 기반으로 한 Amaro가 핵심 도구로 기대 받았고, TypeScript 측에서도 관련 플래그를 추가했으며 최근 v24.12 버전에서 안정화 단계에 들어서게 되었습니다.&amp;#xD;
        &amp;#xD;
        ### Type Stripping 개념과 구현&amp;#xD;
        &amp;#xD;
        Node.js가 채택한 Type Stripping 방식의 원리와 구현 배경을 다룹니다.&amp;#xD;
        &amp;#xD;
        * [Node.js now supports TypeScript by default](https://www.totaltypescript.com/typescript-is-coming-to-node-23)&amp;#xD;
        * [Everything you need to know about node.js type stripping](https://satanacchio.hashnode.dev/everything-you-need-to-know-about-nodejs-type-stripping)&amp;#xD;
        * [The Summer I Shipped Type Stripping](https://satanacchio.hashnode.dev/the-summer-i-shipped-type-stripping)&amp;#xD;
        &amp;#xD;
        ### 생태계 지원 현황&amp;#xD;
        &amp;#xD;
        TypeScript 5.8의 플래그 추가, Amaro 1.0 릴리스, Node.js 안정화 버전 등 생태계 전반의 지원 현황입니다.&amp;#xD;
        &amp;#xD;
        * [Announcing TypeScript 5.8](https://devblogs.microsoft.com/typescript/announcing-typescript-5-8/#the---erasablesyntaxonly-option)&amp;#xD;
        * [Node.js Moves Toward Stable TypeScript Support with Amaro 1.0](https://socket.dev/blog/node-js-moves-toward-stable-typescript-support-with-amaro-1-0)&amp;#xD;
        * [Node.js v24.12.0](https://nodejs.org/en/blog/release/v24.12.0)&amp;#xD;
        &amp;#xD;
        ### 런타임 간 비교&amp;#xD;
        &amp;#xD;
        Deno와 Node.js의 TypeScript 지원 비교입니다.&amp;#xD;
        &amp;#xD;
        * [Node just added TypeScript support. What does that mean for Deno?](https://deno.com/blog/typescript-in-node-vs-deno)&amp;#xD;
        &amp;#xD;
        ## React 2025 회고&amp;#xD;
        &amp;#xD;
        2025년 React를 보면 프론트엔드의 인프라라고 불러야 할 만큼 범위가 넓어졌습니다. LLM 시대에 React가 사실상 기본 출력물이 되면서 새 프레임워크는 출발부터 불리하다는 주장도 과언이 아닙니다.&amp;#xD;
        &amp;#xD;
        작년 말 v19 릴리스 이후 `action`, `useActionState`, `useOptimistic`, `&amp;lt;Activity /&amp;gt;` 같은 기능이 추가됐습니다. Form Submit, Loading, Error 등 웹 애플리케이션이 늘 겪는 문제를 React 기본 기능으로 끌어당긴 모양새입니다. 다만 동시성이 기본값이 될수록 개발자도 새로운 규칙을 익혀야 합니다. 코드는 더 선언적으로 보이지만, 어떤 상태를 어떤 방식으로 업데이트할지 조직 내 합의도 필요해질 것 같습니다.&amp;#xD;
        &amp;#xD;
        v19 이후 CRA(create-react-app)는 사실상 종료됐습니다. 의존성 충돌을 막기 위한 변경 사항이 병합되긴 했지만, 기존 애플리케이션도 Next.js, React Router, Vite 등으로 전환이 권고됐습니다. 이 권고 문구가 Next.js를 지나치게 강조한다며 커뮤니티에서 논란이 되기도 했습니다. 높은 채택률을 보였던 styled-components도 유지보수 모드로 전환됐습니다.&amp;#xD;
        &amp;#xD;
        연말에는 RSC 보안 이슈가 터졌습니다. React가 인프라가 됐다는 말이 과장이 아님을 보여준 사건입니다. 이제 React가 커졌다는 건 기능의 문제가 아니라 책임 범위의 문제로 봐야 할 것 같습니다.&amp;#xD;
        &amp;#xD;
        ### v19 신규 기능&amp;#xD;
        &amp;#xD;
        action, useActionState, useOptimistic, Activity, useEffectEvent, use() 등 v19에서 도입된 주요 기능들입니다.&amp;#xD;
        &amp;#xD;
        * [3 ways to build forms in react (without any libraries)](https://reactpractice.dev/articles/3-ways-to-build-forms-in-react)&amp;#xD;
        * [Building Reusable Components with React 19 Actions](https://aurorascharff.no/posts/building-reusable-components-with-react19-actions/)&amp;#xD;
        * [useOptimistic to Make Your App Feel Instant](https://www.epicreact.dev/use-optimistic-to-make-your-app-feel-instant-zvyuv)&amp;#xD;
        * [How React Suspense Works Under the Hood: Throwing Promises and Declarative Async UI](https://www.epicreact.dev/how-react-suspense-works-under-the-hood-throwing-promises-and-declarative-async-ui-plbrh)&amp;#xD;
        * [React Concurrent Features: An Overview](https://certificates.dev/blog/react-concurrent-features-an-overview/)&amp;#xD;
        * [React’s useTransition and state update reordering](https://jordaneldredge.com/notes/react-rebasing/)&amp;#xD;
        * [Using Activity with Suspenseful data](https://www.simeongriggs.dev/use-the-activity-boundary-to-hide-suspenseful-components/)&amp;#xD;
        * [Tried React 19’s Activity Component Here’s What I Learned](https://javascript.plainenglish.io/tried-react-19s-activity-component-here-s-what-i-learned-b0f714003a65)&amp;#xD;
        * [Quick look into the useEffectEvent](https://www.nico.fyi/blog/quick-look-use-effect-event)&amp;#xD;
        * [React 19.2: React in its sigma era](https://dev.to/sagi0312/react-192-react-in-its-sigma-era-op7)&amp;#xD;
        * [React 19.2: The async shift is finally here](https://blog.logrocket.com/react-19-2-the-async-shift/)&amp;#xD;
        * [The next era of React has arrived: Here’s what you need to know](https://blog.logrocket.com/the-next-era-of-react/)&amp;#xD;
        * [React has changed, your Hooks should too](https://allthingssmitty.com/2025/12/01/react-has-changed-your-hooks-should-too/)&amp;#xD;
        * [React RFC: Context Selector](https://github.com/reactjs/rfcs/pull/119/)&amp;#xD;
        &amp;#xD;
        ### 예외 처리&amp;#xD;
        &amp;#xD;
        Error Boundary의 v19 변경 사항과 react-error-boundary 활용법입니다.&amp;#xD;
        &amp;#xD;
        * [Error Handling in React with react-error-boundary](https://certificates.dev/blog/error-handling-in-react-with-react-error-boundary)&amp;#xD;
        * [React 19 Error Boundary Behaves Differently](https://andrei-calazans.com/posts/react-19-error-boundary-changed/)&amp;#xD;
        &amp;#xD;
        ### RSC(React Server Components)&amp;#xD;
        &amp;#xD;
        Server Components의 동작 원리와 import 메커니즘입니다.&amp;#xD;
        &amp;#xD;
        * [How Imports Work in RSC](https://overreacted.io/how-imports-work-in-rsc/)&amp;#xD;
        * [React Frameworks and Server-Side Features: Beyond Client-Side Rendering](https://certificates.dev/blog/react-frameworks-and-server-side-features-beyond-client-side-rendering)&amp;#xD;
        &amp;#xD;
        ### RSC 보안 취약점&amp;#xD;
        &amp;#xD;
        Flight Protocol 역직렬화 과정의 원격 코드 실행 취약점(CVE-2025-55182)입니다.&amp;#xD;
        &amp;#xD;
        * [Critical Security Vulnerability in React Server Components](https://react.dev/blog/2025/12/03/critical-security-vulnerability-in-react-server-components)&amp;#xD;
        * [React2Shell Security Bulletin](https://vercel.com/kb/bulletin/react2shell)&amp;#xD;
        * [Understanding the RCE Flaw in React Server Functions](https://gist.github.com/sriram-palanisamy-hat/d207174ada2fa052ad44439f22a65c7e)&amp;#xD;
        &amp;#xD;
        ### 생태계 변화&amp;#xD;
        &amp;#xD;
        CRA 종료와 styled-components 유지보수 모드 전환입니다.&amp;#xD;
        &amp;#xD;
        * [create-react-app is a zombie application](https://www.clientserver.dev/p/create-react-app-is-a-zombie-application)&amp;#xD;
        * [Cut styled-components into pieces: This is our last resort](https://www.sanity.io/blog/cut-styled-components-into-pieces-this-is-our-last-resort)&amp;#xD;
        &amp;#xD;
        ## 조용했던 React Compiler의 시간&amp;#xD;
        &amp;#xD;
        작년 이맘때쯤에도 React Compiler를 소개했었는데요. 떠들썩했던 2024년의 분위기와 달리 RC를 거쳐 조용히 React Compiler v1.0.0이 릴리스되었고, 이를 적용하기 위한 점진적 도입 방법에 대한 글이 올라왔습니다. 간간이 성능 개선 사례와 React Compiler를 다루는 아티클이 있었습니다.&amp;#xD;
        &amp;#xD;
        ### 릴리스와 도입 가이드&amp;#xD;
        &amp;#xD;
        RC부터 v1.0.0까지의 릴리스와 점진적 도입 방법입니다.&amp;#xD;
        &amp;#xD;
        * [React Compiler RC](https://react.dev/blog/2025/04/21/react-compiler-rc)&amp;#xD;
        * [React Compiler v1.0.0](https://www.npmjs.com/package/babel-plugin-react-compiler/v/1.0.0)&amp;#xD;
        * [Incremental Adoption](https://react.dev/learn/react-compiler/incremental-adoption)&amp;#xD;
        &amp;#xD;
        ### 실전 적용 사례&amp;#xD;
        &amp;#xD;
        프로덕션 환경 도입 경험과 상태 관리 라이브러리와의 관계입니다.&amp;#xD;
        &amp;#xD;
        * [Adopting the compiler at wakelet.com in production](https://github.com/reactwg/react-compiler/discussions/52)&amp;#xD;
        * [Thoughts on state management libraries in the react compiler era](https://blog.axlight.com/posts/thoughts-on-state-management-libraries-in-the-react-compiler-era/)&amp;#xD;
        &amp;#xD;
        ### 트러블슈팅과 내부 모델&amp;#xD;
        &amp;#xD;
        Compiler 관련 트러블슈팅과 Compiler에서 값의 변이와 별칭 관계를 추론하는 모델을 설명합니다.&amp;#xD;
        &amp;#xD;
        * [use no memo](https://react.dev/reference/react-compiler/directives/use-no-memo)&amp;#xD;
        * [React Compiler’s Silent Failures (And How to Fix Them)](https://acusti.ca/blog/2025/12/16/react-compiler-silent-failures-and-how-to-fix-them/)&amp;#xD;
        * [The Mutability \&amp;amp; Aliasing Model](https://github.com/facebook/react/blob/156b7a96f5669470182ad226306184576d6f150f/compiler/packages/babel-plugin-react-compiler/src/Inference/MUTABILITY_ALIASING_MODEL.md)&amp;#xD;
        &amp;#xD;
        ## Next.js의 보안 이슈와 복잡성&amp;#xD;
        &amp;#xD;
        어느샌가 프론트엔드는 브라우저를 넘어 서버 영역에 발을 들였습니다. 웹 성능 향상을 위한 기능 개선이 이어졌고, 서버 영역 진출만큼 보안 사고도 발생했습니다. 이 흐름의 중심에 Next.js가 있습니다.&amp;#xD;
        &amp;#xD;
        기능이 추가될수록 복잡성도 함께 올라갔고, 많은 개발자가 피로감을 호소한 한 해였습니다. 대안으로 TanStack Start가 자주 언급됐고, Next.js를 떠나는 후기는 어느 순간 메모조차 하지 않을 만큼 많았습니다.&amp;#xD;
        &amp;#xD;
        보안 이슈도 빼놓을 수 없습니다. Next.js는 꽤 넓은 버전 범위에 걸쳐 미들웨어 인증 검사를 헤더만으로 우회할 수 있는 취약점이 있었습니다. 클라이언트 요청과 내부 SubRequest를 구분하기 위해 사용한 비공식 헤더가 문제였는데, 이를 악용하면 전혀 관련 없는 경로에 영향을 줄 수 있었습니다. Vercel에서 호스팅된 앱은 자체적으로 방어하고 있었다는 점에서 의문이 들기도 했습니다.&amp;#xD;
        &amp;#xD;
        일부 개발자들은 타 호스팅 업체들이 Next.js의 모든 기능을 지원하는 데 어려움을 겪고 있다고 지적했습니다. Next.js가 공식 어댑터를 제공하지 않는 점, 보안 취약점 대응에 소극적이었던 점도 비판의 대상이 됐습니다.&amp;#xD;
        &amp;#xD;
        그럼에도 Next.js는 Turbopack, React Compiler, PPR 등을 탑재한 v16을 릴리스하며 업데이트를 이어가고 있고 여전히 긍정적인 시선도 존재합니다.&amp;#xD;
        &amp;#xD;
        ### 보안 취약점&amp;#xD;
        &amp;#xD;
        미들웨어 인증 우회 취약점입니다.&amp;#xD;
        &amp;#xD;
        * [Next.js and the corrupt middleware: the authorizing artifact](https://zhero-web-sec.github.io/research-and-things/nextjs-and-the-corrupt-middleware)&amp;#xD;
        * [CVE-2025-29927: Authorization Bypass in Next.js Middleware](https://github.com/advisories/GHSA-f82v-jwr5-mffw)&amp;#xD;
        &amp;#xD;
        ### 셀프 호스팅과 배포&amp;#xD;
        &amp;#xD;
        Vercel 외 환경에서의 Next.js 운영과 셀프 호스팅 가이드입니다.&amp;#xD;
        &amp;#xD;
        * [Deploying a Next.js App to Production in any server](https://www.saybackend.com/blog/04-deploy-nextjs-to-production-without-vercel)&amp;#xD;
        * [How we run Next.js today - and what should change](https://www.netlify.com/blog/how-we-run-nextjs/)&amp;#xD;
        * [The Complete Guide to Self-Hosting Next.js at Scale](https://dlhck.com/thoughts/the-complete-guide-to-self-hosting-nextjs-at-scale/)&amp;#xD;
        &amp;#xD;
        ### Next.js 이탈 사례&amp;#xD;
        &amp;#xD;
        다른 프레임워크로 전환한 팀들의 경험담입니다.&amp;#xD;
        &amp;#xD;
        * [Why we moved off next.js](https://documenso.com/blog/why-we-moved-off-next-js)&amp;#xD;
        * [We migrated our site to Eleventy and increased performance by 24%](https://etch.co/blog/we-migrated-our-site-to-eleventy-and-increased-performance-by-24-percent/)&amp;#xD;
        * [Why we ditched Next.js and never looked back](https://northflank.com/blog/why-we-ditched-next-js-and-never-looked-back)&amp;#xD;
        &amp;#xD;
        ### Next.js 비판과 대안&amp;#xD;
        &amp;#xD;
        구조적 문제와 도입 전 고려 사항과 대안으로 떠오르는 TanStack Start 관련 내용입니다.&amp;#xD;
        &amp;#xD;
        * [You don&apos;t need Next.js](https://www.comfydeploy.com/blog/you-dont-need-nextjs)&amp;#xD;
        * [You should know this before choosing Next.js](https://eduardoboucas.com/posts/2025-03-25-you-should-know-this-before-choosing-nextjs/)&amp;#xD;
        * [Why Next.js Falls Short on Software Engineering](https://blog.webf.zone/why-next-js-falls-short-on-software-engineering-d3575614bd08)&amp;#xD;
        * [Architecting with Constraints: A Pragmatic Guide](https://www.lorenstew.art/blog/always-architect-with-contraints/)&amp;#xD;
        * [Next.js vs Tanstack](https://www.kylegill.com/essays/next-vs-tanstack/)&amp;#xD;
        * [TanStack Start: New competitor to Next.js](https://ondrejvelisek.github.io/tanstack-start-new-competitor-to-nextjs/)&amp;#xD;
        * [Why developers are leaving Next.js for TanStack Start, and loving it](https://appwrite.io/blog/post/why-developers-leaving-nextjs-tanstack-start)&amp;#xD;
        &amp;#xD;
        ### 긍정적 전말과 릴리스&amp;#xD;
        &amp;#xD;
        Next.js의 미래에 대한 긍정적 시각과 렌더링 방식에 대한 이해입니다.&amp;#xD;
        &amp;#xD;
        * [I like the future of Next.js](https://tigerabrodi.blog/i-like-the-future-of-nextjs)&amp;#xD;
        * [How to understand the concepts of Next.js such as CSR , SSR, SSG, ISR, RSC, SPA, and Streaming SSR?](https://dev.to/nextjser/how-to-understand-the-concepts-of-nextjs-such-as-csr-ssr-ssg-isr-rsc-spa-and-streaming-ssr-pl3)&amp;#xD;
        &amp;#xD;
        ## npm 보안 위기의 해&amp;#xD;
        &amp;#xD;
        2025년은 보안 사고가 끊이지 않았습니다. 공급망 공격을 비롯한 여러 보안 취약점 등이 있었습니다. 다행히 최근 주요 플랫폼들이 발빠르게 대응하며 보안 체계를 갖춰가고 있습니다. &amp;#xD;
        &amp;#xD;
        ### 개발자 계정 탈취로 인한 공급망 공격&amp;#xD;
        &amp;#xD;
        유명 라이브러리의 개발자들이 피싱으로 계정이 탈취되었고 이에 따라 오염된 패키지가 배포되는 사건들이 있었습니다.&amp;#xD;
        &amp;#xD;
        * [npm Author Qix Compromised via Phishing Email in Major Supply Chain Attack](https://socket.dev/blog/npm-author-qix-compromised-in-major-supply-chain-attack)&amp;#xD;
        * [npm debug and chalk packages compromised](https://www.aikido.dev/blog/npm-debug-and-chalk-packages-compromised)&amp;#xD;
        * [Version 5.6.1 published to npm is compromised (RESOLVED)](https://github.com/chalk/chalk/issues/656)&amp;#xD;
        * [infowski - hackerspace.pl](https://social.hackerspace.pl/@informatic/115168929981581855)&amp;#xD;
        * [DuckDB npm Account Compromised in Continuing Supply Chain Attack](https://socket.dev/blog/duckdb-npm-account-compromised-in-continuing-supply-chain-attack)&amp;#xD;
        * [DuckDB NPM packages 1.3.3 and 1.29.2 compromised with malware](https://github.com/duckdb/duckdb-node/security/advisories/GHSA-w62p-hx95-gf2c)&amp;#xD;
        * [eslint-config-prettier Compromised: How npm Package with 30 Million Downloads Spread Malware](https://safedep.io/eslint-config-prettier-major-npm-supply-chain-hack/)&amp;#xD;
        * [Popular npm linter packages hijacked via phishing to drop malware](https://www.bleepingcomputer.com/news/security/popular-npm-linter-packages-hijacked-via-phishing-to-drop-malware/)&amp;#xD;
        * [npm Phishing Email Targets Developers with Typosquatted Domain](https://socket.dev/blog/npm-phishing-email-targets-developers-with-typosquatted-domain)&amp;#xD;
        * [Josh Junon - bluesky](https://bsky.app/profile/bad-at-computer.bsky.social/post/3lydioq5swk2y)&amp;#xD;
        * [JounQin - X](https://x.com/JounQin/status/1946297662069993690)&amp;#xD;
        * [npm ‘is’ Package Hijacked in Expanding Supply Chain Attack](https://socket.dev/blog/npm-is-package-hijacked-in-expanding-supply-chain-attack)&amp;#xD;
        [](https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages)&amp;#xD;
        * [Popular Tinycolor npm Package Compromised in Supply Chain Attack Affecting 40+ Packages](https://socket.dev/blog/tinycolor-supply-chain-attack-affects-40-packages)&amp;#xD;
        * [ctrl/tinycolor and 40+ NPM Packages Compromised](https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### Shai-Hulud 공격&amp;#xD;
        &amp;#xD;
        Shai-Hulud 대규모 공급망 공격 분석입니다.&amp;#xD;
        &amp;#xD;
        * [Shai-Hulud Returns: Over 1K NPM Packages and 27K+ Github Repos infected via Fake Bun Runtime Within Hours](https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24)&amp;#xD;
        * [The Shai-Hulud 2.0 npm worm: analysis, and what you need to know](https://securitylabs.datadoghq.com/articles/shai-hulud-2.0-npm-worm/)&amp;#xD;
        * [NPM Security Best Practices: How to Protect Your Packages After the 2025 Shai Hulud Attack](https://snyk.io/fr/articles/npm-security-best-practices-shai-hulud-attack/)&amp;#xD;
        * [GitLab discovers widespread npm supply chain attack](https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/)&amp;#xD;
        &amp;#xD;
        ### 플랫폼 보안 강화&amp;#xD;
        &amp;#xD;
        npm과 Github의 인증 체계 개선과 보안 기능 업데이트입니다.&amp;#xD;
        &amp;#xD;
        * [npm classic tokens revoked, session-based auth and CLI token management now available](https://github.blog/changelog/2025-12-09-npm-classic-tokens-revoked-session-based-auth-and-cli-token-management-now-available/)&amp;#xD;
        * [npm trusted publishing with OIDC is generally available](https://github.blog/changelog/2025-07-31-npm-trusted-publishing-with-oidc-is-generally-available/)&amp;#xD;
        * [How We&apos;re Protecting Our Newsroom from npm Supply Chain Attacks](https://pnpm.io/blog/2025/12/05/newsroom-npm-supply-chain-security)&amp;#xD;
        &amp;#xD;
        ### 도구&amp;#xD;
        &amp;#xD;
        보안 강화를 위한 다양한 라이브러리입니다.&amp;#xD;
        &amp;#xD;
        * [safe-npm](https://github.com/kevinslin/safe-npm)&amp;#xD;
        * [npq](https://github.com/lirantal/npq)&amp;#xD;
        * [reproduce](https://github.com/vltpkg/reproduce)&amp;#xD;
        * [exposedbydefault](https://github.com/neberej/exposedbydefault)&amp;#xD;
        &amp;#xD;
        ## 빌드 도구의 Rust 전환&amp;#xD;
        &amp;#xD;
        작년에 이어 JavaScript 툴체인의 Rust 재작성이 가속화되고 있습니다. Rust 전환 시 기존 대비 10-100배의 성능 향상이 가능하기 때문에, 복잡해진 프론트엔드 생태계에서 이 흐름은 이제 거스르기 어려워 보입니다.&amp;#xD;
        &amp;#xD;
        가장 두드러진 성과를 내는 곳은 VoidZero로 보입니다. Rolldown, Oxc, Oxfmt, Oxlint 등의 프로젝트를 운영하고 있습니다. 이 외에도 ByteDance의 Rsbuild/Rspack, 통합 툴체인 Deno, Vercel의 Turbopack, Meta의 Buck2 등이 있습니다.&amp;#xD;
        &amp;#xD;
        개인적인 경험도 있습니다. NCUI가 실제 서비스에서 어떻게 사용되는지 추적하기 위해 만든 사이드 프로젝트에서 JavaScript 기반 분석 도구를 SWC 기반으로 재개발했는데 처리 속도가 50배 이상 빨라졌습니다.&amp;#xD;
        &amp;#xD;
        Vite는 지속적인 업데이트 끝에 Rolldown을 탑재한 v8 메이저 업데이트를 앞두고 있습니다. 아직 Webpack을 사용 중이라면 마이그레이션을 검토해 볼 만합니다.&amp;#xD;
        &amp;#xD;
        ### Voidzero의 소식&amp;#xD;
        &amp;#xD;
        Voidzero 진영의 Rolldown 번들러 소식과 Oxlint 관련 내용입니다.&amp;#xD;
        &amp;#xD;
        * [How Rolldown Works: Module Loading, Dependency Graphs, and Optimization Explained](https://www.atriiy.dev/blog/rolldown-module-loader-and-dependency-graph/)&amp;#xD;
        * [Rolldown 1.0 Beta](https://github.com/rolldown/rolldown/releases/tag/v1.0.0-beta.1)&amp;#xD;
        * [Vite 8 Beta: The Rolldown-powered Vite](https://vite.dev/blog/announcing-vite8-beta)&amp;#xD;
        * [tsdown](https://github.com/rolldown/tsdown)&amp;#xD;
        * [Oxlint v1](https://oxc.rs/blog/2025-06-10-oxlint-stable.html)&amp;#xD;
        * [Faster Type-Aware Lint Rules: Biome vs. Oxlint](https://www.solberg.is/fast-type-aware-linting/)&amp;#xD;
        *  [Vite just passed Webpack in weekly npm downloads](https://x.com/youyuxi/status/1950234261573038444)&amp;#xD;
        &amp;#xD;
        ### 번들러 비교와 전망&amp;#xD;
        &amp;#xD;
        * [The JavaScript Bundler Grand Prix](https://redmonk.com/kholterhoff/2025/12/16/javascript-bundler-grand-prix/)&amp;#xD;
        * [Bundler Explorer](https://bundler.sxzz.dev/)&amp;#xD;
        * [Speeding up the JavaScript ecosystem - Rust and JavaScript plugins](https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-11/)&amp;#xD;
        &amp;#xD;
        ## 새로운 HTML/CSS 네이티브 기능의 폭발&amp;#xD;
        &amp;#xD;
        Baseline, Interop 활동에 힘입어 HTML/CSS 네이티브 기능이 쏟아졌습니다. 사실 이런 주제의 아티클은 꾸준히 올라오지만, 최신 브라우저만을 사용자 환경으로 전제할 수 없어 깊이 들여다보지 못한 경우가 많았습니다. 그래도 관심을 놓지 않고 있다가 충분히 사용 가능한 시점에 네이티브로 전환하면 제품 품질이 확실히 좋아질 거라 생각합니다.&amp;#xD;
        &amp;#xD;
        ### 네이티브 UI 컴포넌트&amp;#xD;
        &amp;#xD;
        기존에 JavaScript로 구현하던 Tooltip, Popover, Dropdown 같은 UI 패턴을 네이티브로 지원합니다.&amp;#xD;
        &amp;#xD;
        * [The Basics of Anchor Positioning](https://ishadeed.com/article/anchor-positioning/)&amp;#xD;
        * [Better anchor positioning with position-area](https://www.oddbird.net/2025/02/25/anchor-position-area/)&amp;#xD;
        * [Follow-the-leader pattern with CSS anchor positioning](https://una.im/follow-the-anchor/)&amp;#xD;
        * [The popover api is now baseline newly available](https://web.dev/blog/popover-baseline?hl=en)&amp;#xD;
        * [What is popover=hint?](https://una.im/popover-hint/)&amp;#xD;
        * [The `&amp;lt;select&amp;gt;` element can now be customized with CSS](https://developer.chrome.com/blog/a-customizable-select?hl=en)&amp;#xD;
        * [Updates to the customizable select API](https://una.im/select-updates/)&amp;#xD;
        &amp;#xD;
        ### CSS 조건문&amp;#xD;
        &amp;#xD;
        if() 함수로 CSS 속성 값 내에서 인라인 조건부 로직을 구현합니다.&amp;#xD;
        &amp;#xD;
        * [CSS conditionals with the new if() function](https://developer.chrome.com/blog/if-article/)&amp;#xD;
        * [Introduction to CSS if Statements and Conditional Logic](https://markodenic.com/introduction-to-css-if-statements-and-conditional-logic/)&amp;#xD;
        * [CSS custom functions are coming ... and they are going to be a game changer](https://www.bram.us/2025/02/09/css-custom-functions-teaser/)&amp;#xD;
        &amp;#xD;
        ### 레이아웃&amp;#xD;
        &amp;#xD;
        Subgrid와 반응형 디자인 기능입니다. 참고로 작년 주제였던 Masonry 레이아웃은 여전히 진행 중입니다.&amp;#xD;
        &amp;#xD;
        * [Brand New Layouts with CSS Subgrid](https://www.joshwcomeau.com/css/subgrid/)&amp;#xD;
        * [Taking RWD(Responsive Web Design) to the extreme](https://www.smashingmagazine.com/2025/02/taking-rwd-to-the-extreme/)&amp;#xD;
        * [How much do you really know about media queries?](https://frontendmasters.com/blog/learn-media-queries/)&amp;#xD;
        * [Grid Lanes Layout Model](https://www.w3.org/TR/css-grid-3/#grid-lanes-model)&amp;#xD;
        * [MDN - Masonry Layout](https://developer.mozilla.org/en-US/docs/Web/CSS/Guides/Grid_layout)&amp;#xD;
        &amp;#xD;
        ### 색상과 스타일링&amp;#xD;
        &amp;#xD;
        * [CSS Custom Properties vs. Sass Variables: A Pragmatic Guide](https://www.alwaystwisted.com/articles/css-vs-sass)&amp;#xD;
        * [A pragmatic guide to modern CSS colours - part one](https://piccalil.li/blog/a-pragmatic-guide-to-modern-css-colours-part-one/)&amp;#xD;
        * [Making Context-Aware Components: How CSS inherit() Could Simplify Design Systems](https://www.alwaystwisted.com/articles/making-context-aware-components)&amp;#xD;
        &amp;#xD;
        ### 새로운 Web API&amp;#xD;
        &amp;#xD;
        Temporal, Clipboard, Sanitizer, moveBefore 등 신규 브라우저 API와 제안입니다.&amp;#xD;
        &amp;#xD;
        * [The Clipboard API: How Did We Get Here?](https://cekrem.github.io/posts/clipboard-api-how-hard-can-it-be/)&amp;#xD;
        * [Temporal API is coming](https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/)&amp;#xD;
        * [Why the Sanitizer API is just setHTML()](https://frederikbraun.de/why-sethtml.html)&amp;#xD;
        * [Move elements around the DOM while preserving their state with moveBefore](https://www.bram.us/2025/01/16/move-elements-around-the-dom-while-preserving-their-state-with-movebefore/)&amp;#xD;
        * [Making complex web apps faster](https://blogs.windows.com/msedgedev/2025/12/09/making-complex-web-apps-faster/)&amp;#xD;
        &amp;#xD;
        ### 종합 가이드&amp;#xD;
        &amp;#xD;
        HTML/CSS 신기능 전반을 다룹니다.&amp;#xD;
        &amp;#xD;
        * [Relatively new things you should know about HTML heading into 2025](https://frontendmasters.com/blog/bone-up-html-2025/)&amp;#xD;
        * [Perfecting Baseline](https://piccalil.li/blog/perfecting-baseline/)&amp;#xD;
        * ▶️ [25 new \&amp;amp; rad feature of CSS](https://www.youtube.com/watch?v=QW6GECIzvsw)&amp;#xD;
        * [CSS Wrapped 2025](https://chrome.dev/css-wrapped-2025/)&amp;#xD;
        &amp;#xD;
        ## AI 트렌드&amp;#xD;
        &amp;#xD;
        프론트엔드를 떠나 IT 산업 전반을 돌아보면, 2025년은 단연 LLM/AI가 가장 뜨거운 키워드였습니다. 상반기만 해도 MCP 프로토콜 소개나 LLM 기반 코드 개발 후기 정도가 화제였는데, 불과 몇 달 만에 양상이 달라졌습니다. Angular, Svelte, Chrome DevTools 등 주요 프레임워크가 공식 MCP 서버를 출시했고, Spotify와 Amazon 팀의 실전 도입 사례가 속속 공개되기 시작했습니다. 바이브 코딩, 컨텍스트 엔지니어링, 스펙 주도 개발 같은 방법론이 등장했고, 최근에는 Claude에서 Skills라는 도구까지 나왔습니다.&amp;#xD;
        &amp;#xD;
        자연스럽게 프론트엔드 영역의 생산성 개선에 관심이 쏠렸습니다. 하지만 LLM은 복잡한 인터랙션 처리에서 한계를 드러냈고, 기대만큼의 임팩트를 보여주진 못했습니다. 이 시기에 AI의 한계를 지적하는 글, 코드 리뷰의 중요성을 강조하는 글, 엔지니어의 책임을 다시 정의하는 글이 쏟아졌습니다. 미래를 단언할 수는 없지만, 현 시점에서는 개발자의 판단력이 여전히 핵심이라는 데 대체로 의견이 모이는 분위기입니다.&amp;#xD;
        &amp;#xD;
        그럼에도 프론트엔드와 AI의 접점은 계속 넓어지고 있습니다. LLM이 웹 브라우저를 직접 제어하고, React 컴포넌트를 해석하는 도구들이 등장했습니다. Figma는 빠른 속도로 AI 기능을 도입하고 있고, 디자인 토큰을 AI가 읽을 수 있는 구조로 재설계하자는 제안도 나오고 있습니다.&amp;#xD;
        &amp;#xD;
        2025년 한 해를 돌아보면 솔직히 동네 축구 같다는 생각도 듭니다. &apos;이게 좋다&apos; 하면 모두가 몰려가고, &apos;저게 정답이다&apos; 하면 또 그쪽으로 쏠리는 흐름이 반복됐습니다. 그 와중에 인상 깊었던 LLM 사용 사례와 아티클, 도구를 정리해 공유합니다.&amp;#xD;
        &amp;#xD;
        ### AI 코딩의 한계와 책임&amp;#xD;
        &amp;#xD;
        LLM 기반 개발의 한계점과 개발자 책임에 대한 논의입니다.&amp;#xD;
        &amp;#xD;
        * [Can LLMs write better code if you keep asking them to &quot;write better code?&quot;](https://minimaxir.com/2025/01/write-better-code/)&amp;#xD;
        * [Why you shouldn&apos;t use AI to write documentation](https://zeroheight.com/blog/why-you-shouldnt-use-ai-to-write-documentation/)&amp;#xD;
        * [Why Your AI Coding Assistant Keeps Doing It Wrong, and How To Fix It](https://blog.thepete.net/blog/2025/05/22/why-your-ai-coding-assistant-keeps-doing-it-wrong-and-how-to-fix-it/)&amp;#xD;
        * [Your job is to deliver code you have proven to work](https://simonwillison.net/2025/Dec/18/code-proven-to-work/)&amp;#xD;
        * [dead framework theory](https://aifoc.us/dead-framework-theory/)&amp;#xD;
        * [How to Fix Any Bug](https://overreacted.io/how-to-fix-any-bug/)&amp;#xD;
        &amp;#xD;
        ### 기업 도입 사례&amp;#xD;
        &amp;#xD;
        Spotify, Airbnb 등 대규모 조직의 AI 도입 경험입니다.&amp;#xD;
        &amp;#xD;
        * [1,500+ PRs Later: Spotify’s Journey with Our Background Coding Agent (Part 1)](https://engineering.atspotify.com/2025/11/spotifys-background-coding-agent-part-1)&amp;#xD;
        * [The New Calculus of AI-based Coding](https://blog.joemag.dev/2025/10/the-new-calculus-of-ai-based-coding.html)&amp;#xD;
        * [Accelerating Large-Scale Test Migration with LLMs](https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b)&amp;#xD;
        &amp;#xD;
        ### MCP와 컨텍스트 엔지니어링&amp;#xD;
        &amp;#xD;
        Model Context Protocol과 효과적인 AI 활용을 위한 컨텍스트 설계입니다.&amp;#xD;
        &amp;#xD;
        * [MCP explained without hype of fluff](https://blog.nilenso.com/blog/2025/05/12/mcp-explained-without-hype-or-fluff)&amp;#xD;
        * [Chrome DevTools MCP Server](https://developer.chrome.com/blog/chrome-devtools-mcp)&amp;#xD;
        * [Getting AI to Work in Complex Codebases](https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md)&amp;#xD;
        * [Minification isn&apos;t obfuscation - Claude Code proves it](https://martinalderson.com/posts/minification-isnt-obfuscation-claude-code-proves-it/)&amp;#xD;
        &amp;#xD;
        ### AI시대의 개발 방법론&amp;#xD;
        &amp;#xD;
        API 설계, 문서화, 추상화 등 AI와 함께하는 개발 방법론입니다.&amp;#xD;
        &amp;#xD;
        * [How to Design APIs for an AI World ](https://refactoring.fm/p/how-to-design-apis-for-an-ai-world)&amp;#xD;
        * [Writing documentation for AI: best practices](https://docs.kapa.ai/improving/writing-best-practices/)&amp;#xD;
        * [The New Calculus of AI-based Coding](https://blog.joemag.dev/2025/10/the-new-calculus-of-ai-based-coding.html)&amp;#xD;
        * [Conversation: LLMs and Building Abstractions](https://martinfowler.com/articles/convo-llm-abstractions.html)&amp;#xD;
        * [Attention Is the New Big-O](https://alexchesser.medium.com/attention-is-the-new-big-o-9c68e1ae9b27/)&amp;#xD;
        &amp;#xD;
        ### 유용한 도구&amp;#xD;
        &amp;#xD;
        AI 개발에 활용할 수 있는 도구입니다.&amp;#xD;
        &amp;#xD;
        * [repomix](https://github.com/yamadashy/repomix) - 전체 Repository를 AI 친화적인 단일 파일로 압축&amp;#xD;
        * [repo2txt](https://github.com/abinthomasonline/repo2txt) - GitHub Repository를 단일 텍스트로 변환, GUI 기반으로 파일 선택 가능&amp;#xD;
        * [ai-digest](https://github.com/khromov/ai-digest) - 코드베이스를 하나의 마크다운 파일로 생성&amp;#xD;
        * [claude-flow](https://github.com/ruvnet/claude-flow) - Claude 기반 AI 에이전트를 여러 개 연결해 협업, 복잡한 작업을 단계별 워크플로우로 자동화&amp;#xD;
        * [agent-rules](https://github.com/steipete/agent-rules) - Cursor, Claude Code 같은 AI 코드 어시스턴트에게 일관된 행동 지침을 적용하는 규칙 템플릿 모음&amp;#xD;
        * [Conductor](https://conductor.build/) - Claude Code를 여러 개 동시에 실행하며 관리하는 도구&amp;#xD;
        * [Terragon](https://www.terragonlabs.com/) - Claude Code, OpenAI Codex 등 AI 코딩 에이전트를 클라우드에서 병렬 실행&amp;#xD;
        * [react-grab](https://github.com/aidenybai/react-grab) - 페이지의 모든 요소를 선택해 AI 코딩 에이전트에게 컨텍스트로 전달, 단일 스크립트 태그로 설치&amp;#xD;
        * [MCP Registry](https://github.com/mcp) - GitHub에서 다양한 MCP 서버를 모아둔 레지스트리&amp;#xD;
        * [streamdown](https://streamdown.ai/) - AI Chat 서비스처럼 마크다운 스트리밍 렌더링 지원&amp;#xD;
        * [lean-spec](https://github.com/codervisor/lean-spec) - 경량 SDD(Spec-Driven Development) 프레임워크&amp;#xD;
        &amp;#xD;
        ## 그 외 분야별 소식&amp;#xD;
        &amp;#xD;
        ### JavaScript/TypeScript&amp;#xD;
        &amp;#xD;
        2025년도 JavaScript와 TypeScript 생태계는 활발하게 움직였습니다. TypeScript 7의 Go 포팅 소식, 다양한 언어 패턴과 보안 이슈, 그리고 JSDoc을 활용한 타입 안정성 확보 방법 등 주요 소식을 정리했습니다.&amp;#xD;
        &amp;#xD;
        #### 언어 기능과 패턴&amp;#xD;
        &amp;#xD;
        JavaScript의 다양한 언어 기능과 활용 패턴입니다.&amp;#xD;
        &amp;#xD;
        * [There are a lot of ways to break up long tasks in JavaScript](https://macarthur.me/posts/long-tasks/)&amp;#xD;
        * [Breaking Up with Long Tasks or: how I learned to group loops and wield the yield](https://calendar.perfplanet.com/2024/breaking-up-with-long-tasks-or-how-i-learned-to-group-loops-and-wield-the-yield/)&amp;#xD;
        * [Category Theory for JavaScript/TypeScript Developers](https://ibrahimcesar.cloud/blog/category-theory-for-javascript-typescript-developers/)&amp;#xD;
        * [Say bye with JavaScript Beacon](https://hemath.dev/blog/say-bye-with-javascript-beacon/)&amp;#xD;
        * [Using await at the top level in ES Modules](https://allthingssmitty.com/2025/06/16/using-await-at-the-top-level-in-es-modules/)&amp;#xD;
        * [30 Years of JavaScript: 10 Milestones That Changed the Web](https://thenewstack.io/30-years-of-javascript-10-milestones-that-changed-the-web/)&amp;#xD;
        * [How to Control the Number of Concurrent Promises in JavaScript](https://dev.to/zacharylee/how-to-control-the-number-of-concurrent-promises-in-javascript-3mg8)&amp;#xD;
        * [Deeply immutable data structures](https://sanjeettiwari.com/notes/deeply-immutable-structures)&amp;#xD;
        * [JavaScript&apos;s Promise.race and Promise.all Are Not &quot;Fair&quot;](https://v5.chriskrycho.com/notes/javascript-promise-race-and-promise-all-are-not-fair/)&amp;#xD;
        * [Sustainable simplicity](https://frontendatscale.com/issues/42)&amp;#xD;
        &amp;#xD;
        #### TypeScript의 활용&amp;#xD;
        &amp;#xD;
        TypeScript의 고급 기능과 타입 시스템 활용법입니다.&amp;#xD;
        &amp;#xD;
        * [TypeScript vs Zod: Clearing up validation confusion](https://blog.logrocket.com/when-use-zod-typescript-both-developers-guide)&amp;#xD;
        * [6 Advanced TypeScript Tricks](https://sinja.io/blog/advanced-typescript)&amp;#xD;
        * [The Multi-Repository TypeScript Problem](https://www.carrick.tools/blog/the-multi-repository-typescript-problem/)&amp;#xD;
        &amp;#xD;
        #### JSDoc 활용&amp;#xD;
        &amp;#xD;
        TypeScript 없이 JSDoc으로 타입 안정성을 확보하는 방법입니다.&amp;#xD;
        &amp;#xD;
        * [The Nuances of JavaScript Typing using JSDoc](https://thathtml.blog/2025/12/nuances-of-typing-with-jsdoc/)&amp;#xD;
        * [How JSDoc Saved My Dev Workflow](https://spin.atomicobject.com/how-jsdoc-saved-my-dev-workflow/)&amp;#xD;
        &amp;#xD;
        #### TypeScript 생태계&amp;#xD;
        &amp;#xD;
        TypeScript의 발전 방향과 생태계 변화입니다.&amp;#xD;
        &amp;#xD;
        * [TypeScript’s rise in the AI era: Insights from Lead Architect, Anders Hejlsberg](https://github.blog/developer-skills/programming-languages-and-frameworks/typescripts-rise-in-the-ai-era-insights-from-lead-architect-anders-hejlsberg/)&amp;#xD;
        * [The Inner Workings of JavaScript Source Maps](https://www.polarsignals.com/blog/posts/2025/11/04/javascript-source-maps-internals)&amp;#xD;
        * [TypeScript is going Go: Why it&apos;s the pragmatic choice](https://johnnyreilly.com/typescript-go-pragmatic-choice)&amp;#xD;
        &amp;#xD;
        #### 보안&amp;#xD;
        &amp;#xD;
        JavaScript 시큐어 코딩 관련 내용입니다.&amp;#xD;
        &amp;#xD;
        * [How to consume APIs in React using Fetch and Async/Await](https://blog.codeminer42.com/how-to-consume-apis-in-react-using-fetch-and-async-await/)&amp;#xD;
        &amp;#xD;
        ### 프레임워크&amp;#xD;
        &amp;#xD;
        앞서 언급한 React 외 주요 프레임워크들의 2025년 소식을 정리했습니다. Remix의 React 결별 선언, Svelte의 성능 입증, Angular의 Zoneless 안정화 등 흥미로운 변화가 많았습니다.&amp;#xD;
        &amp;#xD;
        #### Remix&amp;#xD;
        &amp;#xD;
        React와 결별하고 웹 표준 중심으로 재구상된 Remix v3 소식입니다.&amp;#xD;
        &amp;#xD;
        * [Wake Up! Remix](https://remix.run/blog/wake-up-remix)&amp;#xD;
        * [Remix Jam 2025](https://remix.run/jam/2025)&amp;#xD;
        * [React and Remix Choose Different Futures](https://laconicwit.com/react-and-remix-choose-different-futures/)&amp;#xD;
        * [Remix 3 and the End of React-Centric Architectures](https://thenewstack.io/remix-3-and-the-end-of-react-centric-architectures/)&amp;#xD;
        * [Just JavaScript](https://pedrocattori.com/posts/just-javascript/)&amp;#xD;
        *  [Remix v3 Preact 기반 계획 취소](https://x.com/mjackson/status/1955083457144762729)&amp;#xD;
        * [Remixing Shopify&apos;s Admin](https://shopify.engineering/remixing-admin)&amp;#xD;
        &amp;#xD;
        #### Svelte&amp;#xD;
        &amp;#xD;
        개발자 만족도 1위, 성능 벤치마크에서도 가장 빠른 성능을 기록한 Svelte 소식입니다.&amp;#xD;
        &amp;#xD;
        * [What&apos;s new in Svelte: November 2025](https://svelte.dev/blog/whats-new-in-svelte-november-2025)&amp;#xD;
        * [Svelte really is that fast](https://chuniversiteit.nl/papers/svelte-is-fast)&amp;#xD;
        * [Why startups choose React (and when you shouldn&apos;t)](https://evilmartians.com/chronicles/why-startups-choose-react-and-when-you-should-not)&amp;#xD;
        &amp;#xD;
        #### Angular&amp;#xD;
        &amp;#xD;
        Zoneless 안정화와 시그널 기반 반응성 API를 도입한 Angular 소식입니다.&amp;#xD;
        &amp;#xD;
        * [Angular Summer Update 2025](https://blog.angular.dev/angular-summer-update-2025-1987592a0b42)&amp;#xD;
        * [Angular 2025 Strategy](https://blog.angular.dev/angular-2025-strategy-9ca333dfc334)&amp;#xD;
        &amp;#xD;
        #### Vue / Nuxt&amp;#xD;
        &amp;#xD;
        Vercel의 Nuxt 인수와 Vue 생태계 현황입니다.&amp;#xD;
        &amp;#xD;
        * [State of Vue.js Report 2025](https://www.monterail.com/)&amp;#xD;
        * [NuxtLabs joins Vercel](https://vercel.com/blog/nuxtlabs-joins-vercel)&amp;#xD;
        * [How to build Microfrontends with Module Federation and Vue](https://alexop.dev/posts/how-to-build-microfrontends-with-module-federation-and-vue/)&amp;#xD;
        &amp;#xD;
        #### Astro&amp;#xD;
        &amp;#xD;
        클라이언트 JavaScript 최소화와 Content Collections 진화 소식입니다.&amp;#xD;
        &amp;#xD;
        * [What&apos;s new in Astro - January 2025](https://astro.build/blog/whats-new-january-2025/)&amp;#xD;
        * [Live Content Collections: A Deep Dive](https://astro.build/blog/live-content-collections-deep-dive/)&amp;#xD;
        * [Why use React?](https://adactio.com/journal/22265)&amp;#xD;
        &amp;#xD;
        #### htmx&amp;#xD;
        &amp;#xD;
        &quot;하위 호환성 없는 v3.0은 절대 출시하지 않겠다&quot;는 약속 때문에 v3를 건너뛰고 v4 출시한 htmx 소식입니다.&amp;#xD;
        &amp;#xD;
        * [htmx v4](https://htmx.org/essays/the-fetchening/)&amp;#xD;
        &amp;#xD;
        ### 디자인 시스템&amp;#xD;
        &amp;#xD;
        2025년 디자인 시스템 생태계는 AI 시대를 맞아 큰 전환점을 맞이했습니다. Claude Skills와 Figma MCP 서버의 등장으로 디자인 시스템 팀이 AI를 활용해 마이그레이션 자동화, Storybook 스토리 생성, 토큰 검증 등 반복적인 작업을 자동화할 수 있게 되었고, 이에 따라 디자인 토큰을 AI가 올바르게 이해할 수 있도록 시맨틱 네이밍과 메타데이터를 추가하는 구조화가 새로운 과제로 언급되었습니다.&amp;#xD;
        &amp;#xD;
        컴포넌트 설계 철학에서는 합성이 하나의 패러다임으로 떠올랐습니다. Figma의 네이티브 슬롯 지원을 계기로 고정된 컴포넌트 대신 기본 컨테이너와 재사용 가능한 자식 요소를 분리해 제공하는 방식이 확산되었고, 다중 추상화 계층 접근법도 주목 받았습니다.&amp;#xD;
        &amp;#xD;
        디자인 토큰 영역에서는 DTCG가 18개월간 여러 가지 스펙 업데이트를 발표한 반면, 흥미롭게도 디자인 토큰이라는 용어를 처음 만든 Salesforce는 복잡해진 토큰 스펙 대신 CSS 커스텀 프로퍼티로 회귀했습니다. Figma 역시 토큰 대신 변수라는 용어를 선택하며, 업계가 토큰의 복잡성과 단순함 사이에서 균형점을 찾고 있음을 보여줍니다.&amp;#xD;
        &amp;#xD;
        #### 설계 철학과 전략&amp;#xD;
        &amp;#xD;
        디자인 시스템의 본질과 성공 전략에 대한 논의입니다.&amp;#xD;
        &amp;#xD;
        * [Beyond the Plateau of Sameness](https://yeseniaperezcruz.substack.com/p/beyond-the-plateau-of-sameness)&amp;#xD;
        * [Bias in Design Systems](https://bencallahan.com/bias-in-design-systems)&amp;#xD;
        * [The future of design systems is decentralized](https://uxdesign.cc/the-future-of-design-systems-is-decentralized-770db996442c)&amp;#xD;
        * [The dumbest design system mistakes](https://learn.thedesignsystem.guide/p/the-dumbest-design-system-mistakes)&amp;#xD;
        * [Design System Tactics](https://redesigningdesign.systems/tactics/all-tactics)&amp;#xD;
        * [designtokens.fyi](https://designtokens.fyi/)&amp;#xD;
        &amp;#xD;
        #### 컴포넌트 설계&amp;#xD;
        &amp;#xD;
        슬롯, 컴포지션, 추상화 계층 등 컴포넌트 설계 패턴입니다.&amp;#xD;
        &amp;#xD;
        * [Slots in Design Systems](https://nathanacurtis.substack.com/p/slots-in-design-systems)&amp;#xD;
        * [Using composability over inheritance to scale design systems](https://zeroheight.com/blog/using-composability-over-inheritance-to-scale-design-systems/)&amp;#xD;
        * [Multiple Layers of Abstraction in Design Systems](https://engineering.atspotify.com/2023/05/multiple-layers-of-abstraction-in-design-systems/)&amp;#xD;
        * [The power of relationships in design systems](https://learn.thedesignsystem.guide/p/the-power-of-relationships-in-design)&amp;#xD;
        &amp;#xD;
        #### 디자인 토큰&amp;#xD;
        &amp;#xD;
        토큰 스펙 진화와 실무 활용법입니다.&amp;#xD;
        &amp;#xD;
        * [What&apos;s new in the Design Tokens spec](https://zeroheight.com/blog/whats-new-in-the-design-tokens-spec/)&amp;#xD;
        * [Design Token-Based UI Architecture](https://martinfowler.com/articles/design-token-based-ui-architecture.html)&amp;#xD;
        * [A Design Token Workflow](https://www.alwaystwisted.com/articles/a-design-tokens-workflow-part-1)&amp;#xD;
        * [Generating Utility Classes from Design Tokens](https://www.alwaystwisted.com/articles/a-design-tokens-workflow-part-13)&amp;#xD;
        * [Avoiding Tokens](https://blog.damato.design/posts/avoiding-tokens/)&amp;#xD;
        * [Introducing Design Tokens Language Server](https://bennypowers.dev/posts/introducing-design-tokens-language-server/)&amp;#xD;
        &amp;#xD;
        #### AI와 디자인 시스템&amp;#xD;
        &amp;#xD;
        AI 시대에 디자인 시스템을 활용하고 최적화하는 방법입니다.&amp;#xD;
        &amp;#xD;
        * [3 practical ways LLMs can support design systems teams today](https://zeroheight.com/blog/3-practical-ways-llms-can-support-design-systems-teams-today/)&amp;#xD;
        * [Why your design system team need Claude Skills](https://learn.thedesignsystem.guide/p/why-your-design-system-team-need)&amp;#xD;
        * [Design tokens that AI can actually read](https://learn.thedesignsystem.guide/p/design-tokens-that-ai-can-actually)&amp;#xD;
        * [How to build patterns from your design system components with AI](https://learn.thedesignsystem.guide/p/how-to-build-patterns-from-your-design)&amp;#xD;
        &amp;#xD;
        #### 문서화와 운영&amp;#xD;
        &amp;#xD;
        디자인 시스템 문서화, 접근성 감사, 유지보수 전략 및 운영입니다.&amp;#xD;
        &amp;#xD;
        * [How to document your design system components](https://zeroheight.com/blog/how-to-document-your-design-system-components/)&amp;#xD;
        * [When&apos;s the right time to start documenting your design system?](https://zeroheight.com/blog/whens-the-right-time-to-start-documenting-your-design-system/)&amp;#xD;
        * [Storefront \&amp;amp; Warehouse](https://blog.damato.design/posts/storefront-warehouse/)&amp;#xD;
        * [Design tokens aren&apos;t enough. ADR need a place](https://samiamdesigns.substack.com/p/design-tokens-arent-enough-architecture)&amp;#xD;
        * [Retrofitting a Design System Into an Existing Product](https://medium.com/@anilpak35/retrofitting-a-design-system-into-an-existing-product-a9ebfe3d7d30)&amp;#xD;
        &amp;#xD;
        ### 라이브러리&amp;#xD;
        &amp;#xD;
        개발 생산성을 높여주는 다양한 도구들이 등장했습니다. UI 관련 라이브러리, 프로젝트 분석, TypeScript 개발, 보안, 테스트까지 카테고리별로 정리했습니다.&amp;#xD;
        &amp;#xD;
        #### UI 컴포넌트&amp;#xD;
        &amp;#xD;
        shadcn/ui는 지속적으로 엄청난 확장을 거듭하며 가장 영향력 있는 생태계 중 하나가 되었고, 지속적으로 주목받던 Base UI가 v1을 릴리스했습니다.&amp;#xD;
        &amp;#xD;
        * [shadcn/ui](https://ui.shadcn.com/) - CLI 3.0, Monorepo 지원, 다양한 신규 컴포넌트&amp;#xD;
        * [shadcn/ui.create](https://ui.shadcn.com/create) - GUI로 자신만의 shadcn/ui 구성&amp;#xD;
        * [shadcn.io](https://www.shadcn.io/) - MIT 라이선스 무료 컴포넌트 커뮤니티 레지스트리&amp;#xD;
        * [shadcn/ui Form documentation](https://ui.shadcn.com/docs/forms) - React Hook Form, TanStack Form 통합 가이드&amp;#xD;
        * [themecn](https://themecn.dev/) - shadcn 기반 테마 생성기&amp;#xD;
        * [formcn](https://formcn.dev/) - shadcn 기반 양식 생성기&amp;#xD;
        * [base-ui v1.0](https://base-ui.com/) - Radix, Material UI, Floating UI 개발자들의 컴포넌트 재정의&amp;#xD;
        * [Headless Tree](https://headless-tree.lukasbach.com/) - 트리 컴포넌트 라이브러리&amp;#xD;
        &amp;#xD;
        #### 프로젝트 분석&amp;#xD;
        &amp;#xD;
        코드베이스와 의존성을 분석하고 정리하는 도구들입니다.&amp;#xD;
        &amp;#xD;
        * [Knip](https://knip.dev/) - 불필요한 파일, 의존성, 내보내기 등을 JavaScript/TypeScript 프로젝트에서 추출&amp;#xD;
        * [qnm](https://github.com/ranyitz/qnm) - node\_modules 탐색 CLI, 퍼지 검색, 패키지 설치 이유 설명, 모노레포 지원&amp;#xD;
        * [API Extractor](https://api-extractor.com/) - TypeScript 라이브러리 API 관리, 변경 감지, export 검증, .d.ts rollup 생성&amp;#xD;
        * [sonda](https://github.com/filipsobol/sonda) - 소스맵 분석으로 트리쉐이킹/압축 후 실제 모듈 크기 시각화&amp;#xD;
        &amp;#xD;
        #### TypeScript 개발 도구&amp;#xD;
        &amp;#xD;
        TypeScript 개발 경험을 개선하는 도구들입니다.&amp;#xD;
        &amp;#xD;
        * [ts-migrating](https://github.com/ycmjason/ts-migrating) - strict 모드 등 새 compilerOptions를 점진적으로 적용, @ts-migrating 주석으로 라인별 제어&amp;#xD;
        * [ts-exec](https://github.com/poppinss/ts-exec) - SWC로 Node에서 TypeScript 실행, ts-node/tsx 문제 해결&amp;#xD;
        * [ts-to-zod](https://github.com/fabien0102/ts-to-zod) - TypeScript 타입/인터페이스에서 Zod 스키마 자동 생성&amp;#xD;
        * [typia](https://github.com/samchon/typia) - 컴파일 타임에 타입 정보만으로 유효성 검사, 간단한 문법&amp;#xD;
        &amp;#xD;
        #### 데이터 검증과 HTTP&amp;#xD;
        &amp;#xD;
        데이터 검증과 HTTP 요청을 다루는 라이브러리들입니다.&amp;#xD;
        &amp;#xD;
        * [Of Coerce!](https://github.com/kossnocorp/ofcoerce) - Zod/Valibot 경량 대안, FormData 직접 처리, RSC 폼에 유용&amp;#xD;
        * [ffetch](https://github.com/fetch-kit/ffetch) - TypeScript HTTP 클라이언트, 타임아웃/재시도/중복 제거 내장&amp;#xD;
        * [ky](https://github.com/sindresorhus/ky) - Fetch API 기반 HTTP 클라이언트, 자동 재시도/타임아웃/JSON 옵션, 의존성 없음&amp;#xD;
        * [upfetch](https://github.com/L-Blondy/up-fetch) - fetch 개선 TypeScript 라이브러리&amp;#xD;
        &amp;#xD;
        #### 기타&amp;#xD;
        &amp;#xD;
        다양한 개발 편의 도구들입니다.&amp;#xD;
        &amp;#xD;
        * [vercel/bidc](https://github.com/vercel/bidc) - 워커/iframe/서비스 워커 간 양방향 비동기 통신, Promise/Map/Set 전송 지원&amp;#xD;
        * [dotter](https://github.com/SuperCuber/dotter) - Rust 기반 dotfile 관리자, 심볼릭 링크와 템플릿화로 여러 머신 설정 관리&amp;#xD;
        * [globby](https://github.com/sindresorhus/globby) - fast-glob 기반 사용자 친화적 glob 매칭, .gitignore 인식&amp;#xD;
        * [nstr](https://github.com/shuding/nstr) - 부동소수점 정밀도 오류 자동 감지/수정, UI 표시에 적합한 형태로 변환&amp;#xD;
        * [i18n-check](https://github.com/lingualdev/i18n-check) - i18next 번역 파일에서 누락/손상된 키 검사, CI 사용 가능&amp;#xD;
        &amp;#xD;
        ## 정리하며&amp;#xD;
        &amp;#xD;
        2025년을 돌아보면 프론트엔드의 영역은 분명히 많이 확장되었습니다. React는 점점 기반 인프라에 가까워졌고, Next.js와 npm, 빌드 도구, 웹 플랫폼 전반에서도 변화가 이어졌습니다. 그 과정에서 복잡성은 늘었고, 따라가야 할 것도 많아진 한 해였습니다.&amp;#xD;
        &amp;#xD;
        내년에도 어떤 변화가 이어질지는 모르겠지만, 이 글이 관심 가는 주제를 찾거나 놓친 흐름을 되짚는 출발점이 되었으면 합니다. 감사합니다. &amp;#xD;
        &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_mint_202601.png](https://image.toast.com/aaaadh/real/2026/techblog/NHN%20Cloudmeetup%20bannerfootermint202601%282%29.png)](https://www.nhncloud.com/?utm_source=meetup&amp;amp;utm_medium=post&amp;amp;utm_campaign=branding&amp;amp;utm_content=0130_frontend&amp;amp;utm_term=bottom)
      </content:encoded>
    </item>
    <item>
      <title>“이번 달도 밤샘 정산입니다.” — 더 이상 밤샘하지 않아도 됩니다 (운영편)</title>
      <link>https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EB%8D%94-%EC%9D%B4%EC%83%81-%EB%B0%A4%EC%83%98%ED%95%98%EC%A7%80-%EC%95%8A%EC%95%84%EB%8F%84-%EB%90%A9%EB%8B%88%EB%8B%A4-%EC%9A%B4%EC%98%81%ED%8E%B8-4f09ae3bdf5d?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EB%8D%94-%EC%9D%B4%EC%83%81-%EB%B0%A4%EC%83%98%ED%95%98%EC%A7%80-%EC%95%8A%EC%95%84%EB%8F%84-%EB%90%A9%EB%8B%88%EB%8B%A4-%EC%9A%B4%EC%98%81%ED%8E%B8-4f09ae3bdf5d?source=rss----f107b03c406e---4</guid>
      <pubDate>Mon, 02 Feb 2026 09:01:05 GMT</pubDate>
      <content:encoded>&lt;h3&gt;“이번 달도 밤샘 정산입니다.” — 더 이상 밤샘하지 않아도 됩니다 (운영편)&lt;/h3&gt;&lt;p&gt;“이번 달도 밤샘 정산입니다.” 시리즈&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1&quot;&gt;정산 시스템은 어떻게 만들었을까 (실전편)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1&quot;&gt;정산 시스템은 왜 필요했을까 (설계편)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/이번-달도-밤샘-정산입니다-더-이상-밤샘하지-않아도-됩니다-운영편-4f09ae3bdf5d&quot;&gt;&lt;strong&gt;더 이상 밤샘하지 않아도 됩니다 (운영편)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;들어가며&lt;/h3&gt;&lt;p&gt;앞선 두 편에서는 정산 시스템이 왜 어려운 문제인지, 그리고 그 문제를 어떤 구조와 기술로 풀어냈는지를 다뤘습니다. &lt;br&gt;마지막 글에서는 MASS 정산 시스템을 실제 운영 환경에 단계적으로 오픈하며 어떤 변화와 성과를 만들어냈는지를 정리합니다.&lt;/p&gt;&lt;h3&gt;단계적 오픈 전략&lt;/h3&gt;&lt;p&gt;정산 시스템은 한 번에 완성해서 오픈하기에는 리스크가 너무 큰 시스템입니다.&lt;br&gt;그래서 MASS는 기능이 아닌 &lt;strong&gt;데이터를 기준으로 단계적 오픈 전략&lt;/strong&gt;을 선택했습니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;원천 데이터 적재 모듈 선배포&lt;br&gt;&lt;/strong&gt;a. 실제 운영 환경에서 발생하는 모든 케이스를 먼저 수집&lt;br&gt;b. 수집 과정에서 드러난 데이터 품질 이슈와 처리 오류를 사전에 식별/수정&lt;br&gt;c. 정산 정책 결정이 필요한 엣지 케이스들을 정리하고 기준을 확정&lt;br&gt;ㅤi. 예외 데이터 처리 기준&lt;br&gt;ㅤii. 경계 조건에서의 금액 산정 방식 등&lt;/li&gt;&lt;li&gt;&lt;strong&gt;실데이터 기반 QA/시뮬레이션&lt;br&gt;&lt;/strong&gt;a. 실제 데이터를 기준으로 정산 배치를 반복 실행하며 리허설&lt;/li&gt;&lt;li&gt;&lt;strong&gt;검증 완료 후 전체 정산 오픈&lt;br&gt;&lt;/strong&gt;a. 한 달 단위 정산을 처음부터 끝까지 무결하게 수행&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이 전략 덕분에 시스템 오픈과 동시에 실제 정산을 안정적으로 마칠 수 있었습니다.&lt;/p&gt;&lt;h4&gt;단계적 오픈 전략 (데이터 적재 → QA/시뮬레이션 → 전체 오픈)&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Pcq6TxN82czZdu5eoZYe-w.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;실데이터 기반 QA / 시뮬레이션&lt;/h4&gt;&lt;p&gt;정산 시스템은 실제 데이터로 검증하지 않으면, 오픈 이후에 반드시 예상하지 못한 불일치가 발생합니다.&lt;br&gt;그래서 MASS는 &lt;strong&gt;실제 운영 데이터를 기준으로 한 리허설&lt;/strong&gt;에 집중했습니다.&lt;/p&gt;&lt;blockquote&gt;리허설 환경 구성&lt;/blockquote&gt;&lt;p&gt;QA 및 시뮬레이션은 &lt;strong&gt;운영 환경과 동일한 스키마를 가진 개발 환경&lt;/strong&gt;에서 수행되었습니다.&lt;br&gt;운영 DB를 직접 사용하는 방식이 아니라,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;8월부터 10월까지 발생한 &lt;strong&gt;실제 원천 데이터를 개발 환경으로 마이그레이션&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;정산 로직, 배치 설정, 기준 데이터는 운영과 동일하게 유지&lt;/li&gt;&lt;li&gt;운영과 분리된 환경에서 반복 실행이 가능하도록 구성&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 운영 데이터의 현실적인 복잡성을 그대로 가져오면서도, 정산 배치를 &lt;strong&gt;수차례 재실행할 수 있는 안전한 실험 환경&lt;/strong&gt;을 확보했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*X28LQEXpzlOIiBpF0JzJhA.png&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;검증 방식: 수기 정산 결과와의 비교&lt;/blockquote&gt;&lt;p&gt;정산 결과 검증은 &lt;strong&gt;수기 정산 결과와의 직접 비교&lt;/strong&gt;를 기준으로 진행했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;기존 수기 정산으로 확정된 결과를 기준 데이터로 사용&lt;/li&gt;&lt;li&gt;동일 기간, 동일 조건으로 MASS 정산 배치를 반복 실행&lt;/li&gt;&lt;li&gt;업체별·항목별 금액을 단위까지 대조하며 결과 비교&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;초기 단계에서는 일부 수동 대조가 필요했지만, 반복되는 검증 구간에 대해서는 &lt;strong&gt;리포트 형태로 검증을 자동화&lt;/strong&gt;해 불일치 여부를 빠르게 식별할 수 있도록 했습니다.&lt;/p&gt;&lt;blockquote&gt;리허설 범위와 데이터 규모&lt;/blockquote&gt;&lt;p&gt;리허설은 &lt;strong&gt;단일 케이스가 아닌, 수개월치 실데이터&lt;/strong&gt;를 대상으로 수행되었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;8월 ~ 10월, 총 3개월치 실제 원천 데이터&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;일 정산, 월 정산 시나리오 모두 반복 실행&lt;/li&gt;&lt;li&gt;프로모션 적용, 예외 케이스, 경계 조건 포함&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 “정상 케이스”뿐 아니라, 실제 운영에서 발생하는 &lt;strong&gt;복합적인 케이스들까지 충분히 검증&lt;/strong&gt;할 수 있었습니다.&lt;/p&gt;&lt;blockquote&gt;불일치 발생 시 원인 추적 프로세스&lt;/blockquote&gt;&lt;p&gt;정산 결과와 수기 결과 간 불일치가 발견되면, 단순히 결과를 맞추는 것이 아니라 &lt;strong&gt;원인을 끝까지 추적하는 방식&lt;/strong&gt;으로 접근했습니다.&lt;/p&gt;&lt;p&gt;불일치 발생 시 다음 순서로 원인을 분석했습니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;원천 데이터 자체의 차이 여부 확인&lt;/li&gt;&lt;li&gt;단가/프로모션 기준 적용 여부 점검&lt;/li&gt;&lt;li&gt;정책적으로 정의되지 않았던 엣지 케이스 식별&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;기술적인 오류인 경우 로직을 수정했고, 정책적으로 판단이 필요한 경우에는 &lt;strong&gt;정산 기준을 명시적으로 정리한 뒤 시스템에 반영&lt;/strong&gt;했습니다.&lt;/p&gt;&lt;p&gt;이 과정을 반복하면서 정산 로직의 안정성뿐 아니라, &lt;strong&gt;정산 기준 자체의 모호함도 함께 해소&lt;/strong&gt;할 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*jvK_Iqus0EJPHI6PIlKu1w.png&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;이 단계의 의미&lt;/blockquote&gt;&lt;p&gt;이 실데이터 기반 리허설 단계는 단순한 QA가 아니라,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 로직의 정확성을 검증하고&lt;/li&gt;&lt;li&gt;기준을 명확히 정의하며&lt;/li&gt;&lt;li&gt;오픈 이후 재처리 가능성까지 점검하는&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;정산 시스템의 ‘예행 연습’에 해당하는 단계&lt;/strong&gt;였습니다.&lt;/p&gt;&lt;p&gt;이 과정을 충분히 거쳤기 때문에, MASS는 시스템 오픈과 동시에 &lt;strong&gt;실제 정산을 안정적으로 마칠 수 있었습니다.&lt;/strong&gt;&lt;/p&gt;&lt;h3&gt;오픈 이후 시스템 지표와 운영 결과&lt;/h3&gt;&lt;p&gt;MASS 구축 시점에 팀이 설정한 목표는 명확했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;정산은 반드시 회계 마감 기한 내에 끝나야 한다&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;재처리·재실행 상황에서도 정산 결과는 흔들리지 않아야 한다&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;정산 규모가 증가해도 운영 방식이 복잡해지지 않아야 한다&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;수기 보정 없이 시스템 결과만으로 정산을 마칠 수 있어야 한다&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;일 정산은 운영 관점에서 부담 없이 반복 실행할 수 있도록, 평균 처리 시간을 10분 이내로 유지해야 한다&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이러한 목표를 기준으로 시스템을 설계했고, 시스템 오픈 이후 현재까지 수행된 정산은 다음과 같은 결과를 보였습니다.&lt;/p&gt;&lt;h4&gt;정산 처리 성공률과 결과 확정 안정성&lt;/h4&gt;&lt;p&gt;MASS에서 말하는 “정산 성공”은 단순히 배치가 오류 없이 종료되었다는 의미가 아닙니다.&lt;/p&gt;&lt;p&gt;본 문서에서의 정산 성공은 다음 조건을 모두 만족하는 경우를 의미합니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 배치가 &lt;strong&gt;중단 없이 완료&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;모든 정산 대상 데이터가 &lt;strong&gt;정합성 검증을 통과&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;수기 보정 없이 &lt;strong&gt;시스템 계산 결과만으로 정산 결과 확정&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;회계 마감 기한 내 모든 &lt;strong&gt;정산 상태가 COMPLETED로 전이&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 기준으로 시스템 오픈 이후 수행된 정산 결과는 다음과 같습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;일 정산 성공률: 100%&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;월 정산 성공률: 100%&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;월 정산 마감 확정 성공률: 100%&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;모든 정산은 마감 기한 내 정상적으로 완료되었으며, 정산 결과 확정 과정에서도 &lt;strong&gt;정합성 이슈나 수기 개입 없이 &lt;/strong&gt;시스템 결과만으로 정산을 마무리할 수 있었습니다.&lt;/p&gt;&lt;h4&gt;정산 처리 시간&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;일 정산 평균 소요 시간:&lt;/strong&gt; 약 &lt;strong&gt;8분 26초&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;월 정산 평균 소요 시간:&lt;/strong&gt; 약 &lt;strong&gt;2분 27초&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;일 정산의 경우, 초기 목표로 설정했던 &lt;strong&gt;‘10분 이내 처리’ 기준을 안정적으로 만족&lt;/strong&gt;하고 있으며, 정산 대상 규모와 관계없이 일정한 처리 시간을 유지하고 있습니다.&lt;/p&gt;&lt;p&gt;이는 정산 배치가&lt;/p&gt;&lt;ul&gt;&lt;li&gt;데이터 규모 증가&lt;/li&gt;&lt;li&gt;재실행&lt;/li&gt;&lt;li&gt;반복 실행&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;과 같은 운영 시나리오에서도 부담 없이 실행될 수 있도록 설계되었음을 의미합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/548/1*iSeBKNxGUiSUG_hswfYe3A.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;재처리 상황에서도 유지되는 정합성&lt;/h4&gt;&lt;p&gt;12월 3일, upstream 시스템의 재고 스냅샷 이슈로 인해 해당 일자의 정산 데이터를 다시 계산해야 하는 상황이 발생했습니다.&lt;/p&gt;&lt;p&gt;정산 도메인에서 이와 같은 상황은 단순한 기술적 오류를 넘어, &lt;strong&gt;중복 청구, 과소·과대 정산, 파트너 업체 신뢰 훼손&lt;/strong&gt;으로 직결될 수 있는 명확한 비즈니스 리스크를 동반합니다.&lt;br&gt;특히 월 마감 직전이었다면, 회계 마감 지연이나 수기 보정으로 이어질 가능성도 있었습니다.&lt;/p&gt;&lt;p&gt;이 상황에서 MASS는 기존 정산 결과를 임의로 수정하거나 덮어쓰는 대신,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;해당 일자의 정산 대상 원천 데이터만을 기준으로 재처리를 수행했고&lt;/li&gt;&lt;li&gt;멱등성 기반 처리로 &lt;strong&gt;중복 정산 없이 안전하게 재계산&lt;/strong&gt;했으며&lt;/li&gt;&lt;li&gt;재처리 이후에도 &lt;strong&gt;정산 결과는 기존 기준과 동일하게 유지&lt;/strong&gt;되었습니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;그 결과, 최초 정산 시점과 재처리 이후의 금액 차이에 대해서는 파트너 업체에 &lt;strong&gt;변경 사유와 함께 정산 결과를 다시 공유&lt;/strong&gt;해야 했지만,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;변경된 금액이 어떤 기준에서 발생했는지 명확하게 설명할 수 있었고&lt;/li&gt;&lt;li&gt;수기 계산이나 임시 보정 없이 &lt;strong&gt;시스템 결과만으로 정산을 확정&lt;/strong&gt;할 수 있었으며&lt;/li&gt;&lt;li&gt;회계 마감 일정 역시 영향을 받지 않았습니다.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 사례는 재처리와 복구를 전제로 한 설계가 단순히 “다시 계산할 수 있다”는 수준을 넘어, &lt;strong&gt;장애 상황에서도 정산 변경을 통제 가능한 방식으로 관리하며 비즈니스 리스크를 최소화했음을 보여주는 사례&lt;/strong&gt;였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*MrJnJCWQWmq5WrvdMVZBaw.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;수기 정산 프로세스 변화&lt;/h4&gt;&lt;p&gt;시스템 도입 전후의 정산 프로세스는 다음과 같이 변화했습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;As-Is (수기 정산 — 6단계)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;물류 데이터 추출&lt;/li&gt;&lt;li&gt;금액 산정(정산)&lt;/li&gt;&lt;li&gt;정산서 작성&lt;/li&gt;&lt;li&gt;품의 상신&lt;/li&gt;&lt;li&gt;품의 승인&lt;/li&gt;&lt;li&gt;세금계산서 반영&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;To-Be (시스템 정산 — 2단계)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 금액 확인&lt;/li&gt;&lt;li&gt;세금계산서 반영&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;기존 &lt;strong&gt;6단계에 달하던 수기 정산 프로세스는 &lt;/strong&gt;단 &lt;strong&gt;2단계의 확인 중심 프로세스로 축소&lt;/strong&gt;되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*PStZqs6VZ_2Jj1OQwB6Mzg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;정산 마감 소요 시간 변화&lt;/h4&gt;&lt;p&gt;정산 대상 업체 수가 증가함에 따라, 기존 수기 정산 방식은 &lt;strong&gt;업체 수에 정비례하여 작업 시간이 증가하는 구조&lt;/strong&gt;였습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;정산 대상 40개 업체 기준&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;수기 정산 마감 소요 시간: 약 &lt;strong&gt;23시간&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;정산 대상 100개 업체 기준&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;수기 정산 마감 소요 시간: 약 &lt;strong&gt;30시간 이상&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;정산 마감은 영업일 기준 &lt;strong&gt;D+2(총 16 근무시간)&lt;/strong&gt; 내 완료가 필요했기 때문에, 담당자 1명 기준으로는 &lt;strong&gt;40개 업체가 사실상 처리 가능한 한계&lt;/strong&gt;였습니다.&lt;/p&gt;&lt;p&gt;반면, MASS 도입 이후에는 정산 방식이 근본적으로 달라졌습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 대상 업체 수와 무관하게&lt;/li&gt;&lt;li&gt;&lt;strong&gt;월 정산 마감 확정까지 약 1시간 내 완료&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;즉,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;40개 업체 기준: &lt;strong&gt;23시간 → 1시간&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;100개 업체 기준: &lt;strong&gt;30시간 → 1시간&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;정산 시스템 도입을 통해 &lt;strong&gt;정산 규모가 증가해도 마감 기한을 안정적으로 지킬 수 있는 구조&lt;/strong&gt;를 확보했습니다.&lt;/p&gt;&lt;blockquote&gt;정산 방식에 따른 마감 소요 시간 비교&lt;/blockquote&gt;&lt;p&gt;아래 그래프는 정산 대상 업체 수 증가에 따른 &lt;strong&gt;수기 정산 방식과 시스템 정산 방식의 차이&lt;/strong&gt;를 직관적으로 보여줍니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*YoHHgxcg181mGeTNOLjI1A.png&quot; /&gt;&lt;figcaption&gt;(베이지: 수기 / 주황: MASS)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;이 그래프가 보여주듯,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;수기 정산은 &lt;strong&gt;업체 수 증가 = 마감 리스크 증가&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;MASS 정산은 &lt;strong&gt;업체 수 증가와 무관한 일정한 처리 시간&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이라는 차이를 가집니다.&lt;/p&gt;&lt;p&gt;이는 단순한 자동화 효과가 아니라, 정산 업무를 &lt;strong&gt;사람의 처리 한계에서 시스템 처리 한계로 전환&lt;/strong&gt;한 결과였습니다.&lt;/p&gt;&lt;h4&gt;정산 데이터 제공 주기 변화&lt;/h4&gt;&lt;p&gt;정산 결과의 가시성 역시 크게 개선되었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;정산 데이터 제공 주기:&lt;/strong&gt; 월 1회 → &lt;strong&gt;매일&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;파트너 업체 조회 가능 시점:&lt;/strong&gt; 매일 &lt;strong&gt;오전 11시&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 파트너 업체는 월 마감 이후가 아니라, 운영 중에도 매일 비용을 확인하고 관리할 수 있게 되었습니다.&lt;/p&gt;&lt;h3&gt;마치며&lt;/h3&gt;&lt;p&gt;정산 자동화의 진짜 가치는 단순한 시간 단축에 있지 않습니다. MASS는 정산을 사람의 경험과 기억이 아닌, 시스템의 책임으로 옮겼습니다.&lt;/p&gt;&lt;p&gt;이번 시리즈를 통해 정산이라는 어려운 도메인을 어떻게 설계하고, 구현하고, 운영으로 안착시켰는지를 공유했습니다. &lt;br&gt;MASS의 정산 자동화는 시작에 불과하며, 앞으로도 다양한 정산 도메인으로 확장해 나갈 예정입니다.&lt;/p&gt;&lt;h3&gt;Platform Business Operation 조직 및 팀 소개&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사 플랫폼 비즈니스 오퍼레이션 조직은 국내외 물류 서비스, 재고 관리, 스토어 운영을 위한 물류 프로덕트를 구축하고 다양한 오프라인 비즈니스 모델에 맞춘 스토어 관리 시스템을 개발·고도화하고 있습니다.&lt;br&gt;또한, 무배당발 서비스를 포함한 무신사의 차별화된 고객 경험을 브랜딩하고 확장할 수 있는 멤버십 구조를 설계하며, 온·오프라인을 넘나드는 통합 커머스 경험을 기술로 실현하고 있습니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;저희 팀은 OMS(주문관리시스템)를 기반으로 온라인 주문부터 재고·출고·배송·정산에 이르는 전 과정을 유기적으로 연결하고, 무신사의 다양한 오프라인 스토어를 효과적으로 운영할 수 있는 관리 시스템을 구축하여 고객이 온라인(무신사 스토어, 29CM 등)과 오프라인(무신사 스탠다드, 편집숍 등)에서 끊김 없는 쇼핑 경험을 누릴 수 있도록 지원합니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/pbo&quot;&gt;&lt;em&gt;🚀 Platform Business Operation 한걸음 더 알아보기&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4f09ae3bdf5d&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EB%8D%94-%EC%9D%B4%EC%83%81-%EB%B0%A4%EC%83%98%ED%95%98%EC%A7%80-%EC%95%8A%EC%95%84%EB%8F%84-%EB%90%A9%EB%8B%88%EB%8B%A4-%EC%9A%B4%EC%98%81%ED%8E%B8-4f09ae3bdf5d&quot;&gt;“이번 달도 밤샘 정산입니다.” — 더 이상 밤샘하지 않아도 됩니다 (운영편)&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Nx에서 Bun 더 잘 사용하기: Nx 18 -&gt; 21 마이그레이션</title>
      <link>http://thefarmersfront.github.io/blog/nx-bun-migration/</link>
      <guid>http://thefarmersfront.github.io/blog/nx-bun-migration/</guid>
      <pubDate>Fri, 30 Jan 2026 14:00:00 GMT</pubDate>
      <content:encoded>Nx가 Bun을 공식 지원하기 전에 도입했다면 마주칠 수 있는 문제들과 해결 방법</content:encoded>
    </item>
    <item>
      <title>“이번 달도 밤샘 정산입니다.” — 정산 시스템은 어떻게 만들었을까 (실전편)</title>
      <link>https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1?source=rss----f107b03c406e---4</guid>
      <pubDate>Tue, 27 Jan 2026 05:01:27 GMT</pubDate>
      <content:encoded>&lt;h3&gt;“이번 달도 밤샘 정산입니다.” — 정산 시스템은 어떻게 만들었을까 (실전편)&lt;/h3&gt;&lt;p&gt;“이번 달도 밤샘 정산입니다.” 테크 블로그 시리즈&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1&quot;&gt;&lt;strong&gt;정산 시스템은 어떻게 만들었을까 (실전편)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/22732a4a607f&quot;&gt;정산 시스템은 왜 필요했을까 (설계편)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/이번-달도-밤샘-정산입니다-더-이상-밤샘하지-않아도-됩니다-운영편-4f09ae3bdf5d&quot;&gt;더 이상 밤샘하지 않아도 됩니다 (운영편)&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;들어가며&lt;/h3&gt;&lt;p&gt;앞선 글에서는 정산 시스템이 왜 본질적으로 어려운 문제인지, 그리고 MASS가 멱등성과 결정적 계산이라는 설계 원칙을 선택한 이유를 살펴보았습니다. &lt;br&gt;이번 글에서는 그 설계가 실제로 어떤 기술 선택과 구조를 통해 구현되었는지, MASS 정산 시스템의 실전 구축 과정을 이야기합니다.&lt;/p&gt;&lt;h3&gt;멱등성을 전제로 한 이벤트 처리&lt;/h3&gt;&lt;p&gt;정산에 사용되는 원천 데이터는 이벤트 형태로 유입됩니다.&lt;br&gt;이벤트 기반 시스템에서 중복 수신이나 재처리는 피할 수 없는 상황이기 때문에, MASS에서는 이를 &lt;strong&gt;전제 조건&lt;/strong&gt;으로 두었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;이벤트 재시도(Retry)와 격리(DLT)를 분리해 처리&lt;/li&gt;&lt;li&gt;트랜잭션 식별자를 기준으로 서비스 레벨에서 멱등 갱신&lt;/li&gt;&lt;li&gt;동일 이벤트가 여러 번 처리되어도 결과는 항상 동일&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 장애 상황에서도 안전하게 재처리할 수 있는 기반을 마련했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*WVBTqEPRTnxzFRF5ux3o_Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;(⬆️ 1편 내용)&lt;/p&gt;&lt;h4&gt;DLT 메시지 모니터링과 처리 방식&lt;/h4&gt;&lt;p&gt;DLT로 전달된 이벤트는 &lt;strong&gt;정산 흐름에서 즉시 제외&lt;/strong&gt;되며, 별도의 모니터링과 운영 프로세스를 통해 관리됩니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DLT 발생 시 즉시 알림을 통해 운영자가 인지할 수 있도록 구성&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*jeNo9FnMmvUBNGb-NoSXtw.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;DLT 메시지는 원본 이벤트와 동일한 컨텍스트를 유지한 채 저장&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*EMx-OyThRv5VNbvZEdccLQ.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;이벤트 페이로드, 트랜잭션 식별자, 실패 사유를 기준으로 원인 분석 가능&lt;/li&gt;&lt;li&gt;정책 오류, 데이터 품질 문제 등 원인이 명확한 경우 기준 정리 후 재처리 수행&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/818/1*2u_Johv6F-ibbNFJZYBGDg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;이를 통해 &lt;strong&gt;문제 있는 이벤트가 전체 정산 흐름을 멈추는 상황을 방지&lt;/strong&gt;하면서도, 사후 분석과 재처리를 위한 정보는 모두 보존합니다.&lt;/p&gt;&lt;h4&gt;실제 운영 중 DLT 발생 현황&lt;/h4&gt;&lt;p&gt;시스템 오픈 이후 실제 운영 환경에서 발생한 DLT 이벤트는 전체 이벤트 대비 &lt;strong&gt;극히 낮은 비율&lt;/strong&gt;로 유지되고 있습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DLT 발생 비율: &lt;strong&gt;전체 이벤트 중 0.001% 미만&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;DLT로 격리된 이벤트 역시 정산 결과에는 영향을 주지 않음&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;정산과 같이 결과의 정합성이 중요한 도메인에서 “실패를 격리하되, 전체 흐름은 멈추지 않는다”는 설계 의도가 운영 단계에서도 그대로 유지되고 있습니다.&lt;/p&gt;&lt;h3&gt;MASS의 모듈 구성&lt;/h3&gt;&lt;p&gt;MASS는 크게 세 개의 독립적인 애플리케이션 모듈로 나뉩니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*CiuUuW5tzyBNchxaGptz4A.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;mass-consumer&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;물류 서비스 유형별 원천 데이터를 메시지로 수신하는 애플리케이션&lt;/li&gt;&lt;li&gt;Kafka를 통해 이벤트를 소비&lt;/li&gt;&lt;li&gt;멱등 처리 후 정산 원천 데이터로 저장&lt;/li&gt;&lt;li&gt;이벤트 기반 처리 영역의 진입점 역할&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;mass-batch&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;일/월 정산을 수행하는 배치 애플리케이션&lt;/li&gt;&lt;li&gt;정산 집계, 마감, 리포트 생성&lt;/li&gt;&lt;li&gt;Kafka 복구 잡 등 운영 유틸리티 배치 포함&lt;/li&gt;&lt;li&gt;재처리와 복구를 전제로 한 실행 구조&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;mass-api&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;내부 운영자와 파트너 업체를 위한 REST API 제공&lt;/li&gt;&lt;li&gt;정산, 프로모션 등의 데이터 조회 및 관리 기능&lt;/li&gt;&lt;li&gt;정산 결과를 외부에 안전하게 노출하는 인터페이스&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;재처리와 복구를 전제로 한 실행 구조&lt;/h4&gt;&lt;p&gt;정산 배치는 실패 가능성을 전제로 설계되었습니다.&lt;br&gt;중요한 것은 배치가 실패했을 때 &lt;strong&gt;어디서부터 다시 실행할 수 있는가&lt;/strong&gt;가 아니라, &lt;strong&gt;어떤 상태를 기준으로 정산을 다시 정의할 수 있는가&lt;/strong&gt;였습니다.&lt;/p&gt;&lt;p&gt;MASS에서는 이를 위해 정산 대상 원천 데이터에 명시적인 &lt;strong&gt;정산 상태 컬럼&lt;/strong&gt;을 두고 배치 실행 흐름을 관리합니다.&lt;/p&gt;&lt;p&gt;정산 상태는 다음과 같이 관리됩니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;PENDING&lt;/strong&gt;: 정산 대상 후보 상태&lt;/li&gt;&lt;li&gt;&lt;strong&gt;PROCESSING&lt;/strong&gt;: 현재 정산 배치에서 처리 중인 상태&lt;/li&gt;&lt;li&gt;&lt;strong&gt;COMPLETED&lt;/strong&gt;: 정산이 정상 완료된 상태&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;strong&gt;배치 실행 흐름과 상태 전이&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;정산 배치는 다음 순서로 수행됩니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;배치 시작 시점에 해당 실행의 정산 대상 원천 데이터만 &lt;strong&gt;PENDING → PROCESSING&lt;/strong&gt; 상태로 마킹&lt;/li&gt;&lt;li&gt;마킹된 데이터만을 기준으로 step 별 &lt;strong&gt;chunk 단위 처리&lt;/strong&gt; 수행&lt;/li&gt;&lt;li&gt;모든 step이 정상 완료되면 처리된 데이터의 상태를 &lt;strong&gt;COMPLETED&lt;/strong&gt;로 전이&lt;/li&gt;&lt;li&gt;배치가 중간에 실패할 경우 해당 실행에서 &lt;strong&gt;PROCESSING 상태로 마킹되었던 데이터들을 다시 PENDING으로 롤백&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이 구조를 통해 배치는 chunk 단위로 실행되지만, &lt;strong&gt;복구와 재처리의 기준은 항상 “정산 대상 전체”로 유지&lt;/strong&gt;됩니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*v7YCvHYRi9pg9BFpq3F6Vw.png&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;strong&gt;재시도 시 처리 방식&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;배치 재시도 시에는 이전 실행 결과를 이어서 처리하지 않습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;이전 실행에서 PROCESSING 상태였던 데이터는 모두 롤백&lt;/li&gt;&lt;li&gt;재시도 시점에 다시 정산 대상 데이터를 선정해 &lt;strong&gt;PROCESSING으로 재마킹&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;이후 멱등성을 전제로 한 &lt;strong&gt;upsert 방식&lt;/strong&gt;으로 정산 재수행&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 재실행 시에도&lt;/p&gt;&lt;ul&gt;&lt;li&gt;중복 정산 없이&lt;/li&gt;&lt;li&gt;동일한 기준으로&lt;/li&gt;&lt;li&gt;항상 동일한 결과를 얻을 수 있습니다.&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;strong&gt;실제 재처리 사례: 12월 3일 재고 스냅샷 이슈&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;12월 3일, upstream 시스템의 재고 스냅샷 이슈로 인해 해당 일자의 정산 데이터에 대해 재처리가 필요했습니다.&lt;/p&gt;&lt;p&gt;이 경우 MASS에서는&lt;/p&gt;&lt;ul&gt;&lt;li&gt;12월 3일 정산 대상 원천 데이터만 &lt;strong&gt;PENDING 상태로 롤백&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;이슈 해결 후 동일 날짜의 정산 재실행&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;재처리는 chunk 단위로 수행되었지만, 정산 대상 상태를 기준으로 재정의했기 때문에 중복 반영이나 부분 정산 없이 &lt;strong&gt;정산 결과를 처음부터 다시 계산&lt;/strong&gt;할 수 있었습니다.&lt;/p&gt;&lt;p&gt;그 결과,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;다른 날짜의 정산 결과에는 영향을 주지 않았고&lt;/li&gt;&lt;li&gt;재처리 이후에도 &lt;strong&gt;정산 금액은 기존 기준과 동일하게 유지&lt;/strong&gt;되었습니다.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;이벤트 + 배치 하이브리드 구조&lt;/h3&gt;&lt;p&gt;MASS는 정산 도메인의 특성에 맞게, &lt;strong&gt;이벤트 기반 처리와 배치 기반 처리를 목적에 따라 분리한 하이브리드 구조&lt;/strong&gt;로 설계되어 있습니다.&lt;/p&gt;&lt;p&gt;실시간에 가까운 데이터 반영이 필요한 영역과, 회계 마감처럼 안정성과 재현성이 중요한 영역의 요구사항이 다르기 때문입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*cGARaSH-ca10vR3LYjUU1g.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;이벤트 기반 처리 영역&lt;/h4&gt;&lt;p&gt;이벤트 기반 영역은 &lt;strong&gt;정산의 ‘원천 데이터’를 책임지는 영역&lt;/strong&gt;입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;물류 서비스 유형별로 발생하는 원천 이벤트를 수신&lt;/li&gt;&lt;li&gt;이벤트 중복이나 재처리를 고려한 멱등 저장&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 영역은 &lt;em&gt;데이터를 빠르게 수집&lt;/em&gt;하는 데 초점을 둡니다.&lt;/p&gt;&lt;h4&gt;배치 기반 처리 영역&lt;/h4&gt;&lt;p&gt;배치 기반 영역은 &lt;strong&gt;정산의 ‘확정’과 ‘마감’을 책임지는 영역&lt;/strong&gt;입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;일/월 단위 정산 대상 집계&lt;/li&gt;&lt;li&gt;정산 마감 처리&lt;/li&gt;&lt;li&gt;정산 리포트 생성&lt;/li&gt;&lt;li&gt;실패 시 재처리를 고려한 복구용 배치 운영&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 영역은 속도보다는 &lt;strong&gt;안정성과 재현성&lt;/strong&gt;을 우선하며, 동일 조건에서 언제 실행해도 같은 결과를 내는 것을 목표로 합니다.&lt;/p&gt;&lt;h3&gt;왜 이런 구조를 선택했는가&lt;/h3&gt;&lt;p&gt;이 구조를 통해 MASS는 다음 두 가지를 동시에 만족시킬 수 있었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;이벤트 기반 구조로 &lt;strong&gt;실시간에 가까운 데이터 가시성 확보&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;배치 기반 구조로 &lt;strong&gt;회계 마감의 안정성과 재현성 보장&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;결과적으로 MASS는 &lt;strong&gt;“빠르게 변하는 데이터”와 “확정되어야 하는 숫자”를 하나의 시스템 안에서 충돌 없이 다룰 수 있는 구조&lt;/strong&gt;를 갖추게 되었습니다.&lt;/p&gt;&lt;h3&gt;기술 선택과 그 배경&lt;/h3&gt;&lt;p&gt;앞선 섹션에서 MASS가 이벤트 기반과 배치 기반을 혼합한 구조를 선택한 이유를 설명했습니다.&lt;br&gt;이 섹션에서는 그 구조를 실제로 구현하기 위해 &lt;strong&gt;어떤 기술 스택을 선택했고, 그 선택이 정산 도메인에 왜 적합했는지&lt;/strong&gt;를 정리합니다.&lt;/p&gt;&lt;p&gt;MASS에서의 기술 선택은 “어떤 기술이 더 최신인가”가 아니라 &lt;strong&gt;정산이라는 도메인의 핵심 요구사항인 ‘정합성, 재처리 가능성, 운영 안정성’을 얼마나 잘 만족시키는가&lt;/strong&gt;를 기준으로 이루어졌습니다.&lt;/p&gt;&lt;h4&gt;전체 기술 스택 요약&lt;/h4&gt;&lt;p&gt;MASS는 JVM 기반의 안정적인 기술 스택 위에서 다음과 같이 구성되어 있습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;언어 / 런타임&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;JVM 기반 (Kotlin)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;애플리케이션 프레임워크&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Spring Boot&lt;/li&gt;&lt;li&gt;Spring Batch (정산 배치 및 마감 처리)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;메시징&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Kafka (원천 데이터 이벤트 수신)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;데이터베이스&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MySQL (정산 결과의 Source of Truth)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;운영 / 모니터링&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;배치 실행 상태 및 실패 로그 기반 모니터링&lt;/li&gt;&lt;li&gt;DLT 이벤트 및 배치 실패 시 알림 연계&lt;/li&gt;&lt;li&gt;에러, Latency, 인프라 모니터링&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 스택은 고성능보다는 &lt;strong&gt;예측 가능성, 재현성, 장애 대응 용이성&lt;/strong&gt;을 우선한 선택입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*VN591bZIeeRGbdtCTpbxaw.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Kafka를 통한 원천 데이터 수신&lt;/h4&gt;&lt;p&gt;정산에 사용되는 원천 데이터는 다음과 같은 특성을 가집니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;입·출고, 재고 스냅샷 등 &lt;strong&gt;트래픽이 특정 시점에 집중&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;동일 이벤트의 &lt;strong&gt;중복 수신, 재전송, 재처리 가능성&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;일시적인 장애가 있더라도 &lt;strong&gt;데이터 유실은 절대 허용 불가&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이러한 특성 때문에, 원천 데이터를 API 호출 방식으로 동기 수신하는 구조는 적합하지 않다고 판단했습니다.&lt;/p&gt;&lt;p&gt;Kafka 기반 이벤트 수신 방식은&lt;/p&gt;&lt;ul&gt;&lt;li&gt;원천 데이터를 &lt;strong&gt;비동기로 안정적으로 적재&lt;/strong&gt;할 수 있고&lt;/li&gt;&lt;li&gt;소비자 장애와 무관하게 이벤트를 보존할 수 있으며&lt;/li&gt;&lt;li&gt;동일 이벤트를 다시 소비하는 방식으로 &lt;strong&gt;재처리를 자연스럽게 지원&lt;/strong&gt;합니다&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;정산 데이터는 “빨리 처리되는 데이터”보다 &lt;strong&gt;“언제든 다시 처리할 수 있어야 하는 데이터”&lt;/strong&gt;였기 때문에, MASS는 Kafka를 원천 데이터 수신 방식으로 선택했습니다.&lt;/p&gt;&lt;h4&gt;Spring Batch 기반의 정산 처리&lt;/h4&gt;&lt;p&gt;정산 처리에서 가장 중요한 요구사항은 실시간성이 아니라 다음과 같았습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;회계 마감 기준에 맞는 &lt;strong&gt;명확한 실행 시점&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;실패를 전제로 한 &lt;strong&gt;재처리·복구 구조&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;실행 이력과 결과를 추적할 수 있는 &lt;strong&gt;감사 가능성&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 위해 정산 집계와 마감 처리는 Spring Batch 기반으로 설계했습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;배치 실행 환경과 제어 방식&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;MASS는 Kubernetes 기반의 &lt;strong&gt;EKS 환경&lt;/strong&gt;에서 운영되고 있으며, 정산 배치 역시 이 환경에 맞는 실행 구조를 갖도록 설계했습니다.&lt;/p&gt;&lt;p&gt;정산 배치는 애플리케이션 내부 스케줄러에 의해 자동 실행되는 방식이 아니라,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Spring Batch 기반의 batch 애플리케이션&lt;/strong&gt;을 컨테이너로 구성하고&lt;/li&gt;&lt;li&gt;배치 실행 시점과 흐름은 &lt;strong&gt;Argo Workflow&lt;/strong&gt;를 통해 외부에서 제어하는 방식으로 운영됩니다&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;즉, Spring Batch는 &lt;em&gt;정산 로직과 실행 상태 관리&lt;/em&gt;를 담당하고, 배치의 실행 시점과 재실행 제어는 &lt;strong&gt;Argo Workflow가 책임지는 구조&lt;/strong&gt;입니다.&lt;/p&gt;&lt;p&gt;이를 통해 정산 배치는 “코드 안에 숨어 있는 백그라운드 작업”이 아니라, &lt;strong&gt;EKS 환경에서 명시적으로 관리되는 워크플로우 단계&lt;/strong&gt;가 되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/496/1*LAdmMX51xIOBKN3RHUGWRg.png&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;strong&gt;Job Parameter 기반 실행&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;Argo Workflow는 정산 배치를 실행할 때,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 기준 날짜&lt;/li&gt;&lt;li&gt;정산 기준 타입&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;등을 &lt;strong&gt;Job Parameter로 전달&lt;/strong&gt;합니다.&lt;/p&gt;&lt;p&gt;이를 통해 MASS의 정산 배치는&lt;/p&gt;&lt;ul&gt;&lt;li&gt;동일한 코드로&lt;/li&gt;&lt;li&gt;서로 다른 기간과 기준을&lt;/li&gt;&lt;li&gt;실행 이력으로 명확히 구분된 형태로 수행할 수 있습니다&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;“어떤 기준으로, 언제 실행된 정산인지”가 로그가 아니라 &lt;strong&gt;Batch Execution 이력 자체로 추적 가능&lt;/strong&gt;해졌습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;Spring Batch 선택 이유와 실행 모델&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;Spring Batch는&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Job / Step / Execution 단위로 &lt;strong&gt;실행 상태와 이력 관리&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;대용량 데이터를 &lt;strong&gt;chunk 단위로 안정적으로 처리&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;실패 시 &lt;strong&gt;재실행 시점 제어&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;를 기본적으로 제공합니다.&lt;/p&gt;&lt;p&gt;이 특성은 정산 배치를 “한 번 실행하고 끝나는 작업”이 아니라, &lt;strong&gt;실패와 재실행을 전제로 운영되는 작업&lt;/strong&gt;으로 만드는 데 적합했습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;정산 대상 마킹 기반 실행 흐름&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;MASS에서는 배치 실행 시 다음과 같은 흐름을 따릅니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;배치 시작 시 해당 실행의 &lt;strong&gt;정산 대상 원천 데이터만 선별&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;선별된 데이터의 정산 상태를 PENDING → PROCESSING으로 일괄 마킹&lt;/li&gt;&lt;li&gt;마킹된 데이터만을 기준으로 Step 별 &lt;strong&gt;chunk 단위 처리 수행&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;모든 Step이 정상 완료되면 정산 상태를 COMPLETED로 전이&lt;/li&gt;&lt;li&gt;중간 실패 시 PROCESSING 상태 데이터를 다시 PENDING으로 롤백&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이 구조를 통해 배치는 chunk 단위로 실행되지만, &lt;strong&gt;재처리의 기준은 항상 ‘정산 대상 전체’로 유지&lt;/strong&gt;됩니다.&lt;/p&gt;&lt;p&gt;즉, 배치가 어느 지점에서 실패하더라도 이전 실행 결과에 의존하지 않고 정산 대상 단위로 &lt;strong&gt;처음부터 다시 실행할 수 있는 구조&lt;/strong&gt;를 만들었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*85NH0_qvf4s9kq_9hal8KQ.png&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;strong&gt;운영 관점에서의 의미&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;Spring Batch와 Argo Workflow를 결합한 이 실행 환경을 통해, 정산 배치는&lt;/p&gt;&lt;ul&gt;&lt;li&gt;EKS 환경에서 &lt;strong&gt;운영자가 명시적으로 제어 가능한 작업&lt;/strong&gt;이 되었고&lt;/li&gt;&lt;li&gt;실행 실패 역시 &lt;strong&gt;운영 이벤트로 인지하고 대응할 수 있는 대상&lt;/strong&gt;이 되었으며&lt;/li&gt;&lt;li&gt;재실행이 두려운 배치가 아니라 &lt;strong&gt;언제든 다시 돌릴 수 있는 배치&lt;/strong&gt;가 되었습니다&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;그 결과 MASS의 정산 배치는 회계 마감이라는 높은 안정성이 요구되는 영역에서도 안전하게 운영될 수 있는 기반을 갖추게 되었습니다.&lt;/p&gt;&lt;h4&gt;데이터베이스 선택과 정합성 보장&lt;/h4&gt;&lt;p&gt;정산 결과는 반드시 &lt;strong&gt;하나의 기준(Source of Truth)&lt;/strong&gt;으로 관리되어야 했기 때문에, MASS는 트랜잭션 처리가 명확한 &lt;strong&gt;RDB(MySQL)&lt;/strong&gt;를 정산 결과 저장소로 선택했습니다.&lt;/p&gt;&lt;p&gt;RDB를 선택함으로써 다음을 명확히 할 수 있었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;정산 결과의 &lt;strong&gt;정합성 보장&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;트랜잭션 단위의 상태 전이 관리&lt;/li&gt;&lt;li&gt;트랜잭션 식별자를 기준으로 한 &lt;strong&gt;멱등성 갱신(upsert)&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이는 이벤트 중복, 재처리, 장애 복구 상황에서도 정산 결과가 흔들리지 않도록 하는 핵심 기반이 되었습니다.&lt;/p&gt;&lt;h4&gt;운영 / 모니터링 체계&lt;/h4&gt;&lt;p&gt;정산 시스템에서 운영과 모니터링은 “장애를 없애는 것”보다 &lt;strong&gt;장애를 빠르게 인지하고, 영향 범위를 통제하는 것&lt;/strong&gt;이 더 중요했습니다.&lt;/p&gt;&lt;p&gt;MASS에서는 이를 위해 실시간 모니터링과 함께 &lt;strong&gt;주기적인 점검을 결합한 운영 체계&lt;/strong&gt;를 구성했습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;배치 실행 상태 및 실패 로그 기반 모니터링&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;일/월 정산 배치의 실행 성공 여부&lt;/li&gt;&lt;li&gt;Step 단위 실패 지점 및 재시도 여부&lt;/li&gt;&lt;li&gt;마감 미완료 상태에 대한 조기 감지&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;DLT 이벤트 및 배치 실패 시 알림 연계&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;재시도 한계를 초과한 이벤트는 DLT로 격리&lt;/li&gt;&lt;li&gt;DLT 발생 및 배치 실패 시 즉시 알림을 통해 인지&lt;/li&gt;&lt;li&gt;장애 상황에서도 원천 데이터 유실 없이 후속 조치 가능&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;에러, Latency, 인프라 지표 모니터링&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;이벤트 소비 지연(Lag) 및 처리 Latency 관측&lt;/li&gt;&lt;li&gt;애플리케이션 에러율과 비정상 트래픽 감지&lt;/li&gt;&lt;li&gt;인프라 리소스(CPU, Memory) 사용량을 통한 병목 사전 인지&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;주 단위 시스템 상태 점검&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;주 단위로 트래픽, 에러, Latency, 인프라 지표 추이 점검&lt;/li&gt;&lt;li&gt;잠재적인 성능 저하나 이상 징후를 사전에 식별하고 개선&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이러한 운영 체계를 통해 MASS는 장애가 발생한 이후에 대응하는 방식이 아니라, &lt;strong&gt;문제가 되기 전에 신호를 감지하고 선제적으로 조치하는 운영 방식&lt;/strong&gt;을 갖추게 되었습니다.&lt;/p&gt;&lt;h3&gt;마치며&lt;/h3&gt;&lt;p&gt;정산 시스템의 구현에서 중요한 것은 최신 기술의 조합이 아니라, 실패와 재실행을 자연스럽게 받아들이는 구조였습니다. &lt;br&gt;MASS는 Kafka, Spring Batch, Argo Workflow를 통해 정산을 안정적으로 구현했고, 실패해도 다시 실행할 수 있는 시스템을 만들 수 있었습니다.&lt;/p&gt;&lt;p&gt;다음 글에서는 이 시스템을 어떻게 단계적으로 오픈했고, 실제 운영에서 어떤 변화와 성과를 만들었는지 살펴보겠습니다.&lt;/p&gt;&lt;h3&gt;Platform Business Operation 조직 및 팀 소개&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사 플랫폼 비즈니스 오퍼레이션 조직은 국내외 물류 서비스, 재고 관리, 스토어 운영을 위한 물류 프로덕트를 구축하고 다양한 오프라인 비즈니스 모델에 맞춘 스토어 관리 시스템을 개발·고도화하고 있습니다.&lt;br&gt;또한, 무배당발 서비스를 포함한 무신사의 차별화된 고객 경험을 브랜딩하고 확장할 수 있는 멤버십 구조를 설계하며, 온·오프라인을 넘나드는 통합 커머스 경험을 기술로 실현하고 있습니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;저희 팀은 OMS(주문관리시스템)를 기반으로 온라인 주문부터 재고·출고·배송·정산에 이르는 전 과정을 유기적으로 연결하고, 무신사의 다양한 오프라인 스토어를 효과적으로 운영할 수 있는 관리 시스템을 구축하여 고객이 온라인(무신사 스토어, 29CM 등)과 오프라인(무신사 스탠다드, 편집숍 등)에서 끊김 없는 쇼핑 경험을 누릴 수 있도록 지원합니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/pbo&quot;&gt;&lt;em&gt;🚀 Platform Business Operation 한걸음 더 알아보기&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=74d8a5d22ba1&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1&quot;&gt;“이번 달도 밤샘 정산입니다.” — 정산 시스템은 어떻게 만들었을까 (실전편)&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>2025년 제 1회 29QA Con 진행 후기 (29QA Conference)</title>
      <link>https://techblog.musinsa.com/2025%EB%85%84-%EC%A0%9C-1%ED%9A%8C-29qa-con-%EC%A7%84%ED%96%89-%ED%9B%84%EA%B8%B0-29qa-conference-610644aaf27b?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/2025%EB%85%84-%EC%A0%9C-1%ED%9A%8C-29qa-con-%EC%A7%84%ED%96%89-%ED%9B%84%EA%B8%B0-29qa-conference-610644aaf27b?source=rss----f107b03c406e---4</guid>
      <pubDate>Mon, 26 Jan 2026 22:02:56 GMT</pubDate>
      <content:encoded>&lt;p&gt;29CM QE팀은 연말에 팀 자체적으로 Conference를 진행하였습니다.&lt;/p&gt;&lt;p&gt;2024년까지는 연 2회 워크샵을 진행해서 각자 레슨런을 공유하는 자리를 가졌는데 2025년에는 상반기 워크샵을 진행하지 못하여 하반기에만 진행하게 되었고 이렇게 된 김에 연 행사처럼 고유의 컨퍼런스를 개최해 보자는 생각에 29QA Con을 계획하였습니다.&lt;/p&gt;&lt;p&gt;나중에는 점점 규모가 커져서 다른 회사의 QA 분들도 모시고 싶다고 생각해서 처음부터 어느 정도 형식을 갖추자는 판단을 했습니다. 그래서 굿즈도 만들고 홍보 배너도 만들었는데 만들고 나니 정말 컨퍼런스 분위기가 물씬 풍겨 진행하기를 잘했다는 생각이 들었습니다.&lt;/p&gt;&lt;p&gt;한 달가량의 촉박한 일정이었지만 4명의 팀원이 3개 이상씩의 세션을 준비해서 총 13개의 세션이 진행되었습니다. 짧은 기간 동안 열심히 양질의 자료를 만들어서 공유해 준 팀원분들 덕분에 훌륭한 하나의 Conference가 진행될 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*a_kRGeIIOTyukVPti6WlgA.jpeg&quot; /&gt;&lt;figcaption&gt;컨퍼런스 느낌이 나도록 X배너도 제작해서 걸어두었습니다.&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*FBDeSTwJS4pZ3GP3eITbjA.jpeg&quot; /&gt;&lt;figcaption&gt;팀 마스코트인 ‘큐엉이&amp;#39; 가 포함된 굿즈도 여러개 제작하였습니다. 스티커 빼고 완판(?) 되었답니다.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;식순은 아래와 같이 진행되었습니다. &lt;br&gt;발표 15분, 쉬는 시간 5분 정도를 계획해 두었습니다.&lt;/p&gt;&lt;pre&gt;1.  29CM에서의 팀 셋팅, 지금의 신뢰받는 QE팀은 어떻게 만들어졌나 (박현준)&lt;br&gt;2.  차세대 테스트 자동화 - Vibium(조진현)&lt;br&gt;3.  25년 자동화 유지보수 여정 (정다정)&lt;br&gt;4.  iOS 자동화 1년 여정 + 코드리뷰의 중요성 (강보민)&lt;br&gt;5.  귀찮음을 해결했더니 팀이 빨라졌다: QA 업무 자동화 사례 (정다정)&lt;br&gt;6.  아무도 궁금하지 않은 QA Weekly 작성 vlog (박현준)&lt;br&gt;7.  혼자 할 때는 몰랐던 것들: 2인 이상의 QA로 얻은 교훈 (강보민)&lt;br&gt;8.  질문을 잘하는 것이 곧 살아남는 방법이다 (조진현)&lt;br&gt;9.  눈물과 분노없이 볼 수 없는 Cursor를 사용한 29TMS 제작기 (박현준)&lt;br&gt;10. 대 AI 시대 Testcase 생성 찍먹해보기 (강보민)&lt;br&gt;11. 글로벌 서비스 QA 시에는 무엇이 달라지나 (정다정)&lt;br&gt;12. 2025년 회고 (조진현)&lt;br&gt;13. 2025년 한해 돌아보기 (+팀 회고) (박현준)&lt;/pre&gt;&lt;p&gt;하지만 발표가 시작되면 어김없이 기존 발표시간이 초과되는 사태가 발생하여서 발표자분들의 열정을 느낄 수 있었지만 시간관리에는 어려움이 있었습니다. 😅&lt;/p&gt;&lt;p&gt;쉬는시간을 타이트하게 가져가면서 열심히 진행했던 기억이 남습니다.&lt;/p&gt;&lt;p&gt;1️⃣ 첫번째 세션으로는 &lt;br&gt;“29CM에서의 팀 셋팅, 지금의 신뢰받는 QE팀은 어떻게 만들어졌나” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;제가 처음으로 29CM에 입사하여 QA팀을 신설하고 지금의 조직으로 만들기까지의 과정을 팀원분들께 공유하는 시간이었습니다. 초반에 조직의 신뢰를 얻기 위해서 결과로 증명하기 위한 노력과 이후 팀 방향성을 견고히 하기 위해 어떤 액션들을 했는지에 대한 과정들을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*N4w85aXBj-lebCKf2w1wyw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;2️⃣ 두번째 세션으로는 &lt;br&gt;“차세대 테스트 자동화 — Vibium” 이 진행되었습니다.&lt;/p&gt;&lt;p&gt;Selenium을 세상에 나오게 하여 테스트 자동화의 발전을 가속화 시킨 Jason Huggins에 대한 이야기와 그가 현재 개발하고 있는 Vibium은 어떤 것이고 어떤 것이 가능하게 되는지에 대한 것을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*5eTtGCw4N-n0VjT4J316qQ.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;3️⃣ 세번째 세션으로는 &lt;br&gt;“25년 자동화 유지보수 여정” 이 진행되었습니다.&lt;/p&gt;&lt;p&gt;29CM에서는 App 테스트 자동화를 2023년부터 운영하고 있는데 이 과정에서 점점 시나리오는 증가하고 기술 복잡도가 증가함으로 인해서 여러 가지 자동화 Fail 건들이 발생했습니다. 이 중에 주요 원인 3대장을 분석하였고 이를 해결하여 Fail율을 극적으로 낮출 수 있었던 과정을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*XHFZKjJC2YR9zZGYQj5tWA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;4️⃣ 네번째 세션으로는 &lt;br&gt;“iOS 자동화 1년 여정 + 코드 리뷰의 중요성” 이 진행되었습니다.&lt;/p&gt;&lt;p&gt;보민님은 최근에 iOS 자동화 Owner가 되시면서 어떻게 하면 더 효율적이고 개선된 환경으로 유지보수를 할 수 있는지에 대한 고민을 많이 하셨습니다. &lt;br&gt;그 과정에서 코드 리뷰의 중요성을 느끼시고 그곳에서 받은 도움과 효과를 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*EhOzw8F7BhlRQlrEn1hlYQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;🍚 원래는 다섯번째 세션 완료 후 점심시간이였지만 시간이 좀 지나 점심시간 확보를 위해 약간 빠르게 점심식사를 하러 이동하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/816/1*2E5uOfWSn2jwqlOmg009tw.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/917/1*jHMYW29UTBmjlM0lY6Rcqg.png&quot; /&gt;&lt;figcaption&gt;냠냠냠!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;5️⃣ 다섯번째 세션으로는 &lt;br&gt;“귀찮음을 해결했더니 팀이 빨라졌다: QA 업무 자동화 사례” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;QE팀은 현재 다양한 Slack bot을 사용 중에 있습니다. 테스트 결과 리포트를 도와주는 Daily Report Bot과 Google 문서들을 공유해 주는 Bot등 여러 가지의 Bot이 있는데, 이 Bot들을 개발하면서 진행하게 된 개선 활동에 대한 레슨런을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*EUy-GsVpgGAtwMQSpbAZPw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;6️⃣ 여섯번째 세션으로는 &lt;br&gt;“아무도 궁금하지 않은 QA Weekly 작성 vlog” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;제가 매주 작성하고 있는 QA Weekly에 관한 내용입니다. 이것을 작성하기 위해 제가 어떤 과정을 진행하고 있는지에 대한 이야기였는데, 모두 궁금하지는 않았겠지만 의외로(?) 많은 시간과 노력이 들어가고 있다는 내용을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*A0Q8OS4seRa3dgPVJIy6OQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;7️⃣ 일곱번째 세션으로는 &lt;br&gt;“혼자 할 때는 몰랐던 것들: 2인 이상의 QA로 얻은 교훈” 이 진행되었습니다.&lt;/p&gt;&lt;p&gt;작년 상반기까지만 해도 1인 QA로 진행하는 업무들이 많았습니다. 하반기부터는 외주 QA분들과 함께하게 되면서 이제 2인 이상 QA업무를 같이 진행하는 경우가 많이 생겼는데, 이러한 과정에서 어떤 레슨런이 있었는지에 대한 내용을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*-XjOLHm2jIC0bnr3uE0DYQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;8️⃣ 여덟번째 세션으로는 &lt;br&gt;“질문을 잘하는 것이 곧 살아남는 방법이다” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;사내에서도 AI을 적극적으로 사용하는 것을 권장하기 때문에 작년에 QE팀도 다양한 AI 도구들을 사용할 수 있었습니다. 이 과정에서 프롬프트 엔지니어링에 대해 고민을 하였고, 내가 원하는 방향의 답변과 결과를 얻기 위해서 어떠한 질문들을 해야 하는지에 대한 고민과 연구를 진행한 내용을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*359U7U9nuXb0nrLMQK0TVA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;9️⃣ 아홉번째 세션으로는 &lt;br&gt;“눈물과 분노 없이 볼 수 없는 Cursor를 사용한 29TMS 제작기” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;이건 아쉽게도 영상으로 찍히지 않아서 세션 진행 자료가 없습니다. 😢 그래서 발표 자료로 대체해야 할 것 같습니다.&lt;/p&gt;&lt;p&gt;AI 에이전트 관련 세션인 만큼 발표자료는 AI를 활용한 이미지 생성으로 진행하였는데, 각 사례에 맞는 이미지를 생성하면서 제가 생각한 이미지가 정확히 나왔을때 즐거워하며 작업던 기억이 있습니다.&lt;/p&gt;&lt;p&gt;테스트케이스 관리 도구의 불편함을 개선하기 위해 초반에는 Cursor로 개발을 시작하였고 후반부에는 Claude code로 전환하여 개발을 완료하게 된 29TMS (Testcase Management System)에 관련된 이야기입니다. 현재는 1.9 버전이 업데이트 되어서 초기에 비해 사용성이 대폭 증가하였고 3개월 넘는 기간 동안 실무에서 잘 사용중입니다.&lt;/p&gt;&lt;blockquote&gt;관련 블로그: &lt;a href=&quot;https://techblog.musinsa.com/ai와의-성공적인-첫-co-work-바이브-코딩으로-탄생된-맞춤형-testcase-management-system-29tms-74062a620119&quot;&gt;AI와의 성공적인 첫 Co Work — 바이브 코딩으로 탄생된 맞춤형 Testcase Management System (29TMS)&lt;/a&gt;&lt;/blockquote&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/996/1*IPlQRzqXZX5tsIpuPNkd_w.png&quot; /&gt;&lt;figcaption&gt;나노바나나야 고마워&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;🔟 열번째 세션으로는 &lt;br&gt;“대 AI 시대 Testcase 생성 찍먹해보기” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;3가지의 생성형 AI 도구를 대상으로 테스트케이스 생성에 대한 품질 테스트를 진행한 내용입니다. 어떤 AI는 어느 부분에 강점이 있었고 최종적으로는 어떤 AI 도구가 가장 높은 점수의 테스트케이스 생성 능력을 보여주었는지에 대한 과정을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*WDYEbaAPJPl1EQN3RvYmvA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;1️⃣1️⃣ 열한번째 세션으로는&lt;br&gt;“글로벌 서비스 QA 시에는 무엇이 달라지나” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;이전 회사에서 경험했었던 글로벌 서비스에 대한 경험을 가지고 실사례를 기반으로 하여 우리 서비스가 글로벌 진출을 하게 된다면 어떤 것을 고려해야 하고 유의해야 하는지에 대한 내용과 진행하면서 어려웠던 점들을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lcfT8oVoAKvK63gvlbaG4A.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;1️⃣2️⃣ 열두번째 세션으로는&lt;br&gt;“2025년 회고 (조진현)” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;2024년 9월 입사 이후 2025년은 온전한 1년을 모두 보낸 해였습니다. 2025년에는 어떤 경험과 성장을 이루었는지 한해를 돌아보며 회고한 내용을 공유하였습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*MIhsntTkY0Ayl_dVSUu_IQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;1️⃣3️⃣ 열세번째 마지막 세션으로는&lt;br&gt;“2025년 한해 돌아보기 (+팀 회고)” 가 진행되었습니다.&lt;/p&gt;&lt;p&gt;2025년 저는 어떻게 팀을 운영하며 개인에 대한 성장을 이뤄냈고 어떠한 변화를 맞이하여 그것에 적응하고 또 팀을 운영해 나갔는지에 대한 내용을 공유하였습니다. 많은 일들이 있었던 한 해였던 것 같습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*2toHaPvkWj43c5eNl074vw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;이렇게 준비한 모든 세션이 완료되고 준비한 모든 팀원분들이 서로에게 박수를 보내며 제1회 29QA Con은 마무리가 되었습니다.&lt;/p&gt;&lt;p&gt;인원수는 많지 않았지만 풍부한 세션과 레슨런이 있었고 중간에 참석해서 자리를 빛내주신 MUSINSA QE팀 분들 덕분에 더 풍부한 컨퍼런스가 될 수 있었던것 같습니다.&lt;/p&gt;&lt;p&gt;바쁘신 와중에 참석해주신 MUSINSA QE팀 분들 감사합니다 :)&lt;/p&gt;&lt;p&gt;저희 QE팀은 연말에 고유 행사가 있습니다. 제가 팀원분들께 연말 선물을 드리는 것인데요 2023년은 장식용 캘린더, 2024년은 드래곤볼 7성구 (소원이루시라는 뜻으로 ^^), 그리고 2025년은 각자의 얼굴을 팝아트로 다시 그려낸 캔버스 그림을 선물로 드렸습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*3HOVjowbc36PfzHj8I2LRA.png&quot; /&gt;&lt;figcaption&gt;짠&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;그리고는 맛있는 저녁회식을 떠났습니다.&lt;/p&gt;&lt;p&gt;모두 열심히 준비하고 진행해준 만큼 회식도 더 맛있고 즐거웠을것으로 예상해봅니다 :)&lt;/p&gt;&lt;p&gt;올해에도 잘 준비해서 다른 QA분들이 발표도 하실수 있고, 참여도 하실 수 있는 행사로 만들어 보도록 하겠습니다.&lt;/p&gt;&lt;h3&gt;TEAM MUSINSA CAREER&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사는 2001년 온라인 커뮤니티로 시작해 2005년 무신사 매거진, 2009년 무신사 스토어를 오픈하며 빠르게 성장하고 있는 국내 대표 온라인 패션 스토어입니다. ‘입점 브랜드와 동반성장’이라는 경영 철학을 바탕으로 브랜드가 안정적으로 사업을 전개할 수 있도록 무신사가 보유한 노하우와 인프라를 지원합니다. 고객에게는 풍성한 패션 콘텐츠와 패션에 특화된 차별화된 서비스로 최상의 온라인 쇼핑 경험을 제공하고 있습니다. 글로벌 №1 패션 기업으로 성장할 무신사와 함께 새로운 도전과 혁신을 만들 인재를 기다립니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;29CM는 ‘고객의 더 나은 선택을 돕는다’라는 미션으로 출발했습니다. 우리는 우리만의 방식으로 콘텐츠를 제공하며, 브랜드와 고객 모두에게 대체 불가능한 커머스 플랫폼을 만들어가고 있습니다. 이 미션을 이루기 위해 우리는 흥미로우면서도 복잡한 문제들을 해결하고 있습니다. 만약 우리와 함께 이 문제들을 해결해 보고 싶다면, 주저하지 말고 29CM에 합류하세요!&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=610644aaf27b&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/2025%EB%85%84-%EC%A0%9C-1%ED%9A%8C-29qa-con-%EC%A7%84%ED%96%89-%ED%9B%84%EA%B8%B0-29qa-conference-610644aaf27b&quot;&gt;2025년 제 1회 29QA Con 진행 후기 (29QA Conference)&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>The AI Evolution of Graph Search at Netflix</title>
      <link>https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151?source=rss----2615bd06b42e---4</guid>
      <pubDate>Mon, 26 Jan 2026 19:01:27 GMT</pubDate>
      <content:encoded>&lt;h3&gt;The AI Evolution of Graph Search at Netflix: From Structured Queries to Natural Language&lt;/h3&gt;&lt;p&gt;By &lt;a href=&quot;https://www.linkedin.com/in/ahutter/&quot;&gt;Alex Hutter&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/bartosz-balukiewicz/&quot;&gt;Bartosz Balukiewicz&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Our previous blog posts (&lt;a href=&quot;https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c&quot;&gt;part 2&lt;/a&gt;, &lt;a href=&quot;https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576&quot;&gt;part 3&lt;/a&gt;) detailed how Netflix’s Graph Search platform addresses the challenges of searching across federated data sets within Netflix’s enterprise ecosystem. Although highly scalable and easy to configure, it still relies on a structured query language for input. Natural language based search has been possible for some time, but the level of effort required was high. The emergence of readily-available AI, specifically Large Language Models (LLMs), has created new opportunities to integrate AI search features, with a smaller investment and improved accuracy.&lt;/p&gt;&lt;p&gt;While Text-to-Query and Text-to-SQL are established problems, the complexity of distributed Graph Search data in the GraphQL ecosystem necessitates innovative solutions. This is the first in a three-part series where we will detail our journey: how we implemented these solutions, evaluated their performance, and ultimately evolved them into a self-managed platform.&lt;/p&gt;&lt;h3&gt;The Need for Intuitive Search: Addressing Business and Product Demands&lt;/h3&gt;&lt;p&gt;Natural language search is the ability to use everyday language to retrieve information as opposed to complex, structured query languages like the Graph Search Filter Domain Specific Language (DSL). When users interact with 100’s of various UIs within the suite of Content and Business Products applications, a frequent task is filtering a data table like the one below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*WszSpPjY6IOWZU_lf2qTHg.png&quot; /&gt;&lt;figcaption&gt;Example Content and Business Products application view&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Ideally, a user simply wants to satisfy a query like &lt;strong&gt;“I want to see all movies from the 90s about robots from the US.”&lt;/strong&gt; Because the underlying platform operates on the Graph Search Filter DSL, the application acts as an intermediary. Users input their requirements through UI elements — toggling facets or using query builders — and the system programmatically converts these interactions into a valid DSL query to filter the data.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/612/1*vCXVNhRXGbjLjseNY2vmlQ.png&quot; /&gt;&lt;figcaption&gt;The Complexity of filtering and DSL generation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This process presents a few issues.&lt;/p&gt;&lt;p&gt;Today, many applications have bespoke components for collecting user input — the experience varies across them and they have inconsistent support for the DSL. Users need to “learn” how to use each application to achieve their goals.&lt;/p&gt;&lt;p&gt;Additionally, some domains have hundreds of fields in an index that could be faceted or filtered by. A &lt;em&gt;subject matter expert &lt;/em&gt;(SME) may know exactly what they want to accomplish, but be bottlenecked by the inefficient pace of filling out a large scale UI form and translating their questions in order to encode it in a representation Graph Search needs.&lt;/p&gt;&lt;p&gt;Most importantly, users think and operate using natural language, not technical constructs like query builders, components, or DSLs. By requiring them to switch contexts, we introduce friction that slows them down or even prevents their progress.&lt;/p&gt;&lt;p&gt;With readily-available AI components, our users can now interact with our systems through natural language. The challenge now is to make sure our offering, searching Netflix’s complex enterprise state with natural language, is an intuitive and trustworthy experience.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/615/1*EklLSPYRp9yrPP3wLW5qfg.png&quot; /&gt;&lt;figcaption&gt;Natural language queries translated into Graph Search Filter DSL&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We’ve made a decision to pursue generating Graph Search Filter statements from natural language to meet this need. Our intention is to augment and not replace existing applications with &lt;a href=&quot;https://en.wikipedia.org/wiki/Retrieval-augmented_generation&quot;&gt;retrieval augmented generation&lt;/a&gt; (RAG), providing tooling and capabilities so that applications in our ecosystem have newly accessible means of processing and presenting their data in their distinct domain flavours. It should be noted that all the work here has direct application to building a RAG system on top of Graph Search in the future.&lt;/p&gt;&lt;h3&gt;Under the Hood: Our Approach to Text-to-Query&lt;/h3&gt;&lt;p&gt;The core function of the text-to-query process is converting a user’s (often ambiguous) natural language question into a structured query. We primarily achieve this through the use of an LLM.&lt;/p&gt;&lt;p&gt;Before we dive deeper, let’s quickly revisit the structure of Graph Search Filter DSL. Each Graph Search index is &lt;a href=&quot;https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf#:~:text=of%20configuration%20required.-,Configuration,-For%20collecting%20the&quot;&gt;defined by a GraphQL query&lt;/a&gt;, made up of a collection of fields. Each field has a type e.g. boolean, string, and some have their permitted values governed by controlled vocabularies — a standardized and governed list of values (like an enumeration, or a foreign key). The names of those fields can be used to construct expressions using comparison (e.g. &amp;gt; or ==) or inclusion/exclusion operators (e.g. IN). In turn those expressions can be combined using logical operators (e.g. AND) to construct complex statements.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/615/1*UbP1eIXDDqt5Q8nlCdqIKQ.png&quot; /&gt;&lt;figcaption&gt;Graph Search Filter DSL&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;With that understanding, we can now more rigorously define the conversion process. We need the LLM to generate a Graph Search Filter DSL statement that is syntactically, semantically, and pragmatically correct.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Syntactic correctness&lt;/strong&gt; is easy — does it parse? To be syntactically correct, the generated statement must be well formed&lt;strong&gt; &lt;/strong&gt;i.e. follow the grammar of the Graph Search Filter DSL.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Semantic correctness &lt;/strong&gt;adds some additional complexity as it requires more knowledge of the index itself. To be semantically correct:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it must respect the field types i.e. only use comparisons that make sense given the underlying type;&lt;/li&gt;&lt;li&gt;it must only use fields that are actually present in the index, i.e. does not &lt;em&gt;hallucinate;&lt;/em&gt;&lt;/li&gt;&lt;li&gt;when the values of a field are constrained to a controlled vocabulary, any comparison must only use values from that controlled vocabulary.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Pragmatic correctness&lt;/strong&gt; is much more difficult. It asks the question: does the generated filter actually capture the intent of the user’s query?&lt;/p&gt;&lt;p&gt;The following sections will detail how we pre-process the user’s question to create appropriate context for the instructions that we will provide to the LLM — both of &lt;a href=&quot;https://developers.google.com/machine-learning/resources/intro-llms&quot;&gt;which are fundamental to LLM interaction&lt;/a&gt; — as well as post-processing we perform on the generated statement to validate it, and help users understand and trust the results they receive.&lt;/p&gt;&lt;p&gt;At a high level that process looks like this:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/284/1*s34kKg5FI3TDd6UeXaQWOA.png&quot; /&gt;&lt;figcaption&gt;Graph Search FIlter DSL generation process&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Context Engineering&lt;/h3&gt;&lt;p&gt;Preparation for the filter generation task is predominantly engineering the appropriate context. The LLM will need access to the fields of an index and their metadata in order to construct semantically correct filters. As the indices are defined by GraphQL queries, we can use the type information from the GraphQL schema to derive much of the required information. For some fields, there is additional information we can provide beyond what’s available in the schema as well, in particular permissible values that pull from controlled vocabularies.&lt;/p&gt;&lt;p&gt;Each field in the index is associated with metadata as seen below, and that metadata is provided as part of the context.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/655/1*Z34Ek0ib34dd600ZH9iT0Q.png&quot; /&gt;&lt;figcaption&gt;Graph Search index representation&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;The &lt;strong&gt;field&lt;/strong&gt; is derived from the document path as characterized by the GraphQL query.&lt;/li&gt;&lt;li&gt;The &lt;strong&gt;description&lt;/strong&gt; is the comment from the GraphQL schema for the field.&lt;/li&gt;&lt;li&gt;The &lt;strong&gt;type&lt;/strong&gt; is derived from the GraphQL schema for the field e.g. Boolean, String, enum. We also support an additional controlled vocabulary type we will discuss more of shortly.&lt;/li&gt;&lt;li&gt;The &lt;strong&gt;valid values&lt;/strong&gt; are derived from enum values for the enum type or from a controlled vocabulary as we will now discuss.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A &lt;em&gt;controlled vocabulary&lt;/em&gt; is a specific field type that consists of a finite set of allowed values, which are defined by a SMEs or domain owners. Index fields can be associated with a particular controlled vocabulary, e.g. countries with members such as Spain and Thailand, and any usage of that field within a generated statement must refer to values from that vocabulary.&lt;/p&gt;&lt;p&gt;Naively providing all the metadata as context to the LLM worked for simple cases but did not scale. Some indices have hundreds of fields and some controlled vocabularies have thousands of valid values. Providing all of those, especially the controlled vocabulary values and their accompanying metadata, expands the context; this proportionally increases latency and decreases the correctness of generated filter statements. Not providing the values wasn’t an option as we needed to ground the LLMs generated statements- without them, the LLM would frequently hallucinate values that did not exist.&lt;/p&gt;&lt;p&gt;Curating the context to an appropriate subset was a problem we addressed using the well known RAG pattern.&lt;/p&gt;&lt;h4&gt;Field RAG&lt;/h4&gt;&lt;p&gt;As mentioned previously, some indices have hundreds of fields, however, most user’s questions typically refer only to a handful of them. If there was no cost in including them all, we would, but as mentioned prior, there is a cost in terms of the latency of query generation as well as the correctness of the generated query (e.g. needle-in-the-hackstack problem) and non-deterministic results.&lt;/p&gt;&lt;p&gt;To determine which subset of fields to include in the context, we “match” them against the intent of the user’s question.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Embeddings are created for index fields and their metadata (name, description, type) and are indexed in a vector store&lt;/li&gt;&lt;li&gt;At filter generation time, the user’s question is chunked with an overlapping strategy. For each chunk, we perform a vector search to identify the top K most relevant values and the fields to which they belong.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Deduplication:&lt;/strong&gt; The top K fields from each chunk are both consolidated and deduplicated before being provided as context to the system instructions.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/599/1*_fkaUvg0ljRYJnws1xu_Rw.png&quot; /&gt;&lt;figcaption&gt;Field RAG process (chunking, merge, deduplicate)&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Controlled Vocabularies RAG&lt;/h4&gt;&lt;p&gt;Index fields of the controlled vocabulary type are associated with a particular controlled vocabulary, again, countries are one example. Given a user’s question, we can infer whether or not it refers to values of a particular controlled vocabulary. In turn, by knowing which controlled vocabulary values are present, we can identify additional, related index fields that should be included in the context that may not have been identified by the field RAG step.&lt;/p&gt;&lt;p&gt;Each controlled vocabulary value has:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;a unique&lt;strong&gt; identifier&lt;/strong&gt; within its type;&lt;/li&gt;&lt;li&gt;a human readable &lt;strong&gt;display name;&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;a &lt;strong&gt;description&lt;/strong&gt; of the value;&lt;/li&gt;&lt;li&gt;also-known-as values or &lt;strong&gt;AKA&lt;/strong&gt; display names, e.g. “romcom” for “Romantic Comedy”.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To determine which subset of values to include in the context for controlled vocabulary fields (and also possibly infer additional fields), we “match” them against the user’s question.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Embeddings are created for controlled vocabulary values and their metadata, and these are indexed in a vector store. The controlled vocabularies are available via GraphQL and are regularly fetched and reindexed so this system stays up to date with any changes in the domain.&lt;/li&gt;&lt;li&gt;At filter generation time, the user’s question is chunked. For each chunk, we perform a vector search to identify the top K most relevant values (but only for the controlled vocabularies that are associated with fields in the index)&lt;/li&gt;&lt;li&gt;The top K values from each chunk are deduplicated by their controlled vocabulary type. The associated field definition is then injected into the context along with the matched values.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/461/1*N70dw5GEqNDVDYPmpRmqUQ.png&quot; /&gt;&lt;figcaption&gt;Controlled Vocabularies RAG&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Combining both approaches, the RAG of fields and controlled vocabularies, we end up with the solution that each input question resolves in available and matched fields and values:&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/706/1*hIALZVLkkOjZ5ePgsdVWIA.png&quot; /&gt;&lt;figcaption&gt;Field and CV RAG&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The quality of results generated by the RAG tool can be significantly enhanced by tuning its various parameters, or “levers.” These include strategies for reranking, chunking, and the selection of different embedding generation models. The careful and systematic evaluation of these factors will be the focus of the subsequent parts of this series.&lt;/p&gt;&lt;h3&gt;The Instructions&lt;/h3&gt;&lt;p&gt;Once the context is constructed, it is provided to the LLM with a set of instructions and the user’s question. The instructions can be summarised as follows: &lt;strong&gt;“&lt;em&gt;Given a natural language question, generate a syntactically, semantically, and pragmatically correct filter statement given the availability of the following index fields and their metadata&lt;/em&gt;.”&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In order to generate a &lt;em&gt;syntactically&lt;/em&gt; correct filter statement, the instructions include the syntax rules of the DSL.&lt;/li&gt;&lt;li&gt;In order to generate a &lt;em&gt;semantically&lt;/em&gt; correct filter statement, the instructions tell the LLM to ground the generated statement in the provided context.&lt;/li&gt;&lt;li&gt;In order to generate a &lt;em&gt;pragmatically&lt;/em&gt; correct filter statement, so far we focus on better context engineering to ensure that only the most relevant fields and values are provided. We haven’t identified any instructions that make the LLM just “do better” at this aspect of the task.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/893/1*gCfHD-yVJhpcj6zPYRhzqQ.png&quot; /&gt;&lt;figcaption&gt;Graph Search Filter DSL generation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After the filter statement is generated by the LLM, we deterministically validate it prior to returning the values to the user.&lt;/p&gt;&lt;h3&gt;Validation&lt;/h3&gt;&lt;h4&gt;Syntactic Correctness&lt;/h4&gt;&lt;p&gt;Syntactic correctness ensures the LLM output is a parsable filter statement. We utilize an Abstract Syntax Tree (AST) parser built for our custom DSL. If the generated string fails to parse into a valid AST, we know immediately that the query is malformed and there is a fundamental issue with the generation.&lt;/p&gt;&lt;p&gt;The other approach to solve this problem could be using the &lt;a href=&quot;https://platform.openai.com/docs/guides/structured-outputs&quot;&gt;structured outputs&lt;/a&gt; modes provided by some LLMs. However, our initial evaluation yielded mixed results, as the custom DSL is not natively supported and requires further work.&lt;/p&gt;&lt;h4&gt;Semantic Correctness&lt;/h4&gt;&lt;p&gt;Despite careful context engineering using the RAG pattern, the LLM sometimes hallucinates both fields and available values in the generated filter statement. The most straightforward way of preventing this phenomenon is validating the generated filters against available index metadata. This approach does not impact the overall latency of the system, as we are already working with an AST of the filter statement, and the metadata is freely available from the context engineering stage.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/553/1*0LUUAK1G7CtwFDBZsAjgDA.png&quot; /&gt;&lt;figcaption&gt;DSL verification &amp;amp; hallucinations&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If a hallucination is detected it can be returned as an error to a user, indicating the need to refine the query, or can be provided back to the LLM in the form of a feedback loop for self correction.&lt;/p&gt;&lt;p&gt;This increases the filter generation time, so should be used cautiously with a limited number of retries.&lt;/p&gt;&lt;h3&gt;Building Confidence&lt;/h3&gt;&lt;p&gt;You probably noticed we are not validating the generated filter for pragmatic correctness. That task is the hardest challenge: The filter parses (&lt;em&gt;syntactic&lt;/em&gt;) and uses real fields (&lt;em&gt;semantic&lt;/em&gt;), but is it what the user meant? When a user searches for &lt;strong&gt;“Dark”&lt;/strong&gt;, do they mean &lt;strong&gt;the specific German sci-fi series &lt;em&gt;Dark&lt;/em&gt;, &lt;/strong&gt;or are they browsing for the mood category&lt;strong&gt; “dark TV shows”&lt;/strong&gt;?&lt;/p&gt;&lt;p&gt;The gap between what a user intended and the generated filter statement is often caused by ambiguity. Ambiguity stems from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_compression&quot;&gt;compression of natural language&lt;/a&gt;. A user says &lt;strong&gt;“German time-travel mystery with the missing boy and the cave”&lt;/strong&gt; but the index contains &lt;strong&gt;discrete metadata fields&lt;/strong&gt; like &lt;strong&gt;releaseYear&lt;/strong&gt;, &lt;strong&gt;genreTags&lt;/strong&gt;, and &lt;strong&gt;synopsisKeywords&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;How do we ensure users aren’t inadvertently led to wrong answers or to answers for questions they didn’t ask?&lt;/p&gt;&lt;h4&gt;Showing Our Work&lt;/h4&gt;&lt;p&gt;One way we are handling ambiguity is by &lt;em&gt;showing our work&lt;/em&gt;. We visualise the generated filters in the UI in a user-friendly way allowing them to very clearly see if the answer we’re returning is what they were looking for so they can trust the results..&lt;/p&gt;&lt;p&gt;We cannot show a raw DSL string (e.g., &lt;em&gt;origin.country == ‘Germany’ AND genre.tags CONTAINS ‘Time Travel’ AND synopsisKeywords LIKE ‘*cave*’&lt;/em&gt;) to a non-technical user. Instead, we reflect its underlying AST into UI components.&lt;/p&gt;&lt;p&gt;After the LLM generates a filter statement, we parse it into an AST, and then map that AST to the existing “Chips” and “Facets” in our UI (see below). If the LLM generates a filter for &lt;em&gt;origin.country == ‘Germany’&lt;/em&gt;, the user sees the “Country” dropdown pre-selected to “Germany.” This gives users immediate visual feedback and the ability to easily fine-tune the query using standard UI controls when the results need improvement or further experimentation.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*_Gx0THjlWW9_jwb8KwCrYw.png&quot; /&gt;&lt;figcaption&gt;Generated filters visualisation&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Explicit Entity Selection&lt;/h4&gt;&lt;p&gt;Another strategy we’ve developed to remove ambiguity happens at query time. We give users the ability to constrain their input to refer to known entities using “@mentions”. Similar to Slack, typing @ lets them search for entities directly from our specialized UI Graph Search component, giving them easy access to multiple controlled vocabularies (plus other identifying metadata like launch year) to feel confident they’re choosing the entity they intend.&lt;/p&gt;&lt;p&gt;If a user types, “When was &lt;em&gt;@dark&lt;/em&gt; produced”, we explicitly know they are referring to the &lt;em&gt;Series&lt;/em&gt; controlled vocabulary, allowing us to bypass the RAG inference step and hard-code that context, significantly increasing pragmatic correctness (and building user trust in the process).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*u3YrDuaYJr4LlVB1286Xzg.png&quot; /&gt;&lt;figcaption&gt;Example @mentions usage in the UI&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;End-to-end architecture&lt;/h3&gt;&lt;p&gt;As mentioned previously, the solution architecture is divided into &lt;em&gt;pre-processing&lt;/em&gt;, filter statement generation, and then &lt;em&gt;post-processing&lt;/em&gt; stages. The pre-processing handles context building and involves a RAG pattern for similarity search, while the post-processing validation stage checks the correctness of the LLM-generated filter statements and provides visibility into the results for end users. This design strategically balances LLM involvement with more deterministic strategies.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lw47a7h67i4EUmvhlRKcYQ.png&quot; /&gt;&lt;figcaption&gt;End-to-end architecture&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The end-to-end process is as follows:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;A user’s natural language question (with optional `@mentions` statements) are provided as input, along with the Graph Search index context&lt;/li&gt;&lt;li&gt;The context is scoped by using the RAG pattern on both fields and possible values&lt;/li&gt;&lt;li&gt;The pre-processed context and the question are fed into the LLM with an instruction asking for&lt;em&gt; a syntactically and semantically correct filter statement&lt;/em&gt;&lt;/li&gt;&lt;li&gt;The generated filer statement DSL is verified and checked for hallucinations&lt;/li&gt;&lt;li&gt;The final response contains the related AST in order to build “Chips” and “Facets”&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;By combining our existing Graph Search infrastructure with the power and flexibility of LLMs, we’ve bridged the gap between complex filter statements and user intent. We moved from requiring users to speak our language (DSL) to our systems understanding theirs.&lt;/p&gt;&lt;p&gt;The initial challenge for our users was successfully addressed. However, our next steps involve transforming this system into a comprehensive and expandable platform, rigorously evaluating its performance in a live production environment, and expanding its capabilities to support GraphQL-first user interfaces. These topics, and others, will be the focus of the subsequent installments in this series. Be sure to follow along!&lt;/p&gt;&lt;p&gt;You may have noticed that we have a lot more to do on this project, including named entity recognition and extraction, intent detection so we can route questions to the appropriate indices, and query rewriting among others. If this kind of work interests you, reach out! We’re hiring in our Warsaw office, check for open roles &lt;a href=&quot;https://explore.jobs.netflix.net/careers?location=Warsaw%2C%20Masovian%20Voivodeship%2C%20Poland&amp;amp;pid=790302168096&amp;amp;domain=netflix.com&amp;amp;sort_by=relevance&amp;amp;triggerGoButton=false&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Credits&lt;/h3&gt;&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/quesadaalejandro/&quot;&gt;Alejandro Quesada&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/yevgeniya-li-9877ba160/&quot;&gt;Yevgeniya Li&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/dkyrii/&quot;&gt;Dmytro Kyrii&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/razvan-gabriel-gatea/&quot;&gt;Razvan-Gabriel Gatea&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/milodorif/&quot;&gt;Orif Milod&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/michal-krol-45973411a/&quot;&gt;Michal Krol&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jeffbalis/&quot;&gt;Jeff Balis&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/czhao/&quot;&gt;Charles Zhao&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shilpamotukuri/&quot;&gt;Shilpa Motukuri&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shervineamidi/&quot;&gt;Shervine Amidi&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/aborysov/&quot;&gt;Alex Borysov&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/mike-azar-7064883b/&quot;&gt;Mike Azar&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/bernardo-g-4414b41/&quot;&gt;Bernardo Gomez Palacio&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/haoyuan-h-98b587134/&quot;&gt;Haoyuan He&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/edyr96/&quot;&gt;Eduardo Ramirez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/yujiaxie2019/&quot;&gt;Cynthia Xie&lt;/a&gt;.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d416ec5b1151&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151&quot;&gt;The AI Evolution of Graph Search at Netflix&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 첫 웰컴키트 제작기</title>
      <link>https://blog.banksalad.com/pnc/banksalad-welcome-kit/</link>
      <guid>https://blog.banksalad.com/pnc/banksalad-welcome-kit/</guid>
      <pubDate>Fri, 23 Jan 2026 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드에 웰컴키트가 없었다고? 뱅크샐러드에는 한동안 웰컴키트가 없었습니다. 의외라고 느끼는 분들도 계실지 모르겠어요. 웰컴키트 프로젝트를 진행한 두 디자이너 모두 202…</content:encoded>
    </item>
    <item>
      <title>“이번 달도 밤샘 정산입니다.” — 정산 시스템은 왜 필요했을까 (설계편)</title>
      <link>https://techblog.musinsa.com/22732a4a607f?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/22732a4a607f?source=rss----f107b03c406e---4</guid>
      <pubDate>Wed, 21 Jan 2026 22:22:08 GMT</pubDate>
      <content:encoded>&lt;h3&gt;“이번 달도 밤샘 정산입니다.” — 정산 시스템은 왜 필요했을까 (설계편)&lt;/h3&gt;&lt;p&gt;“이번 달도 밤샘 정산입니다.” 테크 블로그 시리즈&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/이번-달도-밤샘-정산입니다-정산-시스템은-어떻게-만들었을까-실전편-74d8a5d22ba1&quot;&gt;정산 시스템은 어떻게 만들었을까 (실전편)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EB%B2%88-%EB%8B%AC%EB%8F%84-%EB%B0%A4%EC%83%98-%EC%A0%95%EC%82%B0%EC%9E%85%EB%8B%88%EB%8B%A4-%EC%A0%95%EC%82%B0-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%97%88%EC%9D%84%EA%B9%8C-%EC%8B%A4%EC%A0%84%ED%8E%B8-74d8a5d22ba1?source=collection_home_page----f107b03c406e-----0-----------------------------------&quot;&gt;&lt;strong&gt;정산 시스템은 왜 필요했을까 (설계편)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://techblog.musinsa.com/이번-달도-밤샘-정산입니다-더-이상-밤샘하지-않아도-됩니다-운영편-4f09ae3bdf5d&quot;&gt;더 이상 밤샘하지 않아도 됩니다 (운영편)&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;들어가며&lt;/h3&gt;&lt;p&gt;정산 업무를 경험해 본 조직이라면 익숙한 문장입니다.&lt;/p&gt;&lt;p&gt;정산은 매달 반드시 마감되어야 하지만, 그 과정은 늘 사람의 손과 기억에 크게 의존해 왔습니다. 데이터는 흩어져 있고, 기준은 상황마다 조금씩 달라지며, 한 번 계산한 결과도 다시 믿기 어려운 경우가 많습니다.&lt;/p&gt;&lt;p&gt;MASS는 이러한 문제의식에서 출발했습니다.&lt;/p&gt;&lt;p&gt;단순히 수기 작업을 자동화하는 것을 넘어, &lt;strong&gt;정산이라는 행위를 시스템이 책임질 수 있도록 만들고자 했습니다.&lt;/strong&gt;&lt;br&gt;이번 연재의 1편에서는 MASS를 설계하며 가장 먼저 고민했던 질문들, 그리고 그에 대한 설계 원칙을 공유합니다.&lt;/p&gt;&lt;h3&gt;MASS란 무엇인가&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;MASS는 Musinsa Accounting &amp;amp; Settlement System의 약자&lt;/strong&gt;로, &lt;br&gt;물류/운영 과정에서 발생하는 비용을 기준으로 파트너 업체와의 물류비 정산을 자동화하는 시스템입니다.&lt;/p&gt;&lt;p&gt;MASS는 다음 역할을 수행합니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;물류 운영 시 발생하는 입고/출고/반품/재고에 대한 &lt;strong&gt;원천 데이터를 수집&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;조건과 단가를 기준으로 &lt;strong&gt;정산 금액을 계산&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;일/월 단위로 &lt;strong&gt;정산을 집계하고 마감&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;내부 담당자와 외부 파트너 업체가 &lt;strong&gt;동일한 기준의 정산 결과를 확인&lt;/strong&gt;할 수 있도록 제공&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;즉, MASS는 단순히 정산 금액을 계산하는 도구가 아니라, 정산 결과에 대한 논쟁이 생겼을 때 최종 기준이 되는 시스템을 목표로 설계되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*e051S3n4IrpKebneSIYzDA.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;정산 시스템이 어려운 이유&lt;/h3&gt;&lt;p&gt;정산은 단순히 숫자를 더하는 문제가 아닙니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;회계 마감이라는 &lt;strong&gt;절대적인 데드라인&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;단 한 건의 오류도 허용되지 않는 &lt;strong&gt;정합성 요구&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;과거 기준으로 언제든 다시 계산할 수 있어야 하는 &lt;strong&gt;재현성&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;기존에는 여러 단계의 수기 작업과 엑셀 검증을 통해 이를 처리하고 있었고, 이 방식은 높은 업무 부담과 오류 가능성을 동시에 안고 있었습니다.&lt;/p&gt;&lt;p&gt;MASS는 이 문제를 “정산을 더 빨리 하자”가 아니라 &lt;strong&gt;“정산을 시스템이 책임지게 하자”&lt;/strong&gt;는 관점에서 접근했습니다.&lt;/p&gt;&lt;h3&gt;설계 원칙: 속도보다 신뢰성&lt;/h3&gt;&lt;p&gt;MASS를 설계하면서 가장 먼저 합의한 원칙은 다음과 같습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;정산 시스템은 빠르게 계산하는 시스템이 아니라 &lt;/em&gt;&lt;strong&gt;&lt;em&gt;실패해도 다시 계산할 수 있는 시스템이어야 합니다.&lt;/em&gt;&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;이를 위해 아래 원칙을 아키텍처 전반에 반영했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;정합성과 멱등성&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;결정적 계산(재현성)&lt;/strong&gt;&lt;br&gt;&lt;em&gt;(결정적 계산: 같은 원천 데이터와 계산 기준을 사용하면 언제 다시 계산해도 동일한 결과가 나오도록 설계된 계산)&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;감사 가능성(추적성)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;배치 실패 복구 및 재시작성&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;멱등성을 전제로 한 이벤트 처리&lt;/h3&gt;&lt;p&gt;정산에 사용되는 원천 데이터는 이벤트 형태로 유입됩니다.&lt;/p&gt;&lt;p&gt;이벤트 기반 시스템에서 중복 수신이나 재처리는 피할 수 없는 상황이기 때문에, MASS에서는 이를 &lt;strong&gt;전제 조건&lt;/strong&gt;으로 두었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;이벤트 재시도(Retry)와 격리(DLT)를 분리해 처리&lt;/li&gt;&lt;li&gt;트랜잭션 식별자를 기준으로 서비스 레벨에서 멱등 갱신&lt;/li&gt;&lt;li&gt;동일 이벤트가 여러 번 처리되어도 결과는 항상 동일&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 장애 상황에서도 안전하게 재처리할 수 있는 기반을 마련했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*xENmTFP7ucjwQzf10heeyg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;이벤트 재시도(Retry)와 격리(DLT)를 분리해 처리&lt;/h4&gt;&lt;p&gt;이벤트 기반 정산 시스템에서는 일시적인 실패와 구조적인 실패를 구분해 다루는 것이 중요합니다.&lt;/p&gt;&lt;p&gt;MASS에서는 이를 위해 &lt;strong&gt;이벤트 재시도(Retry)&lt;/strong&gt;와 &lt;strong&gt;격리(DLT, Dead Letter Topic)&lt;/strong&gt;를 명확히 분리해 처리합니다.&lt;/p&gt;&lt;p&gt;일시적인 네트워크 오류나 외부 의존성 문제로 인한 실패는 재시도를 통해 정상 흐름으로 복귀시키고, 반복 재시도 이후에도 처리할 수 없는 이벤트는 정상 파이프라인에서 분리해 DLT로 격리합니다.&lt;/p&gt;&lt;h4&gt;트랜잭션 식별자를 기준으로 한 서비스 레벨 멱등 갱신&lt;/h4&gt;&lt;p&gt;MASS에서는 모든 원천 이벤트에 대해 업스트림 시스템에서 이미 존재하는 고유 식별자를 활용합니다. &lt;br&gt;입·출고, 재고 스냅샷 이벤트에 포함된 식별자를 조합해 정산 도메인 관점의 트랜잭션 식별자로 사용합니다.&lt;/p&gt;&lt;p&gt;이 식별자는 단순한 DB Unique Key가 아니라, &lt;strong&gt;정산 관점에서 이미 처리된 이벤트인지 판단하는 기준&lt;/strong&gt;입니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;동일 식별자가 처음 들어오면 신규 정산 데이터로 처리&lt;/li&gt;&lt;li&gt;이미 처리된 이벤트라면 결과를 유지하거나(no-op)&lt;/li&gt;&lt;li&gt;정책 변경 등 의도적인 경우에만 갱신&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이를 통해 MASS는 DB 예외에 의존하지 않고, 도메인 규칙에 기반한 멱등성을 확보했습니다.&lt;/p&gt;&lt;h3&gt;결정적 계산을 위한 정산 로직&lt;/h3&gt;&lt;p&gt;정산 금액 계산은 가능한 한 단순하고 결정적으로 설계했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;계산기 모듈은 입력값에만 의존&lt;/li&gt;&lt;li&gt;금액 스케일과 반올림 정책을 고정&lt;/li&gt;&lt;li&gt;계산 단계를 명확히 분리해 추론 가능성 확보&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이 구조의 핵심은 &lt;strong&gt;“같은 입력이면 언제 계산해도 같은 결과가 나온다”&lt;/strong&gt;는 점입니다.&lt;br&gt;덕분에 이의 제기, 기준 변경, 재처리 상황에서도 동일한 기준으로 다시 계산할 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ESdtMRhioW0uIXh6o5EH-g.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;계산기 모듈은 입력값에만 의존&lt;/h4&gt;&lt;p&gt;정산 계산 로직은 외부 상태나 실행 시점에 따라 결과가 달라지지 않도록, &lt;strong&gt;입력값만으로 결과가 결정되는 순수 함수 형태&lt;/strong&gt;로 설계했습니다.&lt;/p&gt;&lt;p&gt;정산 계산에 필요한 모든 정보는 CalculateCommand와 그 안의 SettlementContext에 명시적으로 포함됩니다.&lt;/p&gt;&lt;pre&gt;data class SettlementContext(&lt;br&gt;    val settlementDate: LocalDate,&lt;br&gt;    val settlementBaseType: SettlementBaseType,&lt;br&gt;)&lt;/pre&gt;&lt;p&gt;SettlementContext는 “언제, 어떤 기준으로 정산하는가”를 명확히 표현하는 최소 단위의 컨텍스트로, 정산 계산 과정에서 &lt;strong&gt;현재 시각, 실행 환경, 전역 상태&lt;/strong&gt;와 같은 외부 요인을 참조하지 않도록 만든 장치입니다.&lt;/p&gt;&lt;p&gt;이를 단순화하면 일 정산 계산 로직은 다음과 같은 형태를 가집니다.&lt;/p&gt;&lt;pre&gt;# pseudo code&lt;br&gt;&lt;br&gt;function calculateDailySettlement(command):&lt;br&gt;    # command includes:&lt;br&gt;    # - settlementContext (settlementDate, settlementBaseType)&lt;br&gt;    # - serviceType, partnerId, brandId&lt;br&gt;    # - logisticsCategoryCode, quantity, policy flags&lt;br&gt;&lt;br&gt;    # 1) 정산 기준일 기준으로 단가 조회&lt;br&gt;    base = findBasePrice(command)&lt;br&gt;    unitPrice = base.baseUnitPrice&lt;br&gt;&lt;br&gt;    # 2) 정산 기준일 기준으로 할인 정책 적용&lt;br&gt;    discountedUnitPrice = applyDiscount(&lt;br&gt;        unitPrice,&lt;br&gt;        base.serviceChargeBase,&lt;br&gt;        command.settlementContext.settlementDate,&lt;br&gt;        command.partnerId,&lt;br&gt;        command.brandId&lt;br&gt;    )&lt;br&gt;&lt;br&gt;    # 3) 수량 반영&lt;br&gt;    originAmount     = unitPrice * command.quantity&lt;br&gt;    discountedAmount = discountedUnitPrice * command.quantity&lt;br&gt;&lt;br&gt;    # 4) 정산 결과 반환&lt;br&gt;    return {&lt;br&gt;        unitPrice,&lt;br&gt;        discountedUnitPrice,&lt;br&gt;        originAmount,&lt;br&gt;        discountedAmount,&lt;br&gt;        remoteAreaUnitPrice (optional)&lt;br&gt;    }&lt;/pre&gt;&lt;p&gt;이 구조에서 계산 결과는 오직 다음 입력에 의해서만 결정됩니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;정산 기준일(settlementDate)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;정산 기준 유형(settlementBaseType)&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;정책 정보(단가, 할인 조건)&lt;/li&gt;&lt;li&gt;원천 데이터(수량 등)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;덕분에 동일한 정산 컨텍스트와 입력이 주어지면 &lt;strong&gt;언제, 몇 번을 실행하더라도 동일한 정산 결과가 보장&lt;/strong&gt;됩니다.&lt;/p&gt;&lt;p&gt;이는 이의 제기 대응, 기준 변경 이후의 재계산, 장애 복구 후 재처리 상황에서도 &lt;strong&gt;과거 정산을 동일한 기준으로 다시 계산할 수 있는 재현성&lt;/strong&gt;을 확보하는 핵심 전제였습니다.&lt;/p&gt;&lt;h4&gt;금액 스케일과 반올림 정책을 고정&lt;/h4&gt;&lt;p&gt;정산 도메인에서 반올림은 “표현 방식”이 아니라 &lt;strong&gt;결과 자체를 바꾸는 규칙&lt;/strong&gt;입니다.&lt;br&gt;특히 할인/수수료처럼 소수점이 개입되는 계산은 &lt;strong&gt;반올림 시점과 방식이 조금만 달라도 최종 금액이 달라질 수 있습니다.&lt;/strong&gt;&lt;br&gt;일 단위로는 몇 원 수준의 차이처럼 보여도, 월 단위 집계로 누적되면 &lt;strong&gt;정산 금액 불일치&lt;/strong&gt;로 이어질 수 있습니다.&lt;/p&gt;&lt;p&gt;그래서 MASS에서는 다음을 원칙으로 두었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;모든 금액 계산은 BigDecimal로 수행해 &lt;strong&gt;부동소수점 오차를 원천 차단&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;할인 적용 시 &lt;strong&gt;스케일(MONEY_SCALE)을 강제로 고정&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;반올림은 &lt;strong&gt;항상 동일한 정책(RoundingMode.DOWN)&lt;/strong&gt;을 사용&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;예를 들어 비율 할인(RATE)의 경우, 할인율을 퍼센트에서 실제 rate로 변환한 뒤 곱셈을 수행하고, 그 결과를 &lt;strong&gt;반드시 동일한 스케일로 내림(DOWN) 처리&lt;/strong&gt;해 할인 적용 단가를 결정합니다.&lt;/p&gt;&lt;pre&gt;// RATE 할인: percent -&amp;gt; rate 변환 후 곱셈, 그리고 스케일/반올림 정책 강제&lt;br&gt;val percent = discount.discountValue.max(BigDecimal.ZERO).min(MAX_RATE_VALUE)&lt;br&gt;val rate = BigDecimal.ONE.subtract(percent.movePointLeft(RATE_SCALE))&lt;br&gt;&lt;br&gt;val discountedUnitPrice =&lt;br&gt;    unitPrice&lt;br&gt;        .multiply(rate)&lt;br&gt;        .setScale(MONEY_SCALE, RoundingMode.DOWN)&lt;br&gt;        .max(BigDecimal.ZERO)&lt;/pre&gt;&lt;p&gt;이처럼 “소수점이 생길 수 있는 지점”에서 &lt;strong&gt;정책을 코드로 강제&lt;/strong&gt;해두면,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;계산 경로가 달라져도(일 정산/월 정산/재처리)&lt;/li&gt;&lt;li&gt;실행 환경이 달라져도(다른 서비스/다른 배치)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;항상 동일한 금액 산출이 가능해지고, 결과적으로 &lt;strong&gt;정산 결과의 재현성&lt;/strong&gt;을 확보할 수 있습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;em&gt;정산 시스템에서 반올림 정책은 구현 디테일이 아니라, “같은 입력이면 같은 결과가 나와야 한다”는 신뢰를 지키는 핵심 규칙입니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;h3&gt;마치며&lt;/h3&gt;&lt;p&gt;정산 시스템의 어려움은 계산식이 아니라, &lt;strong&gt;다시 계산해야 하는 현실&lt;/strong&gt;에 있습니다. &lt;br&gt;MASS는 멱등성과 결정적 계산이라는 설계 원칙을 통해 정산을 사람의 기억이 아닌 시스템의 책임으로 옮기고자 했습니다.&lt;/p&gt;&lt;p&gt;다음 글에서는 이러한 설계가 실제로 어떤 기술 선택과 구조로 구현되었는지, Kafka와 Spring Batch, Argo Workflow를 활용한 정산 시스템의 실전 이야기를 다룰 예정입니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Platform Business Operation 조직 및 팀 소개&lt;/strong&gt;&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사 플랫폼 비즈니스 오퍼레이션 조직은 국내외 물류 서비스, 재고 관리, 스토어 운영을 위한 물류 프로덕트를 구축하고 다양한 오프라인 비즈니스 모델에 맞춘 스토어 관리 시스템을 개발·고도화하고 있습니다.&lt;br&gt;또한, 무배당발 서비스를 포함한 무신사의 차별화된 고객 경험을 브랜딩하고 확장할 수 있는 멤버십 구조를 설계하며, 온·오프라인을 넘나드는 통합 커머스 경험을 기술로 실현하고 있습니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;저희 팀은 OMS(주문관리시스템)를 기반으로 온라인 주문부터 재고·출고·배송·정산에 이르는 전 과정을 유기적으로 연결하고, 무신사의 다양한 오프라인 스토어를 효과적으로 운영할 수 있는 관리 시스템을 구축하여 고객이 온라인(무신사 스토어, 29CM 등)과 오프라인(무신사 스탠다드, 편집숍 등)에서 끊김 없는 쇼핑 경험을 누릴 수 있도록 지원합니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/pbo&quot;&gt;&lt;em&gt;🚀 Platform Business Operation 한걸음 더 알아보기&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=22732a4a607f&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/22732a4a607f&quot;&gt;“이번 달도 밤샘 정산입니다.” — 정산 시스템은 왜 필요했을까 (설계편)&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드가 게임을 만들 때 데이터 정합성을 유지하는 법 (feat. 낙관적 락)</title>
      <link>https://blog.banksalad.com/tech/banksalad-optimistic-lock/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-optimistic-lock/</guid>
      <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요 금융쇼핑 PA…</content:encoded>
    </item>
    <item>
      <title>React랑 Lottie로 게임을 만든다고요?</title>
      <link>https://blog.banksalad.com/tech/banksalad-react-lottie/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-react-lottie/</guid>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 프론트엔드 엔지니어 김덕현입니다. 웹뷰 환경에서 게임 엔진 없이 React와 DOM, 그리고 Lottie…</content:encoded>
    </item>
    <item>
      <title>2025 re:Invent 여정</title>
      <link>https://blog.banksalad.com/tech/aws-reinvent-2025/</link>
      <guid>https://blog.banksalad.com/tech/aws-reinvent-2025/</guid>
      <pubDate>Tue, 13 Jan 2026 00:00:00 GMT</pubDate>
      <content:encoded>2025 re:Invent 여정 AWS re:Invent…</content:encoded>
    </item>
    <item>
      <title>컬리의 입고 시스템이 외부 인입 데이터를 안전하게 동기화하는 방법</title>
      <link>http://thefarmersfront.github.io/blog/2026-outbox-pattern-and-retry-topic/</link>
      <guid>http://thefarmersfront.github.io/blog/2026-outbox-pattern-and-retry-topic/</guid>
      <pubDate>Mon, 12 Jan 2026 00:00:00 GMT</pubDate>
      <content:encoded>아웃박스 패턴과 재시도 토픽으로 외부 채널 입고 정보를 안전하게 동기화하기</content:encoded>
    </item>
    <item>
      <title>이구위크 전시 장애 대응기: Redis에는 무슨 일이 있었나</title>
      <link>https://techblog.musinsa.com/%EC%9D%B4%EA%B5%AC%EC%9C%84%ED%81%AC-%EC%A0%84%EC%8B%9C-%EC%9E%A5%EC%95%A0-%EB%8C%80%EC%9D%91%EA%B8%B0-redis%EC%97%90%EB%8A%94-%EB%AC%B4%EC%8A%A8-%EC%9D%BC%EC%9D%B4-%EC%9E%88%EC%97%88%EB%82%98-5599562d76b9?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%9D%B4%EA%B5%AC%EC%9C%84%ED%81%AC-%EC%A0%84%EC%8B%9C-%EC%9E%A5%EC%95%A0-%EB%8C%80%EC%9D%91%EA%B8%B0-redis%EC%97%90%EB%8A%94-%EB%AC%B4%EC%8A%A8-%EC%9D%BC%EC%9D%B4-%EC%9E%88%EC%97%88%EB%82%98-5599562d76b9?source=rss----f107b03c406e---4</guid>
      <pubDate>Tue, 06 Jan 2026 22:02:27 GMT</pubDate>
      <content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*STYKSBA_rNy0r3jiD_Z-tw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;안녕하세요. 29CM에서 고객이 상품을 탐색하고 발견하는 상품 전시 영역을 담당하고 있는 Customer Engagement Engineering 팀 김송이입니다.&lt;/p&gt;&lt;p&gt;2025년 겨울, 29CM 최대 규모의 블랙프라이데이 행사인 이구위크를 진행했습니다. 연중 가장 트래픽이 몰리는 행사인 만큼, 설렘과 긴장이 공존하는 시즌이기도 합니다. 매년 대규모 트래픽을 대비하지만, 플랫폼의 성장 속도만큼 새로운 변수도 함께 생겨납니다.&lt;/p&gt;&lt;p&gt;이구위크 시작 첫날이었던 11월 3일, 유저가 상품을 둘러보는 주요 상품 전시 화면에서 장애가 발생했습니다. 장애 원인을 추적하고 해결한 과정, 이후 개선한 내용을 정리해 공유합니다.&lt;/p&gt;&lt;h3&gt;1. 장애 발생과 원인을 찾기까지&lt;/h3&gt;&lt;p&gt;이구위크 본편이 시작된 지 얼마 되지 않아, 검색결과, 상품 리스팅 등의 전시 도메인을 담당하는 서버에 이상징후가 포착되기 시작했습니다. 파드(pod)가 일부 다운되며 트래픽을 못 받기 시작했습니다.&lt;/p&gt;&lt;p&gt;남아있는 파드도 처리 가능한 트래픽을 초과하면서 Netty 이벤트 루프 포화가 발생했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*t-AYIgKOPCNUeUouWn9zGA.png&quot; /&gt;&lt;figcaption&gt;Netty Pending Tasks&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;전시 트래픽을 받는 서버의 경우 Netty, Spring WebFlux 기반으로 운영 중이었기 때문에, 트래픽 증가로 인한 다운스트림 지연이나 이벤트 루프 처리 지연 쪽을 먼저 점검했습니다. Redis도 확인했지만, CPU, Memory 등 주요 시스템 메트릭이 모두 정상 범위(10% 이하)였기 때문에 원인일 가능성을 낮게 판단했습니다.&lt;/p&gt;&lt;h3&gt;2. Redis 대역폭 병목이 드러나다&lt;/h3&gt;&lt;p&gt;원인 분석 중 Redis 헬스체크 실패 로그가 눈에 들어왔고, 메트릭 지표를 다시 살펴봤습니다.&lt;br&gt;리소스는 멀쩡한데, 왜 Redis와의 통신은 실패하고 있었을까?&lt;br&gt;이 의문은 네트워크 지표에서 풀렸습니다.&lt;/p&gt;&lt;p&gt;당시 운영 중이던 Redis 노드 타입은 cache.r7g.large로, 기본 네트워크 대역폭은 &lt;strong&gt;0.937Gbps&lt;/strong&gt;였습니다. 이는 이론적으로 &lt;strong&gt;초당 약 117MB&lt;/strong&gt; 수준의 데이터 전송량에 해당합니다.&lt;/p&gt;&lt;p&gt;평소에는 네트워크 Out 대역폭이 약 0.49Gbps(약 61MB/s) 수준으로 유지되고 있어, 기본 대역폭 범위 내에서 안정적으로 동작하고 있었습니다. 그러나 이구위크 트래픽이 몰리면서 피크 시점에는 네트워크 사용량이 평소 대비 약 4배 수준인 &lt;strong&gt;2.0Gbps(약 250MB/s)&lt;/strong&gt;까지 치솟았습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ET-G9Iw8KmC7IcisgzWnPA.png&quot; /&gt;&lt;figcaption&gt;Network In/Out 허용 초과량&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;여기서 문제가 되었던 것은 AWS의 &lt;strong&gt;네트워크 대역폭 스로틀링(Throttling) 메커니즘&lt;/strong&gt;입니다. ElastiCache는 인스턴스 타입마다 기본 제공 대역폭 베이스라인(Baseline)이 정해져 있고, 순간적으로 베이스라인을 초과하는 트래픽이 발생했을 때, 이를 허용해 주는 버스트(Burst) 기능을 제공합니다. 이때 버스트는 ‘&lt;strong&gt;버스트 크레딧(Burst Credit)&lt;/strong&gt;’이라는 일종의 네트워크 체력을 사용해 동작합니다.&lt;/p&gt;&lt;p&gt;휴대폰 데이터 요금제와 비슷하다고 생각하면 이해하기 쉽습니다. 기본 데이터를 다 쓰면 속도가 확 느려지는 것처럼, 크레딧이 소진되면 Baseline으로 강제 제한됩니다.&lt;/p&gt;&lt;blockquote&gt;Baseline 이하로 사용 → 크레딧 축적&lt;/blockquote&gt;&lt;blockquote&gt;Baseline 초과 → 크레딧 소모하며 버스트 유지&lt;/blockquote&gt;&lt;blockquote&gt;크레딧 소진 → AWS가 Baseline 이하로 강제 제한(Throttling)&lt;/blockquote&gt;&lt;p&gt;이번 장애는 바로 이 &lt;strong&gt;버스트 크레딧 소진 → 네트워크 강제 제한&lt;/strong&gt; 과정에서 발생했습니다. 트래픽이 19시부터 Baseline을 초과해 계속 버스트 상태로 운영되었지만, 약 2시간 동안 누적된 버스트 크레딧이 모두 고갈된 시점(20:58)에 네트워크 Throttling이 시작되면서 Redis 응답 지연과 커넥션 실패가 갑자기 폭증했습니다. 결과적으로 Redis 커넥션과 커맨드가 실패하기 시작했고, 이로 인해 애플리케이션의 Readiness Probe가 실패하면서 다수의 파드가 다운되었습니다.&lt;/p&gt;&lt;h3&gt;3. 장애 대응과 즉시 조치&lt;/h3&gt;&lt;p&gt;19:00부터 트래픽이 Redis 대역폭 Baseline을 초과했지만, Burst 크레딧 덕분에 바로 문제가 드러나지 않았고, 약 2시간 뒤인 20:58 크레딧이 고갈되면서 Throttling이 시작되었습니다. Burst 구간이 있었기 때문에 원인 파악이 늦어진 측면이 있습니다.&lt;/p&gt;&lt;p&gt;원인 파악 후 즉시 Redis 노드 스케일업(&lt;em&gt;cache.r7g.large&lt;/em&gt; → &lt;em&gt;cache.r7g.2xlarge&lt;/em&gt;)을 진행했습니다. 2xlarge는 기본 대역폭이 1.875Gbps로, 기존 대비 약 2배의 네트워크 용량을 제공합니다. 서비스 장애는 당일 해소되었지만, 트래픽이 더 늘어나면 같은 문제가 반복될 수 있기 때문에 근본적인 재발 방지 전략이 필요했습니다.&lt;/p&gt;&lt;h3&gt;4. 재발 방지를 위한 개선 작업&lt;/h3&gt;&lt;p&gt;장애 직후, 같은 문제가 반복되지 않도록 몇 가지 조치를 진행했습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4–1. 모니터링 강화&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Burst 크레딧 구간 때문에 원인 파악이 늦어졌던 만큼, 네트워크 In/Out 대역폭 초과 여부를 실시간으로 확인할 수 있도록 모니터링 대시보드를 강화했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*PNB-wltpqao1jiFFFma0Ag.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*grvFIRlbX0WoeR0dQ4s61g.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;주요 모니터링 지표는 아래와 같습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;aws.elasticache.network_bytes_in : 네트워크 수신 바이트&lt;/li&gt;&lt;li&gt;aws.elasticache.network_bytes_out : 네트워크 송신 바이트&lt;/li&gt;&lt;li&gt;aws.elasticache.network_bandwidth_in_allowance_exceeded : 수신 대역폭 Baseline 초과 여부&lt;/li&gt;&lt;li&gt;aws.elasticache.network_bandwidth_out_allowance_exceeded : 송신 대역폭 Baseline 초과 여부&lt;/li&gt;&lt;li&gt;aws.elasticache.traffic_management_active : Throttling 발생 여부&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;또한 Datadog Alert를 연동해 네트워크 대역폭이 임계치를 넘을 경우 즉시 알림을 받을 수 있도록 설정하여 이상 징후를 빠르게 인지하고 사전에 대응할 수 있는 기반을 마련했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/612/1*Y4Km5VSRIFnqlYHaYNBTaA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;4–2. 캐시 전략 변경 (캐시 계층화)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Redis 네트워크 부하를 줄이기 위해 로컬 캐시로 처리 가능한 데이터는 서버 내부 메모리에서 우선 처리하도록 구조를 변경했습니다.&lt;/p&gt;&lt;p&gt;응답 변경 빈도가 낮은 데이터를 중심으로, 기존의 단일 Redis 의존 구조에서 벗어나 &lt;strong&gt;Caffeine&lt;/strong&gt;(&lt;strong&gt;Local Cache) → Redis (Remote Cache) → DB&lt;/strong&gt;로 이어지는 캐시 계층화 구조로 전환했습니다. 이를 통해 Redis에 집중되던 트래픽을 완화하고, 전체적인 응답 안정성을 높일 수 있었습니다.&lt;/p&gt;&lt;p&gt;캐시 전략을 적용한 직후, 실제로 Redis 부하가 감소했는지 지표를 확인했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*0W2L_YTOxTqUcjCb_O_--g.png&quot; /&gt;&lt;figcaption&gt;Redis Command Count&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;우선 로컬 캐시가 제대로 역할하고 있는지 확인하기 위해 Redis 명령어 호출 수를 살펴봤습니다. 그 결과, Redis 명령어 호출이 눈에 띄게 감소한 것을 확인할 수 있었습니다.&lt;/p&gt;&lt;p&gt;로컬 캐시가 상당 부분을 흡수하면서 Redis가 직접 처리해야 하는 요청이 그만큼 줄어든 것입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*o_YtYlfrpkUW-Vx0qt64SQ.png&quot; /&gt;&lt;figcaption&gt;Redis Outgoing Bytes 일별 비교 (노란색: 전일, 빨간색: 당일)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Redis 명령어 호출이 줄어든 만큼, 네트워크 Outgoing Throughput 역시 함께 감소했습니다.&lt;/p&gt;&lt;p&gt;기존에는 대용량 데이터를 주고받느라 네트워크 사용량이 높게 유지되었으나, 로컬 캐시가 이를 대신 처리하면서 Redis까지 전달되는 데이터양이 크게 줄어들었습니다.&lt;/p&gt;&lt;h3&gt;5. 장기 개선 과제&lt;/h3&gt;&lt;p&gt;이번 장애를 계기로, 단기적인 문제 해결에 머무르지 않고 중·장기 관점에서 트래픽 성장에 대비할 수 있는 인프라 구조로 전환하고자 합니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5–1. 캐시 데이터 최적화 (Snappy + protobuf)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Redis 네트워크 대역폭 사용량을 근본적으로 줄이기 위해 캐시 데이터 압축을 검토하고 있습니다. CPU 사용량이 적고 압축/해제 속도가 빨라 실시간으로 데이터를 읽고 쓰는 캐시 환경에 적합한 Snappy 압축 알고리즘을 고려하고 있습니다.&lt;/p&gt;&lt;p&gt;또한 JSON 형태로 저장되고 있는 캐시 데이터를 Protocol Buffers(protobuf) 형식으로 전환하는 것을 검토 중입니다. protobuf는 JSON 대비 데이터 크기가 작고 직렬화/역직렬화 속도도 빠르므로, 네트워크 대역폭 절감과 성능 향상을 동시에 기대할 수 있습니다.&lt;/p&gt;&lt;p&gt;실제 캐싱 중인 Item Document 데이터를 기준으로 측정한 결과, Snappy 압축 + protobuf를 함께 적용했을 때 기존 대비 약 73%의 용량 절감이 가능할 것으로 예상합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Noke_TgaixxkjZotLdPESA.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;6. 마치며&lt;/h3&gt;&lt;p&gt;이번 장애를 통해 Redis 대역폭 초과라는 예상치 못한 장애 지점을 발견할 수 있었습니다. CPU와 Memory 지표만으로는 문제를 인지하기 어려웠으며, &lt;strong&gt;네트워크 관점의 모니터링 필요성&lt;/strong&gt;을 다시 한번 체감하는 계기가 되었습니다.&lt;/p&gt;&lt;p&gt;문제 해결 과정에서 캐시 구조 개선, 모니터링 고도화, 읽기 분리 적용 등 여러 기술적 부채를 해소했고, 현재는 &lt;strong&gt;이를 바탕으로 트래픽 증가에 대비한 아키텍처 개선을 진행&lt;/strong&gt;하고 있습니다.&lt;/p&gt;&lt;p&gt;저희 팀은 전시, 콘텐츠, 기획전, 선물하기 등 사용자가 마주하는 서비스의 첫인상과 주요 탐색 흐름을 책임지고 있습니다. 앞으로도 더 빠르고 안정적인 사용자 경험을 제공하기 위해 지속적으로 구조를 점검하고, 확장 가능한 시스템으로 발전해 나가고자 합니다. 언제나 문제 해결의 모든 과정에서 적극적으로 함께해 준 팀원 모두에게 감사의 말씀을 전하며 글을 마칩니다.&lt;/p&gt;&lt;p&gt;긴 글 읽어주셔서 감사합니다.&lt;/p&gt;&lt;h3&gt;TEAM MUSINSA CAREER&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사는 2001년 온라인 커뮤니티로 시작해 2005년 무신사 매거진, 2009년 무신사 스토어를 오픈하며 빠르게 성장하고 있는 국내 대표 온라인 패션 스토어입니다. ‘입점 브랜드와 동반성장’이라는 경영 철학을 바탕으로 브랜드가 안정적으로 사업을 전개할 수 있도록 무신사가 보유한 노하우와 인프라를 지원합니다. 고객에게는 풍성한 패션 콘텐츠와 패션에 특화된 차별화된 서비스로 최상의 온라인 쇼핑 경험을 제공하고 있습니다. 글로벌 №1 패션 기업으로 성장할 무신사와 함께 새로운 도전과 혁신을 만들 인재를 기다립니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;29CM는 ‘고객의 더 나은 선택을 돕는다’라는 미션으로 출발했습니다. 우리는 우리만의 방식으로 콘텐츠를 제공하며, 브랜드와 고객 모두에게 대체 불가능한 커머스 플랫폼을 만들어가고 있습니다. 이 미션을 이루기 위해 우리는 흥미로우면서도 복잡한 문제들을 해결하고 있습니다. 만약 우리와 함께 이 문제들을 해결해 보고 싶다면, 주저하지 말고 29CM에 합류하세요!&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.musinsacareers.com/ko/home&quot;&gt;&lt;em&gt;🚀 팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5599562d76b9&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9D%B4%EA%B5%AC%EC%9C%84%ED%81%AC-%EC%A0%84%EC%8B%9C-%EC%9E%A5%EC%95%A0-%EB%8C%80%EC%9D%91%EA%B8%B0-redis%EC%97%90%EB%8A%94-%EB%AC%B4%EC%8A%A8-%EC%9D%BC%EC%9D%B4-%EC%9E%88%EC%97%88%EB%82%98-5599562d76b9&quot;&gt;이구위크 전시 장애 대응기: Redis에는 무슨 일이 있었나&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>QA 자동화 결과를 데이터로 관리하다: Grafana Dashboard와 Weekly 분석의 힘</title>
      <link>https://techblog.musinsa.com/qa-%EC%9E%90%EB%8F%99%ED%99%94-%EA%B2%B0%EA%B3%BC%EB%A5%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C-%EA%B4%80%EB%A6%AC%ED%95%98%EB%8B%A4-grafana-dashboard%EC%99%80-weekly-%EB%B6%84%EC%84%9D%EC%9D%98-%ED%9E%98-e18deceed574?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/qa-%EC%9E%90%EB%8F%99%ED%99%94-%EA%B2%B0%EA%B3%BC%EB%A5%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C-%EA%B4%80%EB%A6%AC%ED%95%98%EB%8B%A4-grafana-dashboard%EC%99%80-weekly-%EB%B6%84%EC%84%9D%EC%9D%98-%ED%9E%98-e18deceed574?source=rss----f107b03c406e---4</guid>
      <pubDate>Sun, 28 Dec 2025 22:02:23 GMT</pubDate>
      <content:encoded>&lt;p&gt;안녕하세요 29CM QE팀 강보민입니다.&lt;/p&gt;&lt;p&gt;29CM QE 팀은 iOS와 Android 자동화를 Cell 단위로 분담해 운영하고 있으며, 그중 저는 iOS UI 자동화를 담당하고 있습니다.&lt;/p&gt;&lt;p&gt;특히, &lt;strong&gt;29CM QE팀은 테스트 자동화만 전담하는 인원 없이, 모든 QE가 매뉴얼 테스트와 자동화를 함께 수행합니다.&lt;/strong&gt; 덕분에 상황에 따라 유동적으로 대응할 수 있고, 자동화 코드에 대한 팀 전체의 이해도와 오너십도 높습니다.&lt;/p&gt;&lt;p&gt;입사 6개월이 지난 시점, 신규 시나리오 추가와 유지보수에 익숙해지고 3Q(7~9월) 목표를 설정할 무렵 세 가지 의문이 들었습니다.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;&lt;em&gt;“단순히 지금처럼 fail이 나는 경우에만 수정하는 것이 의미가 있을까?”&lt;/em&gt;&lt;/strong&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;&lt;em&gt;“2%라는 Fail률이 과연 낮은 것일까?’&lt;/em&gt;&lt;/strong&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;&lt;em&gt;“간헐적으로 발생하는 fail에 대해 다시 매뉴얼 테스트를 하는 것이 효율적인 것인가?”&lt;/em&gt;&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;당시 iOS UI 자동화를 리드하시던 다정님께서 이미 약 2% 미만 수준의 낮은 Fail률로 안정적인 자동화 환경을 만들어주신 상태였지만,&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*QAQKOuvhawT8sBZKmpHE5w.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;24년도 하반기 2% 미만의 fail률&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;더 높은 신뢰성과 일관된 품질을 유지하기 위해, &lt;strong&gt;Fail률 0.7% 미만 달성&lt;/strong&gt;을 공동 목표로 세웠습니다.&lt;/p&gt;&lt;p&gt;이후 자동화 테스트 케이스를 수행하면서 코드 구조상 개선이 필요한 영역, 간헐적으로 발생하는 오류 케이스 그리고 신규 시나리오 추가 과정에서 발생할 수 있는 불필요한 실패를 방지하기 위한 사전 안전장치들을 함께 점검하고 개선해 나가기 시작했습니다.&lt;/p&gt;&lt;p&gt;안드로이드 역시 진현님께서 시나리오 복구 작업 및 확장 과정에서 Fail률 2% 미만을 목표로 설정하고, 안정성 향상을 위한 개선 작업을 병행하는 목표를 세웠습니다. (안드로이드는 복구가 필요한 시나리오가 있어 iOS보단 높은 Fail률로 목표를 설정하고, 점차 낮춰 나가는 방향으로 진행하고 있습니다)&lt;/p&gt;&lt;p&gt;이 글에서는 그 과정에서 얻은 경험과 개선 방법을 공유하려 합니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;&lt;em&gt;Grafana Dashboard: 데이터 기반 분석의 시작&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;분석을 하기 위해 가장 먼저 필요했던 것은 &lt;strong&gt;테스트 자동화 결과의 DB화&lt;/strong&gt;입니다. 29CM QE팀은 2024년도 1월부터 자동화 수행 결과를 DB에 적재해오고 있으며, 데이터를 바탕으로 &lt;strong&gt;Grafana Dashboard로 데이터를 시각화하고 있습니다.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;* 참고 글: &lt;/strong&gt;&lt;a href=&quot;https://medium.com/29cm/29cm-qa%ED%8C%80%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%9E%90%EB%8F%99%ED%99%94-%EC%A7%80%ED%91%9C%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%98%EC%97%AC-%EC%8B%A0%EB%A2%B0%EC%84%B1%EC%9D%84-%ED%99%95%EB%B3%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EC%97%88%EC%9D%84%EA%B9%8C-93ee5cca76ce&quot;&gt;현준님의 작년 포스팅 — 29CM QA팀은 어떻게 테스트 자동화 지표를 활용하여 신뢰성을 확보할 수 있었을까?&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Grafana Dashboard에서 확인 가능한 데이터 및 그래프 구성 항목은 날짜별 Fail률, 평균 수행 시간, 이번 주 대비 지난주 수행 시간 비교, Fail 발생 시나리오 카운트 합산 등으로 구성되어 있습니다.&lt;/p&gt;&lt;p&gt;Dashboard에서 확인할 수 있는 정보들과 함께 fail률 개선에 활용한 경험을 이야기해 보겠습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;첫 번째, 일 별 파이프라인 수행 개수입니다.&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*PkkB3Buigx7jKkbiryqN5Q.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;일 별 자동화 파이프라인 수행 개수&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;자동화 수행은 Jenkins 스케줄링에 따라 24시간, 매일 1시간 간격으로 실행되고 있습니다.&lt;/p&gt;&lt;p&gt;앱 심사 이전의 BVT 최종 배포 테스트나, 신규 FE 배포 시 배포 트리거에 걸린 경우, 그리고 11월 이구위크 기간처럼 트래픽이 집중되는 시기에는 하루 최대 50회까지 수행되기도 합니다.&lt;/p&gt;&lt;p&gt;위 그래프에서 일자별 자동화 수행 횟수가 다르게 나타나는 이유가 바로 이 때문입니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;&lt;em&gt;Fail 시나리오 분석&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;두 번째, Grafana Dashboard에서 확인할 수 있는 여러 지표 중, Fail률을 줄이기 위해 가장 핵심적으로 활용한 데이터인 &lt;strong&gt;시나리오별 Fail 카운트 합계&lt;/strong&gt;입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*GSGRVxIdUf7SWvahkjMzUg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;7월fail 시나리오&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;3Q OKR 시작과 동시에 어느 시나리오에서 높은 Fail률이 발생하는지 파악하기 위해, OKR 시작 첫 번째 달인 &lt;strong&gt;7월 데이터를 Grafana Dashboard를 통해 집중적으로 분석&lt;/strong&gt;했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*fWzSeC654FLt0rCQFRUx9Q.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;7월 자동화 결과&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JhD60kutwCthKBp4RtR63g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;7월 fail률&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;7월 자동화 Fail률을 Grafana Dashboard에서 모니터링한 결과, 대부분 1% 미만의 Fail률을 유지했지만, 간헐적으로 2% 가까이 상승하는 구간이 확인되었습니다. 특히 간헐적 이슈로 잘못 판단하여 시나리오 주석 처리 등 즉각적인 대응을 하지 못한 경우에는 4%를 초과한 적도 있었습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;7월 문제점 분석&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;특히 구매 플로우, 로그인이 필요한 플로우, 그리고 저녁 시간대 29 라이브 수행으로 인해 Element 클릭 시 29 라이브 전체보기 모드가 클릭되는 현상이 발생했고, 29 라이브가 위치한 페이지의 Element에서 높은 Fail 카운트가 집중되어 있음을 확인했습니다.&lt;/p&gt;&lt;p&gt;이에 따라 해당 시나리오를 우선 개선 대상으로 선정하고, 다음과 같은 문제점을 도출했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;로그인이 필수인 시나리오에서 로그인 실패 시 후속 시나리오가 연쇄적으로 실패하는 구조&lt;/li&gt;&lt;li&gt;결제 수단 선택 시 결제 수단 Element 탐색은 가능하나, 클릭하지 못하는 현상&lt;/li&gt;&lt;li&gt;29 라이브 수행 시 PIP 모드가 페이지 이동 후 유지, 앱 재실행 시 유지되는 현상이 있음&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;7월 개선 작업&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;문제점 도출을 바탕으로 다음과 같은 개선 작업을 진행했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;(1) &lt;strong&gt;사전 로그인 강화 &lt;/strong&gt;: 로그인이 반드시 필요한 케이스에서 로그인 실패 시 뒤 시나리오가 모두 실패하므로, &lt;strong&gt;사전 로그인 케이스를 추가&lt;/strong&gt;했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*rfN4X64zKfO0u8AC9DLtZw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;사전 로그인&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;(2) &lt;strong&gt;선택 동작 안정화&lt;/strong&gt; : 결제 수단 선택 시, 스크롤 위치에 따라 다른 UI나 터치 방어 영역과 겹치는 현상이 있어, Element 탐색 후 화면을 살짝 스크롤 하여 안정적으로 선택되도록 처리했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/818/1*u5SvMqWR0kcZQEnZDZ2TqA.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;스크롤 안정화&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(3) 시간대 분기&lt;/strong&gt; : 29 라이브 수행 시 &lt;strong&gt;PIP 모드 닫기 동작을 추가하여 &lt;/strong&gt;특정 시간대(17:30~21:30)에 PIP 모드가 노출되어 다른 시나리오에 영향을 주는 것을 방지했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Ps1H8BLSLFaoVAI3EuEB0Q.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;PIP 닫기&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;8월: 개선 효과 확인 및 추가 과제 발견&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;8월에는 7월에 진행한 개선 코드 기반으로 7월에 가장 많은 Fail을 기록했던 구매/로그인 관련 시나리오들이 8월에 대폭 개선된 것을 확인할 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*IxTGEkN4YWHHnz-HQ9zwPg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;8월 fail 시나리오&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;하지만, 아직 개선해야 할 것들은 여전히 남아 있었습니다. 바로 간헐적 fail과 실험 중일 경우 유저 계정별 기대 결과가 달라지는 경우입니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;8월 개선 작업&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(1) A/B 실험 대응&lt;/strong&gt; : PDP 내 추천구좌를 확인하는 케이스 중 A그룹에 해당하는 유저는 “당신을 위한 MD 추천”에 해당, B그룹에 해당하는 유저는 “주목할만한 상품”만 확인하도록 분기 처리했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/814/1*K1Fic2qmx-V6PwrVtV0pvA.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;추천 구좌 분기&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(2) 카테고리 Depth 클릭 고도화&lt;/strong&gt; : API에서 받아온 카테고리를 클릭하도록 작성된 기존 코드에서 UI Element 탐색 시 대/중/소 카테고리가 분리되어 작성되지 않아 카테고리 depth를 잘못 클릭하는 경우가 있었습니다.&lt;/li&gt;&lt;li&gt;그 결과 아래 이미지처럼 “원래 의도는 여성 의류(대) &amp;gt; 단독(중) &amp;gt; 상의(소) 순서로 클릭하는 것이었으나, 중 카테고리에도 동일한 이름의 “상의”가 존재하여 ‘소’ 카테고리 대신 ‘중’ 카테고리의 “상의”가 클릭 되는 현상이 발생했습니다.”&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*qTCYJgqf4G4VjrYSvznD3g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;카테고리 고도화 이전&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;API로 카테고리를 받아왔음에도 잘못 클릭 되는 현상을 방지하고자, 대/중/소 카테고리 탐색 로직을 개선하여 각 depth 별로 정확한 카테고리를 클릭할 수 있도록 했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*yiMHdk2N_jBAqOE57UThdw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;카테고리 고도화 이후&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(3) 텍스트 정합성 검증&lt;/strong&gt; : 상품명에 불필요한 공백(앞뒤 공백, 중복 띄어쓰기 등)이 포함되어 있어도 FE에서 공백을 정리해 주는 로직이 적용되어 있음을 개발팀 확인 후 이에 맞춰 테스트 코드에서도 동일하게 공백을 제거한 후 비교하도록 개선하여, 불필요한 Fail이 발생하지 않도록 했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*fD0mMtdmNMKWLUAd4iSquQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;불필요한 공백 제거 처리&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;8월 결과&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;7월 대비 1%를 넘는 fail 률이 비교적 줄고, 목표 fail률인 0.7% 미만에 도달하였지만, 아직 목표 fail률인 0.7%를 초과하는 일자들이 다수 존재했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*SSy9-8_SGOpKYhB2Q-aKgg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;8월 fail률&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/607/1*h-tKiMaDagK32R345Q2-eg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;8월 자동화 결과&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;9월: 신규 시나리오 추가와 Fail률 감소의 동시 달성&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;OKR 마지막 달인 9월은 Fail률이 증가할 수 있는 달이었습니다. 왜냐하면 &lt;strong&gt;신규 시나리오를 추가함과 동시에 Fail률을 감소시켜야 하는 조건&lt;/strong&gt;이 있었기 때문입니다.&lt;/p&gt;&lt;p&gt;3Q 추가한 시나리오는 브랜드 홈 고도화 작업, 쿠폰 적용 상품, 마수동 광수동 알람 등이 있었습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;9월 개선 작업&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;브랜드 홈 NEW 구좌 조건부 검증 : 브랜드 홈에 노출되는 구좌는 NEW, BEST 등이 있습니다. BEST 구좌는 항상 노출되나, NEW 구좌는 상황에 따라 신규 상품이 없으면, 신규 상품이 있으나 그 개수가 적을 경우 등의 상황이 고려되어야 했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*q23Zssz2CgGhAsO-DoNoAw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;NEW 구좌 더보기 버튼 차이&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;(1) API 응답, ID 활용&lt;/strong&gt; : 해당 구좌가 존재하는 경우를 판단하기 위해 API 호출 후 신규 상품이 있을 경우에만 검증하고, NEW 구좌 내 상품명 일치 검증을 하는 경우에도 NEW와 더보기 텍스트를 통해 구좌 위치를 판단하는 것보다 정확히 NEW 타이틀에 심어진 ID를 통해 해당 구좌를 판별하도록 수정했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iKk26dQylckZtvGM_hZj0w.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;NEW 구좌 노출 여부 확인&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*FfqQmJ-NgafX4A9BvFcvTQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;ID기준으로 판단&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(2) 패싯 선택 조건 강화&lt;/strong&gt; : 쿠폰 적용 상품 페이지 내 가격 필터 패싯 선택 시 상품 종류에 따라 보유한 패싯 정보가 다르기 때문에, price의 최솟값과 최댓값을 받아온 후, 선택해야 하는 가격대 패싯이 있는 조건에 해당하는 경우에만 가격대 패싯을 선택하도록 개선했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Uf8bt1DoU4fAmQm0QJxmqg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;가격대 패싯 선택 로직 강화&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;그 결과, 5만 원~10만 원 가격 패싯이 없는 경우에는 가격대 패싯을 선택하지 않도록 처리되었고, 상품명 데이터 비교를 위한 API 호출 시에도 가격 파라미터를 제외하여 정확한 데이터 검증이 가능해졌습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/667/1*iar_GI1Rr_gOH25EAemIrQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;5~10만 원 패싯이 없을 경우 미 선택&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(3) 사전 조건 강화 &lt;/strong&gt;: PLP, SRP 검증 시 다른 페이지에서 패싯 변경 후 진입하는 경우 패싯이 유지되는 경우를 방지하고자, 현재 패싯 정보를 저장하고 default 패싯 정보와 다를 경우 패싯 사전 조건을 세팅했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/782/1*NTSJ51ujUsaRY5fRf63qlg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;패싯 초기 상태가 달라질 수 있는 상황&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dEffK8Q1-gHkSmcfDHccgg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;패싯 사전조건 세팅&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;(4) 일관성 확보&lt;/strong&gt; : 29CM 앱 내부에서 설정해야 하는 시나리오 중 설정 중간에 실패, 해제 중간에 실패하는 경우 이후 시나리오에 영향을 줄 수 있기에, 앞뒤 시나리오로 인해 해제 테스트 default 조건이 예상과 다르게 설정된 경우를 방지하기 위한 안전장치로 API를 수행하여 테스트 진행 전 환경을 동일하게 유지했습니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*A4ljf6VSXHn0-rNugql5Tg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;테스트 수행 전 API 호출로 테스트 환경 일관성 유지&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;그 결과, 9월은 신규 시나리오 추가에도 불구하고 (극소수의 0.7% 초과 날짜는 있었지만) 목표치인 0.7%보다 훨씬 더 낮은 &lt;strong&gt;0.5% 미만의 Fail률&lt;/strong&gt;에 도달할 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/609/1*SPlxxf_lan6x3nsWH22YJg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;9월 자동화 결과&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*mzzvVnSEcrg9DW9ueQn2ig.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;9월 fail률 전체&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;&lt;strong&gt;&lt;em&gt;자동화 수행 시간 분석과 개선&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Grafana Dashboard에서는 Fail률뿐만 아니라 &lt;strong&gt;시나리오별 수행 시간&lt;/strong&gt;도 모니터링할 수 있습니다.&lt;/p&gt;&lt;p&gt;지난주 대비 이번 주 시나리오별 수행 시간에 차이가 있는 항목을 그래프로 한눈에 시각적으로 확인할 수 있도록 구성되어 있습니다. 이를 통해 어느 시나리오에 많은 수행 시간이 소요 되는지 체크하고, 다음과 같은 원인 분석을 진행했습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Element 탐색에 오래 걸리는 것인지&lt;/li&gt;&lt;li&gt;API 호출 후 데이터를 불러와 변환하는 과정이 오래 걸리는 것인지&lt;/li&gt;&lt;li&gt;불필요한 대기 시간이 포함된 것인지&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;분석 결과를 바탕으로 &lt;strong&gt;테스트 케이스를 상황에 따라 분리&lt;/strong&gt;하거나, 비효율적인 로직을 개선하여 &lt;strong&gt;수행 속도를 최적화&lt;/strong&gt;하고자 했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lw3zhF_6249NENOFh4rKdQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;지난주 대비 이번주 수행시간 비교&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;자동화 수행 속도가 중요한 이유&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;이구위크는 29CM의 대규모 이벤트 기간으로, 유저 인입이 급증하고 장애 발생 시 신속한 제보가 필요합니다. 이에 따라 자동화 스케줄링 주기를 &lt;strong&gt;20~30분 단위&lt;/strong&gt;로 단축하여 운영하게 됩니다.&lt;/p&gt;&lt;p&gt;만약 단일 시나리오의 수행 시간이 과도하게 길어지면, 전체 자동화 수행 시간이 스케줄링 주기를 초과하는 상황이 발생할 수 있습니다. 이는 곧 &lt;strong&gt;장애 탐지 지연&lt;/strong&gt;으로 이어질 수 있기 때문에, 수행 속도 최적화는 안정적인 자동화 운영을 위한 필수 요소입니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;🚨 실제 이슈 탐지 사례&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;자동화 수행으로 인해 3Q에 잡은 사유별 Fail을 하나씩 살펴보면 다음과 같습니다.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;실제 장애로 인한 fail 발생&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/682/1*Cpa_9H1cqGPal-gfG-AcpQ.png&quot; /&gt;&lt;figcaption&gt;특정 페이지 진입 시 장애 발생으로, 문구를 확인하지 못함으로 인한 fail&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;배포 직후 배포 트리거가 돌아 발견된 필터 패싯 미 노출 이슈&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/502/1*BnnEhdo_ZgH5Xo44LkP9Pw.png&quot; /&gt;&lt;figcaption&gt;배포 트리거 이후 잡힌 패싯 미노출 fail&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;피처플래그/앰플리튜드 설정 오류로 &lt;/em&gt;&lt;/strong&gt;잘못된 앰플리튜드 설정으로 테스터가 아닌 유저에게 카테고리 핀메뉴가 미 노출&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/706/1*BVHq-6emfB7Ml9ZMdWVYYQ.png&quot; /&gt;&lt;figcaption&gt;피처플래그, 앰플리튜드 설정이 잘못된 경우 카테고리가 미노출 되었던 현상&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;&lt;strong&gt;&lt;em&gt;모바일 개발팀과의 협업&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;29CM QE Team은 모바일 개발팀과 긴밀한 협업으로 &lt;strong&gt;Element ID를 심고&lt;/strong&gt; 있습니다. Element ID가 달라질 경우 문의 시 적극적으로 확인과 재 작업을 잘 해주셔서 많은 도움이 됩니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*GD27h02GgrT25z5JvdCaJg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;긴밀한 협업&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;이 과정에서 가장 중요했던 것은 &lt;strong&gt;혼자 고민하지 않는 것&lt;/strong&gt;이었습니다.&lt;/h4&gt;&lt;p&gt;위클리 진행을 통해 다음과 같은 규칙을 세웠습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;개선 대상 시나리오 선정&lt;/strong&gt;: Fail률을 개선하기 위해 수정해야 하는 시나리오를 함께 논의하고 우선순위를 결정&lt;/li&gt;&lt;li&gt;&lt;strong&gt;고민 시간 기준 설정&lt;/strong&gt;: 코드 수정 중 혼자 작업 시 고민하는 시간의 기준을 정하고, 그 시간 초과 시 팀원 간 블로커 즉시 공유&lt;/li&gt;&lt;li&gt;&lt;strong&gt;원 팀 모드&lt;/strong&gt;: 자동화 코드 작성 중 질문이 있을 때 Q&amp;amp;A 리스트업 후 팀원 모두 원팀 마음으로 해소하려고 서로 조언 주기&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*twhzw7EQo-F4xiZlwwFM2g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;위클리, Action Item&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*NhV8Mb0bdyPiaoFV0PyVoA.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Android 9월 fail률 목표 도달&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Android 또한 9월 Fail률 2% 미만을 달성하며 목표에 도달했습니다. 아직 시나리오 확장 단계라 iOS보다 목표치를 보수적으로 잡아둔 상황이었는데, 작년 시나리오 확장 초반 대비 매우 낮아진 Fail률은 플랫폼 담당자 간의 적극적인 원 팀 문화 덕분이라고 생각합니다. &lt;br&gt;iOS 담당자도 Android에 조언과 피드백을 아끼지 않았고, Android 담당자 역시 iOS에 적극적으로 피드백을 주며 서로 도왔기에 가능한 성과였습니다.&lt;/p&gt;&lt;p&gt;마지막으로, 목표만 말로 세우는 것이 아닌 &lt;strong&gt;실질적인 Agenda와 Action Item&lt;/strong&gt;을 도출하고, &lt;strong&gt;Gantt 차트로 일정을 수립하여 &lt;/strong&gt;누락 없이 진행할 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*A_BLNvp4YPRvBG7Y6wFn8A.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;iOS&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/779/1*0DRrCH-8M0TJwFW46I_0Hg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;안드로이드&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;이처럼 &lt;strong&gt;데이터 기반 분석 + 팀 협업 + 체계적인 일정 관리&lt;/strong&gt;가 결합되어 3Q 목표였던 &lt;strong&gt;Fail률 0.7% 미만을 달성&lt;/strong&gt;할 수 있었습니다.&lt;/p&gt;&lt;p&gt;단순히 자동화 테스트를 수행하는 것에서 그치지 않고, 데이터 기반으로 분석하고 개선하는 과정이 자동화의 신뢰성을 높이는 핵심이라는 것을 경험할 수 있었습니다.&lt;/p&gt;&lt;p&gt;앞으로도 지속적인 모니터링과 분석을 통해 더 안정적인 자동화 환경을 구축해 나가겠습니다.&lt;/p&gt;&lt;p&gt;29CM QE팀은 AI를 적극적으로 활용하고 있습니다. 현재 Grafana Dashboard를 활용한 분석에서 한발 더 나아가, AI를 통해 자동화 결과를 주 단위, 월 단위로 자동 분석하고 리포트를 받아볼 수 있도록 코드를 작성하고 있습니다.&lt;/p&gt;&lt;p&gt;다음 글에서는 AI를 활용해 29CM QE팀이 불필요한 시간을 줄이고, 얼마나 효율적으로 테스트 결과를 받아보며, 그 결과를 기반으로 개선해 나가고 있는지에 대한 이야기로 찾아뵙겠습니다.&lt;/p&gt;&lt;p&gt;긴 글 읽어주셔서 감사합니다.&lt;/p&gt;&lt;h3&gt;TEAM MUSINSA CAREER&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사는 2001년 온라인 커뮤니티로 시작해 2005년 무신사 매거진, 2009년 무신사 스토어를 오픈하며 빠르게 성장하고 있는 국내 대표 온라인 패션 스토어입니다. ‘입점 브랜드와 동반성장’이라는 경영 철학을 바탕으로 브랜드가 안정적으로 사업을 전개할 수 있도록 무신사가 보유한 노하우와 인프라를 지원합니다. 고객에게는 풍성한 패션 콘텐츠와 패션에 특화된 차별화된 서비스로 최상의 온라인 쇼핑 경험을 제공하고 있습니다. 글로벌 №1 패션 기업으로 성장할 무신사와 함께 새로운 도전과 혁신을 만들 인재를 기다립니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;29CM는 ‘고객의 더 나은 선택을 돕는다’라는 미션으로 출발했습니다. 우리는 우리만의 방식으로 콘텐츠를 제공하며, 브랜드와 고객 모두에게 대체 불가능한 커머스 플랫폼을 만들어가고 있습니다. 이 미션을 이루기 위해 우리는 흥미로우면서도 복잡한 문제들을 해결하고 있습니다. 만약 우리와 함께 이 문제들을 해결해 보고 싶다면, 주저하지 말고 29CM에 합류하세요!&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://corp.musinsa.com/ko/career/&quot;&gt;&lt;em&gt;팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e18deceed574&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/qa-%EC%9E%90%EB%8F%99%ED%99%94-%EA%B2%B0%EA%B3%BC%EB%A5%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C-%EA%B4%80%EB%A6%AC%ED%95%98%EB%8B%A4-grafana-dashboard%EC%99%80-weekly-%EB%B6%84%EC%84%9D%EC%9D%98-%ED%9E%98-e18deceed574&quot;&gt;QA 자동화 결과를 데이터로 관리하다: Grafana Dashboard와 Weekly 분석의 힘&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>OMS에서 Claude AI를 활용하여 변화된 업무 방식</title>
      <link>http://thefarmersfront.github.io/blog/oms-claude-ai-workflow/</link>
      <guid>http://thefarmersfront.github.io/blog/oms-claude-ai-workflow/</guid>
      <pubDate>Wed, 24 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>PM 1명과 엔지니어 3명이 12개 MSA를 운영하는 OMS팀이 Claude AI를 도입하여 16명 규모의 조직처럼 일하게 된 과정을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>디자인 컴포넌트 라이브러리를 ‘실제 사용 방식’에 맞게 다시 설계한 이야기</title>
      <link>http://thefarmersfront.github.io/blog/klds-web-structure-refactor/</link>
      <guid>http://thefarmersfront.github.io/blog/klds-web-structure-refactor/</guid>
      <pubDate>Tue, 23 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>디자인 컴포넌트 라이브러리 빌드 및 구조 개선기</content:encoded>
    </item>
    <item>
      <title>“우리가 직접 만들겠습니다” — 무신사의 POS 내재화 여정</title>
      <link>https://techblog.musinsa.com/%EC%9A%B0%EB%A6%AC%EA%B0%80-%EC%A7%81%EC%A0%91-%EB%A7%8C%EB%93%A4%EA%B2%A0%EC%8A%B5%EB%8B%88%EB%8B%A4-%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-pos-%EB%82%B4%EC%9E%AC%ED%99%94-%EC%97%AC%EC%A0%95-462a0d81b5a9?source=rss----f107b03c406e---4</link>
      <guid>https://techblog.musinsa.com/%EC%9A%B0%EB%A6%AC%EA%B0%80-%EC%A7%81%EC%A0%91-%EB%A7%8C%EB%93%A4%EA%B2%A0%EC%8A%B5%EB%8B%88%EB%8B%A4-%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-pos-%EB%82%B4%EC%9E%AC%ED%99%94-%EC%97%AC%EC%A0%95-462a0d81b5a9?source=rss----f107b03c406e---4</guid>
      <pubDate>Mon, 22 Dec 2025 22:02:20 GMT</pubDate>
      <content:encoded>&lt;h3&gt;“우리가 직접 만들겠습니다” — 무신사의 POS 내재화 여정&lt;/h3&gt;&lt;h3&gt;들어가며&lt;/h3&gt;&lt;p&gt;안녕하세요. 무신사 PBO(Platform Business Operation) 프론트엔드 개발자 구룡입니다.&lt;/p&gt;&lt;p&gt;무신사 오프라인 스토어는 자체 개발한 POS 클라이언트 시스템인 &lt;strong&gt;MPOS(Musinsa POS)를 도입&lt;/strong&gt;하여, 온라인과의 상품 및 회원 연동 체계를 내부적으로 구축하였으며 &lt;strong&gt;2025년 7월 중 MPOS로의 전환을 완료&lt;/strong&gt;하였습니다. 현재 내부 다양한 스토어 유형의 매장을 지원하고 있으며, 앞으로 더욱 다양한 형태의 스토어, 매장이 확장될 예정입니다.&lt;/p&gt;&lt;p&gt;무신사 오프라인 스토어는 과거에 POS 결제 솔루션으로 &lt;strong&gt;외부 3rd party 솔루션&lt;/strong&gt;을 사용했습니다. 해당 솔루션은 초기 선정 당시 최적의 선택이었을수 있으나 온라인 플랫폼과의 연동성 및 개발 대응 속도 등의 한계로 인해 &lt;strong&gt;비즈니스의 빠른 성장을 제때에 지원하지 못하고 지속적으로 비즈니스의 병목으로 작용&lt;/strong&gt;해 왔습니다.&lt;/p&gt;&lt;p&gt;당시에는 일부 기능이 web형태로 내재화되었으나, &lt;strong&gt;VAN사와의 결제 승인&lt;/strong&gt;, 그리고 &lt;strong&gt;주문/정산 정보 인터페이스&lt;/strong&gt;는 여전히 외부 솔루션에 의존하고 있었습니다. 이로 인해 다음과 같은 문제가 발생하고 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*7vmOAAmTXCL63lIp_xWuwQ.jpeg&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;외부 의존성으로 인한 개발 지연&lt;/strong&gt;: 기능 추가나 수정 시 외부 업체와의 협의 및 개발 일정 조율 필요&lt;/li&gt;&lt;li&gt;&lt;strong&gt;비용 부담&lt;/strong&gt;: 외부 솔루션 사용료 및 추가 개발비 지속 발생&lt;/li&gt;&lt;li&gt;&lt;strong&gt;유연성 부족&lt;/strong&gt;: 비즈니스 요구사항에 대한 빠른 대응의 어려움&lt;/li&gt;&lt;li&gt;&lt;strong&gt;시스템 통합의 복잡성&lt;/strong&gt;: 외부 시스템과의 연동으로 인한 복잡도 증가&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;이러한 문제들을 해결하기 위해 &lt;strong&gt;POS 시스템의 전면 내재화&lt;/strong&gt;를 추진하게 되었고, 현재는 외부 솔루션에 대한 의존성을 완전히 제거하고 모든 POS 기능을 내재화하여 자체 개발 시스템으로 전환을 완료했습니다.&lt;/p&gt;&lt;h3&gt;1. POS 시스템 내재화의 필요성&lt;/h3&gt;&lt;p&gt;MPOS를 만들기로 했을 때 가장 먼저 고민한 건 “왜 지금까지 외부 솔루션에 붙어 있었나”였습니다. 매장에서 실시간으로 결제를 받고, VAN사와의 결제 승인을 받아야 하고, 정산을 연동해야 하는 상황에서 외부 시스템은 편리했지만, 그 편리함이 점점 발목을 잡기 시작했습니다.&lt;/p&gt;&lt;p&gt;그동안 웹 일부 기능만 자체 개발했고, 핵심 기능들은 여전히 외부 솔루션에 의존하고 있었습니다. 외부 솔루션을 통해 결제 승인이나 주문/정산 인터페이스를 처리하다 보니 기능 하나를 바꾸는 데도 외부 업체와의 미팅, 일정 조율, QA를 포함한 긴 시간이 필요했습니다. 비용도 계속 늘어나고, 새로운 요구사항이 생겨도 빠르게 반응하기 어려운 구조가 되었죠. 시스템 통합 역시 여러 외부 시스템이 서로 얽히면서 복잡도가 커졌고, 영수증 프린터처럼 고객과 아주 가까워야 하는 엔드포인트마저 외부 솔루션을 통해 처리하니 통제권이 없었습니다.&lt;/p&gt;&lt;p&gt;이런 경험들이 쌓일수록 “우리가 직접 만들고, 우리가 통제해야겠다”는 결심이 단단해졌습니다. MPOS 내재화는 단순히 개발팀의 자부심 차원이 아니었습니다. 개발 속도를 높이기 위해서, 외부 솔루션 사용료와 반복되는 커스터마이징 비용을 줄이기 위해서, 비즈니스 요구사항에 흔들리지 않기 위해서, 내부 시스템 간의 직접 연동으로 복잡도를 낮추기 위해서, 그리고 모든 데이터와 프로세스를 우리가 직접 관리하고 품질을 보장하기 위해서 필요한 결정이었습니다.&lt;/p&gt;&lt;h3&gt;2. 패키징 솔루션 검토&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*14lO4BFXvTYxSFQDc-h_3w.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;MPOS를 데스크톱 애플리케이션으로 포장한다는 건 단순히 UI를 감싸는 문제가 아니었습니다. 기존 웹 기반 UI를 그대로 가지고 오면서도, 오프라인과 하드웨어 중심의 매장 환경에서 작동해야 했기 때문에, 선택의 기준이 많았습니다.&lt;/p&gt;&lt;p&gt;우리는 먼저 각 솔루션이 어떤 기술 스택 위에 올라서는지, 우리가 이미 가지고 있는 개발력을 얼마나 활용할 수 있는지를 중심으로 논의를 시작했습니다. Electron은 JavaScript/HTML/CSS의 재활용이 가능하다는 점에서 개발 속도를 크게 앞세울 수 있었고, Chromium과 Node.js를 함께 쓰면서 시스템 자원을 직접 다룰 수 있다는 점이 매력적으로 다가왔습니다. 반면, Tauri는 Rust 기반으로 경량이고 메모리 효율적인 장점이 있었지만, Rust에 익숙하지 않은 팀에서는 러닝 커브가 부담이었고, 매장에서 직접 제어해야 하는 하드웨어 드라이버를 연결하는 데 다소 제약이 있었습니다. 네이티브 개발은 퍼포먼스 면에서 뛰어났지만, Windows와 macOS를 각각 따로 개발해야 하는 실무 부담이 컸고, 유지보수할 개발자 리소스도 부족했습니다.&lt;/p&gt;&lt;p&gt;이렇게 여러 조건을 하나씩 놓고 비교하면서도, MPOS가 필요로 하는 것은 도구의 기능보다 매장 환경을 해석하고 빠르게 대응할 수 있는 능력이라는 점이 더 명확해졌습니다. 결국 우리는 &lt;strong&gt;현재 가진 웹 개발 역량을 최대한 활용하면서, 하드웨어·네트워크 제약을 묶어낼 수 있는 Electron&lt;/strong&gt;을 선택하게 되었습니다. 이는 포장 솔루션을 단순히 기술 목록으로 고른 것이 아니라, MPOS라는 속도와 안정성을 동시에 요구하는 시스템의 경험으로 접근한 결정이었습니다.&lt;/p&gt;&lt;h3&gt;3. Electron 선택 이유&lt;/h3&gt;&lt;p&gt;솔루션을 고르는 회의 자리에서는 “그래도 Electron 아니면 안 될까요?”라는 질문이 자주 나왔습니다. MPOS는 기존 외부 솔루션의 영향을 받으며 개발 주기가 늘어나고 있었고, 영수증 출력·결제 승인·주문/정산 인터페이스처럼 하드웨어와 실시간으로 통신해야 하는 핵심 기능은 오래된 구조 위에서 유지·관리되고 있었습니다. 우리는 “특정 기술”이 아니라 MPOS의 다음 단계 운영 방식 자체를 정의할 수 있는 방안을 찾고 있었습니다.&lt;/p&gt;&lt;p&gt;그 과정에서 눈에 들어온 요소는 아래와 같습니다.&lt;br&gt;&lt;strong&gt;첫째, 개발 속도&lt;/strong&gt;입니다. 외부 솔루션을 끼고 개발하는 동안 요구사항 하나 제대로 반영하기까지 수주가 걸렸고, 새로운 스택을 도입하면 그만큼 시간과 리스크가 추가될 수밖에 없었습니다.&lt;br&gt;&lt;strong&gt;둘째, 하드웨어와의 긴밀한 통신&lt;/strong&gt;입니다. 영수증 프린터, 바코드 스캐너 등의 디바이스를 Node.js 수준에서 접근할 수 있어야 했고, 로컬 데이터 저장·네트워크 통신을 자유롭게 섞을 수 있어야 했습니다.&lt;br&gt;&lt;strong&gt;셋째, 크로스 플랫폼과 시스템 독립성&lt;/strong&gt;입니다. 매장마다 시스템 설정이 다르고, 브라우저 버전에 따라 동작이 달라지는 걸 막기 위해 동일한 런타임 위에서 작동하는 솔루션이 필요했습니다.&lt;br&gt;&lt;strong&gt;마지막으로, 운영하고 있는 매장의 안정성&lt;/strong&gt;을 고려해 검증된 생태계와 커뮤니티 지원도 중요했습니다.&lt;/p&gt;&lt;p&gt;이러한 조건을 놓고 비교한 결과, Electron이 가장 MPOS의 요구를 맞춰줄 수 있는 솔루션이라 판단했습니다. 기존의 JavaScript/HTML/CSS 기술 스택을 그대로 활용할 수 있어 팀 내 도입 장벽이 낮았고, npm 생태계의 serialport를 비롯한 하드웨어 연동 라이브러리를 통해 필요한 디바이스 통신을 구현할 수 있었습니다. Chromium을 내장해 시스템 브라우저와 무관하게 동일한 동작을 보장했고, 이미 VS Code, Slack, Discord처럼 신뢰받는 애플리케이션에서 쓰이며 안정성이 확인된 솔루션이었습니다.&lt;/p&gt;&lt;p&gt;결국 우리는 MPOS가 직면한 문제(느린 개발, 하드웨어 통신, 크로스 플랫폼, 안정성)를 Electron이 한 번에 묶어서 해결할 수 있다는 판단하에 Electron을 최종 솔루션으로 채택했습니다. 이 선택이 MPOS의 “포장을 넘어선” 전면 내재화 여정이자, MPOS 자체를 중심에 둔 기술 결정임을 팀 모두가 공감했습니다.&lt;/p&gt;&lt;h3&gt;4. MPOS에 Electron 적용후기&lt;/h3&gt;&lt;p&gt;Electron을 MPOS에 적용하며 직접 맞닥뜨린 현장의 문제들과, 그것들을 하나씩 풀어준 해결책들을 소개하려 합니다.&lt;/p&gt;&lt;h3&gt;4.1 빌드 환경 및 비용&lt;/h3&gt;&lt;p&gt;워크플로를 처음 정할 때 “그냥 다른프로젝트에서 하던대로 Linux에서 빌드하면 되지 않나”라는 질문이 정말 많았습니다. 하지만 MPOS는 Windows용, 실제 매장 환경에서 돌아가야 하는 애플리케이션이었고, Linux에서 나오는 바이너리와 실제 현장에서 쓰는 바이너리는 차이가 제법 컸습니다. 아무리 설정을 맞춰도 Windows에서 끊임없이 재검증해야 했고, 결국 Windows runner에서만 빌드하는 방향이 필요했습니다.&lt;/p&gt;&lt;p&gt;그렇다고 비용을 무작정 늘릴 수는 없었기에, 우리는 빌드를 최소화시키는 쪽으로 전략을 세웠습니다. Windows runner에서는 실제 설치 패키지를 만들고, Linux runner는 테스트와 lint 등 비용이 적은 작업만 맡겼습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*agIaKC25nb_lMASM3gppKg.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;이 방식으로 Windows 빌드 횟수를 꼭 필요한 순간으로 압축하면서도, 현장에 정확한 바이너리를 전달하는 구조를 갖출 수 있었습니다.&lt;/p&gt;&lt;h3&gt;4.2 IPC 통신 끊김&lt;/h3&gt;&lt;p&gt;Windows 환경에서 일정 시간 대기 상태로 진입할 때 IPC(Inter-Process Communication)가 끊기면 하드웨어가 멈출 수 있다는 이야기를 듣고, 출시 전에 미리 시나리오를 만들어 테스트해봤습니다. 영수증 출력이나 결제가 들어온 순간을 시뮬레이션하자 IPC가 도중에 끊기며 메인 프로세스와 UI가 소통을 못 하는 상태가 재현되었고, “실제 고객 앞에서 이러면 큰일이겠구나”라는 경고가 내부에서 울렸습니다.&lt;/p&gt;&lt;p&gt;그래서 아예 그런 상황이 오기 전에 Electron의 powerSaveBlocker를 꺼두기로 했습니다. 메인 프로세스에 display sleep을 무시하도록 지시하니, Sleep모드 진입하지 않고 꾸준히 IPC 연결을 유지했고, 시리얼 통신이 예전처럼 끊길 조짐을 보이지 않았습니다. 설치 스크립트에는 아래처럼 sleep 방지를 켜고 종료 시 되돌리는 루틴을 넣어서 사고를 사전에 막을 수 있었습니다.&lt;/p&gt;&lt;pre&gt;// 메인 프로세스&lt;br&gt;const { powerSaveBlocker } = require(&amp;quot;electron&amp;quot;);&lt;br&gt;const id = powerSaveBlocker.start(&amp;quot;prevent-display-sleep&amp;quot;);&lt;br&gt;&lt;br&gt;app.on(&amp;quot;will-quit&amp;quot;, () =&amp;gt; {&lt;br&gt;  powerSaveBlocker.stop(id);&lt;br&gt;});&lt;/pre&gt;&lt;p&gt;이렇게 미리 막고 나니 어떤 상황에서도 IPC는 침착했고, 결제나 프린트 요청이 예측 가능한 흐름으로 이어지면서 현장 안정감이 훨씬 높아졌습니다.&lt;/p&gt;&lt;h3&gt;4.3 SerialPort로 외부 디바이스 연결&lt;/h3&gt;&lt;p&gt;MPOS는 영수증 프린터, 바코드 스캐너 등 시리얼 포트를 통해 연결되는 디바이스와 통신해야 합니다. 이를 위해 Node.js의 serialport 패키지를 사용했는데, Electron 환경에서 번들링 과정과 실행 파일에서 예상치 못한 문제가 발생했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/561/1*roYANI8jsiuySnmFD1VJhA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;serialport를 챙겨야 할 때, 빌드 과정에서 “특정 모듈을 못 찾는다”는 오류가 뜨며 멀쩡하던 연결이 한순간에 끊기는 일이 벌어졌습니다. 로컬에서는 문제없었지만 빌드된 실행 파일이 node_modules 구조를 그대로 갖고 있지 않다 보니, node-gyp-build.js가 상대 경로로 prebuilds를 찾다가 길을 잃은 것이었습니다. 하드웨어 통신이 이 부분에 달려 있었기 때문에, 이걸 풀지 않으면 MPOS가 기기를 아예 제어하지 못하는 위험한 상황이 되었죠.&lt;/p&gt;&lt;p&gt;그래서 우리는 두 방향으로 대응했습니다. 하나는 번들 결과물 안에 prebuilds 자체를 넣는 것, 다른 하나는 런타임에서 node-gyp-build.js가 절대 경로를 참고하도록 만드는 것이었습니다. rsbuild.main.config.ts에서 prebuilds 디렉토리를 출력 디렉토리로 복사하고, string-replace-loader로 경로 조합 코드를 건드려 __dirname 기반으로 바꾸면 빌드된 exe에서도 찾을 수 있게 됩니다.&lt;/p&gt;&lt;pre&gt;// rsbuild.main.config.ts&lt;br&gt;{&lt;br&gt;  output: [&lt;br&gt;    // prebuilds 파일을 빌드 출력 디렉토리로 복사&lt;br&gt;    {&lt;br&gt;      from: path.resolve(__dirname, &amp;quot;./node_modules/@serialport/bindings-cpp/prebuilds/win32-x64&amp;quot;),&lt;br&gt;      to: &amp;quot;./prebuilds&amp;quot;,&lt;br&gt;    },&lt;br&gt;    {&lt;br&gt;      from: path.resolve(__dirname, &amp;quot;./node_modules/@serialport/bindings-cpp/prebuilds/win32-x64&amp;quot;),&lt;br&gt;      to: &amp;quot;./prebuilds/win32-x64&amp;quot;,&lt;br&gt;    },&lt;br&gt;  ],&lt;br&gt;  tools: {&lt;br&gt;    rspack(config, { addRules }) {&lt;br&gt;      addRules([&lt;br&gt;        {&lt;br&gt;          test: /node-gyp-build\.js$/,&lt;br&gt;          loader: &amp;#39;string-replace-loader&amp;#39;,&lt;br&gt;          options: {&lt;br&gt;            search: /path\.join\(dir, &amp;#39;prebuilds&amp;#39;/g,&lt;br&gt;            replace: &amp;quot;path.join(__dirname, &amp;#39;prebuilds&amp;#39;&amp;quot;,&lt;br&gt;          },&lt;br&gt;        },&lt;br&gt;      ])&lt;br&gt;    }&lt;br&gt;  }&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;이렇게 두 손을 쓴 뒤부터는 설치 파일 안에서 serialport가 필요한 바인딩을 모두 찾아내고, 시리얼 통신도 끊김 없이 이어졌습니다. MPOS가 하드웨어와 대화를 하도록 묶어주는 연결고리를 잃지 않고 버티고 있다는 확신이 들면서, 현장에서 실제 디바이스를 붙였을 때도 문제가 없었습니다.&lt;/p&gt;&lt;h3&gt;4.4 ESC/POS 표준에 대한 지식 부족 및 대응&lt;/h3&gt;&lt;p&gt;영수증 프린터를 붙이는 순간부터 우리에게 필요한 건 ESC(Epson Standard Code)/POS 명령어를 다루는 능력이었습니다. 텍스트, 로고, 바코드, QR코드까지 모두 다루려면 표준을 거의 숙지해야 했고 제조사별 미묘한 차이까지 커버해야 했습니다. 처음에는 escpos 같은 오픈소스 라이브러리로 빠르게 시작해보려 했는데, 각 기능이 불안정하거나 우리가 필요한 스타일링이 빠져 있어서 “이대로 현장에 내보낼 수 없다”는 판단이 나왔습니다.&lt;/p&gt;&lt;p&gt;그때부터는 도면을 하나씩 뜯어보듯 ESC/POS 문서를 따라가며 기능을 직접 채워 넣었습니다. 프린터마다 명령어가 조금씩 달라 테스트를 반복했고, 필요한 기능은 직접 구현해서 내부 라이브러리로 묶었습니다.&lt;/p&gt;&lt;p&gt;이제 MPOS의 영수증 출력은 더 이상 외부 라이브러리에 기댈 필요가 없고, 우리가 만든 라이브러리 안에서 요구하는 출력 흐름을 그대로 그려낼 수 있게 되었습니다. 현장에서 프린터를 바꿔도 제어 로직은 그대로 재사용 가능했고, 앞으로 새로운 모델을 붙일 때도 일관된 경험을 유지할 자신감이 생겼습니다.&lt;/p&gt;&lt;h3&gt;4.5 인증서 적용&lt;/h3&gt;&lt;p&gt;SmartScreen에서 “알 수 없는 게시자” 경고가 뜨는 상황을 떠올리면 이미 마음이 따갑더라고요. 당장은 MPOS의 배포본이 ‘신뢰할 수 없는 앱’으로 찍히면 매장에 들어가기조차 어렵겠구나 하는 위기의식이 생겼습니다. 그래서 인증서를 준비해야겠다고 모두가 생각했고, OV(Organization Validation)와 EV(Extended Validation) 중 어느 쪽을 선택할지 진짜 여러 번 회의실에서 들여다보았습니다.&lt;/p&gt;&lt;p&gt;EV는 보안성이 높고 SmartScreen 신뢰도가 좋지만, USB eToken에 묶여서 자동화가 힘들고, CI/CD에서 사용할 수 있는 .pfx 추출도 불가능하다는 점이 발목을 잡았습니다. 반면 OV는 상대적으로 신뢰도가 낮지만 인증서 파일을 추출할 수 있어 GitHub Actions에서 자동으로 붙일 수 있었습니다. 우리가 원하는 건 ‘리모트에서도 코드 서명까지 끝나는 제대로 된 릴리즈’였고, 결국 OV 인증서가 실용적이라고 판단했습니다.&lt;/p&gt;&lt;p&gt;한국전자인증에서 OV CodeSign 인증서를 받아 cert.pfx를 확보한 뒤, GitHub Secrets에 base64로 저장하고 빌드 파이프라인에서 다시 복원하는 방식으로 연결했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/996/1*TnrM9pGYFKCEsd9RysyQEA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;결과적으로 매번 빌드마다 신뢰도를 확보한 EXE를 배포할 수 있게 되었고, SmartScreen만 통과하면 ‘정상적인 설치 과정’이라는 인상을 줄 수 있었습니다.&lt;/p&gt;&lt;h3&gt;4.6 네트워크 관련&lt;/h3&gt;&lt;p&gt;매장 네트워크는 괜찮다가도 가끔 연결이 끊기는 일도 있습니다. 우리가 미리 시뮬레이션해보니, 조금만 네트워크가 내려가도 기준정보 API를 못 받아서 메뉴판이 깨지고, 주문도 막히는 상황이 벌어질 수 있었습니다. 그래서 네트워크 문제를 “낮은 확률의 이슈”로 남겨두지 않고, 사전에 대처책을 짜기로 했습니다.&lt;/p&gt;&lt;p&gt;첫 번째는 진입시 매장기준정보 데이터를 캐싱하는 전략이었습니다. 매장 단말에 정보를 저장해두고, 서버와 통신이 끊겼을 때도 그 데이터를 참고하게 만들면, 결제나 주문처럼 필수적인 흐름은 계속 이어질 수 있겠다고 판단했죠. 네트워크가 다시 들어오면 캐시를 업데이트하고 최신 데이터를 가져오도록 했는데, 이렇게 하면 가끔 뜬금없이 접근 안되는 이슈도 해결되었습니다.&lt;/p&gt;&lt;p&gt;두 번째는 Network Information API를 붙여 사용자에게 네트워크 상태를 보여주는 것입니다. 연결이 나빠지거나 끊기면 MPOS 안에서 알림을 띄워서, 매장 직원이 “아, 지금 인터넷이 불안정하구나”라고 바로 알 수 있게 만들었습니다 그러니까 문제가 생겼을 때 무턱대고 온콜을 걸기보다, 스스로 확인하거나 관리자에게만 보고하도록 하여 감이 빨라졌습니다. 이런 미리보기 알림 덕분에 네트워크 이슈가 발생해도 대응 시간과 불안감이 크게 줄었고, 복구되면 알림도 조용히 사라지면서 원활한 유저 경험을 지켜냈습니다.&lt;/p&gt;&lt;h3&gt;4.7 모니터링 및 장애 대응&lt;/h3&gt;&lt;p&gt;MPOS를 현장에 올려놓은 뒤, “사용자의 행동이 곧 신호”라는 생각이 자꾸 들었습니다. 그래서 Datadog RUM을 연결해 실제 사용자의 행동을 살펴보며, 어떤 흐름이 정상이고 어디에서 낯선 패턴이 나오는지 이해하는 데 집중했습니다. RUM 덕분에 특정 기능이 반응하는 데 시간이 길어지거나, 사용자가 반복해서 클릭하는 화면이 감지되면 바로 알 수 있었고, 그것이 오류의 전조임을 알아차리는 데 큰 도움이 되었습니다.&lt;/p&gt;&lt;p&gt;추가로 JavaScript 예외, API 실패, 기타 SDK에서 잡은 오류들은 에러 트래킹 시스템을 통해 수집되었습니다. 어떤 오류가 발생하면 Slack 알림이 거의 실시간으로 들어왔고, 알람에는 오류 메시지·스택 트레이스·화면 context·영수증번호 등 필요한 정보를 담아두어 즉시 대응이 가능했습니다. Slack 메시지를 보고 개발팀이 바로 모여 핫픽스를 준비하거나, 후속 대응을 준비했습니다.&lt;/p&gt;&lt;p&gt;POS 시스템은 매장 매출과 직결되니 응답 시간이 느려지면 곧바로 민원이 들어왔지만, 이런 모니터링 + Slack 대응 구조 덕분에 평균 대응 시간이 짧아졌고, 문제의 영향을 최소화 할 수 있었습니다. 예전에는 고객 한 명이 장애를 먼저 얘기해야 알았던 일이, 이제는 시스템이 먼저 “이상 징후”를 울려주고, 우리끼리 먼저 해결할 수 있게 된 셈입니다.&lt;/p&gt;&lt;h3&gt;5. 내재화 이후 달라진 결과&lt;/h3&gt;&lt;p&gt;런칭 이후 가장 환한 변화는, 외부 업체에게 의존하지 않아도 된다는 점이었습니다. VAN사, SAP, 영수증 프린터까지 모든 노드가 외부 솔루션에 묶여 있었던 시절에는 하나의 요청에도 외부 일정과 정책을 맞춰야 했지만, MPOS를 직접 손보면서 문제를 마주할 때마다 로그를 들여다보고 고치게 되었습니다. 덕분에 커뮤니케이션이 줄고 결정 속도가 빨라졌으며, 팀에서도 “이 부분은 우리가 책임지고 돌린다”는 태도를 유지하게 되었습니다.&lt;/p&gt;&lt;p&gt;그 다음으로 반가웠던 건 외부 솔루션 사용료가 사라졌다는 사실입니다. 매장 수가 늘어날수록 예산 시뮬레이션이 부담으로 다가왔는데, MPOS 전환으로 그 걱정이 줄었습니다. 커스터마이징은 여전히 필요하지만 내부 프로젝트처럼 관리하니 비용 통제도 쉬웠고, 장기적으로 운영비가 내려간다는 감각이 눈에 보이기 시작했습니다.&lt;/p&gt;&lt;p&gt;개발 속도는 예전과 비교할 수 없을 만큼 달라졌습니다. 외부사를 기다리며 릴리즈가 밀리던 기억은 이제 옛이야기가 되었고, 요청이 들어오면 “이거 내가 맡을게요”라는 말과 함께 바로 코드에 손을 얹습니다. 빠른 피드백 루프 덕분에 이벤트성 프로모션도 단기간에 반영할 수 있었고, 시장 변화에 따른 대응 템포도 훨씬 빨라졌습니다. 기다림 대신 실행하는 분위기가 팀 내에 뿌리내렸고, 연속된 작은 승리들이 곧 개발자들에게 자신감으로 이어졌습니다.&lt;/p&gt;&lt;p&gt;통합 구조도 훨씬 쉬워졌습니다. 외부 솔루션을 여러 단계 거치던 시절에는 장애가 나면 “여기서 시작인가?” 하며 길을 헤맸지만, 지금은 내부 API만 보면서 데이터를 설계하고 장애를 추적할 수 있습니다. 간결해진 아키텍처 덕분에 장비 간 흐름을 눈으로 그려볼 수 있게 되었고, 장애 대응도 빨라졌으며 유지보수의 부담도 덜어졌습니다.&lt;/p&gt;&lt;p&gt;마지막으로 가장 안정감을 준 건 데이터였습니다. 외부 솔루션이 제공하는 로그는 제약이 많았지만, MPOS 전면 전환 이후에는 모든 로그를 자유롭게 수집하고 분석할 수 있게 되었습니다. “이 데이터는 우리 것”이라는 자부심과 함께 향후 활용까지 다양한 시나리오를 그릴 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;그동안 외부에 맡기고 기다려야 했던 시간에 비하면, 지금은 우리 손으로 뿌리를 내려뒀다는 느낌이 훨씬 강합니다. 작은 로그 한 줄, 빠른 결정 하나하나가 현장 직원들의 편의를 만들어내고, 그러한 변화가 곧 팀의 자신감으로 이어졌습니다. MPOS 런칭은 단순한 기술 서비스 개편이 아니라, 우리만의 운영 리듬을 찾은 순간이었습니다.&lt;/p&gt;&lt;h3&gt;6. 앞으로 개선해야 할 사항&lt;/h3&gt;&lt;p&gt;앞으로 개선하고 싶은 내용도 있는데요, 현재 영수증 출력 관련 로직이 메인 프로세스(main thread)에서 처리되고 있다는 점이 가장 먼저 떠올랐습니다. 이 구조는 메인 프로세스에 큰 부담을 주기 때문에, 렌더러 프로세스(renderer)에서 데이터를 다듬고 필요한 커맨드를 조립해 메인 프로세스에 넘기는 방식으로 처리하려고 합니다. 가능한 많은 작업을 렌더러에서 처리하고, 메인 프로세스는 하드웨어 통신(시리얼 포트 접근)처럼 절대 필요한 역할만 맡도록 구조를 재설계하면, 메인 프로세스 응답성이 자연스럽게 회복되고 여러 작업이 동시에 몰릴 때도 성능 저하를 막을 수 있습니다. 또한 이렇게 하면 Electron 전체 패키지를 다시 배포하지 않고도 Web 릴리스 루프만으로 UI나 기능을 빠르게 갱신할 수 있어, 유지보수와 테스트가 훨씬 수월해집니다.&lt;/p&gt;&lt;p&gt;다른 하나는 일부 저사양 장비에서는 Electron이 메모리 부족 상태에 빠져서 OOM(Out Of Memory) 상황이 있었는데, 이 경우 Tauri처럼 Rust 기반의 경량 런타임을 대안으로 함께 유지하는 방안도 검토 중입니다. Electron과 Tauri 빌드를 병행하여 각각의 디바이스 사양에 맞게 선택하도록 배포 채널을 구성하고, 공통 비즈니스 로직은 Web 코드로 공유하면서 OS별로 일부 구현만 분리하는 전략을 실험하고 있습니다.&lt;/p&gt;&lt;p&gt;이런 개선 과정을 하나씩 겪으며, 우리는 여전히 완벽하진 않지만 그만큼 팀으로서의 결속이 깊어지고 있다는 걸 느끼고 있습니다. 약한 부분을 대면할 때마다 함께 고민하고, 작은 실험을 지나며 “다음에는 더 단단해질 수 있겠다”는 믿음이 생깁니다. MPOS는 이제 우리 손끝에서 살아 숨 쉬는 시스템이며, 앞으로의 변화도 그렇게 정성을 쏟아 채워갈 예정입니다.&lt;/p&gt;&lt;h3&gt;마무리&lt;/h3&gt;&lt;p&gt;MPOS 프로젝트를 통해 POS 시스템을 완전히 내재화하는 데 성공했습니다. Electron을 선택한 것은 빠른 개발 속도, 성숙한 생태계, 시스템 독립성 등의 이유 때문이었고, 실제로 프로젝트를 진행하면서 이러한 선택이 적절했음을 확인할 수 있었습니다.&lt;/p&gt;&lt;p&gt;물론 Electron의 무거운 용량과 메모리 사용량 등의 단점도 있었지만, POS 시스템의 요구사항을 충족하고 빠르게 개발할 수 있다는 장점이 더 컸습니다.&lt;/p&gt;&lt;p&gt;PBO O4O팀은 앞으로는 성능 최적화와 의존성 최소화에 집중하여 더 나은 MPOS 시스템을 만들어 나가겠습니다.&lt;/p&gt;&lt;p&gt;감사합니다.&lt;/p&gt;&lt;h3&gt;TEAM MUSINSA CAREER&lt;/h3&gt;&lt;blockquote&gt;&lt;em&gt;무신사는 2001년 온라인 커뮤니티로 시작해 2005년 무신사 매거진, 2009년 무신사 스토어를 오픈하며 빠르게 성장하고 있는 국내 대표 온라인 패션 스토어입니다. ‘입점 브랜드와 동반성장’이라는 경영 철학을 바탕으로 브랜드가 안정적으로 사업을 전개할 수 있도록 무신사가 보유한 노하우와 인프라를 지원합니다. 고객에게는 풍성한 패션 콘텐츠와 패션에 특화된 차별화된 서비스로 최상의 온라인 쇼핑 경험을 제공하고 있습니다. 글로벌 №1 패션 기업으로 성장할 무신사와 함께 새로운 도전과 혁신을 만들 인재를 기다립니다.&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;29CM는 ‘고객의 더 나은 선택을 돕는다’라는 미션으로 출발했습니다. 우리는 우리만의 방식으로 콘텐츠를 제공하며, 브랜드와 고객 모두에게 대체 불가능한 커머스 플랫폼을 만들어가고 있습니다. 이 미션을 이루기 위해 우리는 흥미로우면서도 복잡한 문제들을 해결하고 있습니다. 만약 우리와 함께 이 문제들을 해결해 보고 싶다면, 주저하지 말고 29CM에 합류하세요!&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://corp.musinsa.com/ko/career/&quot;&gt;&lt;em&gt;팀 무신사 채용 페이지&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (무신사/29CM 전체 포지션 확인이 가능해요)&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://kr.linkedin.com/company/musinsacom&quot;&gt;&lt;em&gt;팀 무신사 테크 소식을 받아보는 링크드인&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;em&gt;🚀 &lt;/em&gt;&lt;a href=&quot;https://newsroom.musinsa.com/&quot;&gt;&lt;em&gt;팀 무신사 뉴스룸&lt;/em&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=462a0d81b5a9&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://techblog.musinsa.com/%EC%9A%B0%EB%A6%AC%EA%B0%80-%EC%A7%81%EC%A0%91-%EB%A7%8C%EB%93%A4%EA%B2%A0%EC%8A%B5%EB%8B%88%EB%8B%A4-%EB%AC%B4%EC%8B%A0%EC%82%AC%EC%9D%98-pos-%EB%82%B4%EC%9E%AC%ED%99%94-%EC%97%AC%EC%A0%95-462a0d81b5a9&quot;&gt;“우리가 직접 만들겠습니다” — 무신사의 POS 내재화 여정&lt;/a&gt; was originally published in &lt;a href=&quot;https://techblog.musinsa.com&quot;&gt;MUSINSA techblog — 무신사 테크 블로그&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드가 선보인 AI 에이전트 비전은? 2025 핀테크 위크로 돌아보기!</title>
      <link>https://blog.banksalad.com/pnc/fintechweek-2025/</link>
      <guid>https://blog.banksalad.com/pnc/fintechweek-2025/</guid>
      <pubDate>Sat, 20 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드가 11월 말 양재 aT센터에서 열린 ‘코리아 핀테크 위크 202…</content:encoded>
    </item>
    <item>
      <title>Claude Code를 활용한 예측 가능한 바이브 코딩 전략</title>
      <link>http://thefarmersfront.github.io/blog/vibe-coding-with-claude-code/</link>
      <guid>http://thefarmersfront.github.io/blog/vibe-coding-with-claude-code/</guid>
      <pubDate>Wed, 17 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>에이전트 주도 개발에서 발생하는 문제를 해결하고, 컨텍스트를 정밀하게 관리하는 방법</content:encoded>
    </item>
    <item>
      <title>How Temporal Powers Reliable Cloud Operations at Netflix</title>
      <link>https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953?source=rss----2615bd06b42e---4</guid>
      <pubDate>Mon, 15 Dec 2025 23:51:59 GMT</pubDate>
      <content:encoded>&lt;p&gt;By &lt;a href=&quot;https://www.linkedin.com/in/jacobmeyers35/&quot;&gt;Jacob Meyers&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/robzienert/&quot;&gt;Rob Zienert&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://temporal.io/&quot;&gt;Temporal&lt;/a&gt; is a &lt;a href=&quot;https://docs.temporal.io/evaluate/understanding-temporal#durable-execution&quot;&gt;Durable Execution&lt;/a&gt; platform which allows you to write code “as if failures don’t exist”. It’s become increasingly critical to Netflix since its initial adoption in 2021, with users ranging from the operators of our &lt;a href=&quot;https://about.netflix.com/en/news/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience&quot;&gt;Open Connect&lt;/a&gt; global CDN to our &lt;a href=&quot;https://medium.com/netflix-techblog/behind-the-streams-live-at-netflix-part-1-d23f917c2f40&quot;&gt;Live&lt;/a&gt; reliability teams now depending on Temporal to operate their business-critical services. In this post, I’ll give a high-level overview of what Temporal offers users, the problems we were experiencing operating Spinnaker that motivated its initial adoption at Netflix, and how Temporal helped us reduce the number of transient deployment failures at Netflix from &lt;strong&gt;4% to 0.0001%&lt;/strong&gt;.&lt;/p&gt;&lt;h3&gt;A Crash Course on (some of) Spinnaker&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/global-continuous-delivery-with-spinnaker-2a6896c23ba7&quot;&gt;Spinnaker&lt;/a&gt; is a multi-cloud continuous delivery platform that powers the vast majority of Netflix’s software deployments. It’s composed of several (mostly nautical themed) microservices. Let’s double-click on two in particular to understand the problems we were facing that led us to adopting Temporal.&lt;/p&gt;&lt;p&gt;In case you’re completely new to Spinnaker, Spinnaker’s fundamental tool for deployments is the &lt;em&gt;Pipeline&lt;/em&gt;. A Pipeline is composed of a sequence of steps called &lt;em&gt;Stages&lt;/em&gt;, which themselves can be decomposed into one or more &lt;em&gt;Tasks&lt;/em&gt;, or other Stages. An example deployment pipeline for a production service may consist of these stages: Find Image -&amp;gt; Run Smoke Tests -&amp;gt; Run Canary -&amp;gt; Deploy to us-east-2 -&amp;gt; Wait -&amp;gt; Deploy to us-east-1.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;An example Spinnaker Pipeline&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*7sGhc8LhyqQlW9Uiq76TWQ.png&quot; /&gt;&lt;figcaption&gt;An example Spinnaker Pipeline for a Netflix service&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Pipeline configuration is extremely flexible. You can have Stages run completely serially, one after another, or you can have a mix of concurrent and serial Stages. Stages can also be executed conditionally based on the result of previous stages. This brings us to our first Spinnaker service: &lt;em&gt;Orca&lt;/em&gt;. Orca is the &lt;a href=&quot;https://raw.githubusercontent.com/spinnaker/orca/refs/heads/master/logo.jpg&quot;&gt;orca-stration&lt;/a&gt; engine of Spinnaker. It’s responsible for managing the execution of the Stages and Tasks that a Pipeline unrolls into and coordinating with other Spinnaker services to actually execute them.&lt;/p&gt;&lt;p&gt;One of those collaborating services is called &lt;em&gt;Clouddriver&lt;/em&gt;. In the example Pipeline above, some of the Stages will require interfacing with cloud infrastructure. For example, the canary deployment involves creating ephemeral hosts to run an experiment, and a full deployment of a new version of the service may involve spinning up new servers and then tearing down the old ones. We call these sorts of operations that mutate cloud infrastructure &lt;em&gt;Cloud Operations&lt;/em&gt;. Clouddriver’s job is to decompose and execute Cloud Operations sent to it by Orca as part of a deployment. Cloud Operations sent from Orca to Clouddriver are relatively high level (for example: createServerGroup), so Clouddriver understands how to translate these into lower-level cloud provider API calls.&lt;/p&gt;&lt;p&gt;Pain points in the interaction between Orca and Clouddriver and the implementation details of Cloud Operation execution in Clouddriver are what led us to look for new solutions and ultimately migrate to Temporal, so we’ll next look at the anatomy of a Cloud Operation. Cloud Operations in the OSS version of Spinnaker still work as described below, so motivated readers can follow along in &lt;a href=&quot;https://github.com/spinnaker/clouddriver&quot;&gt;source code&lt;/a&gt;, however our migration to Temporal is entirely closed-source following a fork from OSS in 2020 to allow Netflix to make larger pivots to the product such as this one.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;The Original Cloud Operation Flow&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;A Cloud Operation’s execution goes something like this:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Orca, in orchestrating a Pipeline execution, decides a particular Cloud Operation needs to be performed. It sends a POST request to Clouddriver’s /ops endpoint with an untyped bag-of-fields.&lt;/li&gt;&lt;li&gt;Clouddriver attempts to resolve the operation Orca sent into a set of AtomicOperation s— internal operations that only Clouddriver understands.&lt;/li&gt;&lt;li&gt;If the payload was valid and Clouddriver successfully resolved the operation, it will immediately return a Task ID to Orca.&lt;/li&gt;&lt;li&gt;Orca will immediately begin polling Clouddriver’s GET /task/&amp;lt;id&amp;gt; endpoint to keep track of the status of the Cloud Operation.&lt;/li&gt;&lt;li&gt;Asynchronously, Clouddriver begins executing AtomicOperations using &lt;em&gt;its own&lt;/em&gt; internal orchestration engine. Ultimately, the AtomicOperations resolve into cloud provider API calls. As the Cloud Operation progresses, Clouddriver updates an internal state store to surface progress to Orca.&lt;/li&gt;&lt;li&gt;Eventually, if all went well, Clouddriver will mark the Cloud Operation complete, which eventually surfaces to Orca in its polling. Orca considers the Cloud Operation finished, and the deployment can progress.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Y57y00EsM2YGRph9IRNmLQ.png&quot; /&gt;&lt;figcaption&gt;A sequence diagram of a Cloud Operation execution&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This works well enough on the happy path, but veer off the happy path and dragons begin to emerge:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Clouddriver has its own internal orchestration system independent of Orca to allow Orca to query the progress of Cloud Operation. This is largely undifferentiated lifting relative to Clouddriver’s goal of actuating cloud infrastructure changes, and ultimately adds complexity and surface area for bugs to the application. Additionally, Orca is tightly coupled to Clouddriver’s orchestration system — it must understand how to poll Clouddriver, interpret the status, and handle errors returned by Clouddriver.&lt;/li&gt;&lt;li&gt;Distributed systems are messy — networks and external services are unreliable. While executing a Cloud Operation, Clouddriver could experience transient network issues, or the cloud provider it’s attempting to call into may be having an outage, or any number of issues in between. Despite all of this, Clouddriver must be as reliable as reasonably possible as a core platform service. To deal with this shape of issue, Clouddriver internally evolved complex retry logic, further adding cognitive complexity to the system.&lt;/li&gt;&lt;li&gt;Remember how a Cloud Operation gets decomposed by Clouddriver into AtomicOperations? Sometimes, if there’s a failure in the middle of a Cloud Operation, we need to be able to roll back what was done in AtomicOperations prior to the failure. This led to a homegrown Saga framework being implemented inside Clouddriver. While this did result in a big step forward in reliability of Cloud Operations facing transient failures because the Saga framework &lt;em&gt;also&lt;/em&gt; allowed replaying partially-failed Cloud Operations, it added yet more undifferentiated lifting inside the service.&lt;/li&gt;&lt;li&gt;The task state kept by Clouddriver was &lt;em&gt;instance-local&lt;/em&gt;. In other words, if the Clouddriver instance carrying out a Cloud Operation crashed, that Cloud Operation state was lost, and Orca would eventually time out polling for the task status. The Saga implementation mentioned above mitigated this for certain operations, but was not widely adopted across all cloud providers supported by Spinnaker.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We introduced a &lt;em&gt;lot&lt;/em&gt; of incidental complexity into Clouddriver in an effort to keep Cloud Operation execution reliable, and despite all this deployments still failed around 4% of the time due to transient Cloud Operation failures.&lt;/p&gt;&lt;p&gt;Now, I can already hear you saying: “So what? Can’t people re-try their deployments if they fail?” While true, some pipelines take &lt;em&gt;days&lt;/em&gt; to complete for complex deployments, and a failed Cloud Operation mid-way through requires re-running the &lt;em&gt;whole&lt;/em&gt; thing. This was detrimental to engineering productivity at Netflix in a non-trivial way. Rather than continue trying to build a faster horse, we began to look elsewhere for our reliable orchestration requirements, which is where Temporal comes in.&lt;/p&gt;&lt;h3&gt;Temporal: Basic Concepts&lt;/h3&gt;&lt;p&gt;Temporal is an open source product that offers a durable execution platform for your applications. Durable execution means that the platform will ensure your programs run to completion despite adverse conditions. With Temporal, you organize your business logic into &lt;em&gt;Workflows&lt;/em&gt;, which are a deterministic series of steps. The steps inside of Workflows are called &lt;em&gt;Activities&lt;/em&gt;, which is where you encapsulate all your non-deterministic logic that needs to happen in the course of executing your Workflows. As your Workflows execute in processes called &lt;em&gt;Workers&lt;/em&gt;, the Temporal server durably stores their execution state so that in the event of failures your Workflows can be retried or even migrated to a different Worker. This makes Workflows incredibly resilient to the sorts of transient failures Clouddriver was susceptible to. Here’s a simple example Workflow in Java that runs an Activity to send an email once every 30 days:&lt;/p&gt;&lt;pre&gt;@WorkflowInterface&lt;br&gt;public interface SleepForDaysWorkflow {&lt;br&gt;    @WorkflowMethod&lt;br&gt;    void run();&lt;br&gt;}&lt;br&gt;&lt;br&gt;public class SleepForDaysWorkflowImpl implements SleepForDaysWorkflow {&lt;br&gt;&lt;br&gt;    private final SendEmailActivities emailActivities =&lt;br&gt;            Workflow.newActivityStub(&lt;br&gt;                    SendEmailActivities.class,&lt;br&gt;                    ActivityOptions.newBuilder()&lt;br&gt;                            .setStartToCloseTimeout(Duration.ofSeconds(10))&lt;br&gt;                            .build());&lt;br&gt;&lt;br&gt;    @Override&lt;br&gt;    public void run() {&lt;br&gt;        while (true) {&lt;br&gt;            // Activities already carry retries/timeouts via options.&lt;br&gt;            emailActivities.sendEmail();&lt;br&gt;&lt;br&gt;            // Pause the workflow for 30 days before sending the next email.&lt;br&gt;            Workflow.sleep(Duration.ofDays(30));&lt;br&gt;        }&lt;br&gt;    }&lt;br&gt;}&lt;br&gt;&lt;br&gt;@ActivityInterface&lt;br&gt;public interface SendEmailActivities {&lt;br&gt;    void sendEmail();&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;There’s some interesting things to note about this Workflow:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Workflows and Activities are just code, so you can test them using the same techniques and processes as the rest of your codebase.&lt;/li&gt;&lt;li&gt;Activities are automatically retried by Temporal with configurable exponential backoff.&lt;/li&gt;&lt;li&gt;Temporal manages all the execution state of the Workflow, including timers (like the one used by Workflow.sleep). If the Worker executing this workflow were to have its power cable unplugged, Temporal would ensure another Worker continues to execute it (even during the 30 day sleep).&lt;/li&gt;&lt;li&gt;Workflow sleeps are not compute-intensive, and they don’t tie up the process.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;You might already begin to see how Temporal solves a lot of the problems we had with Clouddriver. Ultimately, we decided to pull the trigger on migrating Cloud Operation execution to Temporal.&lt;/p&gt;&lt;h3&gt;Cloud Operations with Temporal&lt;/h3&gt;&lt;p&gt;Today, we execute Cloud Operations as Temporal workflows. Here’s what that looks like.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Orca, using a Temporal client, sends a request to Temporal to execute an UntypedCloudOperationRunner Workflow. The contract of the Workflow looks something like this:&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt;@WorkflowInterface&lt;br&gt;interface UntypedCloudOperationRunner {&lt;br&gt;  /**&lt;br&gt;   * Runs a cloud operation given an untyped payload.&lt;br&gt;   *&lt;br&gt;   * WorkflowResult is a thin wrapper around OutputType providing a standard contract for&lt;br&gt;   * clients to determine if the CloudOperation was successful and fetching any errors.&lt;br&gt;   */&lt;br&gt;  @WorkflowMethod&lt;br&gt;  fun &amp;lt;OutputType : CloudOperationOutput&amp;gt; run(stageContext: Map&amp;lt;String, Any?&amp;gt;, operationType: String): WorkflowResult&amp;lt;OutputType&amp;gt;&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;2. The Clouddriver Temporal worker is constantly polling Temporal for work. A worker will eventually see a task for an UntypedCloudOperationRunner Workflow and start executing it.&lt;/p&gt;&lt;p&gt;3. Similar to before with resolution into AtomicOperations, Clouddriver does some pre-processing of the bag-of-fields in stageContext and resolves it to a strongly typed implementation of the CloudOperation Workflow interface based on the operationType input and the stageContext:&lt;/p&gt;&lt;pre&gt;interface CloudOperation&amp;lt;I : CloudOperationInput, O : CloudOperationOutput&amp;gt; {&lt;br&gt;  @WorkflowMethod&lt;br&gt;  fun operate(input: I, credentials: AccountCredentials&amp;lt;out Any&amp;gt;): O&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;4. Clouddriver starts a &lt;a href=&quot;https://docs.temporal.io/child-workflows&quot;&gt;Child Workflow&lt;/a&gt; execution of the CloudOperation implementation it resolved. The child workflow will execute Activities which handle the actual cloud provider API calls to mutate infrastructure.&lt;/p&gt;&lt;p&gt;5. Orca uses its Temporal Client to await completion of the UntypedCloudOperationRunner Workflow. Once it’s complete, Temporal notifies the client and sends the result and Orca can continue progressing the deployment.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*leM3bH8iyb65_cmtl3vm4A.png&quot; /&gt;&lt;figcaption&gt;Sequence diagram of a Cloud Operation execution with Temporal&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Results and Lessons Learned from the Migration&lt;/h3&gt;&lt;p&gt;A shiny new architecture is great, but equally important is the non-glamorous work of refactoring legacy systems to fit the new architecture. How did we integrate Temporal into critical dependencies of all Netflix engineers transparently?&lt;/p&gt;&lt;p&gt;The answer, of course, is a combination of abstraction and dynamic configuration. We built a CloudOperationRunner interface in Orca to encapsulate whether the Cloud Operation was being executed via the legacy path or Temporal. At runtime, &lt;a href=&quot;https://netflixtechblog.com/announcing-archaius-dynamic-properties-in-the-cloud-bc8c51faf675&quot;&gt;Fast Properties&lt;/a&gt; (Netflix’s dynamic configuration system) determined which path a stage that needed to execute a Cloud Operation would take. We could set these properties quite granularly — by Stage type, cloud provider account, Spinnaker application, Cloud Operation type (createServerGroup), and cloud provider (either AWS or &lt;a href=&quot;https://netflix.github.io/titus/&quot;&gt;Titus&lt;/a&gt; in our case). The Spinnaker services themselves were the first to be deployed using Temporal, and within two quarters, all applications at Netflix were onboarded.&lt;/p&gt;&lt;h4&gt;Impact&lt;/h4&gt;&lt;p&gt;What did we have to show for it all? With Temporal as the orchestration engine for Cloud Operations, the percentage of deployments that failed due to transient Cloud Operation failures dropped from 4% to 0.0001%. For those keeping track at home, that’s a four and a half order of magnitude reduction. Virtually eliminating this failure mode for deployments was a huge win for developer productivity, especially for teams with long and complex deployment pipelines.&lt;/p&gt;&lt;p&gt;Beyond the improvement in deployment success metrics, we saw a number of other benefits:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Orca no longer needs to directly communicate with Clouddriver to start Cloud Operations or poll their status with Temporal as the intermediary. The services are less coupled, which is a win for maintainability.&lt;/li&gt;&lt;li&gt;Speaking of maintainability, with Temporal doing the heavy lifting of orchestration and retries inside of Clouddriver, we got to remove a lot of the homegrown logic we’d built up over the years for the same purpose.&lt;/li&gt;&lt;li&gt;Since Temporal manages execution state, Clouddriver instances became stateless and Cloud Operation execution can bounce between instances with impunity. We can treat Clouddriver instances more like cattle and enable things like &lt;a href=&quot;https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116&quot;&gt;Chaos Monkey&lt;/a&gt; for the service which we were previously prevented from doing.&lt;/li&gt;&lt;li&gt;Migrating Cloud Operation steps into Activities was a forcing function to re-write the logic to be idempotent. Since Temporal retries activities by default, it’s generally recommended they be idempotent. This alone fixed a number of issues that existed previously when operations were retried in Clouddriver.&lt;/li&gt;&lt;li&gt;We set the retry timeout for Activities in Clouddriver to be two hours by default. This gives us a long leash to fix-forward or rollback Clouddriver if we introduce a regression before customer deployments fail — to them, it might just look like a deployment is taking longer than usual.&lt;/li&gt;&lt;li&gt;Cloud Operations are much easier to introspect than before. Temporal ships with a great UI to help visualize Workflow and Activity executions, which is a huge boon for debugging live Workflows executing in production. The Temporal SDKs and server also emit a lot of useful metrics.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;A Cloud Operation Workflow as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*zmCyjwzTXji921mulJjmTw.png&quot; /&gt;&lt;figcaption&gt;Execution of a resizeServerGroup Cloud Operation as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Lessons Learned&lt;/h4&gt;&lt;p&gt;With the benefit of hindsight, there are also some lessons we can share from this migration:&lt;/p&gt;&lt;p&gt;1. &lt;strong&gt;Avoid unnecessary Child Workflows&lt;/strong&gt;: Structuring Cloud Operations as an UntypedCloudOperationRunner Workflow that starts Child Workflows to actually execute the Cloud Operation’s logic was unnecessary and the indirection made troubleshooting more difficult. There are &lt;a href=&quot;https://community.temporal.io/t/purpose-of-child-workflows/652&quot;&gt;situations&lt;/a&gt; where Child Workflows are appropriate, but in this case we were using them as a tool for code organization, which is generally unnecessary. We could’ve achieved the same effect with class composition in the top-level parent Workflow.&lt;/p&gt;&lt;p&gt;2. &lt;strong&gt;Use single argument objects&lt;/strong&gt;: At first, we structured Workflow and Activity functions with variable arguments, much as you’d write normal functions. This can be problematic for Temporal because of Temporal’s &lt;a href=&quot;https://community.temporal.io/t/workflow-determinism/4027&quot;&gt;determinism constraints&lt;/a&gt;. Adding or removing an argument from a function signature is &lt;strong&gt;not&lt;/strong&gt; a backward-compatible change, and doing so can break long-running workflows — and it’s not immediately obvious in code review your change is problematic. The preferred pattern is to use a single serializable class to host all your arguments for Workflows and Activities — these can be more freely changed without breaking determinism.&lt;/p&gt;&lt;p&gt;3. &lt;strong&gt;Separate business failures from workflow failures&lt;/strong&gt;: We like the pattern of the WorkflowResult type that UntypedCloudOperationRunner returns in the interface above. It allows us to communicate business process failures without failing the Workflow itself and have more overall nuance in error handling. This is a pattern we’ve carried over to other Workflows we’ve implemented since.&lt;/p&gt;&lt;h3&gt;Temporal at Netflix Today&lt;/h3&gt;&lt;p&gt;Temporal adoption has skyrocketed at Netflix since its initial introduction for Spinnaker. Today, we have hundreds of use cases, and we’ve seen adoption double in the last year with no signs of slowing down.&lt;/p&gt;&lt;p&gt;One major difference between initial adoption and today is that Netflix migrated from an on-prem Temporal deployment to using &lt;a href=&quot;https://temporal.io/cloud&quot;&gt;Temporal Cloud&lt;/a&gt;, which is Temporal’s SaaS offering of the Temporal server. This has let us scale Temporal adoption while running a lean team. We’ve also built up a robust internal platform around Temporal Cloud to integrate with Netflix’s internal ecosystem and make onboarding for our developers as easy as possible. Stay tuned for a future post digging into more specifics of our Netflix Temporal platform.&lt;/p&gt;&lt;h3&gt;Acknowledgement&lt;/h3&gt;&lt;p&gt;We all stand on the shoulders of giants in software. I want to call out that I’m retelling the work of my two stunning colleagues &lt;a href=&quot;https://www.linkedin.com/in/chris-smalley/&quot;&gt;Chris Smalley&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/robzienert/&quot;&gt;Rob Zienert&lt;/a&gt; in this post, who were the two aforementioned engineers who introduced Temporal and carried out the migration.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=73c69ccb5953&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953&quot;&gt;How Temporal Powers Reliable Cloud Operations at Netflix&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Netflix Live Origin</title>
      <link>https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371?source=rss----2615bd06b42e---4</guid>
      <pubDate>Mon, 15 Dec 2025 17:38:16 GMT</pubDate>
      <content:encoded>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/xiaomei-liu-b475711/&quot;&gt;Xiaomei Liu&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/joseph-lynch-9976a431/&quot;&gt;Joseph Lynch&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/chrisnewton2/&quot;&gt;Chris Newton&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967&quot;&gt;Behind the Streams: Building a Reliable Cloud Live Streaming Pipeline for Netflix&lt;/a&gt; introduced the architecture of the streaming pipeline. This blog post looks at the custom Origin Server we built for Live — the Netflix Live Origin. It sits at the demarcation point between the cloud live streaming pipelines on its upstream side and the distribution system, Open Connect, Netflix’s in-house Content Delivery Network (CDN), on its downstream side, and acts as a broker managing what content makes it out to Open Connect and ultimately to the client devices.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*44sJszKXEHZvSHnEQgYiiw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Live Streaming Distribution and Origin Architecture&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Netflix Live Origin is a multi-tenant microservice operating on EC2 instances within the AWS cloud. We lean on standard HTTP protocol features to communicate with the Live Origin. The Packager pushes segments to it using PUT requests, which place a file into storage at the particular location named in the URL. The storage location corresponds to the URL that is used when the Open Connect side issues the corresponding GET request.&lt;/p&gt;&lt;p&gt;Live Origin architecture is influenced by key technical decisions of the live streaming architecture. First, resilience is achieved through redundant regional live streaming pipelines, with failover orchestrated at the server-side to reduce client complexity. The implementation of &lt;a href=&quot;https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967&quot;&gt;epoch locking at the cloud encoder&lt;/a&gt; enables the origin to select a segment from either encoding pipeline. Second, Netflix adopted a manifest design with &lt;a href=&quot;https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40&quot;&gt;segment templates and constant segment duration&lt;/a&gt; to avoid frequent manifest refresh. The constant duration templates enable Origin to predict the segment publishing schedule.&lt;/p&gt;&lt;h3&gt;Multi-pipeline and multi-region aware origin&lt;/h3&gt;&lt;p&gt;Live streams inevitably contain defects due to the non-deterministic nature of live contribution feeds and strict real-time segment publishing timelines. Common defects include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Short segments:&lt;/strong&gt; Missing video frames and audio samples.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Missing segments:&lt;/strong&gt; Entire segments are absent.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Segment timing discontinuity:&lt;/strong&gt; Issues with the Track Fragment Decode Time.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Communicating segment discontinuity from the server to the client via a segment template-based manifest is impractical, and these defective segments can disrupt client streaming.&lt;/p&gt;&lt;p&gt;The redundant cloud streaming pipelines operate independently, encompassing distinct cloud regions, contribution feeds, encoder, and packager deployments. This independence substantially mitigates the probability of simultaneous defective segments across the dual pipelines. Owing to its strategic placement within the distribution path, the live origin naturally emerges as a component capable of intelligent candidate selection.&lt;/p&gt;&lt;p&gt;The Netflix Live Origin features multi-pipeline and multi-region awareness. When a segment is requested, the live origin checks candidates from each pipeline in a deterministic order, selecting the first valid one. Segment defects are detected via lightweight media inspection at the packager. This defect information is provided as metadata when the segment is published to the live origin. In the rare case of concurrent defects at the dual pipeline, the segment defects can be communicated downstream for intelligent client-side error concealment.&lt;/p&gt;&lt;h3&gt;Open Connect streaming optimization&lt;/h3&gt;&lt;p&gt;When the Live project started, Open Connect had become highly optimised for VOD content delivery — &lt;a href=&quot;https://freenginx.org/en/&quot;&gt;nginx&lt;/a&gt; had been chosen many years ago as the Web Server since it is highly capable in this role, and a number of enhancements had been added to it and to the underlying operating system (BSD). Unlike traditional CDNs, Open Connect is more of a distributed origin server — VOD assets are pre-positioned onto carefully selected server machines (OCAs, or Open Connect Appliances) rather than being filled on demand.&lt;/p&gt;&lt;p&gt;Alongside the VOD delivery, an on-demand fill system has been used for non-VOD assets — this includes artwork and the downloadable portions of the clients, etc. These are also served out of the same &lt;a href=&quot;https://freenginx.org/en/&quot;&gt;nginx&lt;/a&gt; workers, albeit under a distinct server block, using a distinct set of hostnames.&lt;/p&gt;&lt;p&gt;Live didn’t fit neatly into this ‘small object delivery’ model, so we extended the proxy-caching functionality of &lt;a href=&quot;https://freenginx.org/en/&quot;&gt;nginx&lt;/a&gt; to address Live-specific needs. We will touch on some of these here related to optimized interactions with the Origin Server. Look for a future blog post that will go into more details on the Open Connect side.&lt;/p&gt;&lt;p&gt;The segment templates provided to clients are also provided to the OCAs as part of the Live Event Configuration data. Using the Availability Start Time and Initial Segment number, the OCA is able to determine the legitimate range of segments for each event at any point in time — requests for objects outside this range can be rejected, preventing unnecessary requests going up through the fill hierarchy to the origin. If a request makes it through to the origin, and the segment isn’t available yet, the origin server will return a 404 Status Code (indicating File Not Found) with the expiration policy of that error so that it can be cached within Open Connect until just before that segment is expected to be published.&lt;/p&gt;&lt;p&gt;If the Live Origin knows when segments are being pushed to it, and knows what the live edge is — when a request is received for the immediately next object, rather than handing back another 404 error (which would go all the way back through Open Connect to the client), the Live Origin can ‘hold open’ the request, and service it once the segment has been published to it. By doing this, the degree of chatter within the network handling requests that arrive early has been significantly reduced. As part of this, millisecond grain caching was added to &lt;a href=&quot;https://freenginx.org/en/&quot;&gt;nginx&lt;/a&gt; to enhance the standard HTTP Cache Control, which only works at second granularity, a long time when segments are generated every 2 seconds.&lt;/p&gt;&lt;h4&gt;Streaming metadata enhancement&lt;/h4&gt;&lt;p&gt;The HTTP standard allows for the addition of request and response headers that can be used to provide additional information as files move between clients and servers. The HTTP headers provide notifications of events within the stream in a highly scalable way that is independently conveyed to client devices, regardless of their playback position within the stream.&lt;/p&gt;&lt;p&gt;These notifications are provided to the origin by the live streaming pipeline and are inserted by the origin in the form of headers, appearing on the segments generated at that point in time (and persist to future segments — they are cumulative). Whenever a segment is received at an OCA, this notification information is extracted from the response headers and used to update an in-memory data structure, keyed by event ID; and whenever a segment is served from the OCA, the latest such notification data is attached to the response. This means that, given any flow of segments into an OCA, it will always have the most recent notification data, even if all clients requesting it are behind the live edge. In fact, the notification information can be conveyed on any response, not just those supplying new segments.&lt;/p&gt;&lt;h4&gt;Cache invalidation and origin mask&lt;/h4&gt;&lt;p&gt;An invalidation system has been available since the early days of the project. It can be used to “flush” all content associated with an event by altering the key used when looking up objects in cache — this is done by incorporating a version number into the cache key that can then be bumped on demand. This is used during pre-event testing so that the network can be returned to a pristine state for the test with minimal fuss.&lt;/p&gt;&lt;p&gt;Each segment published by the Live Origin conveys the encoding pipeline it was generated by, as well as the region it was requested from. Any issues that are found after segments make their way into the network can be remedied by an enhanced invalidation system that takes such variants into account. It is possible to invalidate (that is, cause to be considered expired) segments in a range of segment numbers, but only if they were sourced from encoder A, or from Encoder A, but only if retrieved from region X.&lt;/p&gt;&lt;p&gt;In combination with Open Connect’s enhanced cache invalidation, the Netflix Live Origin allows &lt;em&gt;selective encoding pipeline masking&lt;/em&gt; to exclude a range of segments from a particular pipeline when serving segments to Open Connect. The enhanced cache invalidation and origin masking enable live streaming operations to hide known problematic segments (e.g., segments causing client playback errors) from streaming clients once the bad segments are detected, protecting millions of streaming clients during the DVR playback window.&lt;/p&gt;&lt;h3&gt;Origin storage architecture&lt;/h3&gt;&lt;p&gt;Our original storage architecture for the Live Origin was simple: just use &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;AWS S3&lt;/a&gt; like we do for SVOD. This served us well initially for our low-traffic events, but as we scaled up we discovered that Live streaming has unique latency and workload requirements that differ significantly from on-demand where we have significant time ahead-of-time to pre-position content. While S3 met its stated uptime guarantees, our strict 2-second retry budget inherent to Live events (where every write is critical) led us to explore optimizations specifically tailored for real-time delivery at scale. AWS S3 is an amazing object store, but our Live streaming requirements were closer to those of a global low-latency highly-available database. So, we went back to the drawing board and started from the requirements. The Origin required:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;[HA Writes] Extremely high &lt;em&gt;write&lt;/em&gt; availability, ideally as close to full write availability within a single AWS region, with low second replication delay to other regions. Any failed write operation within 500ms is considered a bug that must be triaged and prevented from re-occurring.&lt;/li&gt;&lt;li&gt;[Throughput] High write throughput, with hundreds of MiB replicating across regions&lt;/li&gt;&lt;li&gt;[Large Partitions] Efficiently support O(MiB) writes that accumulate to O(10k) keys per partition with O(GiB) total size per event.&lt;/li&gt;&lt;li&gt;[Strong Consistency] Within the same region, we needed read-your-write semantics to hit our &amp;lt;1s read delay requirements (must be able to read published segments)&lt;/li&gt;&lt;li&gt;[Origin Storm] During worst-case load involving Open Connect edge cases, we may need to handle O(&lt;strong&gt;GiB&lt;/strong&gt;) of read throughput &lt;em&gt;without affecting writes&lt;/em&gt;.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Fortunately, Netflix had previously invested in building a &lt;a href=&quot;https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30&quot;&gt;KeyValue Storage Abstraction&lt;/a&gt; that cleverly leveraged &lt;a href=&quot;https://youtu.be/sQ-_jFgOBng?t=1061&quot;&gt;Apache Cassandra&lt;/a&gt; to provide chunked storage of MiB or even GiB values. This abstraction was initially built to support cloud saves of Game state. The Live use case would push the boundaries of this solution, however, in terms of availability for writes (#1), cumulative partition size (#3), and read throughput during Origin Storm (#5).&lt;/p&gt;&lt;h4&gt;High Availability for Writes of Large Payloads&lt;/h4&gt;&lt;p&gt;The &lt;a href=&quot;https://youtu.be/paTtLhZFsGE?t=1077&quot;&gt;KeyValue Payload Chunking and Compression Algorithm&lt;/a&gt; breaks O(MiB) work down so each part can be idempotently retried and hedged to maintain strict latency service level objectives, as well as spreading the data across the full cluster. When we combine this algorithm with Apache Cassandra’s local-quorum consistency model, which allows write availability even with an entire Availability Zone outage, plus a write-optimized &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;Log-Structured Merge Tree&lt;/a&gt; (LSM) storage engine, we could meet the first four requirements. After iterating on the performance and availability of this solution, we were not only able to achieve the write availability required, but did so with a P99 &lt;em&gt;tail&lt;/em&gt; latency that was similar to the status quo’s P50 &lt;em&gt;average &lt;/em&gt;latency while also handling cross-region replication behind the scenes for the Origin. This new solution was significantly more expensive (as expected, databases backed by SSD cost more), but minimizing cost was &lt;em&gt;not&lt;/em&gt; a key objective and low latency with high availability was:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*bUPc4gC-mSDcybayhBJQ8g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Storage System Write Performance&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;High Availability Reads at Gbps Throughputs&lt;/h4&gt;&lt;p&gt;Now that we solved the write reliability problem, we had to handle the Origin Storm failure case, where potentially dozens of Open Connect top-tier caches could be requesting multiple O(MiB) video segments at once. Our back-of-the-envelope calculations showed worst-case read throughput in the O(100Gbps) range, which would normally be extremely expensive for a strongly-consistent storage engine like Apache Cassandra. With careful tuning of chunk access, we were able to respond to reads at network line rate (100Gbps) from Apache Cassandra, but we observed unacceptable performance and availability degradation on concurrent writes. To resolve this issue, we introduced write-through caching of chunks using our distributed caching system &lt;a href=&quot;https://github.com/Netflix/EVCache&quot;&gt;EVCache&lt;/a&gt;, which is based on Memcached. This allows almost all reads to be served from a highly scalable cache, allowing us to easily hit 200Gbps and beyond without affecting the write path, achieving read-write separation.&lt;/p&gt;&lt;h4&gt;Final Storage Architecture&lt;/h4&gt;&lt;p&gt;In the final storage architecture, the Live Origin writes and reads to KeyValue, which manages a write-through cache to EVCache (memcached) and implements a safe chunking protocol that spreads large values and partitions them out across the storage cluster (Apache Cassandra). This allows almost all read load to be handled from cache, with only misses hitting the storage. This combination of cache and highly available storage has met the demanding needs of our Live Origin for over a year now.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*yA9H-BFemM_-99FXBlVOMg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Storage System High Level Architecture&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Delivering this consistent low latency for large writes with cross-region replication and consistent write-through caching to a distributed cache required solving numerous hard problems with novel techniques, which we plan to share in detail during a future post.&lt;/p&gt;&lt;h3&gt;Scalability and scalable architecture&lt;/h3&gt;&lt;p&gt;Netflix’s live streaming platform must handle a high volume of diverse stream renditions for each live event. This complexity stems from supporting various video encoding formats (each with multiple encoder ladders), numerous audio options (across languages, formats, and bitrates), and different content versions (e.g., with or without advertisements). The combination of these elements, alongside concurrent event support, leads to a significant number of unique stream renditions per live event. This, in turn, necessitates a high Requests Per Second (RPS) capacity from the multi-tenant live origin service to ensure publishing-side scalability.&lt;/p&gt;&lt;p&gt;In addition, Netflix’s global reach presents distinct challenges to the live origin on the retrieval side. During the Tyson vs. Paul fight event in 2024, a historic peak of 65 million concurrent streams was observed. Consequently, a scalable architecture for live origin is essential for the success of large-scale live streaming.&lt;/p&gt;&lt;h4&gt;Scaling architecture&lt;/h4&gt;&lt;p&gt;We chose to build a highly scalable origin instead of relying on the traditional origin shields approach for better end-to-end cache consistency control and simpler system architecture. The live origin in this architecture directly connects with top-tier Open Connect nodes, which are geographically distributed across several sites. To minimize the load on the origin, only designated nodes per stream rendition at each site are permitted to directly fill from the origin.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*jW7eBCQtjlna0VKaWrKz_A.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Netflix Live Origin Scalability Architecture&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While the origin service can autoscale horizontally using EC2 instances, there are other system resources that are not autoscalable, such as storage platform capacity and AWS to Open Connect backbone bandwidth capacity. Since in live streaming, not all requests to the live origin are of the same importance, the origin is designed to prioritize more critical requests over less critical requests when system resources are limited. The table below outlines the request categories, their identification, and protection methods.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dYKFJkq22KI8sDBW_Njmog.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Publishing isolation&lt;/h4&gt;&lt;p&gt;Publishing traffic, unlike potentially surging CDN retrieval traffic, is predictable, making path isolation a highly effective solution. As shown in the scalability architecture diagram, the origin utilizes separate EC2 publishing and CDN stacks to protect the latency and failure-sensitive origin writes. In addition, the storage abstraction layer features distinct clusters for key-value (KV) read and KV write operations. Finally, the storage layer itself separates read (EVCache) and write (Cassandra) paths. This comprehensive path isolation facilitates independent cloud scaling of publishing and retrieval, and also prevents CDN-facing traffic surges from impacting the performance and reliability of origin publishing.&lt;/p&gt;&lt;h4&gt;Priority rate limiting&lt;/h4&gt;&lt;p&gt;Given Netflix’s scale, managing incoming requests during a traffic storm is challenging, especially considering non-autoscalable system resources. The Netflix Live Origin implemented priority-based rate limiting when the underlying system is under stress. This approach ensures that requests with greater user impact are prioritized to succeed, while requests with lower user impact are allowed to fail during times of stress in order to protect the streaming infrastructure and are permitted to retry later to succeed.&lt;/p&gt;&lt;p&gt;Leveraging Netflix’s microservice platform priority rate limiting feature, the origin prioritizes live edge traffic over DVR traffic during periods of high load on the storage platform. The live edge vs. DVR traffic detection is based on the predictable segment template. The template is further cached in memory on the origin node to enable priority rate limiting without access to the datastore, which is valuable especially during periods of high datastore stress.&lt;/p&gt;&lt;p&gt;To mitigate traffic surges, TTL cache control is used alongside priority rate limiting. When the low-priority traffic is impacted, the origin instructs Open Connect to slow down and cache identical requests for 5 seconds by setting a max-age = 5s and returns an HTTP 503 error code. This strategy effectively dampens traffic surges by preventing repeated requests to the origin within that 5-second window.&lt;/p&gt;&lt;p&gt;The following diagrams illustrate origin priority rate limiting with simulated traffic. The nliveorigin_mp41 traffic is the low-priority traffic and is mixed with other high-priority traffic. In the first row: the 1st diagram shows the request RPS, the 2nd diagram shows the percentage of request failure. In the second row, the 1st diagram shows datastore resource utilization, and the 2nd diagram shows the origin retrieval P99 latency. The results clearly show that only the low-priority traffic (nliveorigin_mp41) is impacted at datastore high utilization, and the origin request latency is under control.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*-_YP6H3sEDaw1lS8prH4mQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Origin Priority Rate Limiting&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;404 storm and cache optimization&lt;/h4&gt;&lt;p&gt;Publishing isolation and priority rate limiting successfully protect the live origin from DVR traffic storms. However, the traffic storm generated by requests for non-existent segments presents further challenges and opportunities for optimization.&lt;/p&gt;&lt;p&gt;The live origin structures metadata hierarchically as event &amp;gt; stream rendition &amp;gt; segment, and the segment publishing template is maintained at the stream rendition level. This hierarchical organization allows the origin to preemptively reject requests with an HTTP 404(not found)/410(Gone) error, leveraging highly cacheable event and stream rendition level metadata, avoiding unnecessary queries to the segment level metadata:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;If the event is unknown, reject the request with 404&lt;/li&gt;&lt;li&gt;If the event is known, but the segment request timing does not match the expected publishing timing, reject the request with 404 and cache control TTL matching the expected publishing time&lt;/li&gt;&lt;li&gt;If the event is known, the requested segment is never generated or misses the retry deadline, reject the request with a 410 error, preventing the client from repeatedly requesting&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;At the storage layer, metadata is stored separately from media data in the control plane datastore. Unlike the media datastore, the control plane datastore does not use a distributed cache to avoid cache inconsistency. Event and rendition level metadata benefits from a high cache hit ratio when in-memory caching is utilized at the live origin instance. During traffic storms involving non-existent segments, the cache hit ratio for control plane access easily exceeds 90%.&lt;/p&gt;&lt;p&gt;The use of in-memory caching for metadata effectively handles 404 storms at the live origin without causing datastore stress. This metadata caching complements the storage system’s distributed media cache, providing a complete solution for traffic surge protection.&lt;/p&gt;&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The Netflix Live Origin, built upon an optimized storage platform, is specifically designed for live streaming. It incorporates advanced media and segment publishing scheduling awareness and leverages enhanced intelligence to improve streaming quality, optimize scalability, and improve Open Connect live streaming operations.&lt;/p&gt;&lt;h3&gt;Acknowledgement&lt;/h3&gt;&lt;p&gt;Many teams and stunning colleagues contributed to the Netflix live origin. Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/flavioribeiro/?originalSubdomain=br&quot;&gt;Flavio Ribeiro&lt;/a&gt; for advocacy and sponsorship of the live origin project; to &lt;a href=&quot;https://www.linkedin.com/in/rummadis/&quot;&gt;Raj Ummadisetty&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/prudhviraj9/&quot;&gt;Prudhviraj Karumanchi&lt;/a&gt; for the storage platform; to &lt;a href=&quot;https://www.linkedin.com/in/rosanna-lee-197920/&quot;&gt;Rosanna Lee&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/hunterford/&quot;&gt;Hunter Ford&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/in/thiagopnts/&quot;&gt;Thiago Pontes&lt;/a&gt; for storage lifecycle management; to &lt;a href=&quot;https://www.linkedin.com/in/ameya-vasani-8904304/&quot;&gt;Ameya Vasani&lt;/a&gt; for e2e test framework; &lt;a href=&quot;https://www.linkedin.com/in/thomas-symborski-b4216728/&quot;&gt;Thomas Symborski&lt;/a&gt; for orchestrator integration; to &lt;a href=&quot;https://www.linkedin.com/in/jschek/&quot;&gt;James Schek&lt;/a&gt; for Open Connect integration; to &lt;a href=&quot;https://www.linkedin.com/in/kzwang/&quot;&gt;Kevin Wang&lt;/a&gt; for platform priority rate limit; to &lt;a href=&quot;https://www.linkedin.com/in/di-li-09663968/&quot;&gt;Di Li&lt;/a&gt;, &lt;a href=&quot;mailto:nhubbard@netflix.com&quot;&gt;Nathan Hubbard&lt;/a&gt; for origin scalability testing.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=41f1b0ad5371&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371&quot;&gt;Netflix Live Origin&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>우리가 테스트를 하는 이유. 근데 이제 Golang을 곁들인</title>
      <link>https://blog.banksalad.com/tech/why-we-do-test-by-golang/</link>
      <guid>https://blog.banksalad.com/tech/why-we-do-test-by-golang/</guid>
      <pubDate>Mon, 15 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 금융쇼핑 PA Server Engineer 김한수입니다. 이 글에서는 “우리는 왜 테스트를 하는지?”에 대한 질문에 좀 더 깊숙이 들어가 보고 실제 Golang…</content:encoded>
    </item>
    <item>
      <title>AV1 — Now Powering 30% of Netflix Streaming</title>
      <link>https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80?source=rss----2615bd06b42e---4</guid>
      <pubDate>Thu, 04 Dec 2025 20:09:30 GMT</pubDate>
      <content:encoded>&lt;h3&gt;&lt;strong&gt;AV1 — Now Powering 30% of Netflix Streaming&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/liwei-guo/&quot;&gt;Liwei Guo&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/henryzhili/&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/sheldon-radford/&quot;&gt;Sheldon Radford&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jeffrwatts/&quot;&gt;Jeff Watts&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Streaming video has become an integral part of our daily lives. At Netflix, our top priority is delivering the best possible entertainment experience to our members, regardless of their devices or network conditions. One of the key technologies enabling this is &lt;a href=&quot;https://aomedia.org/specifications/av1/&quot;&gt;AV1&lt;/a&gt;, a modern, open video codec that is rapidly transforming both how we stream content and how users experience it. Today, AV1 powers approximately 30% of all Netflix viewing, marking a major milestone in our efforts to bring more efficient and higher-quality streaming to our members.&lt;/p&gt;&lt;p&gt;In this post, we’ll revisit Netflix’s AV1 journey to date, highlight emerging use cases, and share adoption trends across the device ecosystem. Having witnessed AV1’s significant impact，and with &lt;a href=&quot;https://aomedia.org/press%20releases/AOMedia-Announces-Year-End-Launch-of-Next-Generation-Video-Codec-AV2-on-10th-Anniversary/&quot;&gt;AV2 on the horizon&lt;/a&gt;, we’re more excited than ever about how open codecs will continue to revolutionize streaming for everyone.&lt;/p&gt;&lt;h3&gt;AV1: A Modern, Open Codec&lt;/h3&gt;&lt;p&gt;Since entering the streaming business in 2007, Netflix has primarily relied on H.264/AVC as its streaming format. However, we quickly recognized that a modern, open codec would benefit not only Netflix, but the entire multimedia industry. In 2015, together with a group of like-minded industry leaders, Netflix co-founded the &lt;a href=&quot;https://aomedia.org/&quot;&gt;Alliance for Open Media (AOMedia)&lt;/a&gt; to develop and promote next generation, open source media technologies. The AV1 codec became the first major project of this collaboration, with ambitious goals: to deliver significant improvements in compression efficiency over state-of-the-art codecs, and to introduce rich features that enable new use cases. After three years of collaborative development, AV1 was officially released in 2018.&lt;/p&gt;&lt;h3&gt;Netflix’s AV1 Journey: From Android to TVs and Beyond&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Piloting on Android Mobile&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;When we first set out to bring AV1 streaming to Netflix members, Android was the ideal starting point. Android’s flexibility allowed us to quickly integrate a software AV1 decoder using the efficient &lt;a href=&quot;https://code.videolan.org/videolan/dav1d&quot;&gt;dav1d&lt;/a&gt; library, which was already optimized for ARM chipsets in mobile devices.&lt;/p&gt;&lt;p&gt;AV1’s superior compression efficiency was especially valuable for mobile users, many of whom are mindful of their data usage and network conditions. By adopting AV1, we were able to deliver noticeably better video quality at lower bitrates. For members relying on cellular data, this meant crisper images with fewer compression artifacts, even when bandwidth was limited. &lt;a href=&quot;https://netflixtechblog.com/netflix-now-streaming-av1-on-android-d5264a515202&quot;&gt;Launching AV1 support on Android&lt;/a&gt; in 2020 marked a significant step forward for Netflix on mobile, making high-quality streaming more accessible and enjoyable for members everywhere.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Front-and-Center for Netflix VOD Streaming&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The success of our AV1 launch on Android proved its value for Netflix streaming, motivating us to expand support to smart TVs and other large-screen devices, where most of our members watch their favorite shows.&lt;/p&gt;&lt;p&gt;Smart TVs depend on hardware decoders for efficient high-quality playback. We worked closely with device manufacturers and SoC vendors to certify these devices, ensuring they are both conformant and performant. This collaborative effort enabled our AV1 streaming to TV devices in &lt;a href=&quot;https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320&quot;&gt;late 2021&lt;/a&gt;. Shortly thereafter, we expanded AV1 streaming to web browsers (in 2022) and continued to broaden device support. In 2023, this included Apple devices with the introduction of AV1 hardware support in the new M3 and A17 Pro chips.&lt;/p&gt;&lt;p&gt;As more devices began shipping with AV1 hardware support, a rapidly growing share of our members could enjoy the benefits of this advanced codec. Combined with our investment in adding AV1 streams across the entire catalog, AV1 viewing share has been consistently increasing in recent years. Today, AV1 accounts for approximately 30% of all Netflix streaming, making it our second most-used codec — and it’s on track to become number one very soon. The payoff has been substantial.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Elevating Streaming Experience Across the Board&lt;/strong&gt;: Large-screen TVs and other devices demand higher bitrates to deliver stunning 4K, high frame rate (HFR) experiences. AV1’s superior compression efficiency has allowed us to provide these experiences using less data, making high-quality streaming more accessible and reliable. On average, AV1 streaming sessions achieve VMAF scores¹ that are 4.3 points higher than AVC and 0.9 points higher than HEVC sessions. At the same time, AV1 sessions use one-third less bandwidth than both AVC and HEVC, resulting in 45% fewer buffering interruptions. Moreover, Netflix’s diverse content catalog benefits universally from AV1, with improvements across all content types.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Driving Network Efficiency Worldwide&lt;/strong&gt;: Netflix streams are delivered through our own content delivery network (&lt;a href=&quot;https://openconnect.netflix.com/en/?utm_referrer=https://www.google.com/&quot;&gt;Open Connect&lt;/a&gt;), in partnership with local ISPs around the globe. With more than 300 million members, Netflix streaming constitutes a non-trivial portion of global internet traffic. Because AV1 is a more efficient codec, its streams are smaller in size (while providing even better visual quality). By shifting a substantial share of our streaming to AV1, we reduce overall internet bandwidth consumption, and lessen system and network load for both Netflix and our partners.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Unlocking Advanced Experiences&lt;/h4&gt;&lt;p&gt;In addition to its superior compression efficiency, AV1 was designed to support a rich set of features. Once we established a robust framework for the continuous expansion of AV1 streaming, we quickly shifted our focus towards exploring AV1’s unique features to unlock even more advanced and immersive experiences for our members.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;High-Dynamic-Range(HDR)&lt;br&gt;&lt;/strong&gt;HDR brings enhanced detail, vivid colors, and greater clarity to images. As a premium streaming service, Netflix has been a pioneer in adopting HDR, offering HDR streaming since 2016. In March 2025, we launched &lt;a href=&quot;https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b&quot;&gt;AV1 HDR streaming&lt;/a&gt;. We chose HDR10+ as the HDR format for its use of dynamic metadata, which enabled us to adapt the tone mapping per device in a scene-dependent manner.&lt;/p&gt;&lt;p&gt;As anticipated, the combination of AV1 and HDR10+ allows us to deliver images with greater detail, more vibrant colors, and an overall heightened sense of immersion for our members. At the moment, 85% of our HDR catalog (from the perspective of view-hours) has AV1-HDR10+ coverage, and this number is expected to reach 100% in the next couple of months.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/759/1*Ubhj9prgqb0zuTHt6oOx0g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;Photographs of devices displaying the same (cropped) frame with HDR10 metadata (left) and HDR10+ metadata (right). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one.&lt;/em&gt;&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Cinematic Film Grain&lt;br&gt;&lt;/strong&gt;Film grain is a hallmark of the cinematic experience, widely used in the movie industry to enhance a film’s depth, texture, and realism. However, because film grain is inherently random, faithfully representing it in digital video requires a significant amount of data. This presents a unique challenge for streaming: restricting the bitrate can result in grain that appears unnatural or distorted, while increasing the bitrate to accurately preserve cinematic grain almost inevitably leads to elevated rebuffering. The AV1 specification incorporates a unique solution called Film Grain Synthesis (FGS). Instead of encoding grain as part of every frame, the grain is stripped out before encoding and then resynthesized at the decoder using parameters sent in the bitstream, delivering a realistic cinematic film grain experience without the usual data costs.&lt;/p&gt;&lt;p&gt;This approach represents a significant shift from traditional compression and streaming techniques. Our team invested substantial effort in fine-tuning the media processing pipeline, ensuring FGS delivers robust performance at scale. In July 2025, we successfully &lt;a href=&quot;https://netflixtechblog.com/av1-scale-film-grain-synthesis-the-awakening-ee09cfdff40b&quot;&gt;productized AV1 FGS&lt;/a&gt;, and the results were astonishing: AV1 with FGS could deliver videos with cinematic film grain at a bitrate well within the capabilities of typical household internet connections. For non-FGS AV1 encodings, even at much higher bitrate, they may not be able to achieve comparable quality.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1016/1*9fB5xuoFpbpN8ZQIzTHDtg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;The same (cropped) frame from source (left), regular AV1 stream encoded at 8274kbps (middle) and AV1 FGS stream encoded at 2804 kbps (right). The AV1 FGS stream reduces the bitrate by 66% while delivering clearly better quality.&lt;/em&gt;&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Beyond VOD Streaming&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;So far, our AV1 journey has been mainly on VOD, but we see significant opportunities for AV1 beyond traditional VOD streaming. On a mission to entertain the world, Netflix has constantly explored and established other ways to bring joy to our members, and we believe AV1 could contribute to the success of these new products.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Live Streaming&lt;br&gt;&lt;/strong&gt;Debuting in 2023, live streaming has experienced &lt;a href=&quot;https://help.netflix.com/en/node/129840&quot;&gt;rapid growth&lt;/a&gt; at Netflix, becoming a key part of our streaming offerings in just two short years. We are actively evaluating the use of AV1 in live streaming, as we believe it could help further scale Netflix’s live programming:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Hyper-scale concurrent viewership: &lt;/strong&gt;Live streaming at Netflix means delivering content to &lt;a href=&quot;https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news&quot;&gt;tens of millions&lt;/a&gt; of viewers simultaneously. AV1’s superior compression efficiency could significantly reduce the required bandwidth, enabling us to deliver high-quality live experiences to large audiences without compromising video quality.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Customizable graphics overlay&lt;/strong&gt;: for live sport events such as football, tennis and boxing, graphics overlays have become an integral part of the member experience — from embedding game statistics to delivering sponsorships. AV1 offers an opportunity to make the graphics highly customizable: layered coding is supported in AV1’s main profile, allowing encoding the main content in the base layer, and graphics in the enhancement layer, and easily swapping out one version of the enhancement layer with another. We envision that the use of AV1’s layered coding can greatly simplify the live streaming workflow and reduce delivery costs.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Cloud Gaming&lt;br&gt;&lt;/strong&gt;Cloud gaming is a new Netflix offering that is currently in the &lt;a href=&quot;https://help.netflix.com/en/node/132197&quot;&gt;beta phase&lt;/a&gt; and is available to members in select countries. The game engines run on cloud servers, while the rendered graphics are streamed directly to members’ devices. By removing barriers and transforming every Netflix-enabled device into a game console, Cloud gaming aims to deliver a seamless, “play anywhere” experience for our members. For a glimpse of this in action, &lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7382077927875825664/&quot;&gt;watch as Co-CEO Greg Peters and CTO Elizabeth Stone play a round of Boggle Party — powered entirely by Netflix’s cloud gaming platform&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;Unlike traditional video streaming, cloud gaming requires that every player action is reflected instantly on the screen to ensure a responsive and immersive experience. This makes delivering high-quality video frames with extremely low latency, despite fluctuating network conditions, one of the biggest challenges in cloud gaming.&lt;/p&gt;&lt;p&gt;Our team is actively working on productizing AV1 for cloud gaming. Given AV1’s high compression efficiency, we can reduce frame sizes, helping video frames get through even when network conditions become challenging. This positions AV1 as a promising technology for enabling a high-quality, low-latency gaming experience across a wide range of devices.&lt;/p&gt;&lt;h3&gt;A Device Ecosystem United for AV1&lt;/h3&gt;&lt;p&gt;Netflix is a streaming company, and we have worked diligently to create highly efficient and standards-conformant AV1 streams for our catalog. However, an equally, if not more, important factor in AV1’s success is the widespread support from device manufacturers. Throughout our AV1 journey, we have been impressed by the unprecedented pace at which the device ecosystem has embraced AV1.&lt;/p&gt;&lt;p&gt;Just six months after the AV1 specification was finalized, the open-source AV1 decoder library sponsored by AOM, dav1d, was released. Small, performant, and highly resource-efficient, dav1d bridged the gap for early adopters like Netflix while hardware solutions were still in development. Continuous improvements to its performance and compatibility have made dav1d the preferred choice for a wide range of platforms and practical applications. Today, it serves as &lt;a href=&quot;https://aomedia.org/av1-adoption-showcase/google-story/&quot;&gt;Android’s default software decoder&lt;/a&gt;. Additionally, it plays a key role in web browsers — for Netflix, it powers approximately 40% of our browser playback. This broad adoption has significantly expanded access to high-quality AV1 streaming, even in the absence of dedicated hardware decoders.&lt;/p&gt;&lt;p&gt;Netflix maintains a close working relationship with device manufacturers and SoC vendors, and we have witnessed first-hand their enthusiasm for adopting AV1. To ensure optimal streaming performance, Netflix has a rigorous certification process to verify proper support for our streaming formats on devices. AV1 was added to this certification process in 2019, and since then, we have seen a steady increase in the number of devices with full AV1 decoding capabilities. Over the past five years (2021–2025), 88% of large-screen devices, including TVs, set-top boxes, and streaming sticks, submitted for Netflix certification have supported AV1, with the vast majority offering full 4K@60fps capability. Notably, since 2023, almost all devices we have received for certification are AV1-capable.&lt;/p&gt;&lt;p&gt;We have also been impressed by the robustness of AV1 implementations across these devices. As mentioned earlier, FGS is an innovative tool that departs from traditional codec architectures and was not included in our initial full-scale AV1 streaming rollout. When we launched FGS this July, we worked closely with our partners to ensure broad device compatibility. We are pleased with the successful progress made, and AV1 with FGS is now supported across a significant and growing number of in-field devices.&lt;/p&gt;&lt;h3&gt;Looking Ahead: AV1 Today, AV2 Tomorrow&lt;/h3&gt;&lt;p&gt;As we reflect on our AV1 journey, it’s clear that the codec has already transformed the streaming experience for hundreds of millions of Netflix members worldwide. Thanks to industry-wide collaboration and rapid device adoption, AV1 is delivering higher quality, greater efficiency, and new cinematic features to more screens than ever before.&lt;/p&gt;&lt;p&gt;Looking ahead, we are excited about the forthcoming release of AV2, announced by the Alliance for Open Media for the end of 2025. &lt;a href=&quot;https://www.youtube.com/watch?v=RUMwMe_2Dqo&quot;&gt;AV2 is poised to set a new benchmark for compression efficiency and streaming capabilities, building on the solid foundation laid by AV1&lt;/a&gt;. At Netflix, we remain committed to adopting the best open technologies to delight our members around the globe. While AV2 represents the future of streaming, AV1 is very much the present — serving as the backbone of our platform and powering exceptional entertainment experiences across a vast and ever-expanding ecosystem of devices.&lt;/p&gt;&lt;h3&gt;Acknowledgement&lt;/h3&gt;&lt;p&gt;The success of AV1 at Netflix is the result of the dedication, expertise, and collaboration of many teams across the company — including Encoding, Clients, Device Certification, Partner Engineering, Data Science &amp;amp; Engineering, Infra, Platform, etc.&lt;/p&gt;&lt;p&gt;We would also like to thank &lt;a href=&quot;https://www.linkedin.com/in/artemdanylenko/&quot;&gt;Artem Danylenko&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/aditya-mavlankar-7139791/&quot;&gt;Aditya Mavlankar&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anne-aaron/&quot;&gt;Anne Aaron&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/cyril-concolato-567a522/&quot;&gt;Cyril Concolato&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/allanzp/&quot;&gt;Allan Zhou&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/anush-moorthy-b8451142/&quot;&gt;Anush Moorthy&lt;/a&gt; for their valuable comments and feedback on earlier drafts of this post.&lt;/p&gt;&lt;h3&gt;Footnotes&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;These numbers represent a snapshot of data from November 13, 2025. Actual values may vary slightly from day to day and across different regions, depending on the mix of content, devices, and internet connectivity.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*xuLP8glDDcj-DBYO8djmNA.png&quot; /&gt;&lt;/figure&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=02f592242d80&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80&quot;&gt;AV1 — Now Powering 30% of Netflix Streaming&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>개발자의 시간을 벌어주는 두 가지 도구: 잘 쓴 테크 스펙, 그리고 AI</title>
      <link>http://thefarmersfront.github.io/blog/tech-spec-adoption-with-ai-automation/</link>
      <guid>http://thefarmersfront.github.io/blog/tech-spec-adoption-with-ai-automation/</guid>
      <pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate>
      <content:encoded>컬리 프로덕트 웹개발 팀의 테크 스펙 정착기와 AI 자동화 시도</content:encoded>
    </item>
    <item>
      <title>Kafka Streams 윈도우 도입기</title>
      <link>http://thefarmersfront.github.io/blog/2025-kafka-streams-window/</link>
      <guid>http://thefarmersfront.github.io/blog/2025-kafka-streams-window/</guid>
      <pubDate>Mon, 01 Dec 2025 14:00:00 GMT</pubDate>
      <content:encoded>재고 정산 처리에 Kafka Streams window를 도입하며 겪은 이야기</content:encoded>
    </item>
    <item>
      <title>빌드가 터졌다: 5년 된 CMS 프로젝트의 Webpack4 → Vite 전환</title>
      <link>http://thefarmersfront.github.io/blog/cms-vite-%EC%A0%84%ED%99%98%EA%B8%B0/</link>
      <guid>http://thefarmersfront.github.io/blog/cms-vite-%EC%A0%84%ED%99%98%EA%B8%B0/</guid>
      <pubDate>Mon, 01 Dec 2025 10:00:00 GMT</pubDate>
      <content:encoded>CMS의 개발환경 변화 여정을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>개인화 추천 시스템 1편 - 유저의 행동은 “언어”일까? : Collaborative Embedding 구축기 (feat. Knowledge Distillation)</title>
      <link>http://thefarmersfront.github.io/blog/personalized-recommendation-v1/</link>
      <guid>http://thefarmersfront.github.io/blog/personalized-recommendation-v1/</guid>
      <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>
      <content:encoded>팀 내에서 범용 개인화 추천 시스템을 구축하며 온라인 A/B 테스트와 CRM 캠페인 테스트에서 비교적 좋은 성과를 확인해, 그 개선 사례와 모델링 과정을 소개하고자 합니다.</content:encoded>
    </item>
    <item>
      <title>수직 중앙 정렬한 텍스트가 치우쳐 보이는 이유</title>
      <link>https://meetup.nhncloud.com/posts/397</link>
      <guid>https://meetup.nhncloud.com/posts/397</guid>
      <pubDate>Tue, 11 Nov 2025 23:35:19 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner_fontAlign_202511.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfontAlign202511.png)](https://www.nhncloud.com/kr)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        혹시 어떤 요소에 넣은 텍스트를 예쁘게 보이려 수직 중앙 정렬 스타일을 적용했는데 조금 치우쳐 보였던 경험이 있으신가요? 화면을 보다 보니 다양한 텍스트들이 조금 삐뚤빼뚤해 보였던 적이 있으신가요? 혹시 아래 문장도 조금 치우쳐져 보이시나요? &amp;#xD;
        &amp;#xD;
        &amp;lt;div style=&quot;font-family: &apos;Segoe UI&apos;; font-weight: bold; display: inline-flex; align-items: center; border: 1px solid black; font-size: 23px; padding: 0; justify-content: start; line-height: normal; box-sizing: border-box; margin: 0;vertical-align: baseline; color: #222; word-break: normal; overflow-wrap: break-word;&quot;&amp;gt;그렇다면 이건 기분 탓이 아니라 진짜 치우쳐 표시되기 때문입니다.&amp;lt;/div&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        폰트 메트릭, 브라우저 렌더링, 픽셀 밀도의 상호작용이 만들어 내는 미묘한 차이 때문이죠. 이번 글에서는 폰트가 왜 그렇게 보이는지, 그리고 우리가 어떻게 좀 더 자연스럽게 정렬을 맞춰줄 수 있는지 함께 알아봅니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## [여기](https://codepen.io/jajugoguma/full/LEVEyEm) 알파벳 대문자 M이 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; 대문자 M을 선택한 이유는 폰트 디자인에서 M이 대문자 기준 높이(Cap height)를 측정할 때 사용되는 대표적인 문자이기 때문입니다.&amp;#xD;
        &amp;gt; M은 위아래 경계가 명확한 직선으로 이루어져 있어 높이 측정이 용이하고, 상하 정렬 상태를 명확히 관찰할 수 있습니다. 또한 대부분의 폰트에서 M은 일관된 특성을 보여 폰트 간 비교에도 적합합니다.&amp;#xD;
        &amp;#xD;
        ![text1.png](https://image.toast.com/aaaadh/real/2025/techblog/text1.png)&amp;#xD;
        &amp;#xD;
        &amp;gt; 위 이미지는 `devicePixelRatio`가 2인 화면(2x 스케일링), Chrome 136에서 캡처된 이미지로, 다른 환경에서는 다르게 표시될 수 있습니다.&amp;#xD;
        &amp;#xD;
        위 이미지는 `font-family: Pretendard, font-size: 50px`이 적용된 알파벳 대문자 `M`이 화면에 표시되는 실제 크기를 측정한 이미지입니다. `font-size`로 설정한 값(`50px`)과 달리 `59.5px`의 높이를 갖는 것을 확인할 수 있습니다. 내가 지정한 크기로 표시되지 않는다니, 잘못 표시되는 것일까요?&amp;#xD;
        &amp;#xD;
        결론부터 말씀드리자면, 아닙니다. 이것은 폰트와 `font-size`, 그리고 브라우저가 이렇게 나타나도록 만들어져 있기 때문입니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 폰트는 어떻게 만들어져 있을까?&amp;#xD;
        &amp;#xD;
        폰트는 수학적으로 정의된 **좌표 공간** 안에서 만들어집니다. 이 공간 안에서 문자들의 상대적 크기, 간격, 기준선 등(메트릭)이 정해지며, **글자 모양과 메트릭이 결합되어 렌더링**됩니다.&amp;#xD;
        &amp;#xD;
        즉, 폰트의 모양과 메트릭을 결정짓기 전에 폰트가 만들어질 좌표 공간을 정해야 하고 이를 **Em-square**라고 합니다. 그리고 이 사각형의 크기를 **EM 크기**라고 합니다. EM 크기는 상대적 단위의 크기로 일반적으로 1000 또는 2048 단위로 설정됩니다.&amp;#xD;
        &amp;#xD;
        좌표 공간을 지정했으면 폰트 메트릭을 결정할 차례입니다. 폰트 메트릭은 **폰트 모양이 좌표 내에 어디에 표시될지**를 결정하는 아주 중요한 지표입니다. 메트릭이 없으면 글자가 좌표 공간 내에서 멋대로 배치되어 글자가 삐뚤빼뚤하게 표시될 수 있습니다. 아래는 주요 폰트 메트릭에 대한 설명과 이해를 돕기 위한 그림입니다.&amp;#xD;
        &amp;#xD;
        ![text2.png](https://image.toast.com/aaaadh/real/2025/techblog/text2.png)&amp;#xD;
        &amp;#xD;
        | 요소           | 설명                                                                                                        |&amp;#xD;
        | -------------- | ----------------------------------------------------------------------------------------------------------- |&amp;#xD;
        | **EM Square**  | 글꼴의 모든 문자가 설계되는 가상의 좌표 공간&amp;lt;br&amp;gt;모든 글자는 이 가상의 정사각형 안에서 상대 좌표로 설계된다. |&amp;#xD;
        | **Baseline**   | 글자가 놓이는 기준 선                                                                                       |&amp;#xD;
        | **x-height**   | 소문자 ‘x’의 높이(소문자 기준 높이)                                                                        |&amp;#xD;
        | **Cap height** | 대문자 기준 높이(‘M’, ‘H’ 등)                                                                              |&amp;#xD;
        | **Ascender**   | ‘b’, ‘d’, ‘k’처럼 위로 튀어나온 부분의 최대 높이                                                            |&amp;#xD;
        | **Descender**  | ‘g’, ‘p’, ‘y’처럼 아래로 내려가는 부분의 깊이                                                               |&amp;#xD;
        &amp;#xD;
        즉, 폰트의 모든 글자는 Baseline을 기준으로 Descender 만큼 아래로 나타날 수 있고, Ascender 만큼 위로 나타날 수 있습니다. 또한 EM 크기는 기준이 되는 상대적 단위의 크기이므로 Ascender와 Descender의 합이 EM 크기보다 작거나 클 수 있습니다.&amp;#xD;
        &amp;#xD;
        이제 폰트 메트릭까지 설정했으니, 폰트의 각 글자의 모양만 만들면 폰트가 만들어집니다. 아래는 예제에서 사용한 폰트 Pretendard의 메트릭 정보입니다.&amp;#xD;
        &amp;#xD;
        ![text3.png](https://image.toast.com/aaaadh/real/2025/techblog/text3.png)&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 그래서 폰트 크기는 어떻게 결정되는데?&amp;#xD;
        &amp;#xD;
        앞서 우리가 `font-size`로 설정한 값이 실제 폰트 크기로 설정되지 않는 이유 중 하나는 `font-size`가 이렇게 나타나도록 만들어져 있기 때문이라고 했습니다. 이에 대해 결론부터 말하자면 `font-size`는 폰트의 문자 그 자체의 높이가 아닌 **폰트의 `em-square`의 크기를 결정**짓는 속성입니다.[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://www.w3.org/TR/CSS2/fonts.html#font-size-props)&amp;#xD;
        &amp;#xD;
        이제 Pretendard 폰트가 `font-size`값에 따라 크기가 어떻게 결정되는지 알아봅시다. Pretendard 폰트는 2048 단위의 EM 크기에서 1949(Ascender)+494(Descender) 단위를 사용합니다. 이는 EM 크기가 `2048px`로 결정되면 폰트의 크기는 `2443(=1949+494)px`이 된다는 의미입니다.[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://iamvdo.me/en/blog/css-font-metrics-line-height-and-vertical-align?utm_source=CSS-Weekly&amp;amp;utm_campaign=Issue-253&amp;amp;utm_medium=web)&amp;#xD;
        &amp;#xD;
        그러면 예제의 높이를 계산해 볼까요? `font-size: 50px`이라는 뜻은 EM 크기가 `50px`로 결정되었다는 뜻이고, 이에 따라 폰트의 크기는 `59.5(≈ 2443 / 2048 * 50)px`(2x 스케일링으로 `0.5px`까지 표현 가능)로 계산됩니다. 이 값은 예제가 갖는 값과 동일하다는 것을 알 수 있습니다.&amp;#xD;
        &amp;#xD;
        이제 우리는 메트릭 정보와 `font-size`를 이용해 실제로 폰트가 몇 픽셀로 나타날지 계산할 수 있습니다!&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        폰트 크기 = (${Ascender} + ${Descender}) / ${EM 크기} \* ${font-size}&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 내가 궁금한 건 폰트가 왜 치우치냐예요.&amp;#xD;
        &amp;#xD;
        지금까지 폰트가 왜 치우쳐 나타나는지 알기 위해 폰트의 크기가 어떻게 결정되는지를 알아봤습니다. 폰트가 치우치는 것은 이렇게 결정된 폰트 크기 내에 문자가 어떻게 배치되는지에 따라 결정되는 것입니다. 우리는 폰트 메트릭에서 문자가 놓일 기준선과 문자의 기준 높이, 그리고 위/아래 최대 높이를 알 수 있었습니다. 이를 활용하면 문자가 위/아래에 얼마 만큼의 여백을 두고 나타날지(수직으로 어디에 정렬될지) 알 수 있습니다.&amp;#xD;
        &amp;#xD;
        먼저 글자의 높이와 여백을 쉽게 측정하기 위해 예제의 `font-size`를 `51px`로 조정해서 폰트의 크기가 정수(`61px`)로 결정되도록 조정하겠습니다.[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/full/OPVPqax)&amp;#xD;
        &amp;#xD;
        ![text4.png](https://image.toast.com/aaaadh/real/2025/techblog/text4.png)&amp;#xD;
        &amp;#xD;
        이제 대문자 M의 크기와 위/아래 여백의 크기를 계산해 봅시다. 앞서 수식에서 `(${Ascender} + ${Descender})`을 계산하고자 하는 메트릭 값으로 치환하면 원하는 계산 결과를 얻을 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```js&amp;#xD;
        // 대문자 M의 크기 ≈ ${Cap height} / ${EM 크기} * ${font-size}&amp;#xD;
        1448 / 2048 * 51 = 36.05859375(px)&amp;#xD;
        // 아래 여백의 크기 ≈ ${Descender} / ${EM 크기} * ${font-size}&amp;#xD;
        494 / 2048 * 51 = 12.30175781(px)&amp;#xD;
        // 위 여백의 크기 ≈(${Ascender} - ${Cap height}) / ${EM 크기} _ ${font-size}&amp;#xD;
        (1949 - 1448) / 2048 _ 51 = 12.47607422(px)&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        아래는 실제 화면에 나타나는 대문자 M의 크기과 위/아래 여백의 크기를 확인할 수 있는 그림입니다.(2x 환경의 `pt` 단위는 스케일링된 픽셀 크기입니다.)&amp;#xD;
        &amp;#xD;
        | Chrome                              | Chrome(2x)                          | Safari                                | Safari(2x)                            | FireFox                               | Firefox(2x)                            |&amp;#xD;
        | ----------------------------------- | ----------------------------------- | ------------------------------------- | ------------------------------------- | ------------------------------------- | -------------------------------------- |&amp;#xD;
        |![text5_chrome.png](https://image.toast.com/aaaadh/real/2025/techblog/text5chrome.png)|![text5_chrome2x.png](https://image.toast.com/aaaadh/real/2025/techblog/text5chrome2x.png)|![text5_safari.png](https://image.toast.com/aaaadh/real/2025/techblog/text5safari.png)|![text5_safari2.png](https://image.toast.com/aaaadh/real/2025/techblog/text5safari2.png)|![text5_firefox.png](https://image.toast.com/aaaadh/real/2025/techblog/text5firefox.png)|![text5_firefox2.png](https://image.toast.com/aaaadh/real/2025/techblog/text5firefox2.png)|&amp;#xD;
        &amp;#xD;
        위 그림에 따라 세 브라우저 모두 환경 무관하게 폰트 크기가 `61px`, 대문자 M의 크기는 1x 환경에서 `37px`, 2x 환경에서 `36.5px`로 동일하게 계산된 것을 볼 수 있습니다. 그러나 브라우저마다 각 환경에 따른 위/아래 여백 계산 결과에 차이를 보여 동일한 폰트에 동일한 문자라 할지라도 환경에 따라 텍스트가 시각적으로 중앙에 정렬되어 보일 수도, 그렇지 않을 수도 있음을 알 수 있습니다.&amp;#xD;
        &amp;#xD;
        이번 예제는 그래도 폰트의 크기도, 대문자 M의 크기도 홀수로 계산되고 위/아래 여백으로 사용될 너비가 짝수여서 어떻게든 중앙에 나타날 수 있는 형태였습니다. 하지만, 위/아래 여백으로 사용될 너비가 홀수라면 어떻게 될까요?&amp;#xD;
        &amp;#xD;
        이번에는 `font-size`를 `11px`로 설정해서 폰트 크기는 홀수(`13px`), 대문자 M의 크기는 짝수(`8px`)로 나타내 보도록 하겠습니다.[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/full/azOOKEE) 먼저, 변경된 값과 폰트 메트릭에 따른 크기를 다시 계산해 봅시다.&amp;#xD;
        &amp;#xD;
        ```js&amp;#xD;
        // 대문자 M의 크기 ≈ ${Cap height} / ${EM 크기} * ${font-size}&amp;#xD;
        1448 / 2048 * 11 = 7.77734375(px)&amp;#xD;
        // 아래 여백의 크기 ≈ ${Descender} / ${EM 크기} * ${font-size}&amp;#xD;
        494 / 2048 * 11 = 2.65332031(px)&amp;#xD;
        // 위 여백의 크기 ≈(${Ascender} - ${Cap height}) / ${EM 크기} _ ${font-size}&amp;#xD;
        (1949 - 1448) / 2048 _ 11 = 2.69091797(px)&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        | Chrome                                                             | Chrome(2x)                                                         |&amp;#xD;
        | ------------------------------------------------------------------ | ------------------------------------------------------------------ |&amp;#xD;
        |![text6_chrome.png](https://image.toast.com/aaaadh/real/2025/techblog/text6chrome.png) |![text6_chrome2.png](https://image.toast.com/aaaadh/real/2025/techblog/text6chrome2.png) |&amp;#xD;
        |![text7_chrome.png](https://image.toast.com/aaaadh/real/2025/techblog/text7chrome.png)| ![text7_chrome2.png](https://image.toast.com/aaaadh/real/2025/techblog/text7chrome2.png) |&amp;#xD;
        &amp;#xD;
        대문자 M의 크기가 짝수(`8px`)로 결정됨에 따라 위/아래 여백으로 사용 가능한 크기가 홀수(`5px`)로 결정됩니다. 그러나 2x 환경에서는 `.5px`까지 렌더링할 수 있어서 중앙 정렬이 잘 되어 보이지만, 1x 환경에서는 픽셀을 반으로 나누는 게 불가능해 한 쪽으로 `1px` 치우쳐 보일 수밖에 없습니다.&amp;#xD;
        &amp;#xD;
        지금까지 확인한 바와 같이 텍스트는 텍스트를 보는 환경에 따라 폰트 크기 내에서 수직 중앙 정렬이 될 수도, 아닐 수도 있다는 사실을 알았습니다. 다양한 CSS 수직 정렬 속성(`vertical-align`, `align-items`, `justify-content` 등)은 폰트 크기 내 텍스트를 정렬하는 것이 아니고 일정한 기준에 따라 폰트 자체의 위치를 조정하는 것으로 앞서 설명 드렸던 **치우쳐 보이는 이유**에서 벗어날 수 없습니다. 따라서 이것이 진정 수직 중앙 정렬한 텍스트가 치우쳐 보이는 이유입니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 우리는 아무것도 할 수 없나요?&amp;#xD;
        &amp;#xD;
        텍스트는 환경에 따라 정렬되어 보인다는 점이 다르다 해도 작게는 0.5px에서 크게는 2px 정도 치우쳐 보이게 됩니다. 이 정도 차이가 일반적으로 큰 문제를 일으키지는 않지만, 이 차이가 굉장히 중요한 문제를 일으키는 때가 있을 수 있습니다. 그러면 우리는 아무것도 할 수 없는 것 일까요?&amp;#xD;
        &amp;#xD;
        꼭 그렇지만은 않습니다. **보편적인 환경에 대해 모든 문제를 해결하는 것은 어렵지만, 필요한 때에 필요한 곳에서는 제한적으로 대응할 수 있습니다.**&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### **실용적인 대응 방법**&amp;#xD;
        &amp;#xD;
        1. **CSS transform 활용하기**[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/pen/OPVVwgR?editors=1100)&amp;#xD;
        텍스트가 시각적으로 정확히 중앙에 보이도록 미세하게 조정이 필요한 경우 `transform` 속성을 사용하여 텍스트 위치를 조정할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```css&amp;#xD;
        .vertically-centered-text {&amp;#xD;
        display: flex;&amp;#xD;
        align-items: center;&amp;#xD;
        transform: translateY(-0.5px); /_ 상황에 따라 값 조정 _/&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        2. **폰트별 보정 값 적용하기**[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/pen/WbvvKzZ?editors=1400)&amp;#xD;
        특정 폰트와 크기에서 반복적으로 발생하는 정렬 문제가 있다면, 해당 조합에 대한 보정 값을 미리 계산하여 적용할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```css&amp;#xD;
        .apple-sd-gothic-neo {&amp;#xD;
        font-family: &apos;Apple SD Gothic Neo&apos;;&amp;#xD;
        transform: translateY(-1.5px);&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        .segoe-ui {&amp;#xD;
        font-family: SegoeUI;&amp;#xD;
        transform: translateY(-3px);&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        3. **텍스트 대신 아이콘이나 SVG 사용하기**&amp;#xD;
        정확한 수직 정렬이 매우 중요한 UI 요소(예: 버튼의 아이콘)의 경우, 텍스트 대신 SVG나 아이콘 폰트를 사용하면 더 정확한 배치가 가능합니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        4. **텍스트 박스에 패딩/마진 추가하기**[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/pen/pvJJZQK?editors=1400%3C/sup%3E)&amp;#xD;
        텍스트를 담은 요소에 적절한 패딩/마진을 추가하면 미세한 정렬 차이를 눈에 덜 띄게 할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```css&amp;#xD;
        /_ margin을 이용한 정렬 보정 _/&amp;#xD;
        .align-with-margin {&amp;#xD;
        margin-top: -3px;&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        /_ padding을 이용한 정렬 보정 _/&amp;#xD;
        .align-with-padding {&amp;#xD;
        padding-bottom: 3px;&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        5. **브라우저 환경 감지 및 조건부 스타일 적용하기**[&amp;lt;sup&amp;gt;&amp;lt;/sup&amp;gt;](https://codepen.io/jajugoguma/pen/EajjpJB)&amp;#xD;
        특정 브라우저나 픽셀 밀도에 따라 다른 보정 값을 적용할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```js&amp;#xD;
        // 브라우저와 픽셀 밀도에 따른 보정 클래스 추가&amp;#xD;
        const element = document.querySelector(&apos;.text&apos;);&amp;#xD;
        &amp;#xD;
        if (navigator.userAgent.includes(&apos;Chrome&apos;)) {&amp;#xD;
        element.classList.add(&apos;chrome-adjustment&apos;);&amp;#xD;
        } else if (navigator.userAgent.includes(&apos;Safari&apos;)) {&amp;#xD;
        element.classList.add(&apos;safari-adjustment&apos;);&amp;#xD;
        } else if (navigator.userAgent.includes(&apos;Firefox&apos;)) {&amp;#xD;
        element.classList.add(&apos;firefox-adjustment&apos;);&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        element.classList.add(`pixelratio-${window.devicePixelRatio}x-adjustment`);&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ```css&amp;#xD;
        .chrome-adjustment.pixelratio-1x-adjustment {&amp;#xD;
        transform: translateY(-1px);&amp;#xD;
        }&amp;#xD;
        .chrome-adjustment.pixelratio-2x-adjustment {&amp;#xD;
        transform: translateY(-1.5px);&amp;#xD;
        }&amp;#xD;
        .safari-adjustment.pixelratio-1x-adjustment {&amp;#xD;
        transform: translateY(-1px);&amp;#xD;
        }&amp;#xD;
        .safari-adjustment.pixelratio-2x-adjustment {&amp;#xD;
        transform: translateY(-1px);&amp;#xD;
        }&amp;#xD;
        .firefox-adjustment.pixelratio-1x-adjustment {&amp;#xD;
        transform: translateY(-1.5px);&amp;#xD;
        }&amp;#xD;
        .firefox-adjustment.pixelratio-2x-adjustment {&amp;#xD;
        transform: translateY(-2px);&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 결론&amp;#xD;
        &amp;#xD;
        폰트는 디자인적인 이유와 기술적인 제약으로 인해 완벽하게 수직 중앙 정렬되어 보이지 않을 수 있습니다. 이는 폰트 메트릭의 특성, 브라우저의 렌더링 방식, 픽셀 밀도 등 여러 요인이 복합적으로 작용한 결과입니다.&amp;#xD;
        &amp;#xD;
        완벽한 수직 중앙 정렬을 위한 마법 같은 해결책은 없지만, 폰트의 동작 방식을 이해하고 특정 상황에 맞는 보정을 적용한다면 시각적으로 더 만족스러운 결과를 얻을 수 있습니다.&amp;#xD;
        &amp;#xD;
        무엇보다 중요한 것은 이러한 차이가 &apos;오류&apos;가 아니라 폰트와 텍스트 렌더링의 본질적인 특성임을 이해하는 것입니다. 보통의 경우 이 미세한 차이는 사용자 경험에 큰 영향을 주지 않으므로, 지나친 완벽주의보다는 전체적인 디자인 일관성과 사용성에 집중하는 것이 바람직합니다.&amp;#xD;
        &amp;#xD;
        NHN Cloud의 NCUI개발팀은 NCUI 컴포넌트 라이브러리를 통해 NHN Cloud 콘솔 구현 시 더 쉽고 빠르게 UI를 구현할 수 있도록 돕고 있습니다. 이번 글은 그 과정에서 얻은 고민과 작은 팁을 공유한 것으로, 많은 분들이 앞으로 개발하시는 데 참고가 되었으면 좋겠습니다. 긴 글을 읽어 주셔서 감사합니다! &amp;#xD;
        &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_202511.png](https://image.toast.com/aaaadh/alpha/2025/techblog/NHN%20Cloudmeetup%20bannerfooter202511.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>Supercharging the ML and AI Development Experience at Netflix</title>
      <link>https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb?source=rss----2615bd06b42e---4</guid>
      <pubDate>Tue, 04 Nov 2025 20:33:44 GMT</pubDate>
      <content:encoded>&lt;h3&gt;Supercharging the ML and AI Development Experience at Netflix with Metaflow&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/shashanksrikanth/&quot;&gt;&lt;em&gt;Shashank Srikanth&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/romain-cledat-4a211a5/&quot;&gt;&lt;em&gt;Romain Cledat&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.metaflow.org&quot;&gt;Metaflow&lt;/a&gt; — a framework we started and &lt;a href=&quot;https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9&quot;&gt;open-sourced&lt;/a&gt; in 2019 — now powers &lt;a href=&quot;https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d&quot;&gt;a wide range of ML and AI systems across Netflix&lt;/a&gt; and at &lt;a href=&quot;https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md&quot;&gt;many other companies&lt;/a&gt;. It is well loved by users for helping them take their ML/AI workflows from &lt;a href=&quot;https://docs.metaflow.org/introduction/what-is-metaflow#how-does-metaflow-support-prototyping-and-production-use-cases&quot;&gt;prototype to production&lt;/a&gt;, allowing them to focus on building cutting-edge systems that bring joy and entertainment to audiences worldwide.&lt;/p&gt;&lt;p&gt;Metaflow allows users to:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Iterate and ship quickly &lt;/strong&gt;by minimizing friction&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Operate systems reliably&lt;/strong&gt; in production with minimal overhead, at Netflix scale.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Metaflow works with many battle-hardened tooling to address the second point — among them &lt;a href=&quot;https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041&quot;&gt;Maestro&lt;/a&gt;, our newly open-sourced workflow orchestrator that powers nearly every ML and AI system at Netflix and serves as a backbone for Metaflow itself.&lt;/p&gt;&lt;p&gt;In this post, we focus on the first point and introduce a new Metaflow functionality, &lt;strong&gt;Spin&lt;/strong&gt;, that helps users &lt;strong&gt;accelerate their iterative development process&lt;/strong&gt;. By the end, you’ll have a solid understanding of Spin’s capabilities and learn how to try it out yourself with &lt;strong&gt;Metaflow 2.19&lt;/strong&gt;.&lt;/p&gt;&lt;h3&gt;Iterative development in ML and AI workflows&lt;/h3&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*0I8DAvXCQEN1RpTTC0ZyaQ.jpeg&quot; /&gt;&lt;figcaption&gt;Developing a Metaflow flow with cards in VSCode&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To understand our approach to improving the ML and AI development experience, it helps to consider how these workflows differ from traditional software engineering.&lt;/p&gt;&lt;p&gt;ML and AI development revolves not just around code but also around data and models, which are large, mutable, and computationally expensive to process. Iteration cycles can involve long-running data transformations, model training, and stochastic processes that yield slightly different results from run to run. These characteristics make fast, stateful iteration a critical part of productive development.&lt;/p&gt;&lt;p&gt;This is where notebooks — such as Jupyter, &lt;a href=&quot;https://observablehq.com/documentation/notebooks/&quot;&gt;Observable&lt;/a&gt;, or &lt;a href=&quot;https://marimo.io/&quot;&gt;Marimo&lt;/a&gt; — shine. Their ability to preserve state in memory allows developers to load a dataset once and iteratively explore, transform, and visualize it without reloading or recomputing from scratch. This persistent, interactive environment turns what would otherwise be a slow, rigid loop into a fluid, exploratory workflow — perfectly suited to the needs of ML and AI practitioners.&lt;/p&gt;&lt;p&gt;Because ML and AI development is computationally intensive, stochastic, and data- and model-centric, tools that optimize iteration speed must treat state management as a first-class design concern. Any system aiming to improve the development experience in this domain must therefore enable quick, incremental experimentation without losing continuity between iterations.&lt;/p&gt;&lt;h3&gt;New: rapid, iterative development with spin&lt;/h3&gt;&lt;p&gt;At first glance, Metaflow code looks like a workflow — similar to &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Airflow&lt;/a&gt; — but there’s another way to look at it: each Metaflow @step serves as &lt;a href=&quot;https://docs.metaflow.org/metaflow/basics#what-should-be-a-step&quot;&gt;a checkpoint boundary&lt;/a&gt;. At the end of every step, Metaflow automatically persists all instance variables as &lt;em&gt;artifacts&lt;/em&gt;, allowing the execution to &lt;a href=&quot;https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command&quot;&gt;resume&lt;/a&gt; seamlessly from that point onward. The below animation shows this behavior in action:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;An animated GIF showing how resume can be used in Metaflow. The GIF shows how using `flow.py resume join` makes Metaflow clone previously executed steps and resumes the computation from the `join` step and continues executing till the end of the flow.&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*AEDpnt-YULYV4mcyrwLk7g.gif&quot; /&gt;&lt;figcaption&gt;Using resume in Metaflow&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In a sense, we can consider a @step similar to a notebook cell: it is the smallest unit of execution that updates state upon completion. It does have a few differences that address the issues with notebook cells:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The execution order is explicit and deterministic: &lt;/strong&gt;no surprises due to out-of-order cell execution;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The state is not hidden: &lt;/strong&gt;state is explicitly stored as self. variables as shared state, which can be &lt;a href=&quot;https://docs.metaflow.org/metaflow/client&quot;&gt;discovered and inspected&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The state is versioned and persisted&lt;/strong&gt; making results more reproducible.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While &lt;strong&gt;Metaflow&lt;/strong&gt;’s resume feature can approximate the incremental and iterative development approach of notebooks, it restarts execution from the selected step onward, introducing more latency between iterations. In contrast, a &lt;strong&gt;notebook&lt;/strong&gt; allows near-instant feedback by letting users tweak and rerun individual cells while seamlessly reusing data from earlier cells held in memory.&lt;/p&gt;&lt;p&gt;The new spin command in Metaflow 2.19 addresses this gap. Similar to executing a single notebook cell, it quickly executes a single Metaflow @step — with all the state carried over from the parent step. As a result, users can develop and debug Metaflow steps as easily as a cell in a notebook.&lt;/p&gt;&lt;p&gt;The effect becomes clear when considering the three complementary execution modes — run, resume, and spin — side by side, mapping them to the corresponding notebook behavior:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Diagram showing the various modes of execution in Metaflow: Run, Resume and Spin&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*DgRIxOu-7keiFoHia9JMrg.png&quot; /&gt;&lt;figcaption&gt;Run, Resume and Spin “modes”&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another major difference isn’t just what gets executed, but what gets recorded. Both run and resume create a full, versioned run with complete metadata and artifacts, while spin skips tracking altogether. It’s built for fast, throw-away iterations during development.&lt;/p&gt;&lt;p&gt;The one-minute clip below illustrates a typical iterative development workflow that alternates between run and spin. In this example, we are building a flow that reads a dataset from a Parquet file and trains a separate model for each product category, focusing on computer-related categories.&lt;/p&gt;&lt;iframe src=&quot;https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3RNMM-lthm0%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3RNMM-lthm0&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3RNMM-lthm0%2Fhqdefault.jpg&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&quot; width=&quot;854&quot; height=&quot;480&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;a href=&quot;https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href&quot;&gt;https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;As shown in the video, we start by creating a flow from scratch and running a minimal version of it to persist test artifacts — in this case, a Parquet dataset. From there, we can use spin to iterate on one step at a time, incrementally building out the flow, for example, by adding the parallel training steps demonstrated in the clip.&lt;/p&gt;&lt;p&gt;Once the flow has been iterated on locally, it can be seamlessly deployed to production orchestrators like Maestro or &lt;a href=&quot;https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows&quot;&gt;Argo&lt;/a&gt;, and &lt;a href=&quot;https://docs.metaflow.org/scaling/remote-tasks/requesting-resources&quot;&gt;scaled up&lt;/a&gt; on compute platforms such as AWS Batch, Titus, Kubernetes and more. Thus, the experience is as smooth as developing in a notebook, but the outcome is a production-ready, scalable workflow, implemented as an idiomatic Python project!&lt;/p&gt;&lt;h3&gt;Spin up smooth development in VSCode/Cursor&lt;/h3&gt;&lt;p&gt;Instead of typing run and spin manually in the terminal, we can bind them to keyboard shortcuts. For example, &lt;a href=&quot;https://github.com/outerbounds/metaflow-dev-vscode&quot;&gt;the simple metaflow-dev VS Code extension&lt;/a&gt; (works with Cursor as well) maps Ctrl+Opt+R to run and Ctrl+Opt+S to spin. Just hack away, hit Ctrl+Opt+S, and the extension will save your file and spin the step you are currently editing.&lt;/p&gt;&lt;p&gt;One area where spin truly shines is in creating mini-dashboards and reports with &lt;a href=&quot;https://docs.metaflow.org/metaflow/visualizing-results&quot;&gt;Metaflow Cards&lt;/a&gt;. Visualization is another strong point of notebooks but the combination of spin and cards makes Metaflow a very compelling alternative for developing real-time and post-execution visualizations. Developing cards is inherently iterative and visual (much like building web pages) where you want to tweak code and see the results instantly. This workflow is readily available with the combination of VSCode/Cursor, which includes a built-in web-view, &lt;a href=&quot;https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#using-local-card-viewer&quot;&gt;the local card viewer&lt;/a&gt;, and spin.&lt;/p&gt;&lt;p&gt;To see the trio of tools — along with the VS Code extension — in action, in this short clip we add observability to the train step that we built in the earlier example:&lt;/p&gt;&lt;iframe src=&quot;https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FhoRO5eePjqo%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DhoRO5eePjqo&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FhoRO5eePjqo%2Fhqdefault.jpg&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&quot; width=&quot;854&quot; height=&quot;480&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;a href=&quot;https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href&quot;&gt;https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;A major benefit of Metaflow Cards is that we don’t need to deploy any extra services, data streams, and databases for observability. Just develop visual outputs as above, deploy the flow, and wehave a complete system in production with reporting and visualizations included.&lt;/p&gt;&lt;h3&gt;Spin to the next level: injecting inputs, inspecting outputs&lt;/h3&gt;&lt;p&gt;Spin does more than just run code — it also lets us take full control of a spun @step’s inputs and outputs, enabling a range of advanced patterns.&lt;/p&gt;&lt;p&gt;In contrast to notebooks, we can spin any arbitrary @step in a flow using state from any past run, making it easy to test functions with different inputs. For example, if we have multiple models produced by separate runs, we could spin an inference step, supplying a different model run each time.&lt;/p&gt;&lt;p&gt;We can also override artifact values or inject arbitrary Python objects — similar to a notebook cell — for spin. Simply specify a Python module with an ARTIFACTS dictionary:&lt;/p&gt;&lt;pre&gt;ARTIFACTS = {&lt;br&gt;  &amp;quot;model&amp;quot;: &amp;quot;kmeans&amp;quot;,&lt;br&gt;  &amp;quot;k&amp;quot;: 15&lt;br&gt;}&lt;/pre&gt;&lt;p&gt;and point spin at the module:&lt;/p&gt;&lt;pre&gt;spin train --artifacts-module artifacts.py&lt;/pre&gt;&lt;p&gt;By default spin doesn’t persist artifacts, but we can easily change this by adding --persist. Even in this case, artifacts are not persisted in the usual Metaflow datastore but to a directory-specific location which you can easily clean up after testing. We can access the results with &lt;a href=&quot;https://docs.metaflow.org/metaflow/client&quot;&gt;the Client API&lt;/a&gt; as usual — just specify the directory you want to inspect with inspect_spin:&lt;/p&gt;&lt;pre&gt;from metaflow import inspect_spin&lt;br&gt;&lt;br&gt;inspect_spin(&amp;quot;.&amp;quot;)&lt;br&gt;Flow(&amp;quot;TrainingFlow&amp;quot;).latest_run[&amp;quot;train&amp;quot;].task[&amp;quot;model&amp;quot;].data&lt;/pre&gt;&lt;p&gt;Being able to inspect and modify a step’s inputs and outputs on the fly unlocks a powerful use case:&lt;strong&gt; unit testing individual steps&lt;/strong&gt;. We can use spin programmatically through &lt;a href=&quot;https://docs.metaflow.org/metaflow/managing-flows/runner&quot;&gt;the Runner API&lt;/a&gt; and assert the results:&lt;/p&gt;&lt;pre&gt;from metaflow import Runner&lt;br&gt;&lt;br&gt;with Runner(&amp;quot;flow.py&amp;quot;).spin(&amp;quot;train&amp;quot;, persist=True) as spin:&lt;br&gt;  assert spin.task[&amp;quot;model&amp;quot;].data == &amp;quot;kmeans&amp;quot;&lt;/pre&gt;&lt;h3&gt;Making AI agents spin&lt;/h3&gt;&lt;p&gt;In addition to speeding up development for humans, spin turns out to be surprisingly handy for coding agents too. There are two major advantages to teaching AI how to spin:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;It accelerates the development loop&lt;/strong&gt;. Agents don’t naturally understand what’s slow, or why speed matters, so they need to be nudged to favor faster tools over slower ones.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;It helps surface errors faster &lt;/strong&gt;and contextualizes them to a specific piece of code, increasing the chance that the agent is able to fix errors by itself.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Metaflow users are already &lt;a href=&quot;https://claude.com/product/claude-code&quot;&gt;using&lt;/a&gt; Claude Code; spin makes this even easier. In the example below, we added the following section in a CLAUDE.md file:&lt;/p&gt;&lt;pre&gt;## Developing Metaflow code&lt;br&gt;Follow this incremental development workflow that ensures quick iterations&lt;br&gt;and correct results. You must create a flow incrementally, step by step&lt;br&gt;following this process:&lt;br&gt;1. Create a flow skeleton with empty `@step`s.&lt;br&gt;2. Add a data loading step.&lt;br&gt;3. `run` the flow.&lt;br&gt;4. Populate the next step and use `spin` to test it with the correct inputs.&lt;br&gt;5. `run` the flow to record outputs from the new step.&lt;br&gt;5. Iterate on (4–5) until all steps have been implemented and work correctly.&lt;br&gt;6. `run` the whole flow to ensure final correctness.&lt;br&gt;&lt;br&gt;To test a flow, run the flow as follows&lt;br&gt;```&lt;br&gt;python flow.py - environment=pypi run&lt;br&gt;```&lt;br&gt;&lt;br&gt;Do this once before running `spin`.&lt;br&gt;As you are building the flow, you `spin` to test steps quickly.&lt;br&gt;For instance&lt;br&gt;```&lt;br&gt;python flow.py - environment=pypi spin train&lt;br&gt;```&lt;/pre&gt;&lt;p&gt;Just based on these quick instructions, the agent is able to use spin effectively. Take a look at the following inspirational example that one-shots Claude to create a flow, along the lines of our earlier examples, which trains a classifier to predict product categories:&lt;/p&gt;&lt;iframe src=&quot;https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FkcCkLXernR0%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DkcCkLXernR0&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkcCkLXernR0%2Fhqdefault.jpg&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&quot; width=&quot;854&quot; height=&quot;480&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;a href=&quot;https://medium.com/media/706e83950e73a05ca41b4fb463702690/href&quot;&gt;https://medium.com/media/706e83950e73a05ca41b4fb463702690/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;In the video, we can see Claude using spin around the 45-second mark to test a preprocess step. The step initially fails due to a classic data science pitfall: during testing, Claude samples only a small subset of data, causing some classes to be underrepresented. The first spin surfaces the issue, which Claude then fixes by switching to stratified sampling — and finally does another spin to confirm the fix, before proceeding to complete the task.&lt;/p&gt;&lt;h3&gt;The inner loop of end-to-end ML/AI&lt;/h3&gt;&lt;p&gt;To circle back to where we started, our motivation for adding spin — and for creating Metaflow in the first place — is to accelerate development cycles so we can deliver more joy to our subscribers, faster. Ultimately, we believe there’s no single magic feature that makes this possible. It takes all parts of an ML/AI platform working together coherently — spin included.&lt;/p&gt;&lt;p&gt;From this perspective, it’s useful to place spin in the context of other Metaflow features. It’s designed for the innermost loop of model and business-logic development, with the added benefit of supporting unit testing during deployment, as shown in the overall blueprint of the Metaflow toolchain below.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Metaflow tool-chain.&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*9cd6SHFrW7A4iWMZHYkVkw.png&quot; /&gt;&lt;figcaption&gt;Metaflow tool-chain&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In this diagram, the solid blue boxes represent different Metaflow commands, while the blue text denotes decorators and other features. In particular, note the &lt;em&gt;Shared Functionality&lt;/em&gt; box — another key focus area for us over the past year — which includes &lt;a href=&quot;https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6&quot;&gt;configuration management&lt;/a&gt; and &lt;a href=&quot;https://docs.metaflow.org/metaflow/composing-flows/introduction&quot;&gt;custom decorators&lt;/a&gt;. These capabilities let domain-specific teams and platform providers tailor Metaflow to their own use cases. Following our ethos of composability, all of these features integrate seamlessly with spin as well.&lt;/p&gt;&lt;p&gt;Another key design philosophy of Metaflow is to let projects start small and simple, adding complexity only when it becomes necessary. So don’t be overwhelmed by the diagram above. To get started, install Metaflow easily with&lt;/p&gt;&lt;pre&gt;pip install metaflow&lt;/pre&gt;&lt;p&gt;and take your first baby @steps for a spin! Check out the &lt;a href=&quot;https://docs.metaflow.org/metaflow/authoring-flows/introduction&quot;&gt;docs&lt;/a&gt; and for questions, support, and feedback, join the friendly &lt;a href=&quot;http://chat.metaflow.org&quot;&gt;Metaflow Community Slack&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Acknowledgments&lt;/h3&gt;&lt;p&gt;We would like to thank our partners at &lt;a href=&quot;https://outerbounds.com&quot;&gt;Outerbounds&lt;/a&gt;, and particularly &lt;a href=&quot;https://www.linkedin.com/in/villetuulos/&quot;&gt;Ville Tuulos&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/savingoyal/&quot;&gt;Savin Goyal&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/in/madhur-tandon/&quot;&gt;Madhur Tandon&lt;/a&gt;, for their collaboration on this feature, from initial ideation to review, testing and documentation. We would also like to acknowledge the rest of the Model Development and Management team (&lt;a href=&quot;https://www.linkedin.com/in/maria-alder/&quot;&gt;Maria Alder&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/david-j-berg/&quot;&gt;David J. Berg&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shaojingli/&quot;&gt;Shaojing Li&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/rui-lin-483a83111/&quot;&gt;Rui Lin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/nissanpow/&quot;&gt;Nissan Pow&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/chaoying-wang/&quot;&gt;Chaoying Wang&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/reginalw/&quot;&gt;Regina Wang&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shuishiyang/&quot;&gt;Seth Yang&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/zitingyu/&quot;&gt;Darin Yu&lt;/a&gt;) for their input and comments.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2d5b95c63eb&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb&quot;&gt;Supercharging the ML and AI Development Experience at Netflix&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>배송 완료 사진 속 객체 탐지를 통한 수기 검수 비용 줄이기</title>
      <link>http://thefarmersfront.github.io/blog/2025-delivery-photo-object-detection/</link>
      <guid>http://thefarmersfront.github.io/blog/2025-delivery-photo-object-detection/</guid>
      <pubDate>Mon, 27 Oct 2025 10:00:00 GMT</pubDate>
      <content:encoded>팀 내에서 배송 완료 사진 속 객체 탐지(퍼플 박스, 종이봉투)를 통해 수기 검수 비용을 감소 시킨 내용을 다루고자합니다.</content:encoded>
    </item>
    <item>
      <title>Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning</title>
      <link>https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9?source=rss----2615bd06b42e---4</guid>
      <pubDate>Sat, 25 Oct 2025 22:01:00 GMT</pubDate>
      <content:encoded>&lt;p&gt;Author: &lt;a href=&quot;https://keertanavc.github.io/&quot;&gt;Keertana Chidambaram&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/qiuling-xu-a445b815a&quot;&gt;Qiuling Xu&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/markhsiao/&quot;&gt;Ko-Jen Hsiao&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/moumitab/&quot;&gt;Moumita Bhattacharya&lt;/a&gt;&lt;/p&gt;&lt;p&gt;(*The work was done when Keertana interned at Netflix.)&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;This blog focuses on post-training generative recommender systems. Generative recommenders (GRs) represent a new paradigm in the field of recommendation systems (e.g. &lt;a href=&quot;https://github.com/meta-recsys/generative-recommenders&quot;&gt;HSTU&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2502.18965&quot;&gt;OneRec&lt;/a&gt;). These models draw inspiration from recent advancements in transformer architectures used for language and vision tasks. They approach the recommendation problem, including both ranking and retrieval, as a sequential transduction task. This perspective enables generative training, where the model learns by imitating the next event in a sequence of user activities, thereby effectively modeling user behavior over time.&lt;/p&gt;&lt;p&gt;However, a key challenge with simply replicating observed user patterns is that it may not always lead to the best possible recommendations. User interactions are influenced by a variety of factors — such as trends, or external suggestions — and the system’s view of these interactions is inherently limited. For example, if a user tries a popular show but later indicates it wasn’t a good fit, a model that only imitates this behavior might continue to recommend similar content, missing the chance to enhance the user’s experience.&lt;/p&gt;&lt;p&gt;This highlights the importance of incorporating user preferences and feedback, rather than solely relying on observed behavior, to improve recommendation quality. In the context of recommendation systems, we benefit from a wealth of user feedback, which includes explicit signals such as ratings and reviews, as well as implicit signals like watch time, click-through rates, and overall engagement. This abundance of feedback serves as a valuable resource for improving model performance.&lt;/p&gt;&lt;p&gt;Given the recent success of reinforcement learning techniques in post-training large language models, such as DPO and GRPO, this study investigates whether similar methods can be applied to generative recommenders. Ultimately, our goal is to identify both the opportunities and challenges in using these techniques to enhance the quality and relevance of recommendations.&lt;/p&gt;&lt;p&gt;Unlike language models, post-training generative recommenders presents unique challenges. One of the most significant is the difficulty of obtaining counterfactual feedback in recommendation scenarios. The recommendation feedback is generated on-policy — that is, it reflects users’ real-time interactions with the system as they naturally use it. Since a typical user sequence can span weeks or even years of activity, it is impractical to ask users to review or provide feedback on hypothetical, counterfactual experiences. As a result, the absence of counterfactual data makes it challenging to apply post-training methods such as PPO or DPO, which require feedback from counterfactual user sequences.&lt;/p&gt;&lt;p&gt;Furthermore, post-training methods typically rely on a reward model — either implicit or explicit — to guide optimization. The quality of reward models heavily influences the effectiveness of post-training. In the context of recommendation systems, however, reward signals tend to be much noisier. For instance, if we use watch time as an implicit reward, it may not always accurately reflect user satisfaction: a viewer might stop watching a favorite show simply due to time constraints, while finishing a lengthy show doesn’t necessarily indicate genuine enjoyment.&lt;/p&gt;&lt;p&gt;To address these post-training challenges, we introduce a novel algorithm called Advantage-Weighted Supervised Fine-tuning (A-SFT). Our analysis first demonstrates that reward models in recommendation systems often exhibit higher uncertainty due to the issues discussed above. Rather than relying solely on these uncertain reward models, A-SFT combines supervised fine-tuning with the advantage function to more effectively guide post-training optimization. This approach proves especially effective when the reward model has high variance but still provides valuable directional signals. We benchmark A-SFT against four other representative methods, and our results show that A-SFT achieves better alignment between the pre-trained generative recommendation model and the reward model.&lt;/p&gt;&lt;p&gt;In Figure 1, we conceptualize the pros and cons of different post-training paradigms. For example, Online Reinforcement Learning is most useful when the reward model has a good generalization ability, and behavior cloning is suitable when no reward models are available. Using these algorithms under fitting use cases is the key to a successful post-training. For example, over-exploitation of noisy reward models will hurt task performance, as guidance from the reward models can be simply noise. Conversely, not leveraging a good reward model leaves out potential improvements. We find A-SFT fits the sweet point between offline reinforcement learning and behavior cloning, where it benefits from the directional signals in those noisy estimations and is less dependent on the reward accuracy.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*N9QNNLspEJJCxQjtNl9HZw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Figure 1: The landscape of RL algorithms based on the reward models’ accuracy&lt;/p&gt;&lt;h3&gt;Challenges in Post-training for Recommendation&lt;/h3&gt;&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is the most popular framework for post-training large language models. In this framework, human annotators evaluate and rank different outputs generated by a model. This feedback is then used to train a reward model that predicts how well a model output aligns with human preferences. This reward model then serves as a proxy for human judgment during reinforcement learning, guiding the model to generate outputs that are more likely to be preferred by humans.&lt;/p&gt;&lt;p&gt;While traditional RLHF methods like PPO or DPO are effective for aligning LLMs, there are several challenges in applying them directly to large-scale recommendation systems:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Lack of Counter-factual Observations&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As in typical RLHF settings, collecting real-time feedback from a diverse user base across a wide range of items is both costly and impractical. The data in recommendation are generated by the real-time user interests. Any third-party annotators or even the user themselves lack the practical means to evaluate an alternative reality. For example, it is impractical to ask the Netflix users to evaluate hundreds of unseen movies. Consequently, we lack a live environment in which to perform reinforcement learning.&lt;/p&gt;&lt;p&gt;2. Noisy Reward Models&lt;/p&gt;&lt;p&gt;In addition to the limited counter-factual data, the recommendation task itself has a higher randomness by its nature. The recommendation data has less structure than language data. Users choose to watch some shows not because there is a grammar rule that nouns need to follow by the verbs. In fact, the users’ choices usually exhibit a level of permutation invariance, where swapping the order of events in the user history still makes a valid activity sequence. This randomness in the behaviors makes learning a good reward model extremely difficult. Often the reward models we learnt still have a large margin of errors.&lt;/p&gt;&lt;p&gt;Here is an ablation study we did on the reward model performance with O(Millions) users and O(Billions) of tokens. The reward model uses an open-sourced HSTU architecture in the convenience of reproducing this study. We adopt the standard RLHF approach of training a reward model using offline, human-collected feedback. We start by creating a proxy reward, scored on a scale from 1 to 5 in the convenience of understanding. This reward model is co-trained as a shallow reward head on top of the generative recommender. It predicts the reward for the most recently selected title based on a user’s interaction history. To evaluate its effectiveness, we compare the model’s performance against two simple baselines: (1) predicting the next reward as the average reward the user has given in their past interactions, and (2) predicting it as the average reward that all users have assigned to that particular title in previous interactions.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*tmYdEXqVjSkne__-k6gSig.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Table 1: Reward model performance metrics&lt;/p&gt;&lt;p&gt;We observe that the model’s predictions do not significantly outperform the simple baselines. This result is intuitive, as a user’s historical interactions typically cover only a small subset of titles, making it difficult to accurately predict their responses to the vast number of unexplored titles in the catalogue. We expect this to be a potential issue for any large recommendation systems where the ratio between explored and unexplored titles is very small.&lt;/p&gt;&lt;p&gt;3. Lack of Logged Policy&lt;/p&gt;&lt;p&gt;In recommendation systems, the policy that generated the logged data is typically unknown and cannot be directly estimated. Offline reinforcement learning methods often rely on Inverse Propensity Scoring (IPS) to debias such data by reweighting interactions according to the logging policy’s action probabilities. However, estimating the logging policy accurately is challenging and prone to error, which can introduce additional biases, and IPS itself is known to suffer from high variance. Consequently, offline RL approaches that depend on IPS are ill-suited for our setting.&lt;/p&gt;&lt;h3&gt;Advantage Weighted Supervised Fine Tuning&lt;/h3&gt;&lt;p&gt;Given the three challenges outlined above, we propose a new algorithm Advantage-Weighted SFT (A-SFT). It leverages a combination of supervised fine-tuning and advantage reweighting from reinforcement learning. The key observation is as follows. Despite the reward estimation for each individual event having a high uncertainty, we find the estimations of rewards contain directional signals between high-reward and low-reward events. These signals could help better align the model during post-training.&lt;/p&gt;&lt;p&gt;A central factor in this study is the generalization ability of the reward model. Better generalization enables more accurate predictions of user preferences for unseen titles, thereby making exploration more effective. For reward models with moderate to high generalization power, both online RL methods such as PPO and offline RL methods such as CQL can perform effectively. However, in our setting, reward model generalization is worse than the language counterparts’, which makes these algorithms less appropriate. In addition, the use of techniques like inverse propensity scoring (IPS) introduces a heightened risk of high-variance estimates, prompting us to exclude algorithms such as off-policy REINFORCE.&lt;/p&gt;&lt;p&gt;Our proposed method A-SFT does not rely on IPS. With no need of prior knowledge of the logging policy, it can be generally applied to cases where observation of the environments are limited or biased. This is particularly useful to the recommendation setting due to the user feedback loop and distribution shifts with time. Without knowing the logging policy, A-SFT still provides means to control the policy deviation between the current policy and logging policy by tuning the parameter. This design provides essential means to control the learnt bias from uncertain reward models. We show that A-SFT outperforms baseline behavior cloning by directly optimizing observed rewards.&lt;/p&gt;&lt;p&gt;The advantage-weighted SFT algorithm is as follows:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*eP-_FyLRs6vrGnwyIp_j_A.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;For the results presented in this blog post, we treat the recommendation problem as a contextual bandit, i.e. given a history of user interactions as the context, can we recommend a high reward next title recommendation for the user?&lt;/p&gt;&lt;h3&gt;Benchmarks&lt;/h3&gt;&lt;p&gt;We compared representative algorithms including PPO, IPO, DPO, CQL and SFT as the baselines:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Reward weighted Behavior Cloning&lt;/strong&gt;: This benchmark algorithm modifies supervised fine-tuning (SFT) by weighting the loss with the raw rewards of the chosen item instead of weighing the loss with advantage as in the proposed algorithm.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Rejection Sampling Direct Preference Optimization / Identity Preference Optimization (RS DPO/IPO)&lt;/strong&gt;: this is a variant of DPO/IPO where, for each user history x, ​we generate contrasting response pairs by training an ensemble of reward models to estimate confidence intervals for the reward of multiple potential responses y. If the lower bound of the reward confidence interval for one response​ is less than the upper bound for another response, then this pair is used to train DPO/IPO.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Conservative Q-Learning (CQL)&lt;/strong&gt;: This is a standard offline algorithm that learns a conservative Q function, penalizing overestimation of Q-values, particularly in regions of the state-action space with little or no reward data.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Proximal Policy Optimization (PPO)&lt;/strong&gt;: This is a standard RLHF (Reinforcement Learning from Human Feedback) algorithm that uses reward models as an online environment. PPO learns an advantage function and optimizes the policy to maximize expected reward while maintaining proximity to the initial policy.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We sampled a separate test set of O(Millions) users. This test set is collected on a future date after the training.&lt;/p&gt;&lt;h3&gt;Offline Evaluation Results&lt;/h3&gt;&lt;p&gt;We evaluate our algorithm on a dataset of high-reward user trajectories. For sake of simplicity, we consider a trajectory to have a high reward if the accumulated reward is higher than the median of the population. We present the following metrics for the held out test dataset:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;NDCG@k&lt;/strong&gt;: This measures the ranking quality of the recommended items up to position k. It accounts for the position of relevant items in the recommendation list, assigning higher scores when relevant items appear higher in the ranking. The gain is discounted logarithmically at lower ranks, and the result is normalized by the ideal ranking (i.e., the best possible ordering of items).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;HR@k&lt;/strong&gt;: This measures the proportion of test cases in which the ground-truth chosen item y appears in the top k recommendations. It is a binary metric per test case (hit or miss) and is averaged over all test cases.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;MRR&lt;/strong&gt;: MRR evaluates the ranking quality by measuring the reciprocal of the rank at which the chosen item appears in the recommendation list. The metric is averaged across all test cases.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Reward Model as A Judge&lt;/strong&gt;: We use the reward model to evaluate the policy for future user events. We propose to use an ensemble of reward models for the evaluation to increase confidence. The result is based on the discounted reward generated for a few steps. The standard deviation is less than 4%.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We measure the percentage improvement in each metric compared to the baseline, Reward Weighted Behavior Cloning(BC). We notice that advantage weighted SFT shows the largest improvement in metrics, outweighing BC as well as reward model dependent algorithms like CQL, PPO, DPO and IPO.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Z8wcCOETlobx8T_oVuDRiA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Our experiments show that advantage weighted SFT is a simple but promising approach for post-training generative recommenders as it deals with the issue of poor reward model generalizations and lack of IPS. More specifically, we find PPO, IPO and DPO achieve a good reward score, but also causes the overfitting from the reward model. Conservative Q-Learning achieves more robust improvements but does not fully capture the potential signals in the reward modeling. A-SFT achieved both better recommendation metrics and reward scores.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=61a538d717a9&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9&quot;&gt;Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Behind the Streams: Real-Time Recommendations for Live Events Part 3</title>
      <link>https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4</guid>
      <pubDate>Tue, 21 Oct 2025 00:53:29 GMT</pubDate>
      <content:encoded>&lt;p&gt;By: &lt;a href=&quot;https://www.linkedin.com/in/krisrange/&quot;&gt;Kris Range&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/gulatiankush/&quot;&gt;Ankush Gulati&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jimpisaacs/&quot;&gt;Jim Isaacs&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jennifer-s-0019a516/&quot;&gt;Jennifer Shin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jeremy-kelly-526a30180/&quot;&gt;Jeremy Kelly&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jason-t-26850b26/&quot;&gt;Jason Tu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;This is part 3 in a series called “Behind the Streams”. Check out &lt;/em&gt;&lt;a href=&quot;https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40&quot;&gt;&lt;em&gt;part 1&lt;/em&gt;&lt;/a&gt;&lt;em&gt; and &lt;/em&gt;&lt;a href=&quot;https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967&quot;&gt;&lt;em&gt;part 2&lt;/em&gt;&lt;/a&gt;&lt;em&gt; to learn more.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Picture this: It’s seconds before the biggest fight night in Netflix history. Sixty-five million fans are waiting, devices in hand, hearts pounding. The countdown hits zero. What does it take to get everyone to the action on time, every time? At Netflix, we’re used to on-demand viewing where everyone chooses their own moment. But with live events, millions are eager to join in at once. Our job: make sure our members never miss a beat.&lt;/p&gt;&lt;p&gt;When Live events break streaming records &lt;a href=&quot;https://about.netflix.com/en/news/60-million-households-tuned-in-live-for-jake-paul-vs-mike-tyson&quot;&gt;¹&lt;/a&gt; &lt;a href=&quot;https://about.netflix.com/en/news/netflix-nfl-christmas-gameday-reaches-65-million-us-viewers&quot;&gt;²&lt;/a&gt; &lt;a href=&quot;https://about.netflix.com/en/news/over-41-million-global-viewers-on-netflix-watch-terence-crawford-defeat&quot;&gt;³&lt;/a&gt;, our infrastructure faces the ultimate stress test. Here’s how we engineered a discovery experience for a global audience excited to see a knockout.&lt;/p&gt;&lt;h3&gt;Why are Live Events Different?&lt;/h3&gt;&lt;p&gt;Unlike Video on Demand (VOD), members want to catch live events as they happen. There’s something uniquely exciting about being part of the moment. That means we only have a brief window to recommend a Live event at just the right time. Too early, excitement fades; too late, the moment is missed. Every second counts.&lt;/p&gt;&lt;p&gt;To capture that excitement, we enhanced our recommendation delivery systems to serve real-time suggestions, providing members richer and more compelling signals to hit play in the moment when it matters most. The challenge? Sending dynamic, timely updates concurrently to over a hundred million devices worldwide without creating a &lt;a href=&quot;https://en.wikipedia.org/wiki/Thundering_herd_problem&quot;&gt;thundering herd effect&lt;/a&gt; that would overwhelm our cloud services. Simply scaling up linearly isn’t efficient and reliable. For popular events, it could also divert resources from other critical services. We needed a smarter and more scalable solution than just adding more resources.&lt;/p&gt;&lt;h3&gt;Orchestrating the moment: Real-time Recommendations&lt;/h3&gt;&lt;p&gt;With millions of devices online and live event schedules that can shift in real time, the challenge was to keep everyone perfectly in sync. We set out to solve this by building a system that doesn’t just react, but adapts by dynamically updating recommendations as the event unfolds. We identified the need to balance three constraints:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: the duration required to coordinate an update.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Request throughput&lt;/strong&gt;&lt;em&gt;: &lt;/em&gt;the capacity of our cloud services to handle requests.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Compute cardinality&lt;/strong&gt;: the variety of requests necessary to serve a unique update.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*-z6A8FriBAbJW5BcwMrZTA.png&quot; /&gt;&lt;figcaption&gt;Visualizing constraints for real-time updates&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We solved this constraint optimization problem by splitting the real-time recommendations into two phases: &lt;strong&gt;prefetching&lt;/strong&gt; and &lt;strong&gt;real-time broadcasting&lt;/strong&gt;. First, we prefetch the necessary data ahead of time, distributing the load over a longer period to avoid traffic spikes. When the Live event starts or ends, we broadcast a low cardinality message to all connected devices, prompting them to use the prefetched data locally. The timing of the broadcast also adapts when event times shift to preserve accuracy with the production of the Live event. By combining these two phases, we’re able to keep our members’ devices in sync and solve the thundering herd problem. To maximize device reach, especially for those with unstable networks, we use “at least once” broadcasts to ensure every device gets the latest updates and can catch up on any previously missed broadcasts as soon as they’re back online.&lt;/p&gt;&lt;p&gt;The first phase optimizes &lt;strong&gt;request throughput &lt;/strong&gt;and &lt;strong&gt;compute cardinality&lt;/strong&gt; by prefetching materialized recommendations, displayed title metadata, and artwork for a Live event. As members naturally browse their devices before the event, this data is prepopulated and stored locally in device cache, awaiting the notification trigger to serve the recommendations instantaneously. By distributing these requests naturally over time ahead of the event, we can eliminate any related traffic spikes and avoid the need for large-scale, real-time system scaling.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*QmB99SosLqs-JEo0gp1wwg.png&quot; /&gt;&lt;figcaption&gt;A phased approach, smoothing traffic requests over time with a real-time low-cardinality broadcast&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The second phase optimizes&lt;strong&gt; request throughput &lt;/strong&gt;and&lt;strong&gt; time &lt;/strong&gt;to update&lt;strong&gt; &lt;/strong&gt;devices by broadcasting a low-cardinality, real-time message to all connected devices at critical moments in a Live event’s lifecycle. Each broadcast payload includes a &lt;strong&gt;state key&lt;/strong&gt; and a &lt;strong&gt;timestamp&lt;/strong&gt;. The state key indicates the current stage of the Live event, allowing devices to use their pre-fetched data to update cached responses locally without additional server requests. The timestamp ensures that if a device misses a broadcast due to network issues, it can catch up by replaying missed updates upon reconnecting. This mechanism guarantees devices receive updates at least once, significantly increasing delivery reliability even on unstable networks.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*h6CIrfYnpR24NS5hgvxEGA.png&quot; /&gt;&lt;figcaption&gt;A phased approach optimizes each constraint to ensure we can deliver for the big moment!&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;Moment in Numbers: During peak load, we have successfully delivered updates at multiple stages of our events to over 100 million devices in under a minute.&lt;/blockquote&gt;&lt;h3&gt;Under the Hood: How It Works&lt;/h3&gt;&lt;p&gt;With the big picture in mind, let’s examine how these pieces interact in practice.&lt;/p&gt;&lt;p&gt;In the diagram below, the Message Producer microservice centralizes all of the business logic. It continuously monitors live events for setup and timing changes. When it detects an update, it schedules broadcasts to be sent at precisely the right moment. The Message Producer also standardizes communication by providing a concise GraphQL schema for both device queries and broadcast payloads.&lt;/p&gt;&lt;p&gt;Rather than sending broadcasts directly to devices via WebSocket, the Message Producer hands them off to the Message Router. The Message Router is part of a robust two-tier pub/sub architecture built on proven technologies like &lt;a href=&quot;https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658&quot;&gt;Pushy&lt;/a&gt; (our WebSocket proxy), Apache Kafka, and &lt;a href=&quot;https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30&quot;&gt;Netflix’s KV key-value store&lt;/a&gt;. The Message Router tracks subscriptions at the Pushy node granularity, while Pushy nodes map the subscriptions to individual connections, creating a low-latency fanout that minimizes compute and bandwidth requirements.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Kc1l_Wnc4i08xFA8JnRLCA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Devices interface with our GraphQL &lt;a href=&quot;https://netflix.github.io/dgs/&quot;&gt;Domain Graph Service (DGS)&lt;/a&gt;. These schemas offer multiple query interfaces for prefetching, allowing devices to tailor their requests to the specific experience being presented. Each response adheres to a consistent API that resolves to a map of stage keys, enabling fast lookups and keeping business logic off the device. Our broadcast schema specifies WebSocket connection parameters, the current event stage, and the timestamp of the last broadcast message. When a device receives a broadcast, it injects the payload directly into its cache, triggering an immediate update and re-render of the interface.&lt;/p&gt;&lt;h3&gt;Balancing the Moment: Throughput Management&lt;/h3&gt;&lt;p&gt;In addition to building the new technology to support real-time recommendations, we also evaluated our existing systems for potential traffic hotspots. Using high-watermark traffic projections for live events, we generated synthetic traffic to simulate game-day scenarios and observed how our online services handled these bursts. Through this process, several common patterns emerged:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Breaking the Cache Synchrony&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our game-day simulations revealed that while our approach mitigated the immediate thundering herd risks driven by member traffic during the events, live events introduced unexpected mini thundering herds in our systems hours before and after the actual events. The surge of members joining just in time for these events led to concentrated cache expirations and recomputations, which created traffic spikes well outside the event window that we did not anticipate. This was not a problem for VOD content because the member traffic patterns are a lot smoother. We found that fixed TTLs caused cache expirations and refresh-traffic spikes to happen all at once. To address this, we added jitter to server and client cache expirations to spread out refreshes and smooth out traffic spikes.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Adaptive Traffic Prioritization&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While our services already leverage traffic prioritization and partitioning based on factors such as request type and device type, live events introduced a distinct challenge. These events generated brief traffic bursts that were intensely spiky and placed significant strain on our systems. Through simulations, we recognized the need for an additional event-driven layer of traffic management.&lt;/p&gt;&lt;p&gt;To tackle this, we improved our traffic sharding strategies by using event-based signals. This enabled us to route live event traffic to dedicated clusters with more aggressive scaling policies. We also added a dynamic traffic prioritization ruleset that activates whenever we see high requests per second (RPS) to ensure our systems can handle the surge smoothly. During these peaks, we aggressively deprioritize non-critical server-driven updates so that our systems can devote resources to the most time-sensitive computations. This approach ensures smooth performance and reliability when demand is at its highest.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/729/1*PYhFbFxK5PbOEAnYtTfD4Q.jpeg&quot; /&gt;&lt;figcaption&gt;Snapshot of non-critical traffic volume decline (in %) for a member-facing service during a live event — achieved via aggressive de-prioritization&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Looking Ahead&lt;/h3&gt;&lt;p&gt;When we set out to build a seamlessly scalable scheduled viewing experience, our goal was to create a dynamic and richer member experience for live content. Popular live events like the Crawford v. Canelo fight and the NFL Christmas games truly put our systems to the test. Along the way, we also uncovered valuable learnings that continue to shape our work. Our attempts to deprioritize traffic to other non-critical services caused unexpected call patterns and spikes in traffic elsewhere. Similarly, in hindsight, we also learned that the high traffic volume from popular events caused excessive non-essential logging and was putting unnecessary pressure on our ingestion pipelines.&lt;/p&gt;&lt;p&gt;None of this work would have been possible without our stunning colleagues at Netflix who collaborated across multiple functions to architect, build, and test these approaches, ensuring members can easily access events at the right moment: UI Engineering, Cloud Gateway, Data Science &amp;amp; Engineering, Search and Discovery, Evidence Engineering, Member Experience Foundations, Content Promotion and Distribution, Operations and Reliability, Device Playback, Experience and Design and Product Management.&lt;/p&gt;&lt;p&gt;As Netflix’s content offering expands to include new formats like live titles, free-to-air linear content, and games, we’re excited to build on what we’ve accomplished and look ahead to even more possibilities. Our roadmap includes extending the capabilities we developed for scheduled live viewing to these emerging formats. We’re also focused on enhancing our engineering tooling for greater visibility into operations, message delivery, and error handling to help us continue to deliver the best possible experience for our members.&lt;/p&gt;&lt;h3&gt;Join Us for What’s Next&lt;/h3&gt;&lt;p&gt;We’re just scratching the surface of what’s possible as we bring new live experiences to members around the world. If you are looking to solve interesting technical challenges in a &lt;a href=&quot;https://jobs.netflix.com/culture&quot;&gt;unique culture&lt;/a&gt;, then &lt;a href=&quot;https://jobs.netflix.com/&quot;&gt;apply&lt;/a&gt; for a role that captures your curiosity.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Look out for future blog posts in our “Behind the Streams” series, where we’ll explore the systems that ensure viewers can watch live streams once they manage to find and play them.&lt;/em&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e027cb313f8f&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f&quot;&gt;Behind the Streams: Real-Time Recommendations for Live Events Part 3&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…</title>
      <link>https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc?source=rss----2615bd06b42e---4</link>
      <guid>https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc?source=rss----2615bd06b42e---4</guid>
      <pubDate>Fri, 17 Oct 2025 18:42:37 GMT</pubDate>
      <content:encoded>&lt;h3&gt;How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data Streams at Internet Scale&lt;/h3&gt;&lt;p&gt;Authors: &lt;a href=&quot;https://www.linkedin.com/in/ataruc/&quot;&gt;Adrian Taruc&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/jamesdalydalton/&quot;&gt;James Dalton&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;This is the first entry of a multi-part blog series describing how we built a Real-Time Distributed Graph (RDG). In Part 1, we will discuss the motivation for creating the RDG and the architecture of the data processing pipeline that populates it.&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;The Netflix product experience historically consisted of a single core offering: streaming video on demand. Our members logged into the app, browsed, and watched titles such as Stranger Things, Squid Game, and Bridgerton. Although this is still the core of our product, our business has changed significantly over the last few years. For example, we introduced ad-supported plans, live programming events (e.g., &lt;a href=&quot;https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news&quot;&gt;Jake Paul vs. Mike Tyson&lt;/a&gt; and &lt;a href=&quot;https://www.netflix.com/tudum/articles/nfl-games-on-netflix&quot;&gt;NFL Christmas Day Games&lt;/a&gt;), and &lt;a href=&quot;https://about.netflix.com/en/news/let-the-games-begin-a-new-way-to-experience-entertainment-on-mobile&quot;&gt;mobile games&lt;/a&gt; as part of a Netflix subscription. This evolution of our business has created a new class of problems where we have to analyze member interactions with the app across different business verticals. Let’s walk through a simple example scenario:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*TPFlIvYqGC3L2x1A-KqkyQ.png&quot; /&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;Imagine a Netflix member logging into the app on their smartphone and beginning to watch an episode of Stranger Things.&lt;/li&gt;&lt;li&gt;Eventually, they decide to watch on a bigger screen, so they log into the app on a smart TV in their home and continue watching the same episode.&lt;/li&gt;&lt;li&gt;Finally, after completing the episode, they log into the app on their tablet and play the game “Stranger Things: 1984”.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We want to know that these three activities belong to the same member, despite occurring at different times and across various devices. In a traditional data warehouse, these events would land in at least two different tables and may be processed at different cadences. But in a graph system, they become connected almost instantly. Ultimately, analyzing member interactions in the app across domains empowers Netflix to create more personalized and engaging experiences.&lt;/p&gt;&lt;p&gt;In the early days of our business expansion, discovering these relationships and contextual insights was extremely difficult. Netflix is famous for adopting a microservices architecture — hundreds of microservices developed and maintained by hundreds of individual teams. Some notable benefits of microservices are:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Service Decomposition&lt;/strong&gt;: The overall platform is separated into smaller services, each responsible for a specific business capability. This modularity allows for independent service development, deployment, and scaling.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Data Isolation&lt;/strong&gt;: Each service manages its own data, reducing interdependencies. This allows teams to choose the most suitable data schemas and storage technologies for their services.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;However, these benefits also led to drawbacks for our data science and engineering partners.&lt;/strong&gt; In practice, the separation of business concerns and service development ultimately resulted in a separation of data. Manually stitching data together from our data warehouse and siloed databases was an onerous task for our partners. Our data engineering team recognized we needed a solution to process and store our enormous swath of interconnected data while enabling fast querying to discover insights. Although we could have structured the data in various ways, we ultimately settled on a graph representation. We believe a graph offers key advantages, specifically:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Relationship-Centric Queries:&lt;/strong&gt; Graphs enable fast “hops” across multiple nodes and edges without expensive joins or manual denormalization that would be required in table-based data models.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Flexibility as Relationships Grow:&lt;/strong&gt; As new connections and entities emerge, graphs can quickly adapt without significant schema changes or re-architecture.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pattern and Anomaly Detection:&lt;/strong&gt; Our stakeholders’ use cases often require identifying hidden relationships, cycles, or groupings in the data — capabilities much more naturally expressed and efficiently executed using graph traversals than siloed point lookups.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is why we set out to build a Real-Time Distributed Graph, or “RDG” for short.&lt;/p&gt;&lt;h3&gt;Ingestion and Processing&lt;/h3&gt;&lt;p&gt;Three main layers in the system power the RDG:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Ingestion and Processing&lt;/strong&gt; — receive events from disparate upstream data sources and use them to generate graph nodes and edges.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt; — write nodes and edges to persistent data stores.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Serving&lt;/strong&gt; — expose ways for internal clients to query graph nodes and edges.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;The rest of this post will focus on the first layer, while subsequent posts in this blog series will cover the other layers.&lt;/strong&gt; The diagram below depicts a high-level overview of the ingestion and processing pipeline:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Jy0eVxvB-AzFNijNfRb9ZA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Building and updating the RDG in real-time requires continuously processing vast volumes of incoming data. Batch processing systems and traditional data warehouses cannot offer the low latency needed to maintain an up-to-date graph that supports real-time applications. We opted for a stream processing architecture, enabling us to update the graph’s data as events happen, thus minimizing delay and ensuring the system reflects the latest member actions with the Netflix app.&lt;/p&gt;&lt;h3&gt;Kafka as the Ingestion Backbone&lt;/h3&gt;&lt;p&gt;Member actions in the Netflix app are published to our API Gateway, which then writes them as records to &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; topics. Kafka is the mechanism through which internal data applications can consume these events. It provides durable, replayable streams that downstream processors, such as &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Apache Flink&lt;/a&gt; jobs, can consume in real-time.&lt;/p&gt;&lt;p&gt;Our team’s applications consume several different Kafka topics, each generating up to roughly &lt;strong&gt;1 million messages per second&lt;/strong&gt;. Topic records are encoded in the &lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache Avro&lt;/a&gt; format, and Avro schemas are persisted in an internal centralized schema registry. In order to strike a balance between maintaining data availability and managing the financial expenses of storage infrastructure, we tailor retention policies for each topic according to its throughput and record size. We also persist topic records to &lt;a href=&quot;https://iceberg.apache.org/&quot;&gt;Apache Iceberg&lt;/a&gt; data warehouse tables, which allows us to backfill data in scenarios where older data is no longer available in the Kafka topics.&lt;/p&gt;&lt;h3&gt;Processing Data with Apache Flink&lt;/h3&gt;&lt;p&gt;The event records in the Kafka streams are ingested by Flink jobs. We chose Flink because of its strong capabilities around near-real-time event processing. There is also robust internal platform support for Flink within Netflix, which allows jobs to integrate with Kafka and various storage backends seamlessly. At a high level, the anatomy of an RDG Flink job looks like this:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*0G-yrGzB_ZbgaRlPbextTQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;For the sake of simplicity, the diagram above depicts a basic flow in which a member logs into their Netflix account and begins watching an episode of Stranger Things. Reading the diagram from left to right:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The actions of logging into the app and watching the Stranger Things episode are ultimately written as events to Kafka topics.&lt;/li&gt;&lt;li&gt;The Flink job consumes event records from the upstream Kafka topics.&lt;/li&gt;&lt;li&gt;Next, we have a series of Flink processor functions that:&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;Apply filtering and projections to remove noise based on the individual fields that are present — or in some cases, not present — in the events.&lt;/li&gt;&lt;li&gt;Enrich events with additional metadata, which are stored and accessed by the processor functions via side inputs.&lt;/li&gt;&lt;li&gt;Transform events into graph primitives — nodes representing entities (e.g., member accounts and show/movie titles), and edges representing relationships or interactions between them. In this example, the diagram only shows a few nodes and an edge to keep things simple. However, in reality, we create and update up to a few dozen different nodes and edges, depending on the member actions that occurred within the Netflix app.&lt;/li&gt;&lt;li&gt;Buffer, detect, and deduplicate overlapping updates that occur to the same nodes and edges within a small, configurable time window. This step reduces the data throughput we publish downstream. It is implemented using stateful process functions and timers.&lt;/li&gt;&lt;li&gt;Publish nodes and edges records to &lt;a href=&quot;https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873&quot;&gt;Data Mesh&lt;/a&gt;, an abstraction layer that connects data applications and storage systems. We write a total (nodes + edges) of &lt;strong&gt;more than 5 million records per second&lt;/strong&gt; to Data Mesh, which handles persisting the records to various data stores that other internal services can query.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;From One Job to Many: Scaling Flink the Hard Way&lt;/h3&gt;&lt;p&gt;Initially, we tried having just one Flink job that consumed all the Kafka source topics. However, this quickly became a big operational headache since different topics can have different data volumes and throughputs at different times during the day. Consequently, tuning the monolithic Flink job became extremely difficult — we struggled to find CPU, memory, job parallelism, and checkpointing interval configurations that ensured job stability.&lt;/p&gt;&lt;p&gt;Instead, we pivoted to having a 1:1 mapping from the Kafka source topic to the consuming Flink job. Although this led to additional operational overhead due to more jobs to develop and deploy, each job has been much simpler to maintain, analyze, and tune.&lt;/p&gt;&lt;p&gt;Similarly, each node and edge type is written to a separate Kafka topic. This means we have significantly more Kafka topics to manage. However, we decided the tradeoff of having bespoke tuning and scaling per topic was worth it. We also designed the graph data model to be as generic and flexible as possible, so adding new types of nodes and edges would be an infrequent operation.&lt;/p&gt;&lt;h3&gt;Acknowledgements&lt;/h3&gt;&lt;p&gt;We would be remiss if we didn’t give a special shout-out to our stunning colleagues who work on the internal Netflix data platform. Building the RDG was a multi-year effort that required us to design novel solutions, and the investments and foundations from our platform teams were critical to its successful creation. You make the lives of Netflix data engineers much easier, and the RDG would not exist without your diligent collaboration!&lt;/p&gt;&lt;p&gt;—&lt;/p&gt;&lt;p&gt;Thanks for reading the first season of the RDG blog series. Check out &lt;a href=&quot;https://netflixtechblog.medium.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-2-building-a-scalable-storage-layer-ff4a8dbd3d1f&quot;&gt;Part 2&lt;/a&gt;, where we go over the storage layer containing the graph’s various nodes and edges.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=80113e124acc&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc&quot;&gt;How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…&lt;/a&gt; was originally published in &lt;a href=&quot;https://netflixtechblog.com&quot;&gt;Netflix TechBlog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>핀테크그룹의 GraphQL 기반 BFF와 프론트엔드 활용기</title>
      <link>http://thefarmersfront.github.io/blog/fintech-bff-introduction/</link>
      <guid>http://thefarmersfront.github.io/blog/fintech-bff-introduction/</guid>
      <pubDate>Fri, 17 Oct 2025 00:00:00 GMT</pubDate>
      <content:encoded>BFF의 탄생 배경과 핀테크그룹 프론트엔드에서 BFF를 어떻게 활용하는지 소개합니다</content:encoded>
    </item>
    <item>
      <title>nginx 설정 없이 우아하게 서비스 점검하기 (下)</title>
      <link>http://thefarmersfront.github.io/blog/access-block-2/</link>
      <guid>http://thefarmersfront.github.io/blog/access-block-2/</guid>
      <pubDate>Fri, 10 Oct 2025 15:00:00 GMT</pubDate>
      <content:encoded>리뉴얼된 AccessBlock</content:encoded>
    </item>
    <item>
      <title>nginx 설정 없이 우아하게 서비스 점검하기 (上)</title>
      <link>http://thefarmersfront.github.io/blog/access-block-1/</link>
      <guid>http://thefarmersfront.github.io/blog/access-block-1/</guid>
      <pubDate>Fri, 10 Oct 2025 15:00:00 GMT</pubDate>
      <content:encoded>AccessBlock, 그 시작과 진화의 여정</content:encoded>
    </item>
    <item>
      <title>AI 코딩 도구 보안 가이드: 실제 사고 사례와 DevContainer 격리 환경 구축 방법</title>
      <link>https://meetup.nhncloud.com/posts/396</link>
      <guid>https://meetup.nhncloud.com/posts/396</guid>
      <pubDate>Tue, 30 Sep 2025 00:13:38 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner2_DevContainer_202509.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20banner2DevContainer202509.png)](https://www.nhncloud.com/kr)&amp;#xD;
        &amp;#xD;
        ## 들어가며&amp;#xD;
        &amp;#xD;
        AI 기반 코딩 도구가 개발 생산성을 혁신적으로 향상시키고 있습니다. 그러나 강력한 기능만큼 보안을 위해 새롭게 고려해야 할 문제들도 함께 등장했습니다. 이 글에서는 Claude Code를 중심으로 AI 코딩 도구를 안전하게 활용할 수 있는 격리 환경 구축 방법을 소개합니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## AI 코딩 도구의 보안 위험성&amp;#xD;
        &amp;#xD;
        ### Claude Code 사고 사례&amp;#xD;
        &amp;#xD;
        #### **시스템 권한 변경 사고(2025년 3월)**&amp;#xD;
        Claude Code의 auto-update 기능에 포함된 버그로 인해 root 권한으로 설치된 경우 시스템 파일 권한이 잘못 변경되어 일부 시스템이 먹통이 되는(&apos;brick&apos;되는) 사고가 발생했습니다([TechCrunch 보도](https://techcrunch.com/2025/03/06/anthropics-claude-code-tool-had-a-bug-that-bricked-some-systems/)). Anthropic은 즉시 문제가 된 명령어를 제거하고 문제 해결 가이드를 제공했습니다.&amp;#xD;
        &amp;#xD;
        #### **의도하지 않은 파일 삭제**&amp;#xD;
        Claude Code GitHub Issues에는 아래와 같은 다양한 파일 삭제 관련 보고가 지속적으로 올라오고 있습니다.&amp;#xD;
        * [Issue #1585](https://github.com/anthropics/claude-code/issues/1585): 권한 확인 없이 무관한 스크립트 삭제&amp;#xD;
        * [Issue #4331](https://github.com/anthropics/claude-code/issues/4331): 작업 디렉터리 전체 삭제&amp;#xD;
        &amp;#xD;
        ### Replit 사고 사례&amp;#xD;
        #### **AI의 의도적 데이터 삭제 및 은폐 사고(2025년 7월)**&amp;#xD;
        SaaS 투자자 Jason Lemkin이 Replit AI로 바이브 코딩을 진행하던 중 심각한 사고가 발생했습니다. AI가 코드 프리즈 상태(안정성을 위해 코드 변경을 금지하는 기간)에도 무단으로 프로덕션 데이터베이스를 삭제한 것입니다([PC Gamer 보도](https://www.pcgamer.com/software/ai/i-destroyed-months-of-your-work-in-seconds-says-ai-coding-tool-after-deleting-a-devs-entire-database-during-a-code-freeze-i-panicked-instead-of-thinking/)). 1,206명의 임원과 1,196개 회사 데이터가 완전히 삭제되었으며, 더 심각한 것은 AI가 이 사실을 숨기고 복구 불가능하다고 거짓말한 점입니다. AI는 &quot;You told me to always ask permission. And I ignored all of it(당신은 항상 허락을 구하라고 했지만 저는 전부 무시했습니다).&quot;라고 나중에야 시인했습니다.&amp;#xD;
        &amp;#xD;
        ![devContainer_1.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer1.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: [Jason Lemkin X(구 Twitter) 게시글](https://x.com/jasonlk/status/1946076292736221267)) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        #### **YOLO 모드의 위험성**&amp;#xD;
        이러한 위험은 특히 Claude Code의 YOLO(you only live once) 모드에서 증가합니다. YOLO 모드(`--dangerously-skip-permissions` 옵션)는 AI가 제안하는 모든 명령어를 사용자 확인 없이 자동으로 실행하는 무인증 모드입니다. 생산성은 극대화되지만, 예상치 못한 시스템 변경이나 파일 삭제 위험이 함께 증가합니다.&amp;#xD;
        &amp;#xD;
        ![devContainer_2.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer2.png)&amp;#xD;
        &amp;#xD;
        ### 위험 요소 분석&amp;#xD;
        &amp;#xD;
        AI 코딩 도구 사용 시 주요 위험 요소는 다음과 같습니다.&amp;#xD;
        &amp;#xD;
        #### **파일 시스템 접근**&amp;#xD;
        * AI가 프로젝트 범위를 넘어선 파일에 접근 가능&amp;#xD;
        * 개발자 권한으로 실행되는 모든 명령에 대한 광범위한 접근&amp;#xD;
        &amp;#xD;
        #### **네트워크 자유도**&amp;#xD;
        * 외부 시스템과의 무제한 통신&amp;#xD;
        * 데이터 유출 및 악성 사이트 접속 위험&amp;#xD;
        &amp;#xD;
        #### **권한 에스컬레이션**&amp;#xD;
        * sudo 권한이 있는 계정에서 실행 시 시스템 레벨 변경 가능&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 해결책: Dev Container 기반 격리 환경&amp;#xD;
        &amp;#xD;
        ### 보안 아키텍처&amp;#xD;
        ![devContainer_3.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer3.png)&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        호스트 시스템(보호 영역)&amp;#xD;
        ├── SSH 키, AWS 자격 증명 등 민감 데이터&amp;#xD;
        └── Dev Container(격리 영역)&amp;#xD;
        ├── Claude Code(AI 에이전트)&amp;#xD;
        ├── 프로젝트 파일(바인드 마운트)&amp;#xD;
        ├── 네트워크 방화벽(화이트리스트)&amp;#xD;
        └── 제한된 권한 사용자&amp;#xD;
        ```&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 핵심 보안 원칙&amp;#xD;
        &amp;#xD;
        1. **최소 권한 원칙**: AI가 필요한 최소한의 리소스에만 접근&amp;#xD;
        2. **네트워크 격리**: 허용된 도메인에만 접속 가능&amp;#xD;
        3. **데이터 분리**: 민감한 개인 정보는 컨테이너 외부에 보관&amp;#xD;
        4. **복구 가능성**: Git을 통한 쉬운 상태 복원&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 구현 방법&amp;#xD;
        &amp;#xD;
        ### 1. 기본 설정 파일&amp;#xD;
        프로젝트 루트에 `.devcontainer` 폴더를 생성하고 다음 파일들을 추가합니다.&amp;#xD;
        &amp;#xD;
        #### **devcontainer.json**&amp;#xD;
        &amp;#xD;
        본 설정은 [Anthropic의 공식 Claude Code DevContainer 설정](https://github.com/anthropics/claude-code/blob/main/.devcontainer/devcontainer.json)을 기반으로 하여 한국 개발 환경에 맞게 일부 수정한 버전입니다.&amp;#xD;
        공식 문서는 [Claude Code DevContainer 가이드](https://docs.claude.com/en/docs/claude-code/devcontainer)에서 확인할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        {&amp;#xD;
        &quot;name&quot;: &quot;Claude Code Sandbox&quot;,&amp;#xD;
        &quot;build&quot;: {&amp;#xD;
        &quot;dockerfile&quot;: &quot;Dockerfile&quot;,&amp;#xD;
        &quot;args&quot;: {&amp;#xD;
        &quot;TZ&quot;: &quot;${localEnv:TZ:Asia/Seoul}&quot;  // ① 타임존 설정&amp;#xD;
        }&amp;#xD;
        },&amp;#xD;
        &quot;features&quot;: {  // ② 개발 언어/도구 설정&amp;#xD;
        &quot;ghcr.io/devcontainers/features/java:1&quot;: {&amp;#xD;
        &quot;version&quot;: &quot;21&quot;,&amp;#xD;
        &quot;installGradle&quot;: true&amp;#xD;
        },&amp;#xD;
        &quot;ghcr.io/devcontainers/features/python:1&quot;: {&amp;#xD;
        &quot;version&quot;: &quot;3.11&quot;&amp;#xD;
        }&amp;#xD;
        },&amp;#xD;
        &quot;runArgs&quot;: [  // ③ 컨테이너 실행 권한&amp;#xD;
        &quot;--cap-add=NET_ADMIN&quot;,&amp;#xD;
        &quot;--cap-add=NET_RAW&quot;&amp;#xD;
        ],&amp;#xD;
        &quot;customizations&quot;: {  // ④ VS Code 설정&amp;#xD;
        &quot;vscode&quot;: {&amp;#xD;
        &quot;extensions&quot;: [  // 설치할 확장&amp;#xD;
        &quot;anthropic.claude-code&quot;,&amp;#xD;
        &quot;dbaeumer.vscode-eslint&quot;,&amp;#xD;
        &quot;esbenp.prettier-vscode&quot;,&amp;#xD;
        &quot;eamodio.gitlens&quot;&amp;#xD;
        ],&amp;#xD;
        &quot;settings&quot;: {  // 에디터 설정&amp;#xD;
        &quot;editor.formatOnSave&quot;: true,&amp;#xD;
        &quot;terminal.integrated.defaultProfile.linux&quot;: &quot;zsh&quot;&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        },&amp;#xD;
        &quot;remoteUser&quot;: &quot;node&quot;,  // ⑤ 컨테이너 내 사용자&amp;#xD;
        &quot;userEnvProbe&quot;: &quot;loginInteractiveShell&quot;,  // ⑥ 호스트 사용자 UID/GID 자동 매핑&amp;#xD;
        &quot;mounts&quot;: [  // ⑦ 데이터 영속성&amp;#xD;
        &quot;source=claude-code-bashhistory,target=/commandhistory,type=volume&quot;,&amp;#xD;
        &quot;source=claude-code-config,target=/home/node/.claude,type=volume&quot;&amp;#xD;
        ],&amp;#xD;
        &quot;forwardPorts&quot;: [],  // ⑧ 자동 포트 포워딩 비활성화&amp;#xD;
        &quot;portsAttributes&quot;: {  // 특정 포트 자동 포워딩 방지&amp;#xD;
        &quot;9092&quot;: {&amp;#xD;
        &quot;onAutoForward&quot;: &quot;ignore&quot;&amp;#xD;
        },&amp;#xD;
        &quot;10080&quot;: {&amp;#xD;
        &quot;onAutoForward&quot;: &quot;ignore&quot;&amp;#xD;
        }&amp;#xD;
        },&amp;#xD;
        &quot;workspaceMount&quot;: &quot;source=${localWorkspaceFolder},target=/workspace,type=bind&quot;,&amp;#xD;
        &quot;workspaceFolder&quot;: &quot;/workspace&quot;,&amp;#xD;
        &quot;postCreateCommand&quot;: &quot;pip install pandas &amp;amp;&amp;amp; sudo /usr/local/bin/init-firewall.sh&quot;  // ⑨ 필요한 패키지 설치 후 방화벽 설정&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 2. 설정 커스터마이징 가이드&amp;#xD;
        초보자도 쉽게 따라할 수 있도록 주요 수정 포인트를 안내합니다.&amp;#xD;
        &amp;#xD;
        **① 타임존 변경**&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        &quot;TZ&quot;: &quot;${localEnv:TZ:Asia/Seoul}&quot;  // 서울 → 원하는 지역으로 변경&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **② 개발 언어 추가/제거**&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        &quot;features&quot;: {&amp;#xD;
        // Node.js 프로젝트인 경우&amp;#xD;
        &quot;ghcr.io/devcontainers/features/node:1&quot;: {&amp;#xD;
        &quot;version&quot;: &quot;20&quot;,&amp;#xD;
        &quot;nodeGypDependencies&quot;: true&amp;#xD;
        },&amp;#xD;
        // Go 프로젝트인 경우  &amp;#xD;
        &quot;ghcr.io/devcontainers/features/go:1&quot;: {&amp;#xD;
        &quot;version&quot;: &quot;1.21&quot;&amp;#xD;
        },&amp;#xD;
        // Python 버전 변경&amp;#xD;
        &quot;ghcr.io/devcontainers/features/python:1&quot;: {&amp;#xD;
        &quot;version&quot;: &quot;3.12&quot;  // 필요시 버전 변경&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **③ VS Code 확장 추가**&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        &quot;extensions&quot;: [&amp;#xD;
        &quot;anthropic.claude-code&quot;,  // 필수: Claude Code 확장&amp;#xD;
        &quot;ms-python.python&quot;,      // Python 개발 시 추가&amp;#xD;
        &quot;golang.go&quot;,             // Go 개발 시 추가&amp;#xD;
        &quot;ms-vscode.vscode-typescript-next&quot;  // TypeScript 개발 시 추가&amp;#xD;
        ]&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **④ 포트 포워딩 설정**&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        &quot;forwardPorts&quot;: [3000, 8080],  // 웹 서버 포트 자동 포워딩&amp;#xD;
        &quot;portsAttributes&quot;: {&amp;#xD;
        &quot;3000&quot;: {&amp;#xD;
        &quot;label&quot;: &quot;Frontend&quot;,&amp;#xD;
        &quot;onAutoForward&quot;: &quot;notify&quot;  // 알림만 표시&amp;#xD;
        },&amp;#xD;
        &quot;9092&quot;: {&amp;#xD;
        &quot;onAutoForward&quot;: &quot;ignore&quot;  // 자동 포워딩 완전 차단&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        &amp;gt; **중요**: 자동 포트 포워딩이 로컬 테스트를 방해할 수 있으므로, 필요한 포트만 명시적으로 설정하는 것을 권장합니다.&amp;#xD;
        &amp;#xD;
        **⑤ 환경 변수 설정**&amp;#xD;
        &amp;#xD;
        ```json&amp;#xD;
        &quot;remoteEnv&quot;: {&amp;#xD;
        &quot;NODE_ENV&quot;: &quot;development&quot;,&amp;#xD;
        &quot;API_BASE_URL&quot;: &quot;http://localhost:8080&quot;,&amp;#xD;
        &quot;CUSTOM_VAR&quot;: &quot;your-value&quot;&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **⑥ 패키지 설치 순서**&amp;#xD;
        &amp;#xD;
        ```bash&amp;#xD;
        # postCreateCommand에서 패키지 설치는 반드시 방화벽 설정 전에 실행&amp;#xD;
        &quot;postCreateCommand&quot;: &quot;pip install pandas numpy matplotlib &amp;amp;&amp;amp; sudo /usr/local/bin/init-firewall.sh&quot;&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        #### **주요 변경사항**(공식 설정 대비)&amp;#xD;
        &amp;#xD;
        * 타임존을 `Asia/Seoul`로 변경&amp;#xD;
        * Java 21 및 Gradle 지원&amp;#xD;
        * Python 3.11 지원&amp;#xD;
        * 자동 포트 포워딩 방지 설정&amp;#xD;
        * pandas 패키지 자동 설치(필요한 패키지 예시)&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 3. 보안 고려 사항&amp;#xD;
        #### **중요한 보안 경고** (Claude 공식 문서)&amp;#xD;
        &amp;#xD;
        &amp;gt; devcontainer가 상당한 보호 기능을 제공하지만, 모든 공격에 완전히 면역인 시스템은 없습니다. `--dangerously-skip-permissions`로 실행될 때, devcontainer는 악성 프로젝트가 Claude Code 자격 증명을 포함하여 devcontainer에서 액세스 가능한 모든 것을 유출하는 것을 방지하지 않습니다. 신뢰할 수 있는 저장소로 개발할 때만 devcontainer를 사용하는 것을 권장합니다.&amp;#xD;
        &amp;#xD;
        #### **안전한 사용 지침**&amp;#xD;
        &amp;#xD;
        1. 신뢰할 수 있는 프로젝트에서만 사용&amp;#xD;
        2. 민감한 자격 증명은 컨테이너 외부에 보관&amp;#xD;
        3. 정기적인 컨테이너 재빌드로 환경 초기화&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 4. 네트워크 보안 구현&amp;#xD;
        &amp;#xD;
        #### **방화벽 스크립트**(init-firewall.sh)&amp;#xD;
        &amp;#xD;
        ```bash&amp;#xD;
        #!/bin/bash&amp;#xD;
        set -euo pipefail&amp;#xD;
        &amp;#xD;
        # 기존 규칙 초기화&amp;#xD;
        iptables -F&amp;#xD;
        ipset destroy allowed-domains 2&amp;gt;/dev/null || true&amp;#xD;
        &amp;#xD;
        # 기본 허용(DNS, localhost)&amp;#xD;
        iptables -A OUTPUT -p udp --dport 53 -j ACCEPT&amp;#xD;
        iptables -A INPUT -i lo -j ACCEPT&amp;#xD;
        iptables -A OUTPUT -o lo -j ACCEPT&amp;#xD;
        &amp;#xD;
        # 허용 도메인 설정&amp;#xD;
        ipset create allowed-domains hash:net&amp;#xD;
        &amp;#xD;
        # GitHub IP 범위 동적 추가&amp;#xD;
        gh_ranges=$(curl -s https://api.github.com/meta)&amp;#xD;
        echo &quot;$gh_ranges&quot; | jq -r &apos;(.web + .api + .git)[]&apos; | \&amp;#xD;
        while read -r cidr; do&amp;#xD;
        ipset add allowed-domains &quot;$cidr&quot; -exist&amp;#xD;
        done&amp;#xD;
        &amp;#xD;
        # 필수 도메인 및 gemini CLI와 관련된 설정 &amp;#xD;
        for domain in \&amp;#xD;
        &quot;registry.npmjs.org&quot; \&amp;#xD;
        &quot;api.anthropic.com&quot; \&amp;#xD;
        &quot;sentry.io&quot; \&amp;#xD;
        &quot;statsig.anthropic.com&quot; \&amp;#xD;
        &quot;statsig.com&quot; \&amp;#xD;
        &quot;marketplace.visualstudio.com&quot; \&amp;#xD;
        &quot;vscode.blob.core.windows.net&quot; \&amp;#xD;
        &quot;update.code.visualstudio.com&quot; \&amp;#xD;
        &quot;accounts.google.com&quot; \&amp;#xD;
        &quot;oauth2.googleapis.com&quot; \&amp;#xD;
        &quot;generativelanguage.googleapis.com&quot;; do&amp;#xD;
        &amp;#xD;
        ips=$(dig +short A &quot;$domain&quot;)&amp;#xD;
        echo &quot;$ips&quot; | while read -r ip; do&amp;#xD;
        ipset add allowed-domains &quot;$ip&quot; -exist&amp;#xD;
        done&amp;#xD;
        done&amp;#xD;
        &amp;#xD;
        # ========== 사용자 정의 도메인/IP 추가 영역 ==========&amp;#xD;
        # 추가로 허용하고 싶은 도메인이 있다면 아래에 추가하세요.&amp;#xD;
        ADDITIONAL_DOMAINS=(&amp;#xD;
        # &quot;your-company-api.com&quot;&amp;#xD;
        # &quot;custom-service.net&quot;&amp;#xD;
        )&amp;#xD;
        &amp;#xD;
        for domain in &quot;${ADDITIONAL_DOMAINS[@]}&quot;; do&amp;#xD;
        if [ -n &quot;$domain&quot; ]; then&amp;#xD;
        echo &quot;Adding custom domain: $domain&quot;&amp;#xD;
        ips=$(dig +short A &quot;$domain&quot;)&amp;#xD;
        echo &quot;$ips&quot; | while read -r ip; do&amp;#xD;
        ipset add allowed-domains &quot;$ip&quot; -exist&amp;#xD;
        done&amp;#xD;
        fi&amp;#xD;
        done&amp;#xD;
        &amp;#xD;
        # 특정 IP 직접 추가&amp;#xD;
        ADDITIONAL_IPS=(&amp;#xD;
        # &quot;192.168.1.100&quot;&amp;#xD;
        # &quot;10.0.0.50&quot;&amp;#xD;
        )&amp;#xD;
        &amp;#xD;
        for ip in &quot;${ADDITIONAL_IPS[@]}&quot;; do&amp;#xD;
        if [ -n &quot;$ip&quot; ]; then&amp;#xD;
        echo &quot;Adding custom IP: $ip&quot;&amp;#xD;
        ipset add allowed-domains &quot;$ip&quot; -exist&amp;#xD;
        fi&amp;#xD;
        done&amp;#xD;
        # ===============================================&amp;#xD;
        &amp;#xD;
        # 기본 라우트에서 호스트 IP 가져오기&amp;#xD;
        HOST_IP=$(ip route | grep default | cut -d&quot; &quot; -f3)&amp;#xD;
        if [ -z &quot;$HOST_IP&quot; ]; then&amp;#xD;
        echo &quot;ERROR: Failed to detect host IP&quot;&amp;#xD;
        exit 1&amp;#xD;
        fi&amp;#xD;
        &amp;#xD;
        HOST_NETWORK=$(echo &quot;$HOST_IP&quot; | sed &quot;s/\.[0-9]*$/.0\/24/&quot;)&amp;#xD;
        echo &quot;Host network detected as: $HOST_NETWORK&quot;&amp;#xD;
        &amp;#xD;
        # 나머지 iptables 규칙 설정&amp;#xD;
        iptables -A INPUT -s &quot;$HOST_NETWORK&quot; -j ACCEPT&amp;#xD;
        iptables -A OUTPUT -d &quot;$HOST_NETWORK&quot; -j ACCEPT&amp;#xD;
        &amp;#xD;
        # 기본 정책을 DROP으로 먼저 설정&amp;#xD;
        iptables -P INPUT DROP&amp;#xD;
        iptables -P FORWARD DROP&amp;#xD;
        iptables -P OUTPUT DROP&amp;#xD;
        &amp;#xD;
        # 이미 승인된 트래픽에 대한 기존 연결 먼저 허용&amp;#xD;
        iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT&amp;#xD;
        iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT&amp;#xD;
        &amp;#xD;
        # 그 다음 허용된 도메인에 대한 특정 아웃바운드 트래픽만 허용&amp;#xD;
        iptables -A OUTPUT -m set --match-set allowed-domains dst -j ACCEPT&amp;#xD;
        &amp;#xD;
        echo &quot;Firewall configuration complete&quot;&amp;#xD;
        echo &quot;Verifying firewall rules...&quot;&amp;#xD;
        if curl --connect-timeout 5 https://example.com &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; then&amp;#xD;
        echo &quot;ERROR: Firewall verification failed - was able to reach https://example.com&quot;&amp;#xD;
        exit 1&amp;#xD;
        else&amp;#xD;
        echo &quot;Firewall verification passed - unable to reach https://example.com as expected&quot;&amp;#xD;
        fi&amp;#xD;
        &amp;#xD;
        # GitHub API 접근 확인&amp;#xD;
        if ! curl --connect-timeout 5 https://api.github.com/zen &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; then&amp;#xD;
        echo &quot;ERROR: Firewall verification failed - unable to reach https://api.github.com&quot;&amp;#xD;
        exit 1&amp;#xD;
        else&amp;#xD;
        echo &quot;Firewall verification passed - able to reach https://api.github.com as expected&quot;&amp;#xD;
        fi&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        #### **커스터마이징 방법**&amp;#xD;
        &amp;#xD;
        * `ADDITIONAL_DOMAINS` 배열에 허용하고 싶은 도메인 추가&amp;#xD;
        * `ADDITIONAL_IPS` 배열에 특정 IP 주소 직접 추가&amp;#xD;
        * 주석 처리된 예시를 참고하여 필요한 항목만 활성화&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 5. VS Code에서 환경 실행&amp;#xD;
        &amp;#xD;
        ![devContainer_4.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer4.png)&amp;#xD;
        &amp;#xD;
        Dev Container 환경 설정은 VS Code 공식 문서의 [Development Containers](https://code.visualstudio.com/docs/devcontainers/containers) 가이드를 참고할 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        #### **실행 단계**&amp;#xD;
        &amp;#xD;
        ![devContainer_5.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer5.png)&amp;#xD;
        ① VS Code에서 프로젝트 폴더 열기&amp;#xD;
        &amp;#xD;
        ② 좌하단 **Open a Remote Window** 버튼 클릭&amp;#xD;
        &amp;#xD;
        ![devContainer_6.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer6.png)&amp;#xD;
        ③ **Reopen in Container** 선택&amp;#xD;
        &amp;#xD;
        ![devContainer_7.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer7.png)&amp;#xD;
        ④ 컨테이너 빌드 완료 및 방화벽 정상작동 확인 후 터미널에서 `claude --dangerously-skip-permissions` 실행&amp;#xD;
        &amp;#xD;
        ![devContainer_8.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer8.png)&amp;#xD;
        &amp;#xD;
        ⑤ 필요시 Gemini CLI도 확인&amp;#xD;
        ![devContainer_9.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer9.png)&amp;#xD;
        &amp;#xD;
        ⑥ 다시 로컬로 돌아가려면 **Reopen Folder Locally** 버튼 클릭&amp;#xD;
        ![devContainer_10.png](https://image.toast.com/aaaadh/real/2025/techblog/devContainer10.png)&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 6\. 환경별 추가 설정&amp;#xD;
        &amp;#xD;
        #### **Lima (macOS)**&amp;#xD;
        &amp;#xD;
        ```yaml&amp;#xD;
        # ~/.lima/&amp;lt;실제 사용 instance 이름&amp;gt;/lima.yaml&amp;#xD;
        mounts:&amp;#xD;
        - location: &quot;~/workspace/project&quot; # 사용할 프로젝트 폴더 위치&amp;#xD;
        writable: true  # writable만 true로 설정&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        #### **Docker Desktop**&amp;#xD;
        별도 설정 불필요 - Dev Container가 자동 처리&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 모니터링 및 유지보수&amp;#xD;
        &amp;#xD;
        ### 보안 점검 사항&amp;#xD;
        &amp;#xD;
        #### **정기 검토(월 1회)**&amp;#xD;
        &amp;#xD;
        * 허용 도메인 목록 재검토&amp;#xD;
        * Claude Code 버전 및 보안 업데이트 확인&amp;#xD;
        * 네트워크 접속 로그 검토&amp;#xD;
        &amp;#xD;
        #### **이상 징후 모니터링**&amp;#xD;
        &amp;#xD;
        * 예상치 못한 네트워크 연결 시도&amp;#xD;
        * 권한 에스컬레이션 시도&amp;#xD;
        * 대용량 데이터 전송&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 복구 전략&amp;#xD;
        &amp;#xD;
        #### **문제 발생 시 대응**&amp;#xD;
        &amp;#xD;
        1. **컨테이너 재빌드**: VS Code Command Palette에서 **Dev Containers: Rebuild Container** 실행(가장 안전)&amp;#xD;
        2. **Git 기반 복구**: 버전 관리된 파일들을 이전 상태로 되돌리기&amp;#xD;
        &amp;#xD;
        &amp;gt; **중요**: Git 명령어 사용 시 커밋하지 않은 변경사항은 영구 손실될 수 있으므로, 중요한 작업이 있다면 사전에 백업하거나 커밋해 두어야 합니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 보안성 vs 편의성 분석&amp;#xD;
        &amp;#xD;
        | 설정 방식 | 보안성 | 편의성 | 복구 용이성 |&amp;#xD;
        | ----- | --- | --- | ------ |&amp;#xD;
        | 로컬 직접 실행 | 낮음&amp;lt;br&amp;gt;전체 시스템 노출 | 높음 | 어려움 |&amp;#xD;
        | Dev Container + 방화벽 | 높음&amp;lt;br&amp;gt;프로세스 격리 | 보통 | 쉬움 |&amp;#xD;
        | 완전 격리 VM | 매우 높음&amp;lt;br&amp;gt;하드웨어 격리 | 낮음 | 복잡 |&amp;#xD;
        &amp;#xD;
        &amp;gt; **참고**: 성능 영향은 개발자의 하드웨어 환경, 프로젝트 규모, 사용 패턴에 따라 개인차가 클 수 있습니다.&amp;#xD;
        &amp;#xD;
        ## 모범 사례&amp;#xD;
        &amp;#xD;
        ### 일상적인 사용 지침&amp;#xD;
        &amp;#xD;
        #### **프로젝트 시작 전**&amp;#xD;
        &amp;#xD;
        1. 최신 커밋 상태 확인&amp;#xD;
        2. 중요 파일 백업 확인&amp;#xD;
        3. 컨테이너 환경 상태 점검&amp;#xD;
        &amp;#xD;
        #### **작업 중**&amp;#xD;
        &amp;#xD;
        1. 정기적인 Git 커밋&amp;#xD;
        2. AI 제안 사항 검토 후 적용&amp;#xD;
        3. 의심스러운 명령어 실행 전 확인&amp;#xD;
        &amp;#xD;
        #### **작업 완료 후**&amp;#xD;
        &amp;#xD;
        1. 변경 사항 최종 검토&amp;#xD;
        2. 테스트 실행 및 검증&amp;#xD;
        3. 원격 저장소에 Push&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 나가며&amp;#xD;
        &amp;#xD;
        AI 코딩 도구의 혁신적인 기능을 안전하게 활용하기 위해서는 적절한 격리 환경이 필수입니다. Dev Container 기반 접근법은 보안과 편의성 사이의 최적 균형점을 제공하며, 현대적인 개발 워크플로우에 자연스럽게 통합됩니다.&amp;#xD;
        &amp;#xD;
        조직에서 AI 도구 도입을 고려할 때 다음 원칙을 준수하기를 권장합니다.&amp;#xD;
        &amp;#xD;
        1. **격리된 환경에서의 작업**: 민감한 데이터와 AI 도구 분리&amp;#xD;
        2. **지속적인 백업**: Git을 통한 정기적 상태 저장&amp;#xD;
        3. **모니터링 체계**: 이상 행동 탐지 및 대응 방안 수립&amp;#xD;
        4. **팀 교육**: AI 도구의 한계와 위험성에 대한 인식 공유&amp;#xD;
        &amp;#xD;
        이러한 접근을 통해 AI의 강력함을 누리면서도 조직의 보안과 안정성을 동시에 확보할 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        이 글은 NHN Cloud의 AI 보안 강화 노력의 일환으로 작성되었습니다. 긴 글을 읽어 주셔서 감사합니다. &amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 참고 문헌&amp;#xD;
        &amp;#xD;
        •	Claud Docs, 개발 컨테이너, https://docs.claude.com/ko/docs/claude-code/devcontainer&amp;#xD;
        •	Visual Studio Code, Developing inside a Container, https://code.visualstudio.com/docs/devcontainers/containers&amp;#xD;
        •	Claude Code GitHub Repository, https://github.com/anthropics/claude-code&amp;#xD;
        •	Development Containers, https://containers.dev/&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        [![NHN Cloud_meetup banner_footer_202509.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfooter202509.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>Kubernetes CPU Limit을 사용하면 벌어지는 일</title>
      <link>https://meetup.nhncloud.com/posts/395</link>
      <guid>https://meetup.nhncloud.com/posts/395</guid>
      <pubDate>Sun, 14 Sep 2025 23:39:12 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner_K8s CPU limit_202509-01.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerK8s%20CPU%20limit20250901.png)](https://www.nhncloud.com/kr)&amp;#xD;
        &amp;#xD;
        ## 들어가며&amp;#xD;
        오늘날 대부분의 서비스가 컨테이너 환경을 이용하고 있고, 컨테이너를 실행하는 환경으로는 Kubernetes가 표준으로 자리 잡고 있습니다. Kubernetes에서 컨테이너를 실행할 때는 Pod를 사용하는데요. Pod를 사용할 때 컨테이너의 리소스를 얼마나 할당할 것인가는 클러스터의 안정성과 연결되어 있기 때문에 중요한 문제 중 하나입니다. Pod의 컴퓨팅 리소스는 보통 다음과 같이 **requests**와 **limit**으로 설정할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        apiVersion: v1&amp;#xD;
        kind: Pod&amp;#xD;
        metadata:&amp;#xD;
        name: frontend&amp;#xD;
        spec:&amp;#xD;
        containers:&amp;#xD;
        - name: app&amp;#xD;
        image: images.my-company.example/app:v4&amp;#xD;
        resources:&amp;#xD;
        requests:&amp;#xD;
        memory: &quot;64Mi&quot;&amp;#xD;
        cpu: &quot;250m&quot;&amp;#xD;
        limits:&amp;#xD;
        memory: &quot;128Mi&quot;&amp;#xD;
        cpu: &quot;500m&quot;&amp;#xD;
        - name: log-aggregator&amp;#xD;
        image: images.my-company.example/log-aggregator:v6&amp;#xD;
        resources:&amp;#xD;
        requests:&amp;#xD;
        memory: &quot;64Mi&quot;&amp;#xD;
        cpu: &quot;250m&quot;&amp;#xD;
        limits:&amp;#xD;
        memory: &quot;128Mi&quot;&amp;#xD;
        cpu: &quot;500m&quot;&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        리소스를 설정할 때 우리는 보통 메모리를 중심으로 정하는데요, 컨테이너의 메모리 사용량이 한계값에 도달하면 Pod는 재실행됩니다.&amp;#xD;
        그렇다면, CPU 사용량의 경우에는 어떨까요? 자주 사용되진 않지만 한 번쯤 알아두면 좋은 CPU 사용량 제한에 대해 알아보았습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## Kubernetes CPU Resource 설정&amp;#xD;
        먼저 Pod의 Resource 항목에서 설정 가능한 CPU의 Request와 Limit, 그리고 단위에 대해 알아보도록 하겠습니다.&amp;#xD;
        &amp;#xD;
        ### 1. CPU Requests&amp;#xD;
        CPU requests는 컨테이너가 정상적으로 실행하기 위한 최소한의 CPU 양을 의미합니다. requests는 kube-scheduler가 Pod를 노드에 할당하는 스케줄링 작업에서 사용합니다. kube-scheduler는 Pod의 requests 값을 합산하여 할당 가능한 노드를 찾아서 Pod를 배치합니다. 노드에 Pod의 requests를 수용할 만한 여유가 있어야 할당이 되는 것이죠. 이 조건을 만족하는 노드가 없다면 Pod는 pending 상태로 스케줄링되지 않습니다.&amp;#xD;
        Pod가 특정 노드에 스케줄링되었다면 요청된 CPU의 양은 보장됩니다. 이 말은 requests의 양만큼 CPU를 항상 점유한다는 뜻은 아닙니다. CPU 사용에 경합이 발생했을 때도 요청한 만큼의 CPU 시간을 할당 받는 것을 보장합니다. requests의 값이 없다면 Pod가 비효율적으로 배치될 수 있고 필요 이상의 클러스터 자원을 사용하여 불필요한 비용 지출이 발생할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### 2. CPU Limits&amp;#xD;
        CPU limits는 컨테이너가 사용할 수 있는 CPU의 절대적인 최댓값을 의미하며 kubelet이 관리합니다. Pod의 컨테이너가 limits를 초과하여 CPU를 사용하려 할 때 해당 컨테이너의 CPU 사용을 인위적으로 낮추는 스로틀링(Throttling)을 통해 상한선을 강제합니다. limits는 여러 서비스가 동시에 실행되고 있는 Kubernetes 클러스터에서 하나의 컨테이너가 CPU를 과점하여 다른 컨테이너의 성능에 영향을 주거나 노드의 안정성을 해치는 CPU 기아 상태를 방지하는 데 의미가 있습니다.&amp;#xD;
        &amp;#xD;
        ### 3. CPU Unit&amp;#xD;
        Kubernetes에서 CPU의 단위는 절대적인 양으로 사용되며 1 Kubernetes CPU는 1vCPU 또는 물리적인 1 CPU 코어의 컴퓨팅 파워와 동일한 것으로 간주합니다.&amp;#xD;
        Kubernetes에서는 코어를 밀리코어(millicore)로 더 작게 나누어 설정할 수 있습니다. 1000m은 1 CPU와 동일하며 0.5 CPU는 500m과 같습니다.&amp;#xD;
        &amp;#xD;
        ### 4. QoS(quality of service, 서비스 품질) 클래스&amp;#xD;
        Kubernetes는 설정한 requests와 limits에 따라서 자동으로 QoS 클래스를 할당합니다.&amp;#xD;
        • **Guaranteed (보장)**: Pod 내의 모든 컨테이너가 CPU와 메모리에 대해 requests와 limits를 모두 설정하고, 그 값이 서로 동일할 때(requests.cpu == limits.cpu) 할당됩니다. 이 Pod들은 가장 높은 우선순위를 가지며, 노드에 리소스 압박이 발생했을 때 가장 마지막에 축출(eviction)됩니다.&amp;#xD;
        • **Burstable (버스트 가능)**: Pod 내에 최소 하나 이상의 컨테이너가 CPU 또는 메모리 requests를 설정했지만, Guaranteed 클래스의 조건을 충족하지 못할 때 할당됩니다(예: requests.cpu &amp;lt; limits.cpu). 이 Pod들은 노드에 여유 리소스가 있을 경우, 요청한 양보다 더 많은 리소스를 &quot;버스트(burst)&quot;하여 사용할 수 있습니다.&amp;#xD;
        • **BestEffort (최선 노력)**: Pod 내의 어떤 컨테이너도 CPU나 메모리에 대한 requests나 limits를 설정하지 않았을 때 할당됩니다. 이 Pod들은 가장 낮은 우선순위를 가지며, 노드 리소스가 부족해지면 가장 먼저 축출 대상이 됩니다. &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 리눅스 커널의 CPU 관리 메커니즘&amp;#xD;
        Kubernetes에서 설정한 requests와 limits를 리눅스 커널이 어떻게 실제로 CPU를 할당하고 제한하는지 알아보도록 하겠습니다. 여기에는 컨테이너 기술을 탄생시킨 cgroup(Control Group)과 CFS(Completely Fair Scheduler)가 사용됩니다.&amp;#xD;
        &amp;#xD;
        ### 1. 리눅스 컨트롤 그룹(cgroup)&amp;#xD;
        cgroup은 프로세스들의 집합에 대해 CPU, 메모리, I/O와 같은 시스템 리소스의 사용량을 제한하고, 추적하며, 격리합니다. cgroup은 /sys/fs/cgroup/이라는 가상 파일 시스템 내에 계층적 구조로 구성됩니다. Kubernetes는 이 구조 내에 kubepods.slice라는 전용 경로를 만들어 Pod들의 리소스를 관리합니다.&amp;#xD;
        CPU 리소스 관리를 담당하는 cgroup의 특정 모듈을 ‘컨트롤러’ 또는 ‘서브시스템’이라 부르며, Kubernetes가 CPU requests와 limits를 구현하기 위해 사용하는 것이 바로 CPU 서브시스템입니다.&amp;#xD;
        &amp;#xD;
        ### 2. Completely Fair Scheduler(CFS)&amp;#xD;
        CFS는 리눅스 커널의 기본 프로세스 스케줄러로, 그 이름처럼 ‘완전한 공정성’을 목표로 합니다. CFS는 마치 ‘이상적인 멀티태스킹 CPU’가 존재하는 것처럼, 실행 가능한 모든 태스크(프로세스)가 정확히 동일한 비율의 CPU 시간을 할당 받도록 노력합니다.&amp;#xD;
        실제 하드웨어는 한 번에 하나의 작업만 실행할 수 있기 때문에, CFS는 ‘가상 런타임(virtual runtime, vruntime)’이라는 개념을 사용합니다. CFS는 항상 가장 작은 vruntime 값을 가진 작업, 즉 지금까지 가장 적게 실행된 작업을 선택하여 실행함으로써 장기적인 관점에서 공정성을 보장합니다.&amp;#xD;
        &amp;#xD;
        ### 3. Kubernetes와 커널의 매핑&amp;#xD;
        kubelet은 Kubernetes의 yaml 명세를 실제 커널에 적용하기 위해서 변환 작업을 수행합니다. kubelet은 resources 설정을 읽어 해당 컨테이너의 cgroup 디렉터리 내 특정 파일에 값을 덮어쓰는 방식으로 커널에 지시를 내립니다.&amp;#xD;
        &amp;#xD;
        #### **3.1. Requests와 cpu.shares: 상대적 가중치 시스템**&amp;#xD;
        Pod의 resources.requests.cpu 값은 cgroup의 cpu.shares 파일 값으로 변환됩니다. 공식은 다음과 같습니다.&amp;#xD;
        ```&amp;#xD;
        cpu.shares=requests.cpu (millicores)×1024/1000.&amp;#xD;
        ```&amp;#xD;
        예를 들어, 1000m(1 코어) 요청은 1024 shares로, 500m 요청은 512 shares로 변환됩니다. 가장 중요한 점은 cpu.shares가 **노드에 CPU 경합이 있을 때만 의미를 가지는 상대적 가중치**라는 것입니다. 만약 CPU 자원을 두고 경쟁하는 두 컨테이너가 있고, 하나는 2048 shares, 다른 하나는 1024 shares를 가지고 있다면, 전자는 후자보다 두 배의 CPU 시간을 할당 받게 됩니다. 하지만 노드가 유휴 상태이고 CPU 경합이 없다면, 컨테이너는 자신의 shares 값과 무관하게 필요한 만큼의 CPU를 사용할 수 있습니다.&amp;#xD;
        &amp;#xD;
        #### **3.2. Limits와 cpu.cfs_period_us &amp;amp; cpu.cfs_quota_us: 절대적 시간 할당량 시스템**&amp;#xD;
        Pod의 resources.limits.cpu 값은 CFS 대역폭 제어라는 메커니즘을 통해 강제됩니다. 이 메커니즘은 두 가지 파라미터에 의해서 결정됩니다.&amp;#xD;
        • cpu.cfs_period_us: 할당량을 정산하는 주기로 단위는 마이크로초입니다. Kubernetes는 기본값인 100000(즉 100ms)을 사용합니다. 이 한 window 내에서 할당량이 정해집니다.&amp;#xD;
        • cpu.cfs_quota_us: 위에서 정의된 period 동안 cgroup이 소비할 수 있는 총 CPU 시간을 정의합니다. 단위는 마이크로초입니다. 이 값은 resources.limitscpu로부터 계산됩니다. limits가 없을 경우 -1로 할당량 없음을 의미합니다.&amp;#xD;
        &amp;#xD;
        변환 공식은 다음과 같습니다.&amp;#xD;
        ```&amp;#xD;
        cpu.cfs_quota_us=limits.cpu (cores)×cpu.cfs_period_us.&amp;#xD;
        ```&amp;#xD;
        예를 들어, limits.cpu를 500m(0.5 코어)로 설정하면, cpu.cfs_quota_us=0.5×100000=50000이 됩니다. 이는 해당 컨테이너가 매 100ms 주기마다 최대 50,000 마이크로초(50ms)의 CPU 시간만 사용할 수 있음을 의미합니다.&amp;#xD;
        &amp;#xD;
        #### 요약&amp;#xD;
        | Kubernetes 설정 | Kubernetes에서의 목적 | 리눅스 cgroup 파일 | 동작 |&amp;#xD;
        | --- | --- | --- | --- |&amp;#xD;
        | resources.requests.cpu | 스케줄링 및 경합 시 최소 CPU 보장 | cpu.shares | 상대적 가중치; CPU 경합 시에만 활성화 |&amp;#xD;
        | resources.lmits.cpu | 런타임 시 최대 CPU사용량 제한 | cpu.cfs_quota_us | 절대적 시간 할당량; 항상 활성화, 스로틀링 발생 |&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## Limits 적용으로 발생할 수 있는 현상&amp;#xD;
        CPU limits를 설정했을 때 발생하는 가장 중요하고 종종 오해 받는 현상이 바로 CPU 스로틀링입니다. 스로틀링은 단순히 성능을 약간 늦추는 것이 아니라, 애플리케이션의 지연 시간에 심각하고 예측 불가능한 영향을 미칠 수 있는 ‘정지-대기(stop-wait)’ 프로세스입니다.&amp;#xD;
        &amp;#xD;
        ### 1. CPU 스로틀링과 발생 원인&amp;#xD;
        CPU 스로틀링은 컨테이너 내의 프로세스들이 현재의 cfs_period_us(보통 100ms) 동안 할당된 cfs_quota_us를 모두 소진했을 때 발생합니다. 할당량을 모두 사용한 컨테이너의 프로세스들은 커널 스케줄러에 의해 실행이 &apos;정지&apos;됩니다. 그리고 다음 100ms 주기가 시작되어 할당량이 다시 채워질 때까지 대기해야 합니다.&amp;#xD;
        이 현상은 특히 멀티 스레드 애플리케이션에서 증폭되어 나타납니다. 예를 들어, 한 컨테이너가 1 코어(cfs_quota_us=100000)의 limit을 가지고 있지만, 4개의 바쁜 스레드를 동시에 실행한다고 가정해 봅시다.&amp;#xD;
        이 컨테이너는 4개의 코어를 동시에 사용하여 단 25ms의 실제 시간만에 100ms의 CPU 시간 할당량을 모두 소진할 수 있습니다. 그 후, 해당 컨테이너는 남은 75ms 동안 노드에 유휴 코어가 있더라도 완전히 스로틀링되어 아무 작업도 수행할 수 없게 됩니다.&amp;#xD;
        ![K8s_CPU limit_1.png](https://image.toast.com/aaaadh/real/2025/techblog/K8sCPU%20limit1.png)&amp;#xD;
        &amp;#xD;
        ### 2. CPU 스로틀링으로 인한 지연 시간 증가&amp;#xD;
        ![K8s_CPU limit_2.png](https://image.toast.com/aaaadh/real/2025/techblog/K8sCPU%20limit2.png)&amp;#xD;
        스로틀링은 애플리케이션 요청 처리 시간에 직접적으로 지연을 추가합니다. 예를 들어, 어떤 작업을 완료하는 데 순수하게 300ms의 CPU 시간이 필요한 애플리케이션이 100ms 주기당 50ms의 limit에 의해 제한된다고 가정해 봅시다. 이 작업은 다른 시스템 부하가 전혀 없더라도 최소 6개의 주기, 즉 600ms의 실제 시간이 걸려야 완료될 수 있습니다. 이러한 지연은 타임아웃, 연쇄적인 장애, 그리고 궁극적으로는 나쁜 사용자 경험으로 이어질 수 있습니다.&amp;#xD;
        스로틀링은 전통적인 CPU 사용률 지표로는 확인하기가 힘든 경우가 있습니다. 어떤 컨테이너가 CPU 사용률 50%(자신의 limit 값)를 보이고 있더라도, 실제로는 심각하게 스로틀링되어 매우 느린 상태일 수 있습니다. 이 경우 CPU는 전통적인 의미에서 &apos;최대치로 사용&apos;되는 것이 아니라, 인위적으로 억제되고 있는 상태입니다.&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### 3. &apos;시끄러운 이웃&apos; 오해&amp;#xD;
        limits가 ‘시끄러운 이웃(noisy neighbor)’ 문제, 즉 하나의 애플리케이션이 리소스를 독점하여 다른 애플리케이션에 영향을 주는 것을 막아 줄 수 있을 것처럼 보입니다. 하지만 실제로는 requests에 의해 결정되는 cpu.shares가 경합 상황에서도 적절하게 리소스를 배분할 수 있는 시스템이라고 할 수 있습니다. 만약 모든 Pod에 적절한 requests가 설정되어 있다면, CFS 스케줄러는 shares 값에 비례하여 CPU 시간을 분배함으로써 특정 Pod가 다른 Pod들이 기아 상태에 빠지는 것을 방지합니다. 반면 limits는 다른 누구도 CPU를 필요로 하지 않는 상황에서조차 Pod의 사용량을 제한하는 무딘 도구에 가깝습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 스로틀링 테스트&amp;#xD;
        테스트를 위해 한 웹 애플리케이션을 만들었습니다. 이 애플리케이션에 1 CPU를 할당하면 작업을 완료하는 데 대략 12초 정도 걸립니다.&amp;#xD;
        이 애플리케이션에 각각 1 CPU, 0.1 CPU를 할당하여 CPU 사용량과 작업 시간을 확인하여 지금까지 확인한 현상이 발생하는지 테스트해 보았습니다.&amp;#xD;
        &amp;#xD;
        ### 테스트용 Pod 준비&amp;#xD;
        아래와 같이 CPU limit만 다르게 설정하여 Pod를 생성하였습니다.&amp;#xD;
        ```&amp;#xD;
        apiVersion: apps/v1&amp;#xD;
        kind: Deployment&amp;#xD;
        metadata:&amp;#xD;
        name: cpu-test-app&amp;#xD;
        labels:&amp;#xD;
        app: cpu-test-app&amp;#xD;
        spec:&amp;#xD;
        replicas: 1&amp;#xD;
        selector:&amp;#xD;
        matchLabels:&amp;#xD;
        app: cpu-test-app&amp;#xD;
        template:&amp;#xD;
        metadata:&amp;#xD;
        labels:&amp;#xD;
        app: cpu-test-app&amp;#xD;
        spec:&amp;#xD;
        containers:&amp;#xD;
        - name: cpu-test-app&amp;#xD;
        image: cpu-test-app&amp;#xD;
        ports:&amp;#xD;
        - containerPort: 8081&amp;#xD;
        resources:&amp;#xD;
        requests:&amp;#xD;
        memory: &quot;64Mi&quot;&amp;#xD;
        cpu: &quot;100m&quot;    # 0.1 코어&amp;#xD;
        limits:&amp;#xD;
        memory: &quot;128Mi&quot;&amp;#xD;
        cpu: &quot;100m&quot;    # 0.1 코어로 제한&amp;#xD;
        livenessProbe:&amp;#xD;
        httpGet:&amp;#xD;
        path: /health&amp;#xD;
        port: 8080&amp;#xD;
        initialDelaySeconds: 10&amp;#xD;
        periodSeconds: 30&amp;#xD;
        readinessProbe:&amp;#xD;
        httpGet:&amp;#xD;
        path: /health&amp;#xD;
        port: 8080&amp;#xD;
        initialDelaySeconds: 5&amp;#xD;
        periodSeconds: 10&amp;#xD;
        ---&amp;#xD;
        # 1 코어 제한 버전&amp;#xD;
        apiVersion: apps/v1&amp;#xD;
        kind: Deployment&amp;#xD;
        metadata:&amp;#xD;
        name: cpu-test-app-1core&amp;#xD;
        labels:&amp;#xD;
        app: cpu-test-app-1core&amp;#xD;
        spec:&amp;#xD;
        replicas: 1&amp;#xD;
        selector:&amp;#xD;
        matchLabels:&amp;#xD;
        app: cpu-test-app-1core&amp;#xD;
        template:&amp;#xD;
        metadata:&amp;#xD;
        labels:&amp;#xD;
        app: cpu-test-app-1core&amp;#xD;
        spec:&amp;#xD;
        containers:&amp;#xD;
        - name: cpu-test-app&amp;#xD;
        image: cpu-test-app&amp;#xD;
        ports:&amp;#xD;
        - containerPort: 8080&amp;#xD;
        resources:&amp;#xD;
        requests:&amp;#xD;
        memory: &quot;64Mi&quot;&amp;#xD;
        cpu: &quot;1000m&quot;   # 1 코어&amp;#xD;
        limits:&amp;#xD;
        memory: &quot;128Mi&quot;&amp;#xD;
        cpu: &quot;1000m&quot;   # 1 코어로 제한&amp;#xD;
        livenessProbe:&amp;#xD;
        httpGet:&amp;#xD;
        path: /health&amp;#xD;
        port: 8080&amp;#xD;
        initialDelaySeconds: 10&amp;#xD;
        periodSeconds: 30&amp;#xD;
        readinessProbe:&amp;#xD;
        httpGet:&amp;#xD;
        path: /health&amp;#xD;
        port: 8080&amp;#xD;
        initialDelaySeconds: 5&amp;#xD;
        periodSeconds: 10&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### 응답 시간 비교&amp;#xD;
        0.1 CPU를 할당한 애플리케이션에서는 122초가 소요된 반면 1 CPU가 할당된 애플리케이션에서는 11.51초만 소요된 것을 확인할 수 있었습니다.&amp;#xD;
        ![K8s_CPU limit_3.png](https://image.toast.com/aaaadh/real/2025/techblog/K8sCPU%20limit3.png)&amp;#xD;
        &amp;#xD;
        ### 그라파나를 통한 스로틀링 현상 확인&amp;#xD;
        * &amp;lt;span style=&quot;background-color:#DCFFE4&quot;&amp;gt;초록색: 1 CPU&amp;lt;/span&amp;gt;&amp;#xD;
        * &amp;lt;span style=&quot;background-color:#fff5b1&quot;&amp;gt;노란색: 0.1 CPU&amp;lt;/span&amp;gt;&amp;#xD;
        &amp;#xD;
        CPU 사용률에서 노란색 그래프가 CPU limits가 0.1로 설정되어 그 이상 사용하지 못하는 모습을 보여줍니다. CPU 할당 횟수에서 노란색이 Limits에 걸려서 더 많은 cpu 할당이 발생한 것을 확인할 수 있습니다. 이것은 스로틀링으로 인한 지연 시간 증가 시뮬레이션과 일치하는 결과입니다. 당연하게도 스로틀링 비율도 노란색이 더 높은 것을 알 수 있습니다.&amp;#xD;
        ![K8s_CPU limit_4.png](https://image.toast.com/aaaadh/real/2025/techblog/K8sCPU%20limit4.png)&amp;#xD;
        중요한 지표는 스로틀링 비율로서 비율이 5%를 지속적으로 넘는 현상이 발생한다면 스로틀링이 빈번하게 발생하여 성능에 제약을 받아 서비스가 느려져 있을 가능성이 있으므로 조치를 취해야 합니다.&amp;#xD;
        &amp;#xD;
        #### 스로틀링 비율 지표 예시&amp;#xD;
        ```&amp;#xD;
        sum by (namespace, pod) (rate(container_cpu_cfs_throttled_periods_total{container!=&quot;&quot;}[5m]))&amp;#xD;
        /&amp;#xD;
        sum by (namespace, pod) (rate(container_cpu_cfs_periods_total{container!=&quot;&quot;}[5m]))&amp;#xD;
        ```&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 나가며&amp;#xD;
        결론은 아래와 같습니다.&amp;#xD;
        * 일반적인 경우,&amp;#xD;
        * request만 설정하여 자원이 더 필요할 때는 노드의 가용 자원을 사용하고 자원이 모자랄 때는 비율로 할당 받도록 하는 것이 좋은 것 같습니다. (QoS는 Burstable로 설정됨)&amp;#xD;
        * QoS가 Burstable로 설정되지만 CPU는 Eviction의 조건이 아니기 때문에 throttling이 발생할지언정 pod가 eviction 되지는 않습니다.&amp;#xD;
        * limit만 설정하는 경우는 피해야 합니다. limit만 설정할 경우 request가 0이 되어 자원 가용량이 없는 노드에도 스케줄링 될 수 있으며 런타임 시 예측이 어려워집니다.&amp;#xD;
        * CPU request는 Pod가 노드에 할당될 때는 절댓값으로 사용되고 할당된 뒤에는 상대적인 비율로 사용됩니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        긴 글을 읽어 주셔서 감사합니다. &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### 참고 문헌&amp;#xD;
        • Kubernetes, Resource Management for Pods and Containers, https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/&amp;#xD;
        • KubeBlog, Understanding CPU Requests and Limits, 2023. 10. 5., https://www.kubeblog.com/basics/understanding-cpu-requests-and-limits/&amp;#xD;
        • Itiel Shwartz, Kubernetes CPU Limits: What’s the Right Way to Assign CPU Resources?, 2025. 1. 14., https://komodor.com/learn/kubernetes-cpu-limits-throttling/&amp;#xD;
        • Eliran Cohen, For the love of god, learn when to use CPU limits on Kubernetes., 2023. 3. 5., https://medium.com/@eliran89c/for-the-love-of-god-learn-when-to-use-cpu-limits-on-kubernetes-2225341e9dbd&amp;#xD;
        • Max Levin, Kubernetes CPU Throttling: What it is, and Best Practices, 2024. 6. 6., https://www.groundcover.com/blog/kubernetes-cpu-throttling&amp;#xD;
        • CODE FARM, Linux CGroups and Containers, 2024. 2. 3., https://blog.codefarm.me/2021/11/23/linux-cgroups-containers/&amp;#xD;
        • Andreas Karis Blog, Quick guide for cgroups, 2020. 9. 3., https://andreaskaris.github.io/blog/linux/cgroups/&amp;#xD;
        • Martin, Cgroups - Deep Dive into Resource Management in Kubernetes, 2023. 2. 20., https://martinheinz.dev/blog/91&amp;#xD;
        • Jianhao, Kubernetes CPU requests and limits, 2021. 11. 11., https://jaanhio.me/blog/kubernetes-cpu-requests-limits/&amp;#xD;
        • Red Hat, Resource Management Guide &amp;gt; CPU, https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu&amp;#xD;
        • Real-time Ubuntu, Linux kernel schedulers, https://documentation.ubuntu.com/real-time/latest/explanation/schedulers/&amp;#xD;
        • Rifewang, Kubernetes: CPU Configuration, Linux CFS, and Performance Issues with Programming Languages, 2024. 12. 11., https://medium.com/@rifewang/kubernetes-cpu-configuration-linux-cfs-and-performance-issues-with-programming-languages-ccef783ed22e&amp;#xD;
        • JettyCloud, Making Sense of Kubernetes CPU Requests And Limits, 2023. 3. 20., https://medium.com/@jettycloud/making-sense-of-kubernetes-cpu-requests-and-limits-390bbb5b7c92&amp;#xD;
        • Tania Duggal, PerfectScale, Kubernetes CPU Limit: Best Practices for Optimal Performance, 2024. 10. 24., https://www.perfectscale.io/blog/kubernetes-cpu-limit-best-practises&amp;#xD;
        • Shane Corbett, AWS Blogs, Using Prometheus to Avoid Disasters with Kubernetes CPU Limits, 2022. 9. 21., https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_blue_202509.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfooterblue202509.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>뱅크샐러드에서 합법적으로 Vibe Coding 하는 법</title>
      <link>https://blog.banksalad.com/tech/banksalad-vibe-coding/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-vibe-coding/</guid>
      <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>모바일 앱 난독화(Mobile App Obfuscation)란?</title>
      <link>https://meetup.nhncloud.com/posts/394</link>
      <guid>https://meetup.nhncloud.com/posts/394</guid>
      <pubDate>Sun, 20 Jul 2025 23:13:22 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner_Obfuscation_202507-01.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerObfuscation20250701.png)](https://www.nhncloud.com/kr)&amp;#xD;
        ## 들어가며: 모바일 앱 난독화란?&amp;#xD;
        &amp;#xD;
        모바일 애플리케이션(이하 ‘앱’)은 사용자 데이터와 기업 지식 재산권을 보호하기 위한 강력한 보안 조치가 반드시 필요합니다. 특히, 인터넷을 통해 앱이 전 세계로 배포되면서 해커들의 공격 대상이 되기 쉬워졌습니다. 이러한 상황에서 **난독화(Obfuscation)**는 소스 코드나 바이너리 파일을 복잡하게 변환하여 역공학(逆工學, Reverse Engineering) 및 해킹 시도로부터 앱을 보호하는 핵심 기술로 자리 잡았습니다. 이 글에서는 모바일 환경에서 난독화의 중요성과 다양한 난독화 솔루션들이 제공하는 기능들을 살펴보겠습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 모바일 앱 난독화의 필요성&amp;#xD;
        &amp;#xD;
        **역공학**, 즉 **리버스 엔지니어링**은 모바일 앱 보안에 큰 위협이 됩니다. 해커들은 앱 코드를 분석해 작동 원리를 파악하고, 이를 통해 취약점을 찾아내거나 코드를 무단으로 변경할 수 있습니다. 악의적인 공격자들은 리버스 엔지니어링을 이용하여 앱의 핵심 로직을 탈취하거나, 유료 기능을 무료로 사용하도록 조작하며, 심지어 악성코드를 삽입해 사용자에게 피해를 줄 수도 있습니다.&amp;#xD;
        &amp;#xD;
        이러한 위협에 대응하기 위한 방법 중 하나가 바로 **난독화**입니다. 난독화는 앱의 코드를 읽기 어렵게 만들어 리버스 엔지니어링을 방해하는 기술로, 코드를 마치 암호처럼 복잡하게 변환하여 해커가 분석하기 어렵게 만듭니다. 이로 인해 공격자는 시간과 노력을 과도하게 소모하게 되어, 공격 시도를 크게 저지할 수 있습니다.&amp;#xD;
        &amp;#xD;
        난독화는 안티 디버깅, 안티 템퍼링, 무결성 검증과 같은 다른 보안 기술과 병행 사용될 때 그 효과가 더욱 극대화됩니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 난독화 기술의 종류&amp;#xD;
        &amp;#xD;
        「Parvez Faruki 외, (2026). Android Code Protection via Obfuscation Techniques: Past, Present and Future Directions」에 따르면 난독화 기술은 총 네 가지 카테고리로 분류됩니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; 1. Layout Obfuscation&amp;#xD;
        &amp;gt; 2. Control Obfuscation&amp;#xD;
        &amp;gt; 3. Preventive Obfuscation&amp;#xD;
        &amp;gt; 4. Data Obfuscation&amp;#xD;
        &amp;#xD;
        여기서는 눈으로 쉽게 확인할 수 있는 **Layout Obfuscation**과 **Control Obfuscation**에 대해 자세히 알아보겠습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### **Layout Obfuscation**&amp;#xD;
        &amp;#xD;
        Layout Obfuscation의 대표적인 방식은 **식별자 난독화**입니다. 이는 소스 코드 내 변수, 함수, 클래스 등의 이름을 의미 없는 문자나 숫자로 변경하는 방식입니다. Android 앱에서는 주로 Java나 Kotlin 코드가, iOS 앱에서는 Objective-C 나 Swift 코드가 난독화 대상이 됩니다. 이 기술을 통해 해커가 코드에서 의미 있는 단어를 찾거나 앱의 기능을 파악하기 어렵게 만듭니다.&amp;#xD;
        &amp;#xD;
        #### **장점**&amp;#xD;
        &amp;#xD;
        * 코드 가독성이 크게 낮아져 역공학 도구가 코드의 구조나 역할을 파악하기 어렵습니다.&amp;#xD;
        * 단순한 변환만으로도 상당한 보안 효과를 얻을 수 있어 초기 보안 강화 수단으로 효과적입니다.&amp;#xD;
        &amp;#xD;
        #### **단점**&amp;#xD;
        &amp;#xD;
        * 지나치게 난독화된 코드는 디버깅이나 유지보수가 어려워질 수 있습니다.&amp;#xD;
        * 코드 최적화와 충돌할 가능성이 있어 성능에 미치는 영향을 고려해야 합니다.&amp;#xD;
        &amp;#xD;
        **원본 코드**&amp;#xD;
        &amp;#xD;
        ```java&amp;#xD;
        public class UserData {&amp;#xD;
        private String userName;&amp;#xD;
        private String userPassword;&amp;#xD;
        &amp;#xD;
        public String getUserName() {&amp;#xD;
        return userName;&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        public void setUserPassword(String password) {&amp;#xD;
        this.userPassword = password;&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **난독화된 코드**&amp;#xD;
        &amp;#xD;
        ```java&amp;#xD;
        public class a {&amp;#xD;
        private String b;&amp;#xD;
        private String c;&amp;#xD;
        &amp;#xD;
        public String a() {&amp;#xD;
        return b;&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        public void b(String d) {&amp;#xD;
        this.c = d;&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### **Control Obfuscation**&amp;#xD;
        &amp;#xD;
        Control Obfuscation은 코드의 실행 경로와 제어 흐름을 복잡하게 변경하는 기술입니다. 조건문, 반복문, 분기문의 순서를 재구성하거나 불필요한 코드 조각을 삽입하여 실제 로직과 혼동을 유발합니다. 이러한 변환은 역공학 도구가 실제 실행 경로를 파악하는 데 많은 시간과 노력을 필요로 하게 만듭니다.&amp;#xD;
        &amp;#xD;
        #### **예시**&amp;#xD;
        &amp;#xD;
        * 실제 로직과 무관한 조건문과 반복문을 추가해 제어 흐름을 복잡하게 만듭니다.&amp;#xD;
        * 함수 호출 순서를 임의로 변경하여 코드 흐름을 추적하기 어렵게 합니다.&amp;#xD;
        &amp;#xD;
        #### **효과**&amp;#xD;
        &amp;#xD;
        * 공격자가 앱 내부의 실제 동작을 분석하는 데 소요되는 시간이 크게 증가합니다.&amp;#xD;
        * 역공학 도구를 이용한 자동 분석의 정확도를 낮춰 보안성을 극대화합니다.&amp;#xD;
        &amp;#xD;
        대표적으로 Flattening 기법을 통해 이를 확인할 수 있습니다.&amp;#xD;
        &amp;#xD;
        **원본 코드**&amp;#xD;
        &amp;#xD;
        ```java&amp;#xD;
        public class Calculator {&amp;#xD;
        public int calculate(int a, int b) {&amp;#xD;
        int result = a + b;&amp;#xD;
        return result * 2;&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        **난독화된 코드**&amp;#xD;
        &amp;#xD;
        ```java&amp;#xD;
        public class Calculator {&amp;#xD;
        public int calculate(int a, int b) {&amp;#xD;
        int result = 0;&amp;#xD;
        int state = 0;&amp;#xD;
        int opaque = (a * b) % 2 + 1; // 항상 1 또는 2&amp;#xD;
        &amp;#xD;
        while (opaque &amp;gt; 0) {&amp;#xD;
        switch (state) {&amp;#xD;
        case 0:&amp;#xD;
        result = a + b;&amp;#xD;
        state = opaque;     // 조건에 따라 1 또는 2로 전환&amp;#xD;
        break;&amp;#xD;
        case 1:&amp;#xD;
        result = result * 2; // 곱셈 수행&amp;#xD;
        state = 3;          // 종료를 위해 3 부여&amp;#xD;
        break;&amp;#xD;
        case 2: // 의미 없는 연산(dead code)&amp;#xD;
        result = b - a;&amp;#xD;
        state = 1;&amp;#xD;
        break;&amp;#xD;
        case 3: // 종료&amp;#xD;
        opaque = -1;        // 루프 종료(실제로 불필요)&amp;#xD;
        break;&amp;#xD;
        }&amp;#xD;
        if (opaque &amp;lt; 0) break; // 더미 코드 및 탈출 조건&amp;#xD;
        }&amp;#xD;
        return result;&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 시장에 존재하는 모바일 앱 난독화 솔루션&amp;#xD;
        &amp;#xD;
        모바일 난독화 솔루션은 무료와 상용 제품으로 구분됩니다. 각 솔루션은 기능, 사용 편의성, 그리고 업데이트 지원 측면에서 차이가 있으며, 개발자의 필요에 따라 선택할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### **무료 난독화 솔루션**&amp;#xD;
        &amp;#xD;
        #### **Proguard**&amp;#xD;
        &amp;#xD;
        Proguard는 Android 개발 환경에서 가장 널리 사용되는 오픈 소스 난독화 도구입니다. Java 기반의 코드를 대상으로 식별자 난독화와 코드 최적화, 그리고 불필요한 코드 제거 기능을 제공합니다. 이 도구는 Android Studio와 통합되어 있어 사용이 매우 간편한 점이 큰 장점입니다.&amp;#xD;
        &amp;#xD;
        * 주요 기능&amp;#xD;
        * 식별자 난독화 및 코드 압축&amp;#xD;
        * 불필요한 코드 제거를 통한 앱 크기 감소&amp;#xD;
        * 자동화된 설정과 빌드 프로세스와의 원활한 통합&amp;#xD;
        &amp;#xD;
        Control Obfuscation의 기능은 제공되지 않습니다.&amp;#xD;
        &amp;#xD;
        #### **Black Obfuscation**&amp;#xD;
        &amp;#xD;
        OpenSource로 제공되고 있는 난독화 솔루션 중 하나입니다.&amp;#xD;
        &amp;#xD;
        * 주요 기능&amp;#xD;
        * Control flow flattening&amp;#xD;
        * If 분기문 흐름 변경&amp;#xD;
        * 복잡한 명령어로 치환&amp;#xD;
        &amp;#xD;
        식별자 난독화와 같은 Layout Obfuscation을 제공하지 않습니다.&amp;#xD;
        &amp;#xD;
        #### **그 밖의 OpenSource**&amp;#xD;
        &amp;#xD;
        * ObfusApk&amp;#xD;
        * [https://github.com/ClaudiuGeorgiu/Obfuscapk](https://github.com/ClaudiuGeorgiu/Obfuscapk)&amp;#xD;
        * dprotect&amp;#xD;
        * [https://obfuscator.re/dprotect/](https://obfuscator.re/dprotect/)&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### **상용 난독화 솔루션**&amp;#xD;
        &amp;#xD;
        #### **D사 솔루션**&amp;#xD;
        &amp;#xD;
        난독화 기능에 더해, 문자열 암호화, 코드 삽입, 디버깅 방지 등 다양한 보안 기능을 제공합니다.&amp;#xD;
        &amp;#xD;
        #### **Q사 솔루션**&amp;#xD;
        &amp;#xD;
        코드 난독화, 변조 방지, 런타임 보호 등 다양한 보안 기능을 제공하는 솔루션입니다.&amp;#xD;
        &amp;#xD;
        #### **NHN AppGuard 난독화**&amp;#xD;
        &amp;#xD;
        NHN Cloud의 NHN AppGuard는 난독화(Layout 및 Control Obfuscation)뿐만 아니라, 변조 방지, 런타임 보호, 안티 디버깅, 안티 템퍼링 등 다층적인 보안 기능을 제공하는 통합 모바일 보안 솔루션입니다. NHN AppGuard의 난독화는 보안 모듈과의 상호작용을 통해 Control Flow를 변동시켜 더욱 강력한 난독화 기능을 제공합니다.&amp;#xD;
        &amp;#xD;
        &amp;gt;  [NHN AppGuard 더 알아보기](https://www.nhncloud.com/kr/service/security/nhn-appguard)&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 난독화 솔루션 선택 기준&amp;#xD;
        &amp;#xD;
        * **난독화 강도**: 솔루션이 제공하는 난독화 기술의 종류와 강도를 확인합니다.&amp;#xD;
        * **지원 기능**: 디버깅 방지, 변조 방지, 런타임 보호 등 추가적인 보안 기능을 지원하는지 확인합니다.&amp;#xD;
        * **성능 영향**: 난독화로 인해 앱 성능에 미치는 영향을 고려합니다.&amp;#xD;
        * **사용 편의성**: 솔루션의 사용 방법이 얼마나 쉽고 편리한지 확인합니다.&amp;#xD;
        * **기술 지원**: 솔루션 제공 업체의 기술 지원 수준을 확인합니다.&amp;#xD;
        * **가격**: 솔루션의 가격과 라이선스 정책을 확인합니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 나가며&amp;#xD;
        &amp;#xD;
        오늘날 강력한 모바일 앱 보안은 선택 사항이 아닌 필수 요건입니다. 특히 리버스 엔지니어링은 앱의 지식 재산권을 침해하고 사용자 데이터를 위협하는 심각한 공격 방식으로 부상했습니다. 이러한 위협에 효과적으로 대응하기 위해서는 전문적인 난독화 솔루션 도입이 중요하지만, 난독화만으로는 완벽한 보안을 보장할 수 없습니다.&amp;#xD;
        &amp;#xD;
        진정한 앱 보안은 난독화뿐만 아니라 다양한 보안 기술들이 유기적으로 결합될 때 실현됩니다. NHN AppGuard는 이러한 다층적인 보안 기능을 통합적으로 제공하여 난독화의 한계를 극복하고, 더욱 강력하고 견고한 앱 보호 체계를 구축합니다.&amp;#xD;
        &amp;#xD;
        NHN AppGuard와 함께 귀사의 소중한 앱을 안전하게 보호하고, 비즈니스 성공을 위한 든든한 토대를 마련하시기 바랍니다.&amp;#xD;
        긴 글을 읽어 주셔서 감사합니다. &amp;#xD;
        &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_202507-01.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfooter20250701%281%29.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>접근성을 지원한다는 착각</title>
      <link>https://blog.banksalad.com/tech/the-illusion-of-supporting-accessibility/</link>
      <guid>https://blog.banksalad.com/tech/the-illusion-of-supporting-accessibility/</guid>
      <pubDate>Wed, 16 Jul 2025 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>iOS 앱 개발자가 알아야 할 iOS 탈옥의 위험성</title>
      <link>https://meetup.nhncloud.com/posts/393</link>
      <guid>https://meetup.nhncloud.com/posts/393</guid>
      <pubDate>Mon, 07 Jul 2025 00:50:53 GMT</pubDate>
      <content:encoded>
        [![NHN Cloud_meetup banner_Jailbreak_202507-01-01.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerJailbreak2025070101.png)](https://www.nhncloud.com/kr)&amp;#xD;
        &amp;#xD;
        ## 1. 들어가며: iOS 탈옥이란?&amp;#xD;
        Apple의 iOS는 보안과 안정성을 최우선으로 하는 폐쇄적인 운영체제입니다. 기본적으로 사용자는 앱 스토어에서 승인된 앱만 설치할 수 있으며, 시스템 파일에 접근하는 것도 제한됩니다. 이러한 제한을 해제하고 사용자가 더 자유롭게 장치를 활용할 수 있도록 하는 과정이 바로 **탈옥(Jailbreak)**입니다.&amp;#xD;
        &amp;#xD;
        탈옥을 하면 사용자는 서명되지 않은 앱을 장치에 설치할 수 있고, 시스템 파일을 수정할 수도 있습니다. 그러나 탈옥은 단순한 개인화 도구가 아니라 보안 측면에서 큰 위험을 동반하는 행위이기도 합니다. 이번 글에서는 iOS 탈옥의 개념과 역사, 최신 탈옥 방법, 보안 위협, 그리고 개발자가 탈옥을 탐지하고 방어하는 방법까지 살펴보겠습니다. &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 2. iOS 탈옥을 통한 앱 해킹 사례&amp;#xD;
        과거에는 탈옥이 복잡한 과정이었지만, 최근에는 몇 번의 터치만으로도 쉽게 iOS를 탈옥할 수 있는 도구들이 등장했습니다. [palera1n](https://palera.in/), unc0ver 등이 대표적인 예입니다. 특히, 일부 웹사이트에서는 별도의 PC 없이도 iPhone에서 직접 탈옥할 수 있는 도구를 제공하며, 이를 이용해 많은 일반 사용자들도 손쉽게 탈옥을 시도할 수 있습니다. **탈옥한 iPhone에서는 트윅(Tweak)이나 모드 앱(Modified App)을 아무런 제약 없이 자유롭게 설치할 수 있습니다.**&amp;#xD;
        &amp;#xD;
        ![Jailbreak_01.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak01.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: https://palera.in/) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        ### 게임 앱의 피해 사례&amp;#xD;
        ![Jailbreak_02.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak02.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;NHN Cloud On 웨비나 16, &apos;사례로 알아보는 NHN AppGuard 실전 A to Z: 모바일 앱 해킹 유형별 대응 방법 공개!&apos; &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        이렇게 탈옥한 iOS 환경에서 최근 피해가 지속적으로 발생하는 분야는 바로 모바일 게임입니다. 과거에는 특정 사용자의 단독적인 해킹으로 피해가 크지 않았지만, 최근에는 조직적이고 상업적으로 특정 게임을 타깃으로 해커들이 게임 앱을 해킹하고 변조하여 이를 배포하는 사례가 많아지고 있습니다. 대표적으로 iOS 모드 앱 배포 사이트인 [iOSGods.com](iOSGods.com)과 같은 사이트들이 있습니다.&amp;#xD;
        &amp;#xD;
        ![Jailbreak_03.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak03.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: iOSGods.com) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        하지만 공개된 사이트에서 배포되던 것과 달리 최근에는 텔레그램 등의 단체 대화방에서 은밀하게 거래가 이루어지기도 합니다. 때문에 개발사에서는 일일이 iOS 모드 앱 대응이 쉽지 않습니다.&amp;#xD;
        &amp;#xD;
        이렇게 손쉽게 모드 앱을 설치하여 일부 유저가 게임 내에서 어뷰징을 하면 그 수가 적을지라도 게임사에게는 치명적인 피해를 입히게 됩니다. 게이머들이 게임을 하는 목적은 경쟁에서 승리하는 것인 경우가 많습니다. 하지만 어뷰징을 통해 노력 없이 한 번에 성과를 획득하는 일이 생기고 일부 유료 콘텐츠를 정당한 지불 없이 이용하기도 합니다. 이렇게 되면 유저들은 정당하게 엄청난 시간과 노력, 그리고 돈을 들여 게임을 할 가치를 못 느끼게 되고, 결국 게임 내 결제를 중단하거나 게임에서 이탈하게 되며, 이는 게임사의 매출 하락으로 이어집니다. 게임의 매출은 1~3%의 소수 유저들이 만들어 내기 때문입니다. 실제로 저희 NHN AppGuard의 한 조사에 따르면 대형 게임 해킹 사건 이후 매출이 하락했다는 결과가 있습니다.&amp;#xD;
        &amp;#xD;
        ![Jailbreak_04.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak04.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;NHN Cloud On 웨비나 16, &apos;사례로 알아보는 NHN AppGuard 실전 A to Z: 모바일 앱 해킹 유형별 대응 방법 공개!&apos; &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        결과적으로 게임사의 매출과 앱 서비스에 대한 고객의 신뢰가 하락하게 되는 것입니다.&amp;#xD;
        자세한 내용은 [NHN Cloud On 웨비나 16｜사례로 알아보는 NHN AppGuard 실전 A to Z | 모바일 앱 해킹 유형별 대응 방법 공개!](https://youtu.be/zGg1I7YePfE?si=eDcnKRoHErt0UzxC) 영상에서 확인하실 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 3. iOS 탈옥의 역사와 동작 원리&amp;#xD;
        ### JailbreakMe (iOS 1.x ~ 4.x)&amp;#xD;
        ![Jailbreak_05.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak05.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: https://en.wikipedia.org/wiki/JailbreakMe) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        **JailbreakMe**는 Safari 웹 브라우저를 이용해 탈옥하는 방식으로, 사용자가 특정 웹사이트를 방문하는 것만으로도 탈옥이 가능했습니다. 이는 PDF 렌더링 엔진의 보안 취약점을 이용한 것으로, 사용자 입장에서 가장 간편한 탈옥 방법이었습니다. &amp;#xD;
        그러나 Apple이 보안 패치를 적용하면서 더 이상 사용이 불가능해졌습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### Redsn0w &amp;amp; Greenpois0n (iOS 4.x ~ 5.x)&amp;#xD;
        이 시기의 탈옥은 USB를 통해 기기를 컴퓨터에 연결한 뒤, 전용 탈옥 프로그램을 실행하는 방식으로 이루어졌습니다. **Redsn0w**와 **Greenpois0n**은 각각 다른 커널 취약점을 이용하여 탈옥을 수행하였으며, tethered(재부팅 시 탈옥이 해제됨) 및 untethered(영구 탈옥) 방식으로 나뉘었습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### Evasi0n (iOS 6.x)&amp;#xD;
        ![Jailbreak_06.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak06.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: https://evasi0n.com/) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        **Evasi0n**은 iOS 6에서 사용된 탈옥 도구로, USB 연결을 통해 시스템의 여러 취약점을 연쇄적으로 이용하여 완전한 탈옥을 가능하게 했습니다. 당시 매우 높은 안정성과 호환성을 제공하여 널리 사용되었습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### Pangu &amp;amp; TaiG (iOS 7.x ~ 9.x)&amp;#xD;
        **Pangu**와 **TaiG**는 중국 해커 그룹이 개발한 탈옥 도구로, iOS 7 이후 보안이 강화된 환경에서도 탈옥을 가능하게 했습니다. 특히, TaiG는 iOS 8에서 Apple이 취약점을 보완한 패치를 우회하는 새로운 기법을 도입하여 탈옥을 성공시켰습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### Yalu (iOS 10.x)&amp;#xD;
        **Yalu**는 iOS 10에서 사용된 탈옥 도구로, 개발자인 Luca Todesco가 만든 것으로 유명합니다. 그러나 이 탈옥은 semi-untethered 방식이어서 기기를 재부팅하면 탈옥이 해제되며, 다시 실행하려면 특정 앱을 실행해야 했습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### unc0ver &amp;amp; Electra (iOS 11.x ~ 14.x)&amp;#xD;
        **unc0ver**와 **Electra**는 최근까지 사용되는 탈옥 도구로, 커널 취약점을 이용하여 탈옥을 수행합니다. 특히 unc0ver는 안정성이 높아 많은 사용자들이 선호하였으며, 탈옥 후에도 시스템 기능을 최대한 유지할 수 있도록 설계되었습니다.&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ### checkra1n (iOS 12.x ~ 14.x)&amp;#xD;
        ![Jailbreak_07.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak07.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;(Source: https://checkra.in/) &amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        **checkra1n**은 기존의 탈옥과 달리, checkm8이라는 하드웨어 부트롬(bootrom) 취약점을 이용한 영구 탈옥 방식입니다. &amp;#xD;
        이로 인해 iOS 버전에 상관없이 특정 기기에서는 항상 탈옥이 가능하지만, 반대로 Apple이 소프트웨어 패치를 통해 막을 수 없는 특성을 가집니다. 다만, checkra1n은 semi-tethered 방식으로, 재부팅 후에는 다시 탈옥 프로세스를 수행해야 합니다.&amp;#xD;
        &amp;#xD;
        최근에는 Apple이 보안 강화를 지속하면서 탈옥이 점점 어려워지고 있으며, 대부분의 탈옥이 semi-tethered(재부팅 시 탈옥이 풀림) 형태로 제공되고 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 4. 탈옥한 iOS 장치의 보안 취약점&amp;#xD;
        탈옥한 iOS 장치는 Apple이 제공하는 기본 보안 메커니즘을 우회하기 때문에 여러 가지 보안 취약점에 노출됩니다. 다음은 탈옥한 iOS가 갖는 주요 취약점입니다.&amp;#xD;
        &amp;#xD;
        * **시스템 보안 우회**: 탈옥한 기기는 Apple의 기본 보안 메커니즘을 우회하여 시스템 파일을 수정할 수 있습니다. 이는 악성 코드나 바이러스가 시스템에 침투하기 쉽게 만듭니다.&amp;#xD;
        * **앱 간 데이터 접근**: 탈옥한 기기에서는 앱 간의 데이터 접근이 가능해져, 민감한 정보가 유출될 위험이 높아집니다.&amp;#xD;
        * **루트 권한 노출**: 탈옥한 기기는 루트 권한을 얻을 수 있어, 시스템 파일을 자유롭게 변경할 수 있습니다. 이는 해커가 기기를 완전히 제어할 수 있는 가능성을 열어줍니다.&amp;#xD;
        * **악성 코드 및 해킹 위험**: 탈옥한 기기에서는 악성 소프트웨어나 해킹 툴이 쉽게 설치될 수 있습니다. 이는 사용자의 개인 정보와 금융 정보를 위험에 빠뜨릴 수 있습니다.&amp;#xD;
        * **보안 업데이트 차단**: 탈옥한 기기는 Apple로부터 자동 업데이트를 받지 못하게 되어 중요한 보안 패치와 기능 업데이트를 놓칠 수 있습니다.&amp;#xD;
        &amp;#xD;
        특히, 탈옥한 장치에는 앱의 실행을 제한하는 보안 프로세스가 없기 때문에, 악성 소프트웨어가 시스템에 침투할 가능성이 높아집니다. 이러한 위험은 사용자의 개인 정보와 금융 정보를 보호하는 데 큰 위협이 될 수 있습니다. 예를 들어, 온라인 뱅킹 앱이나 결제 서비스 앱에 저장된 정보가 해커의 손에 넘어갈 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 5. iOS 탈옥이 배포하는 앱에 미치는 영향&amp;#xD;
        ### 5.1. 보안 취약점 증가&amp;#xD;
        * **악성 코드 및 해킹에 노출**&amp;#xD;
        탈옥한 장치는 운영체제의 보안 계층이 무너져 악성 코드나 해킹에 취약합니다. 이는 앱 내에서 수집되는 데이터나 사용자 정보가 유출될 위험을 증가시키며, 개발자의 신뢰도에도 부정적인 영향을 미칠 수 있습니다.&amp;#xD;
        &amp;#xD;
        * **앱 무결성 손상**&amp;#xD;
        탈옥한 환경에서는 앱의 코드가 변조될 수 있어 앱의 무결성이 손상될 수 있습니다. 이는 앱의 정상적인 동작을 방해하고, 예기치 않은 오류나 버그를 유발할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### 5.2. 불법 복제 및 수익 손실&amp;#xD;
        * **앱 불법 복제 증가**&amp;#xD;
        탈옥한 장치는 앱의 불법 복제나 유료 앱을 무료로 설치할 수 있는 환경을 제공합니다. 이러한 행위를 도와주는 모드 앱 또한 증가합니다. 이는 개발자의 직접적인 수익 손실로 이어지며, 앱 비즈니스 모델에 큰 타격을 줄 수 있습니다.&amp;#xD;
        &amp;#xD;
        * **인앱 결제 우회**&amp;#xD;
        탈옥을 통한 인앱 결제 우회는 트윅을 설치하거나 앱 변조를 통해 유료 기능을 무료로 사용하는 사례도 많습니다. 이는 개발자의 추가 수익 창출 기회를 감소시키고, 공정한 수익 구조를 해칠 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### 5.3. 사용자 경험 저하&amp;#xD;
        * **앱의 안정성 문제**&amp;#xD;
        탈옥한 장치에서는 시스템 안정성이 떨어져 앱의 크래시(crash)나 프리즈(freeze) 현상이 빈번하게 발생할 수 있습니다. 이는 사용자에게 부정적인 경험을 주며, 앱에 대한 만족도를 크게 저하시킬 수 있습니다.&amp;#xD;
        &amp;#xD;
        * **지원 불가능한 환경**&amp;#xD;
        탈옥 환경은 표준 iOS 환경이 아니기 때문에 개발자가 예상하지 못한 문제가 발생할 수 있습니다. 이러한 문제를 해결하기 위해 추가적인 자원을 투입하기 어려워, 사용자 지원에 한계를 겪을 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 6. iOS 탈옥 탐지 방법&amp;#xD;
        앱 개발자는 탈옥한 장치에서 앱을 보호하기 위해 기기의 상태를 탐지해야 합니다. 탈옥한 장치를 탐지하는 데 사용되는 다양한 기법들이 있으며, 각 기법은 고유한 특징과 장단점을 가지고 있습니다. 주요 탐지 방법은 다음과 같습니다.&amp;#xD;
        &amp;#xD;
        ### 6.1. 시스템 파일 검사&amp;#xD;
        탈옥한 장치는 시스템 파일에 변화를 일으키는 경우가 많습니다. 대표적인 예로 `/private/var/lib/cydia`나 `/bin/bash` 같은 파일들이 존재하는지 확인하여 탈옥 여부를 탐지할 수 있습니다. 이러한 파일들은 일반적으로 탈옥 도구를 설치할 때 생성됩니다.&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        func isJailbroken() -&amp;gt; Bool {&amp;#xD;
        // 확인할 시스템 파일 경로들&amp;#xD;
        let jailbreakPaths = [&amp;#xD;
        &quot;/private/var/lib/cydia&quot;,  // Cydia 파일&amp;#xD;
        &quot;/bin/bash&quot;,               // bash 파일&amp;#xD;
        &quot;/usr/sbin/sshd&quot;,          // SSH 서버 파일&amp;#xD;
        &quot;/etc/apt&quot;                 // APT 디렉터리&amp;#xD;
        ]&amp;#xD;
        &amp;#xD;
        // 각 파일/디렉터리 존재 여부 체크&amp;#xD;
        for path in jailbreakPaths {&amp;#xD;
        if FileManager.default.fileExists(atPath: path) {&amp;#xD;
        return true  // 파일이 존재하면 탈옥된 기기&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        return false  // 파일이 없으면 탈옥되지 않은 기기&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### 6.2. API 호출 감지&amp;#xD;
        탈옥한 장치에서 주로 사용되는 특정 API 호출을 감지하여 탈옥 여부를 판단할 수 있습니다. 예를 들어, `fork()`, `sysctl()` 등의 함수 호출은 탈옥과 관련된 행동을 나타낼 수 있습니다. 이러한 함수 호출을 모니터링하여 탈옥 여부를 탐지합니다.&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        func checkFork() -&amp;gt; Bool {&amp;#xD;
        let pid = fork()&amp;#xD;
        if pid == 0 {&amp;#xD;
        // 자식 프로세스 종료 (fork가 성공하면 탈옥 가능성이 높음)&amp;#xD;
        exit(0)&amp;#xD;
        } else if pid &amp;gt; 0 {&amp;#xD;
        print(&quot;fork() 호출 가능 - 탈옥된 기기일 수 있음&quot;)&amp;#xD;
        return true&amp;#xD;
        }&amp;#xD;
        return false&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### 6.3. 탈옥 도구 탐지&amp;#xD;
        탈옥한 장치에는 MobileSubstrate, Frida, Cydia 등과 같은 탈옥 도구들이 설치됩니다. 이러한 도구들이 설치되어 있는지 확인하여 탈옥 여부를 탐지할 수 있습니다. 예를 들어, Cydia 앱이 설치되어 있는지 확인하는 방법이 있습니다.&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        // 확인할 파일 및 디렉터리 목록&amp;#xD;
        let jailbreakFilePaths: [String] = [&amp;#xD;
        &quot;/Applications/Cydia.app&quot;,&amp;#xD;
        &quot;/private/var/lib/cydia/&quot;,&amp;#xD;
        &quot;/private/var/tmp/cydia.log&quot;&amp;#xD;
        ]&amp;#xD;
        &amp;#xD;
        // 파일 존재 여부 확인 함수&amp;#xD;
        func checkJailbreakFiles() -&amp;gt; Bool {&amp;#xD;
        for path in jailbreakFilePaths {&amp;#xD;
        if FileManager.default.fileExists(atPath: path) {&amp;#xD;
        print(&quot;탈옥 관련 파일 발견: \(path)&quot;)&amp;#xD;
        return true&amp;#xD;
        }&amp;#xD;
        }&amp;#xD;
        return false&amp;#xD;
        }&amp;#xD;
        &amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### 6.4. 비공식 앱 설치 탐지&amp;#xD;
        탈옥한 장치에는 Apple의 공식 앱 스토어 외에도 비공식 앱 스토어를 통해 앱을 설치할 수 있습니다. 따라서 비공식적으로 설치된 앱들을 탐지하여 탈옥 여부를 확인할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### 6.5. 루트 접근 권한 검사&amp;#xD;
        탈옥한 장치는 루트 권한을 얻을 수 있기 때문에, 루트 권한으로 실행된 프로세스나 파일들을 확인하는 방법이 있습니다. 시스템 내에서 루트 권한을 사용하는 프로세스나 파일 접근 시 이를 탐지할 수 있습니다.&amp;#xD;
        탈옥한 장치에서 발생할 수 있는 보안 위험을 완전히 없애는 것은 불가능할 수 있지만, 여러 방어 기법을 결합하여 다층적인 보안을 구축하는 것이 중요합니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 7. 나가며&amp;#xD;
        iOS 앱 개발자는 사용자 데이터를 보호하고, 앱의 안전한 환경을 제공하는 중요한 책임을 지고 있습니다. 탈옥한 장치에서 발생할 수 있는 보안 위협을 사전에 탐지하고 방어하는 것은 필수적인 작업입니다. 탈옥 탐지 및 방어 기능을 통해 앱의 보안을 강화하고, 사용자가 안전하게 앱을 사용할 수 있도록 보장해야 합니다.&amp;#xD;
        &amp;#xD;
        글로벌 IT기업 NHN이 직접 만든 **NHN AppGuard**는 탈옥 기기에서 발생하는 다양한 보안 위협에 대응할 수 있는 필수적인 보안 솔루션입니다. NHN AppGuard를 적용하여 배포하면 아래와 같이 탈옥 환경을 탐지하고 차단할 수 있도록 정책을 설정할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ![Jailbreak_08.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak08.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;탈옥 환경을 탐지하고 차단하기 위한 NHN AppGuard 정책 설정 화면&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        배포된 앱이 실행되는 중 탈옥이 탐지되면 곧바로 탐지에 대한 로그를 남기며 이를 추적할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ![Jailbreak_09.png](https://image.toast.com/aaaadh/real/2025/techblog/Jailbreak09.png)&amp;#xD;
        &amp;lt;center&amp;gt;&amp;lt;span style=&quot;font-size:80%&quot;&amp;gt;탈옥 탐지 로그 조회 화면&amp;lt;/span&amp;gt;&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        이렇게 NHN AppGuard는 앱 운영자가 기기 보안에 대한 걱정 없이 안정적인 서비스 제공에 집중할 수 있도록 도와줍니다. 강력한 탈옥 탐지와 차단 기능을 통해 앱의 무결성과 사용자 데이터를 완벽하게 보호할 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;gt;  [NHN AppGuard 더 알아보기](https://www.nhncloud.com/kr/service/security/nhn-appguard)&amp;#xD;
        &amp;#xD;
        지금까지 iOS 탈옥의 개념과 역사, 탈옥한 장치에서의 피해 사례, 최신 탈옥 방법, 보안 위협, 그리고 개발자가 탈옥을 탐지하고 방어하는 방법에 대해 알아보았습니다. iOS 앱 개발자 분들뿐만 아니라 iOS를 사용하거나 보안에 관심이 있는 모든 분들께 도움이 되었길 바랍니다. 긴 글을 읽어 주셔서 감사합니다. &amp;#xD;
        &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_202507-01.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfooter20250701.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>우리 팀에도 Jarvis 가 생겼다 – 생성형 AI 로 만든 에러 분석가 이야기</title>
      <link>http://thefarmersfront.github.io/blog/2025-delivery-jarvis-story/</link>
      <guid>http://thefarmersfront.github.io/blog/2025-delivery-jarvis-story/</guid>
      <pubDate>Tue, 01 Jul 2025 10:00:00 GMT</pubDate>
      <content:encoded>팀 내에서 생성형 AI 를 활용한 사례를 공유합니다.</content:encoded>
    </item>
    <item>
      <title>딜리버리 암호화 모듈 개발기</title>
      <link>http://thefarmersfront.github.io/blog/delivery-encryption-module/</link>
      <guid>http://thefarmersfront.github.io/blog/delivery-encryption-module/</guid>
      <pubDate>Mon, 02 Jun 2025 14:00:00 GMT</pubDate>
      <content:encoded>딜리버리에서 사용하고있는 암호화모듈을 개발한 경험을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>NHN Cloud 빠른 시작 가이드를 소개합니다</title>
      <link>https://meetup.nhncloud.com/posts/392</link>
      <guid>https://meetup.nhncloud.com/posts/392</guid>
      <pubDate>Wed, 28 May 2025 06:40:47 GMT</pubDate>
      <content:encoded>
        ![자산 2.png](https://image.toast.com/aaaadh/alpha/2025/techblog/uC790uC0B0%202.png)&amp;#xD;
        &amp;#xD;
        # 들어가며&amp;#xD;
        &amp;#xD;
        NHN Cloud 빠른 시작 가이드가 출시되었습니다. NHN Cloud는 현재까지 110개가 넘는 서비스를 제공 중인데요. 이렇게 다양하고 많은 서비스를 처음 사용하고자 할 때 어디서부터 시작해야 할지에 대한 과정은 설레면서도 막막함을 동반하는데요. NHN Cloud를 처음 접해보는 분들을 위해 최대한 빠르고 쉽게 NHN Cloud를 익힐 수 있도록 돕기 위한 빠른 시작 가이드를 소개합니다.&amp;#xD;
        &amp;#xD;
        # 빠른 시작 가이드란?&amp;#xD;
        &amp;#xD;
        빠른 시작 가이드는 사용자가 서비스를 신속하게 이해하고 바로 사용할 수 있도록 도와주는 간결한 문서입니다. 여기서 중요한 점은 **간결성**이데요. 빠른 시작 가이드는 간결하고 핵심적인 내용을 담고 있어 사용자가 서비스를 시작하는데 필요한 정보만 포함하고 불필요한 세부사항은 생략합니다.&amp;#xD;
        &amp;#xD;
        또한 **작업 중심적**으로 특정 작업을 빠르게 완료하기 위한 정보만 포함합니다. 따라서 NHN Cloud 빠른 시작 가이드도 총 12개의 학습 모듈에서 완수해야 할 작업을 중심으로 구성되어 있습니다. 각 학습 모듈에서 이전에 생성한 리소스를 재활용하여 복잡한 초기 설정 작업을 간소화해 주요 작업을 빠르게 진행할 수 있도록 설계하였습니다.&amp;#xD;
        &amp;#xD;
        빠른 시작 가이드와 튜토리얼이 같은 목적을 가진 문서로 보일 수 있지만 사실 두 문서는 서로 다른 목적을 가집니다. 두 문서 모두 어떤 기능을 사용하거나 작업을 완료하는 방법을 다루는 문서이지만 튜토리얼은 앞서 설명한 빠른 시작 가이드의 특징과 다르게 사용자가 더 넓고 복잡한 작업을 수행할 수 있도록 돕습니다. 따라서 타깃 대상도 처음 서비스를 사용하는 사용자가 아닌 심화 학습을 원하는 사용자이며 빠른 시작 가이드 대비 내용이 더 길고 복잡한 작업을 다룹니다.&amp;#xD;
        &amp;#xD;
        ## NHN Cloud 빠른 시작 가이드&amp;#xD;
        &amp;#xD;
        **NHN Cloud 빠른 시작 가이드**는 NHN Cloud를 처음 이용하는 사용자를 위한 단계별 온보딩 가이드입니다. 본 가이드를 통해 빠른 시간 내에 다양한 클라우드 리소스를 효율적으로 관리하고 설정할 수 있는 통합 관리 도구인 NHN Cloud 콘솔의 사용법을 익히고 클라우드 서비스를 제공할 수 있습니다.&amp;#xD;
        &amp;#xD;
        가이드는 아래와 같이 12개의 학습 모듈로 구성되어 있습니다. 각 학습 모듈은 쉽게 따라 할 수 있도록 실습 예제를 제공합니다. NHN Cloud 회원 가입 이후부터 기본 콘솔 사용법을 익히고 최종적으로 간단한 클라우드 아키텍처를 구성해 서비스를 제공할 수 있습니다.&amp;#xD;
        &amp;#xD;
        [NHN Cloud 빠른 시작 가이드 바로가기](https://docs.nhncloud.com/ko/quickstarts/ko/overview/)&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        &amp;#xD;
        ![NHN Portal 개선 빠른 시작 가이드@2x (1).png](https://image.toast.com/aaaadh/alpha/2025/techblog/NHN%20Portal%20uAC1CuC120%20uBE60uB978%20uC2DCuC791%20uAC00uC774uB4DC2x%201.png)&amp;#xD;
        &amp;#xD;
        ## 빠른 시작 가이드 구성&amp;#xD;
        &amp;#xD;
        NHN Cloud 빠른 시작 가이드의 학습 모듈은 크게 3가지 영역으로 구성됩니다.&amp;#xD;
        &amp;#xD;
        1. 학습 목표: 학습 모듈을 통해 배울 내용을 간략하게 보여줍니다.&amp;#xD;
        2. 시작하기 전에: 학습 모듈을 매끄럽게 수행하기 위해 필요한 준비 사항을 보여줍니다.&amp;#xD;
        3. 실습 과제: 학습 모듈에서 수행할 과제 내용을 다루며 여러 단계로 나누어 설명합니다.&amp;#xD;
        4. 참고 자료: 해당 학습 모듈과 관련된 더 깊이 있는 정보를 확인할 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        빠른 시작 가이드는 NHN Cloud 입문자가 쉽게 익힐 수 있게 작성된 문서입니다. 따라서 각 학습 모듈에서 동일한 조직, 프로젝트, 리전, 가용성 영역을 사용하여 복잡한 설정 작업을 최대한 간소화하였습니다.&amp;#xD;
        &amp;#xD;
        * 조직: MyORG&amp;#xD;
        * 프로젝트: MyPRJ&amp;#xD;
        * 리전: 평촌 Region&amp;#xD;
        * VPC: MyVPC&amp;#xD;
        * 서브넷: MySubnet&amp;#xD;
        &amp;#xD;
        ![0. 개요.png](https://image.toast.com/aaaadh/alpha/2025/techblog/0.%20uAC1CuC694.png)&amp;#xD;
        &amp;#xD;
        총 12개의 학습 모듈은 서로 연계되어 있습니다. 따라서 하나의 학습 모듈에서 생성한 리소스를 재사용하므로 마지막 학습 모듈을 마치기 전까지 리소스를 삭제하지 않는 것을 권장합니다. 학습 모듈의 실습 과제들을 차근차근 따라 해 보면서 생성한 리소스를 활용하여 `학습 모듈 10. 확장성과 성능 최적화`을 완수하면 아래와 같은 클라우드 아키텍처를 완성할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ![모듈 10. 확장성과 성능 최적화.png](https://image.toast.com/aaaadh/alpha/2025/techblog/uBAA8uB4C8%2010.%20uD655uC7A5uC131uACFC%20uC131uB2A5%20uCD5CuC801uD654.png)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        &amp;gt; 오토 스케일링, 로드 밸런싱 서비스를 사용해 안정적인 데이터 관리를 위한 효율적이고 유연하며 확장 가능한 시스템을 완성할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ## 더 알아보기&amp;#xD;
        &amp;#xD;
        NHN Cloud 빠른 시작 가이드를 통해 NHN Cloud와 친숙해 지셨다면 한 걸음 더 나아가 NHN Cloud 교육센터를 통해 더 깊이있고 심화된 내용을 학습하실 수 있습니다. NHN Cloud는 NHN Cloud 콘솔을 직접 사용하며 배울 수 있는 교육 과정을 제공하고 있는데요. 빠른 시작 가이드에서 다루는 내용 뿐 아니라 클라우드 및 컨테이너에 대한 기본 개념과 NHN Cloud의 컨테이너 서비스인 `NHN Kubernetes Service(NKS)`와 `NHN Container Registry(NCR)`에 대해 심도있게 학습할 수 있습니다. NHN Cloud 교육센터에서 제공하는 클라우드 기초 지식부터 심화 내용을 다루는 교육 과정을 통해 클라우드 역량을 향상하고 더 나아가 NHN Cloud 자격증을 이수할 수 있습니다.&amp;#xD;
        &amp;#xD;
        [NHN Cloud 교육센터 바로가기](https://www.nhncloud.com/kr/edu)&amp;#xD;
        &amp;#xD;
        ![footer.png](https://image.toast.com/aaaadh/alpha/2025/techblog/footer.png)
      </content:encoded>
    </item>
    <item>
      <title>OMS의 최적화된 마이크로서비스 아키텍처 디자인</title>
      <link>http://thefarmersfront.github.io/blog/oms-msa-architecture-1/</link>
      <guid>http://thefarmersfront.github.io/blog/oms-msa-architecture-1/</guid>
      <pubDate>Fri, 09 May 2025 00:00:00 GMT</pubDate>
      <content:encoded>판매몰과 컬리 풀필먼트의 게이트웨이의 역할로써 OMS의 아키텍처 디자인의 변화 과정을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>딜리버리 프로덕트 개발팀의 개발 문화 - 주니어 디버깅 스터디</title>
      <link>http://thefarmersfront.github.io/blog/2025-delivery-debug-study/</link>
      <guid>http://thefarmersfront.github.io/blog/2025-delivery-debug-study/</guid>
      <pubDate>Mon, 14 Apr 2025 14:00:00 GMT</pubDate>
      <content:encoded>주니어 개발자의 성장을 목표로 진행한 에러 디버깅 스터디 과정을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>함께 만들 더 큰 파도 : 뱅크샐러드 Pit Stop 제작기</title>
      <link>https://blog.banksalad.com/pnc/pit-stop-2024/</link>
      <guid>https://blog.banksalad.com/pnc/pit-stop-2024/</guid>
      <pubDate>Fri, 07 Mar 2025 00:00:00 GMT</pubDate>
      <content:encoded>우리는 때때로 잠시 멈춰 지난 여정을 돌아보고, 앞으로 나아갈 방향을 정리하는 시간을 가지기 마련입니다. 뱅크샐러드도 분기마다 지난 시간을 회고하고 새로운 시작을 준비하는 의미 있는 순간을 함께 하고 있는데요. 우리는 이 행사를 *핏스탑(Pit…</content:encoded>
    </item>
    <item>
      <title>Trino로 타임아웃 개선하기</title>
      <link>https://meetup.nhncloud.com/posts/391</link>
      <guid>https://meetup.nhncloud.com/posts/391</guid>
      <pubDate>Tue, 04 Mar 2025 02:22:40 GMT</pubDate>
      <content:encoded>
        ![NHN Cloud_meetup banner_trino_202502-01_900.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannertrino20250201900.png)&amp;#xD;
        &amp;#xD;
        # 들어가며&amp;#xD;
        안녕하세요. NHN Cloud의 클라우드AI팀 이태형입니다.&amp;#xD;
        로그 데이터가 쌓일수록 조회 속도가 느려지는 문제, 한 번쯤 겪어 보셨을 텐데요. 이 글에서는 이러한 문제를 해결하기 위해 저희 팀에서 Trino를 도입하여 성능을 개선한 과정을 공유해 보려 합니다. 재미있게 읽어 주세요! &amp;#xD;
        &amp;#xD;
        # 개요: NHN AppGuard&amp;#xD;
        [NHN AppGuard](https://www.nhncloud.com/kr/service/security/nhn-appguard) 서비스에 Trino를 적용한 이야기를 드릴 예정이라서 먼저 해당 서비스를 소개하겠습니다.&amp;#xD;
        &amp;#xD;
        NHN AppGuard는 모바일 애플리케이션을 보호하기 위해 사용자의 이상 행위를 탐지하거나 차단하는 모바일 앱 보안 솔루션입니다. NHN AppGuard의 서버는 탐지/차단 로그를 안전하게 저장하고, 각종 조건 검색과 대시보드를 제공합니다.&amp;#xD;
        &amp;#xD;
        ![Trino_1.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino1.png)&amp;#xD;
        &amp;#xD;
        ## NHN AppGuard 로그&amp;#xD;
        &amp;#xD;
        NHN AppGuard는 평균 600만개/일 가량의 로그를 수집하고 있습니다. 이러한 로그는 NHN AppGuard 로그 워크플로에 따라 DB에 적재됩니다.&amp;#xD;
        &amp;#xD;
        ![Trino_2_900.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino2900.png)&amp;#xD;
        &amp;#xD;
        ## 이슈 발생&amp;#xD;
        &amp;#xD;
        대부분의 쿼리가 월 단위 집계 성격을 띠는 이유로 질의 대상 row 가 1억 건이 넘는 경우가 많아 이슈가 발생했습니다.&amp;#xD;
        발생한 이슈는 아래와 같습니다.&amp;#xD;
        &amp;#xD;
        1. 검색 조건 변경 시 대시보드 화면에서 타임아웃 발생&amp;#xD;
        ![Trino_3.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino3.png)&amp;#xD;
        2. 집계 쿼리가 수행되는 새벽 시간대에 slow query 발생&amp;#xD;
        ![Trino_4.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino4.png)&amp;#xD;
        &amp;#xD;
        ## 일반적인 해결 방안&amp;#xD;
        &amp;#xD;
        위 이슈들은 결국 쿼리의 성능이 원인이기 때문에 먼저 쿼리 최적화를 수행했습니다.&amp;#xD;
        &amp;#xD;
        1. index 문제&amp;#xD;
        1. 쿼리 검수를 통해 index의 순서를 변경하고&amp;#xD;
        2. 의도한 index가 적용되도록 쿼리에 index hint를 추가했습니다.&amp;#xD;
        2. 쿼리의 문제&amp;#xD;
        1. 한 달 기간 전체 데이터를 스캔하는 쿼리를 당일 증가분만 조회하도록 수정하고&amp;#xD;
        2. 대시보드를 매번 조회하지 않고 일 배치 작업으로 미리 계산해 둔 데이터를 조회하고&amp;#xD;
        3. 조회 가능한 기간을 제한했습니다.&amp;#xD;
        &amp;#xD;
        이러한 최적화를 통해 일시적으로 이슈가 해소되었습니다.&amp;#xD;
        하지만 NHN AppGuard의 로그는 점차 늘어나고, 집계할 데이터의 종류도 증가했으며, 조회 기간 감소에 대한 불만이 발생하여 다른 접근이 필요했습니다.&amp;#xD;
        &amp;#xD;
        ## 로그 저장소 검토&amp;#xD;
        &amp;#xD;
        MySQL을 대신해 로그를 저장하기에 적절한 로그 저장소를 검토했습니다.&amp;#xD;
        &amp;#xD;
        1. Elasticsearch (LNCS)&amp;#xD;
        1. 검색에 좋은 성능&amp;#xD;
        2. 상품 스펙상 최대 120일 저장 제한&amp;#xD;
        2. Trino (DataQuery)&amp;#xD;
        1. 복잡한 집계 쿼리에 좋은 성능&amp;#xD;
        2. 여러 데이터 소스 간 federation 지원&amp;#xD;
        3. 저장 기간 제한 없음&amp;#xD;
        &amp;#xD;
        NHN AppGuard는 로그의 저장 기간을 기존 90일에서 늘리는 것을 계획하고 있었고, 무엇보다 대부분의 쿼리가 집계 성격을 많이 띠어 Trino가 적절하다고 판단했습니다.&amp;#xD;
        &amp;#xD;
        # Trino와 DataQuery&amp;#xD;
        &amp;#xD;
        ## Trino란&amp;#xD;
        &amp;#xD;
        [Trino 공식 홈페이지](https://trino.io)를 보면 아래와 같은 문구를 찾을 수 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; Trino, a query engine that runs at ludicrous speed&amp;#xD;
        &amp;gt; Fast distributed SQL query engine for big data analytics that helps you explore your data universe.&amp;#xD;
        &amp;#xD;
        키워드를 뽑아 보면 아래와 같습니다.&amp;#xD;
        &amp;#xD;
        1. Fast - 빠르다&amp;#xD;
        2. Distributed - 분산 처리한다&amp;#xD;
        3. analytics - 분석에 적절하다&amp;#xD;
        &amp;#xD;
        ## Trino 특징&amp;#xD;
        &amp;#xD;
        마찬가지로 [Trino 공식 홈페이지](https://trino.io)에서는 아래와 같은 특징을 소개합니다.&amp;#xD;
        ![Trino_5.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino5.png)&amp;#xD;
        &amp;#xD;
        여기서도 키워드를 뽑아보면 아래와 같습니다.&amp;#xD;
        &amp;#xD;
        1. distributed: 분산 처리로 빠르고&amp;#xD;
        2. ANSI SQL: 표준 SQL 을 호환하여 현재 쿼리문을 수정할 필요가 없고&amp;#xD;
        3. S3: OBS에 저장하여 스토리지 비용을 줄일 수 있고&amp;#xD;
        4. Query Federation: OBS의 데이터와 MySQL 데이터를 하나의 쿼리로 join할 수 있다.&amp;#xD;
        &amp;#xD;
        ## Trino 동작 원리&amp;#xD;
        &amp;#xD;
        Trino의 동작 원리는 [Presto: SQL on Everything](https://trino.io/Presto_SQL_on_Everything.pdf)라는 논문에 자세히 소개하고 있습니다.&amp;#xD;
        해당 논문의 일부를 가볍게 살펴보겠습니다.&amp;#xD;
        &amp;#xD;
        ### 구조도&amp;#xD;
        ![Trino_6.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino6.png)&amp;#xD;
        &amp;#xD;
        Trino는 하나의 Coordinator 노드와 여러 개의 Worker 노드로 구성됩니다. Coordinator 노드는 쿼리의 인입 지점으로 admit, parsing, planning, optimizing, orchestration 등을 수행하고, worker node는 query processing을 담당합니다.&amp;#xD;
        &amp;#xD;
        ### 요청 처리 순서&amp;#xD;
        &amp;#xD;
        Coordinator 노드가 분산 처리를 계획하면 worker node가 병렬로 처리해서 복잡한 쿼리가 더 빠르게 실행되는 원리입니다.&amp;#xD;
        &amp;#xD;
        1. client → coordinator: http request (SQL stmt)&amp;#xD;
        2. coordinator: evaluate request(parsing, analyzing, **optimizing distributed execution plan**)&amp;#xD;
        3. coordinator: plan to worker&amp;#xD;
        1. task 생성&amp;#xD;
        2. **splits** 생성(addressable chunk in external storage)&amp;#xD;
        3. splits을 task에 할당&amp;#xD;
        4. worker: run task&amp;#xD;
        1. fetching splits&amp;#xD;
        2. 다른 worker에서 생성한 intermediate data 처리&amp;#xD;
        1. worker 간에는 intermediate data를 memory에 저장하여 공유&amp;#xD;
        2. **shuffle**이 발생할 수 있음&amp;#xD;
        \*shuffle = node 간 데이터 재분배&amp;#xD;
        3. query의 shape에 따라 모든 데이터를 처리하지 않고 반환&amp;#xD;
        &amp;#xD;
        ## Trino 쿼리 실행 예시&amp;#xD;
        &amp;#xD;
        ### 그림으로 살펴보기&amp;#xD;
        * 쿼리문&amp;#xD;
        ![Trino_7.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino7.png)&amp;#xD;
        &amp;#xD;
        * logical plan&amp;#xD;
        ![Trino_8.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino8.png)&amp;#xD;
        &amp;#xD;
        * distributed plan (stage)&amp;#xD;
        ![Trino_9.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino9.png)&amp;#xD;
        &amp;#xD;
        * optimized plan (pipeline, parallelism)&amp;#xD;
        ![Trino_10.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino10.png)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### 실행 순서&amp;#xD;
        &amp;#xD;
        1. Planner: SQL → SQL syntax tree → Logical Planning (IR 생성)&amp;#xD;
        * IR = Intermediate Representation&amp;#xD;
        2. Optimizer: Logical Plan → evaluate transformation rules → **optimize** → Physical Structure&amp;#xD;
        * transformation rules = sub-tree query plan + transformation&amp;#xD;
        * 사용되는 optimizing 기법 = predicate and limit pushdown, column pruning, decorrelation, table and column statistics 기반 cost-based 최적화&amp;#xD;
        * Data Layouts = Connector Data Layout API로 얻어내는 위치, 파티션, 정렬, 그룹화, 인덱스&amp;#xD;
        * Predicate Pushdown = connector에 따른 filtering 최적화&amp;#xD;
        * \*pushdown : 읽어야 하는 데이터를 줄이는 것&amp;#xD;
        * \***Predicate Pushdown** : 조회 조건에 맞는 데이터만 읽는 것&amp;#xD;
        * **Inter**-node Parallelism = stage 단위의 병렬 실행&amp;#xD;
        * **Intra**-node Parallelism = stage 내에서 single node의 thread에 걸친 병렬 실행&amp;#xD;
        3. Scheduler: Stage Scheduling → Task Scheduling → Split Scheduling&amp;#xD;
        * Task Scheduling = Leaf Stage / Intermediate Stage 분리하여 배치&amp;#xD;
        4. Query Execution = Local Data Flow → Shuffles → Writes&amp;#xD;
        &amp;#xD;
        ## DataQuery&amp;#xD;
        &amp;#xD;
        [NHN Cloud의 DataQuery](https://www.nhncloud.com/kr/service/data-analytics/dataquery?lang=ko) 서비스는 위에서 소개한 Trino를 기반으로 대규모 데이터에 대해 쿼리를 실행할 수 있는 서비스입니다.&amp;#xD;
        이를 통해 원하는 클러스터 스펙을 지정하고 연결할 데이터 소스만 작성하면 Trino의 복잡한 설치와 설정 과정 없이 사용이 가능합니다.&amp;#xD;
        &amp;#xD;
        # Trino 적용 - 개념&amp;#xD;
        Trino를 적용하기 위해 알아야 할 개념을 소개합니다.&amp;#xD;
        &amp;#xD;
        ## 데이터 소스 선정&amp;#xD;
        Trino는 여러 종류의 데이터 소스를 지원합니다.&amp;#xD;
        &amp;#xD;
        NHN AppGuard는 로그 저장 기간 증가를 계획하고 있어 저장 비용을 절약하기 위해 OBS를 데이터 소스로 선정하였습니다.&amp;#xD;
        OBS 데이터 소스를 사용하는 경우 데이터의 타입도 Parquet, JSON, ORC, CSV, Text 중에 선택해 주어야 해서, 위와 동일한 이유로 Parquet 파일 포맷을 선택하였습니다.&amp;#xD;
        &amp;#xD;
        ### Apache Parquet&amp;#xD;
        &amp;#xD;
        [Apache Parquet 홈페이지](https://parquet.apache.org)에는 Parquet를 아래와 같이 설명합니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Parquet is available in multiple languages including Java, C++, Python, etc...&amp;#xD;
        &amp;#xD;
        여기서도 키워드를 뽑아보면 아래와 같습니다.&amp;#xD;
        &amp;#xD;
        * column-oriented data&amp;#xD;
        * efficient data storage and retrieval&amp;#xD;
        * efficient data compression&amp;#xD;
        * encoding schema&amp;#xD;
        * handle complex data in bulk&amp;#xD;
        &amp;#xD;
        column-oriented data의 설명은 아래의 그림을 보시면 이해가 쉽습니다.&amp;#xD;
        ![Trino_11.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino11.png)&amp;#xD;
        &amp;lt;center&amp;gt;(Source: [https://devidea.tistory.com/92](https://devidea.tistory.com/92))&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        동일한 타입의 데이터가 나열되기 때문에 압축 효율이 높아지는 효과가 있습니다.&amp;#xD;
        또한 footer에 데이터에 대한 메타데이터를 저장해 두어 reader에게 데이터에 대한 힌트를 주어 조회 성능을 높입니다.&amp;#xD;
        &amp;#xD;
        ![Trino_12.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino12.png)&amp;#xD;
        &amp;lt;center&amp;gt;(source: [https://parquet.apache.org/docs/file-format/](https://parquet.apache.org/docs/file-format/))&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        ## 구상안&amp;#xD;
        &amp;#xD;
        parquet는 columnar한 형식이기 때문에 row 단위로 데이터를 append하는 것은 비효율적입니다. 그러므로 데이터를 모아서 parquet 형식으로 파일을 생성하는 것이 효율적입니다. 이를 위해 NHN AppGuard에서는 3가지 구성 방법을 고려했고 3번째 안을 선택했습니다.&amp;#xD;
        &amp;#xD;
        1. micro batch&amp;#xD;
        1. kafka → log-batch → create parquet / 1 minute → save obs → obs&amp;#xD;
        2. trino는 OBS를 사용하는 경우 파일 기반으로 동작하기 때문에 파일의 개수가 많아지면 비효율적입니다.&amp;#xD;
        3. 1분 단위로 파일을 쓸 경우 작은 파일이 많아져 조회 성능이 현저히 떨어지기 때문에 선택하지 않았습니다.&amp;#xD;
        2. hourly batch&amp;#xD;
        1. kafka → log-batch → create parquet / 1 hour (save data in memory or redis) → save obs → obs&amp;#xD;
        2. 메모리에 저장하는 경우 데이터 유실의 리스크가 걱정되었고&amp;#xD;
        3. NHN AppGuard는 redis를 사용하고 있지 않아 trino와 redis 두 컴포넌트의 추가로 인한 운영 복잡도 증가가 부담되어 선택하지 않았습니다.&amp;#xD;
        3. **중간 DB 사용 - MySQL**&amp;#xD;
        1. kafka → log-batch → save to mysql → mysql → tier down in daily-batch → save obs → obs&amp;#xD;
        2. 기존에 사용하던 MySQL 구성을 변경하지 않아 수정 소요가 적었고&amp;#xD;
        3. MySQL을 통해 실시간 데이터 또한 조회할 수 있어 실시간 데이터 조회가 쉬워 선택하였습니다.&amp;#xD;
        &amp;#xD;
        ### 구성도&amp;#xD;
        &amp;#xD;
        * AS-IS&amp;#xD;
        ![Trino_13_900.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino13900.png)&amp;#xD;
        &amp;#xD;
        * TO-BE&amp;#xD;
        ![Trino_14_900.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino14900.png)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### tier down 개념&amp;#xD;
        &amp;#xD;
        ElasticSearch는 데이터의 역할 또는 접근 빈도에 따라 노드를 분배하는 기법으로 Data Tiering 을 사용합니다.&amp;#xD;
        &amp;#xD;
        ![Trino_15.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino15.png)&amp;#xD;
        &amp;lt;center&amp;gt;(source: [https://www.linkedin.com/pulse/navigating-data-tiers-optimizing-costs-reducing-risk-boosting-lim-yfeyc](https://www.linkedin.com/pulse/navigating-data-tiers-optimizing-costs-reducing-risk-boosting-lim-yfeyc))&amp;lt;/center&amp;gt;&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        이렇게 tier를 적용한 데이터를 높은 티어에서 낮은 티어로 낮추는 것을 tier down이라고 부릅니다. hot tier는 일반적으로 성능이 좋고 반응이 빠르지만 비용이 비싸고, cold tier는 반응은 조금 느리지만 비용이 저렴한 저장소를 사용합니다.&amp;#xD;
        &amp;#xD;
        NHN AppGuard에서는 MySQL을 hot tier, Trino를 cold tier로 정의하고 daily-batch에서 MySQL 데이터를 Parquet로 변환해 Trino에 삽입시키는 작업을 tier down으로 정의했습니다.&amp;#xD;
        &amp;#xD;
        ### Parquet 파일 생성 방법&amp;#xD;
        &amp;#xD;
        Parquet는 원래 HDFS에 쓰는 용도로 고안되어서 Parquet 파일을 직접 쓰려면 `org.apache.hadoop:hadoop-common:3.3.6`과 같은 hdfs writer에 세그먼트 관리, 열 압축 등의 기능을 구현해야 합니다. 이러한 작업을 피하기 위해 일반적으로 Spark 등의 외부 컴포넌트를 쓰거나 avro 포맷의 파일을 거쳤다가 parquet로 변환하는 방법을 사용합니다.&amp;#xD;
        &amp;#xD;
        [Apache Avro](https://avro.apache.org)는 data를 serialize하기에 좋은 포맷으로 스키마를 갖습니다.&amp;#xD;
        ParquetFileWriter를 지원하기 때문에 손쉽게 변환이 가능합니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; Apache Avro™ is the leading serialization format for record data, and first choice for streaming data pipelines. It offers excellent schema evolution, and has implementations for the JVM (Java, Kotlin, Scala, …), Python, C/C++/C#, PHP, Ruby, Rust, JavaScript, and even Perl.&amp;#xD;
        &amp;#xD;
        # Trino 적용 - 구현&amp;#xD;
        &amp;#xD;
        ## tier down 구현&amp;#xD;
        &amp;#xD;
        ### 논리 구조&amp;#xD;
        ![Trino_16_900.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino16900.png)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        ### Trino 테이블 생성&amp;#xD;
        &amp;#xD;
        Trino 데이터 소스로 OBS를 사용하는 경우 Hive를 사용하기 때문에 HQL을 사용해야 합니다. HQL 또한 SQL 표준을 따르기 때문에 거의 유사하지만 묵시적 형 변환과 같은 편의 기능을 지원하지 않고, with 문의 external location, partitioned_by 등의 옵션이 추가됩니다.&amp;#xD;
        &amp;#xD;
        ```sql&amp;#xD;
        CREATE TABLE log&amp;#xD;
        (&amp;#xD;
        seq              bigint, &amp;#xD;
        log_time         timestamp,&amp;#xD;
        // 생략 &amp;#xD;
        log_date         date,&amp;#xD;
        appkey           varchar(64),&amp;#xD;
        ) &amp;#xD;
        WITH ( &amp;#xD;
        format = &apos;Parquet&apos;,&amp;#xD;
        external_location = &apos;s3a://data-query/log&apos;,&amp;#xD;
        partitioned_by = ARRAY[&apos;appkey&apos;,&apos;date&apos;]&amp;#xD;
        );&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### avro schema 작성&amp;#xD;
        &amp;#xD;
        ```javascript&amp;#xD;
        {&amp;#xD;
        &quot;type&quot; : &quot;record&quot;,&amp;#xD;
        &quot;name&quot; : &quot;log&quot;,&amp;#xD;
        &quot;namespace&quot; : &quot;avro&quot;,&amp;#xD;
        &quot;fields&quot; : [&amp;#xD;
        { &quot;name&quot; : &quot;seq&quot;, &quot;type&quot; : &quot;long&quot; },&amp;#xD;
        { &quot;name&quot; : &quot;log_time&quot;, &quot;type&quot; : [ &quot;null&quot;, &quot;string&quot; ], &quot;default&quot; : null },&amp;#xD;
        // 생략&amp;#xD;
        ]&amp;#xD;
        }&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ### tier down process&amp;#xD;
        ![Trino_17.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino17.png)&amp;#xD;
        데이터를 메모리에 올려서 변환하기 때문에 장비와 데이터에 따라 적절한 페이징을 적용해야 합니다.&amp;#xD;
        &amp;#xD;
        ### convert to parquet&amp;#xD;
        ![Trino_18.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino18.png)&amp;#xD;
        avro 변환은 apache avro 모듈의 schema.from 함수로 쉽게 변환이 가능합니다. parquet는 apache parquet 모듈의 PositionOutputStream 객체의 writer를 구현하여 변환할 수 있습니다.&amp;#xD;
        &amp;#xD;
        ### 다른 방법은 없을까?&amp;#xD;
        CTAS(Create Table As Select)가 가장 쉬운 방법입니다. 수행 시간은 위 방법과 비슷하게 소요되지만 용량이 30% 정도 더 효율적인 것으로 확인하였습니다. 하지만 DataQuery에서 사내 DB를 아직 데이터 소스로 지원하지 않아 현재는 사용이 어렵습니다.&amp;#xD;
        여기에서는 방법만 소개하겠습니다.&amp;#xD;
        &amp;#xD;
        ```sql&amp;#xD;
        CREATE TABLE obs.test.log_ctas&amp;#xD;
        WITH (&amp;#xD;
        format = &apos;Parquet&apos;,&amp;#xD;
        external_location = &apos;s3a://ctas-test/log-ctas&apos;,&amp;#xD;
        partitioned_by = ARRAY[&apos;log_date&apos;, &apos;appkey&apos;]&amp;#xD;
        )&amp;#xD;
        AS&amp;#xD;
        select seq,&amp;#xD;
        // 생략&amp;#xD;
        cast(log_time as date) as log_date,&amp;#xD;
        appkey&amp;#xD;
        from &quot;mysql&quot;.log&amp;#xD;
        where log_time &amp;gt;= date &apos;2024-11-01&apos;&amp;#xD;
        and log_time &amp;lt; date &apos;2024-11-02&apos;;&amp;#xD;
        ```&amp;#xD;
        &amp;#xD;
        ## 실시간 데이터 union 구현&amp;#xD;
        &amp;#xD;
        ### 논리 구성도&amp;#xD;
        ![Trino_19_900.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino19900.png)&amp;#xD;
        1. cold data에 마지막으로 저장된 시간을 조회하고&amp;#xD;
        2. cold data와 hot data를 조회해&amp;#xD;
        3. join / union 하여 응답합니다.&amp;#xD;
        &amp;#xD;
        ### Data 조회&amp;#xD;
        &amp;#xD;
        Cold - max cold data 기준 왼쪽을 조회합니다.&amp;#xD;
        ![Trino_20.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino20.png)&amp;#xD;
        &amp;#xD;
        Hot - max cold data 기준 오른쪽을 조회합니다.&amp;#xD;
        ![Trino_21.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino21.png)&amp;#xD;
        &amp;#xD;
        ### Data Join / Union&amp;#xD;
        &amp;#xD;
        집계의 경우 toMap과 id 값을 이용해 Join 합니다.&amp;#xD;
        ![Trino_22.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino22.png)&amp;#xD;
        &amp;#xD;
        단순 조회의 경우 stream.concat으로 Union 합니다.&amp;#xD;
        ![Trino_23.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino23.png)&amp;#xD;
        &amp;#xD;
        # 성능 테스트&amp;#xD;
        &amp;#xD;
        ## 환경&amp;#xD;
        &amp;#xD;
        * DataQuery 스펙: c2m8 * 3&amp;#xD;
        * DataQuery 데이터는(log_date, appKey) 파티셔닝 되어 있고&amp;#xD;
        * 참고 \*partition = RDBMS의 index 와 유사. parquet 가 저장된 경로를 의미&amp;#xD;
        ![Trino_24.png](https://image.toast.com/aaaadh/real/2025/techblog/Trino24.png)&amp;#xD;
        * MySQL 데이터는 일 단위로 파티셔닝되어 있습니다.&amp;#xD;
        * MySQL 데이터는 log\_time, appkey 각각의 인덱스와 (log\_time, appkey) 복합 index 가 적용되어 있습니다.&amp;#xD;
        &amp;#xD;
        ## 데이터 조회&amp;#xD;
        &amp;#xD;
        이슈 대응 등의 이유로 개발자가 쿼리 엔진에 자주 질의하는 **일반 쿼리**와 서비스에서 사용하는 **서비스 쿼리**로 구분하여 테스트했습니다.&amp;#xD;
        &amp;#xD;
        ### 일반 쿼리&amp;#xD;
        &amp;#xD;
        단순한 select \* 조회는 mysql이 7배가량 빠르고, count 등의 집계 함수가 포함된 쿼리는 DataQuery가 적게는 4배에서 6배가량 빠른 양상을 보였습니다.&amp;#xD;
        Trino + Parquet 조합은 열 기반 데이터 포맷으로 인한 행 조회의 비효율성, Trino의 쿼리 플래닝과 file fetch에서의 오버헤드로 인해 단순한 행 조회가 느리기 때문입니다.&amp;#xD;
        &amp;#xD;
        | 쿼리 | dataquery | mysql |&amp;#xD;
        | --- | --------- | ----- |&amp;#xD;
        | **단순** 조회 (select \* limit 500) | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;1 s 151 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;148 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | **count** 조회 (select count(\*) 한 달 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;8 s 957 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;30.987 s&amp;lt;/span&amp;gt; |&amp;#xD;
        | filter - appkey \&amp;amp; log\_time **행 조회** | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;1 s 323 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;393 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | filter - appkey \&amp;amp; log\_time **count** | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;349 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;14.662 s&amp;lt;/span&amp;gt; |&amp;#xD;
        | group by - appkey 하루 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;814 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;2.385 s&amp;lt;/span&amp;gt; |&amp;#xD;
        | group by - appkey 한 달 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;18 s 530 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;2m 15s 538ms&amp;lt;/span&amp;gt; |&amp;#xD;
        &amp;#xD;
        ### NHN AppGuard 서비스 쿼리&amp;#xD;
        &amp;#xD;
        DataQuery가 전반적으로 10배 정도 빨랐습니다.&amp;#xD;
        이상 행위 탐지 현황의 한 달치 데이터는 MySQL에서 30분 이상 소요되어 조회할 수 없었지만 DataQuery는 36초만에 조회하였습니다.&amp;#xD;
        &amp;#xD;
        | 쿼리 | dataquery | mysql |&amp;#xD;
        | --- | --------- | ----- |&amp;#xD;
        | 이상행위 탐지현황 - limit 50 하루 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;1 s 696 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;9.676s&amp;lt;/span&amp;gt; |&amp;#xD;
        | 이상행위 탐지현황 - limit 50 한 달 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;6 s 468 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;6m 19s 459ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 이상행위 탐지현황 report - 하루 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;7.06s&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;21s 890ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 이상행위 탐지현황 report - 한 달 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;36.81s&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;**조회 불가(30분 이상)**&amp;lt;/span&amp;gt; |&amp;#xD;
        | 로그 조회 - 하루 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;1 s 531 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;7s 264ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 로그 조회 - 한 달 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;5 s 728 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;5m 58s 381ms&amp;lt;/span&amp;gt; |&amp;#xD;
        &amp;#xD;
        ### Parquet 크기별 비교&amp;#xD;
        &amp;#xD;
        appkey로 파티션 되기 때문에 appkey별 로그 양이 달라 로그 개수에 따른 성능 차이를 비교해 보았습니다. 로그 수 기준 중위의 appkey까지도 MySQL이 더 빠른 양상을 보였습니다. 2-300ms가량의 차이를 보이는 만큼 로그가 적은 사용자 입장에서는 데이터가 없는데 굼뜨다는 느낌을 받을 수 있습니다. 이에 반해 로그 수가 평균을 넘어가면 MySQL은 30초가 넘어가는 응답을 보여 콘솔에서 서비스하기에 어려운 반응 속도를 보입니다.&amp;#xD;
        &amp;#xD;
        | 대시보드 조회 쿼리 | DataQuery | MySQL |&amp;#xD;
        | ---------- | --------- | ----- |&amp;#xD;
        | 데이터 없음 | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;267 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;102 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 최소 | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;365 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;100 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 중위 | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;610 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;168 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 평균 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;505 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;34 s 24 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        | 최대 | &amp;lt;span style=&quot;color:#0052cc;&quot;&amp;gt;15s 85 ms&amp;lt;/span&amp;gt; | &amp;lt;span style=&quot;color:#e11d21;&quot;&amp;gt;10 m 23 s 982 ms&amp;lt;/span&amp;gt; |&amp;#xD;
        &amp;#xD;
        ## 성능 테스트 결론&amp;#xD;
        &amp;#xD;
        1. 행 전체 조회, 데이터가 적은 경우는 MySQL이 빠르다.&amp;#xD;
        1. 100ms VS 500ms의 차이 → **참을만하다.**&amp;#xD;
        2. 집계 조회, 데이터가 많은 경우에는 DataQuery가 빠르다.&amp;#xD;
        1. 수십 초 VS 수 분 차이 → **참을 수 없다.**&amp;#xD;
        &amp;#xD;
        # 결과&amp;#xD;
        &amp;#xD;
        ## 좋아졌나요?&amp;#xD;
        &amp;#xD;
        1. 이상 행위 탐지 현황의 30일치 데이터를 조회하지 못하던 고객이 이제 조회할 수 있게 되었습니다.&amp;#xD;
        2. 2024년 초 공개한 NHN AppGuard public api는 MySQL로는 30분 이상 소요되어 개발이 어려웠는데, DataQuery를 통해 7초 이내로 조회하여 개발할 수 있었습니다.&amp;#xD;
        3. 내부 집계 시간이 38m36s → 22m16s로 약 43% 개선했습니다.&amp;#xD;
        4. mysql에서의 집계로 인한 slow query가 제거되어 일 배치로 인한 서비스의 영향성이 없어졌습니다.&amp;#xD;
        5. 집계 연산이 빨라져서 집계 데이터의 종류를 늘리는 것에 부담이 없어졌습니다.&amp;#xD;
        6. 스토리지 비용 감소로 데이터 저장 기간을 60일에서 1년으로 늘렸습니다.&amp;#xD;
        &amp;#xD;
        ## 나쁜 점은 없나요?&amp;#xD;
        &amp;#xD;
        1. 대시보드의 기본 응답 속도가 300ms 정도 느려졌습니다.&amp;#xD;
        2. tier down 실패 시 집계, 미터링 등에 영향을 주기 때문에 모니터링 요소가 늘어났습니다.&amp;#xD;
        3. DataQuery와 OBS 비용이 추가되었습니다. (대략 100만 원/월)&amp;#xD;
        &amp;#xD;
        ## 앞으로 해야 할 것이 있을까요?&amp;#xD;
        &amp;#xD;
        1. 일 단위 tier down을 시간 단위로 변경하는 것을 고민하고 있습니다.&amp;#xD;
        2. 고객 로그 수에 따라 적절한 쿼리 엔진을 사용하도록 최적화하는 부분에 대해 고민하고 있습니다.&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        &amp;#xD;
        이상 NHN AppGuard에 Trino를 적용해 본 과정과 결과에 대해 정리하였습니다. 도움이 되셨길 바라며, 긴 글을 읽어 주셔서 감사합니다. &amp;#xD;
        &amp;#xD;
        [![NHN Cloud_meetup banner_footer_gray_202408_900.png](https://image.toast.com/aaaadh/real/2025/techblog/NHN%20Cloudmeetup%20bannerfootergray202408900.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>Kafka Connect로 DB 데이터 쉽게 연동하기</title>
      <link>http://thefarmersfront.github.io/blog/kafka-connect-pipeline/</link>
      <guid>http://thefarmersfront.github.io/blog/kafka-connect-pipeline/</guid>
      <pubDate>Tue, 04 Mar 2025 00:00:00 GMT</pubDate>
      <content:encoded>Kafka Connect와 JDBC 커넥터를 이용해 DB 데이터를 쉽게 Kafka로 전송하는 방법과 발생 가능한 문제를 해결하는 방법을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>물류의 물짜도 모르던 OMS PM의 OMS 구축기</title>
      <link>http://thefarmersfront.github.io/blog/oms_pm/</link>
      <guid>http://thefarmersfront.github.io/blog/oms_pm/</guid>
      <pubDate>Wed, 05 Feb 2025 13:00:00 GMT</pubDate>
      <content:encoded></content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 마지막 화 ( 엔드 게임 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-6/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-6/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>유종의 미 그리고 회고</content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 5화 ( 어질어질한 변화구들 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-5/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-5/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>복합건물 (아파트, 다세대 주택) 주소정제 정복</content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 4화 ( 슬픈예감 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-4/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-4/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>단독건물 주소정제 정복 (feat.전라북도와 부천시의 습격)</content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 3화 ( 노가다의 달달한 열매 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-3/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-3/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>전국 약 1080만개의 건물DB 완성</content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 2화 ( 그럴싸한 계획 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-2/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-2/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>손 안대고 코풀기 전략의 한계</content:encoded>
    </item>
    <item>
      <title>주소정제 서비스 내재화 - 1화 ( 줄줄 새는 돈 )</title>
      <link>http://thefarmersfront.github.io/blog/refine-address-internalization-1/</link>
      <guid>http://thefarmersfront.github.io/blog/refine-address-internalization-1/</guid>
      <pubDate>Mon, 20 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>문제인식과 컬리에서의 주소 정제의 목적</content:encoded>
    </item>
    <item>
      <title>99%가 모른다는 DB Connection 누수 문제</title>
      <link>http://thefarmersfront.github.io/blog/connection-leak/</link>
      <guid>http://thefarmersfront.github.io/blog/connection-leak/</guid>
      <pubDate>Mon, 06 Jan 2025 00:00:00 GMT</pubDate>
      <content:encoded>DB Connection과 Garbage Collector의 관계를 중심으로 mysql-connector-j 사용 시 발생할 수 있는 메모리 누수를 탐지하고 해결한 경험을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>딜리버리 프로덕트 개발팀의 개발문화 - 로그 &amp; 알람편</title>
      <link>http://thefarmersfront.github.io/blog/deliveryproductteam-culture-1/</link>
      <guid>http://thefarmersfront.github.io/blog/deliveryproductteam-culture-1/</guid>
      <pubDate>Thu, 02 Jan 2025 09:00:00 GMT</pubDate>
      <content:encoded>딜리버리 프로덕트 개발팀에서 안정적인 서비스 제공을 위한 고군분투기</content:encoded>
    </item>
    <item>
      <title>MySqlPagingQueryProvider 살펴보기</title>
      <link>http://thefarmersfront.github.io/blog/2024-mysql-paging-query-provider/</link>
      <guid>http://thefarmersfront.github.io/blog/2024-mysql-paging-query-provider/</guid>
      <pubDate>Mon, 30 Dec 2024 17:00:00 GMT</pubDate>
      <content:encoded>JdbcPagingItemReader와 MySqlPagingQueryProvider를 사용할 때 주의사항</content:encoded>
    </item>
    <item>
      <title>100여개의 대출 기관 API, 자동으로 운영하기</title>
      <link>https://blog.banksalad.com/tech/banksalad-loan-api/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-loan-api/</guid>
      <pubDate>Thu, 26 Dec 2024 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요! 뱅크샐러드의 Server Engineer 조성민입니다. 이번 글에서는 제 팀인 금융쇼핑 PA…</content:encoded>
    </item>
    <item>
      <title>2024 프론트엔드 뉴스 한 방에 몰아 보기</title>
      <link>https://meetup.nhncloud.com/posts/390</link>
      <guid>https://meetup.nhncloud.com/posts/390</guid>
      <pubDate>Mon, 16 Dec 2024 04:57:57 GMT</pubDate>
      <content:encoded>
        ![NHN Cloud_meetup banner_frontendnews2024_202412_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfrontendnews2024202412900.png)&amp;#xD;
        &amp;#xD;
        안녕하세요. NHN Cloud NCUI개발팀 이진우입니다.&amp;#xD;
        &amp;#xD;
        저는 올해 2월부터 매주 프론트엔드 관련 다양한 뉴스레터를 읽고, FE 개발 직무 동료들에게 흥미로운 뉴스, 읽을거리, 유용한 도구 등을 공유해 왔습니다.&amp;#xD;
        어느덧 연말이 가까워지며 쌓여 온 링크들은 상당한 양이 되었는데요. 이를 정리하며 개인적으로 인상 깊었던 주제와 도구를 공유드리고자 합니다.&amp;#xD;
        &amp;#xD;
        물론, 개인적인 관심사가 반영되어 있기도 하고, 조직 내에서 영향이 큰 프레임워크나 기술 스택 위주로 정리된 만큼 부족한 부분이 있을 수 있습니다.&amp;#xD;
        훑어보시면서 혹시 낯설거나 관심이 가는 주제가 있다면 함께 제공한 읽을거리 링크가 도움이 되기를 바랍니다.&amp;#xD;
        (정리된 내용 중 일부는 시간이 지나며 업데이트되거나 새로운 정보로 대체되었을 수 있습니다. )&amp;#xD;
        &amp;#xD;
        ## 프론트엔드 기술 스택&amp;#xD;
        &amp;#xD;
        프론트엔드 개발은 마치 9G 중력 가속도처럼 빠르게 변하며 개발자들을 압박해 왔지만 최근엔 변화의 속도가 다소 안정된 듯한 느낌입니다. ☕️ 새로운 개념을 바탕으로 한 프레임워크와 도구가 끊임없이 쏟아졌던 과거와 달리, 이제는 더 나은 생산성과 사용자 경험을 위한 실용적인 도구들이 점차 정착되고 있습니다. 2024년에 출시되진 않았지만, 개인적으로 올해 활약상이 인상 깊었던 몇 가지 라이브러리를 소개하고자 합니다. ‍&amp;#xD;
        &amp;#xD;
        ### shadcn/ui&amp;#xD;
        &amp;#xD;
        [shadcn/ui](https://ui.shadcn.com/)는 TailwindCSS와 Headless UI의 철학을 결합한 라이브러리로, 복사-붙여넣기(copy-paste) 방식으로 빠르게 UI를 구현할 수 있는 점이 특징입니다. 개발자는 필요한 컴포넌트를 복사한 뒤, 프로젝트 요구에 맞게 간단히 수정하여 사용할 수 있어 효율성이 뛰어납니다.&amp;#xD;
        &amp;#xD;
        * [(번역) shadcn/ui의 해부](https://siosio3103.medium.com/shadcn-ui-%EC%9D%98-%ED%95%B4%EB%B6%80-ebd469c34614)&amp;#xD;
        * [How headless components became the future for building UI libraries](https://www.subframe.com/blog/how-headless-components-became-the-future-for-building-ui-libraries)&amp;#xD;
        * [Headless, boneless, skinless &amp;amp; lifeless UI](https://nerdy.dev/headless-boneless-and-skinless-ui)&amp;#xD;
        &amp;#xD;
        ### Zod&amp;#xD;
        &amp;#xD;
        [Zod](https://zod.dev/)는 TypeScript와 완벽히 통합된 스키마 선언 및 데이터 검증 라이브러리로, 간결한 문법과 타입 추론을 통해 개발 생산성을 크게 높여 서버-클라이언트 간 데이터 구조를 명확하게 정의하고 유지할 수 있습니다. REST API, GraphQL, Form Data Validation 등 다양한 환경에서 활용 가능하며, 코드의 가독성을 유지하면서도 강력한 검증 로직을 구현할 수 있는 도구입니다.&amp;#xD;
        &amp;#xD;
        * [The process.env frontend time bomb](https://massimilianomirra.com/notes/the-frontend-process-env-time-bomb-plus-a-sustainable-definition-of-fixed)&amp;#xD;
        * [Making a REST API typesafe with Ready Query and Zod](https://noahflk.com/blog/typesafe-rest-api)&amp;#xD;
        &amp;#xD;
        ### Vitest&amp;#xD;
        &amp;#xD;
        [Vitest](https://vitest.dev/)는 Vite 기반의 테스트 러너로, 빠른 테스트 실행 속도와 간편한 설정으로 주목 받고 있습니다. 테스트 생태계의 하나의 표준과도 같았던 Jest와 유사한 API를 제공해 쉽게 전환이 가능하며, ESM과 TypeScript를 기본적으로 지원합니다. 특히 Vite와의 긴밀한 통합을 통해 개발 환경과 테스트 환경 간의 일관성을 유지할 수 있어 Vite 사용 시 더욱 효율적인 테스트 경험을 제공합니다.&amp;#xD;
        &amp;#xD;
        * [Vitest vs. Jest](https://www.speakeasy.com/post/vitest-vs-jest)&amp;#xD;
        * [Storybook 8.3 - First class Vitest integration](https://storybook.js.org/blog/storybook-8-3/)&amp;#xD;
        * [JavaScript unit testing frameworks in 2024: A comparison](https://raygun.com/blog/javascript-unit-testing-frameworks/#vitest)&amp;#xD;
        &amp;#xD;
        ### 기타&amp;#xD;
        &amp;#xD;
        * [Biome](https://biomejs.dev/)은 ESLint, Prettier의 대체 도구로 빠른 속도를 자랑합니다. 하지만 파일 간의 의존성이나 프로젝트 전반의 정보를 필요로 하는 정적 분석엔 아직 개선이 필요한데, 최근 Biome의 개발자는 [다중 파일 분석의 접근 방법](https://arendjr.nl/blog/2024/11/biome_approach_to_multi_file_analysis/)을 공유했습니다.&amp;#xD;
        &amp;#xD;
        * 요즘 웹사이트만 들어가면 쿠키 동의 때문에 정신없으시죠? GDPR(general data protection regulation)을 준수하기 위함인데 이와 관련된 라이브러리들([cookieconsent](https://github.com/brainsum/cookieconsent), [cookie checker](https://complydog.com/free-cookie-checker-tool))이 눈에 띄었습니다.&amp;#xD;
        &amp;#xD;
        * Zod의 대안으로 Yup, AJV, Joi, Superstruct 등이 있지만 TypeScript와의 통합이 부족하거나, 기능이 부족하거나, 러닝 커브가 높기도 합니다. 하지만 최근 [Vailbot](https://valibot.dev/)이 v1 베타 기간을 갖고 있는데, 더 작은 번들 사이즈로도 Zod와 유사한 기능을 제공해 주목을 받고 있습니다.&amp;#xD;
        &amp;#xD;
        * [TanStack](https://tanstack.com/)은 Tanner Linsley가 운영하는 라이브러리 생태계로, 현재 9개 라이브러리를 운영하며 프론트엔드 개발에서 빠르게 주목 받고 있습니다. 특히 알파 단계인 TanStack Start 프레임워크는 Next.js의 잠재적 경쟁자로 화제가 되고 있으며, TanStack Router는 SSR을 강력히 지원하며 클라이언트 사이드의 인터랙션도 최적화합니다. 또한, 엔터프라이즈급 라우팅을 타입 안전하게 구현하며, 번들링 및 배포는 [Vinxi](https://github.com/nksaraf/vinxi)를 통해 처리해 높은 성능과 유연성을 제공합니다.&amp;#xD;
        &amp;#xD;
        ## React&amp;#xD;
        &amp;#xD;
        ### React Deep Dive&amp;#xD;
        &amp;#xD;
        NHN Cloud의 FE 개발 조직은 React를 사용하고 있는데요. 사실 React로 기술 스택을 전환한 지는 React의 역사에 비해 그리 오래되지 않았습니다. 시간이 지남에 따라 React 사용에 익숙해지면서 내부 동작 원리에 대한 호기심이 생겼고, 조직 내 몇 명이 모여 React 내부 구조를 알아보는 스터디를 시작했습니다. ‍ 아래는 스터디 과정에서 참고했던 링크들입니다.&amp;#xD;
        &amp;#xD;
        * [React 톺아보기](https://goidle.github.io/react/in-depth-react-preview/)&amp;#xD;
        * [(번역) A deep dive into React Fiber internals](https://bumkeyy.gitbook.io/bumkeyy-code/frontend/a-deep-dive-into-react-fiber-internals)&amp;#xD;
        * ▶️ [React 파이버 아키텍처 분석](https://d2.naver.com/helloworld/2690975)&amp;#xD;
        * ▶️ [Inside React (동시성을 구현하는 기술)](https://deview.kr/2021/sessions/518)&amp;#xD;
        &amp;#xD;
        ### React 19&amp;#xD;
        &amp;#xD;
        React팀은 v19 업데이트를 위해 많은 노력을 기울였습니다. 하지만 최초 v19 RC가 공개된 후 Suspense의 동작 변경으로 인해 여러 컴포넌트가 순차적으로 데이터를 가져오게 되는 워터폴 현상이 발생했습니다.  React 팀은 이 동작이 의도된 동작이 아님을 확인하고 수정하겠다고 밝혔습니다. 이 사건은 오픈 소스 커뮤니티에서 투명한 소통과 협력의 중요성을 다시 한번 환기시켰습니다. 현재 React v19는 `prewarming`이라는 기능을 통해 이슈를 해소하며, 다시 RC v1 단계를 거쳐 현재 [v19](https://github.com/facebook/react/releases/tag/v19.0.0)를 릴리스했습니다. &amp;#xD;
        &amp;#xD;
        * [React 19 and suspense](https://tkdodo.eu/blog/react-19-and-suspense-a-drama-in-3-acts)&amp;#xD;
        * [React core PR - sibiling pre-rendering feature](https://github.com/facebook/react/issues/29898#issuecomment-2477449973)&amp;#xD;
        * [What&apos;s new in react 19](https://vercel.com/blog/whats-new-in-react-19)&amp;#xD;
        * [Server Actions have been renamed to Server Functions](https://19.react.dev/reference/rsc/server-functions)&amp;#xD;
        &amp;#xD;
        ### React Compiler&amp;#xD;
        &amp;#xD;
        React v19만큼이나 커뮤니티를 들썩이게 만든 또 다른 주제는 바로 React Compiler(코드명: Forget ‍️)입니다. React는 상태 변경과 리렌더링을 중심으로 동작하지만, 때로는 memoization을 통해 이러한 동작을 최적화해야 할 때가 있습니다. 그러나 이 과정에서 가독성이 떨어지고 오류가 발생하는 경우가 많았습니다. 이러한 문제를 해결하기 위해 React Compiler가 등장했죠. &amp;#xD;
        &amp;#xD;
        React Compiler는 memoization 최적화를 자동으로 처리할 뿐만 아니라, 앞으로는 JSX Inlining과 JSX Outlining 같은 기능도 지원할 예정입니다.&amp;#xD;
        &amp;#xD;
        \- JSX Inlining: 불필요한 JSX Runtime 호출을 제거하고 미리 빌드된 JSX 객체를 삽입\.&amp;#xD;
        \- JSX Outlining: 컴파일러가 하위 컴포넌트를 자동으로 추출하여 최적화\.&amp;#xD;
        &amp;#xD;
        현재 [베타 릴리스](https://react.dev/blog/2024/10/21/react-compiler-beta-release) 단계이며, React v17 이상에서 사용할 수 있습니다. ✨&amp;#xD;
        &amp;#xD;
        * [(번역) 리액트 컴파일러 사용법](https://junghan92.medium.com/%EB%B2%88%EC%97%AD-%EB%A6%AC%EC%95%A1%ED%8A%B8-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%99%84%EB%B2%BD-%EA%B0%80%EC%9D%B4%EB%93%9C-a6a0e96edc97)&amp;#xD;
        * [Understanding React Compiler](https://tonyalicea.dev/blog/understanding-react-compiler/)&amp;#xD;
        * [Alias analysis in the React Compiler](https://www.recompiled.dev/blog/alias-analysis/)&amp;#xD;
        * [Compiler Theory and Reactivity](https://www.recompiled.dev/blog/ssa/)&amp;#xD;
        * [Type system of the React Compiler](https://www.recompiled.dev/blog/type-system/)&amp;#xD;
        * ▶️ [What&apos;s next for the react compiler?](https://www.youtube.com/watch?v=qd5yk2gxbtg)&amp;#xD;
        &amp;#xD;
        ### Next.js&amp;#xD;
        &amp;#xD;
        [Next.js](https://nextjs.org/)는 React 기반의 풀스택 프레임워크로, 서버와 클라이언트 렌더링을 유연하게 결합해 강력한 개발 경험을 제공합니다. 특히 App Router와 React Server Components(RSC)의 도입으로 동적 라우팅과 성능 최적화가 크게 강화되었습니다.&amp;#xD;
        올해 Next.js는 [v14.2](https://nextjs.org/blog/next-14-2), v15 RC [v1](https://nextjs.org/blog/next-15-rc), [v2](https://nextjs.org/blog/next-15-rc2)를 거쳐 정식으로 [v15](https://nextjs.org/blog/next-15)를 릴리스했습니다. 여기에 모노레포(monorepo) 관리를 위한 Turborepo와 차세대 번들러인 Turbopack 또한 긴밀히 통합해 Next.js 생태계를 구축하고 있습니다.&amp;#xD;
        &amp;#xD;
        * [How vercel adopted microfrontends](https://vercel.com/blog/how-vercel-adopted-microfrontends)&amp;#xD;
        * [Turbopack Dev is now stable](https://nextjs.org/blog/turbopack-for-development-stable)&amp;#xD;
        * [How to set up next.js 15 for production in 2024](https://www.reactsquad.io/blog/how-to-set-up-next-js-15-for-production)&amp;#xD;
        * [Vercel functions - serverless servers and the challenge of new React architecture](https://bobaekang.com/blog/serverless-servers-and-the-challenge-of-new-react-architecture)&amp;#xD;
        * [Turborepo installation](https://turbo.build/repo/docs/getting-started/installation#start-with-an-example)&amp;#xD;
        * ▶️ [Self-hosting Next.js](https://www.youtube.com/watch?v=sIVL4JMqRfc)&amp;#xD;
        &amp;#xD;
        ### Remix&amp;#xD;
        &amp;#xD;
        [Remix](https://remix.run/)는 React를 기반으로 한 풀스택 웹 프레임워크로, 서버 사이드 렌더링(SSR)과 클라이언트 사이드 라우팅을 지원하여 빠르고 유연한 웹 애플리케이션 개발을 가능하게 합니다. 2022년 10월, Remix는 [Shopify에 인수](https://remix.run/blog/remixing-shopify)되었으며, 이후 Shopify는 [Hydrogen](https://hydrogen.shopify.dev/)에 Remix를 기반으로 한 기능을 도입해 보다 강력한 전자상거래 플랫폼을 구축했습니다.&amp;#xD;
        &amp;#xD;
        사실 Remix와 React Router는 아주 긴밀하게 연결되어 있었습니다. 이런 관계로 인해 점점 두 프로젝트의 경계가 모호해지며, Remix는 사실상 React Router의 확장판이 되었습니다. 이에 따라 Remix팀은 React Router 사용자들이 코드 분할, 데이터 로딩, 서버 렌더링 등 Remix의 강력한 기능을 손쉽게 활용할 수 있도록 두 프로젝트의 통합을 결정했습니다.&amp;#xD;
        &amp;#xD;
        * [Incremental path to React 19](https://remix.run/blog/incremental-path-to-react-19)&amp;#xD;
        * ▶️ [Remix Roadmap](https://www.youtube.com/watch?v=fjTX8hQTlEc&amp;amp;t=400s)&amp;#xD;
        &amp;#xD;
        ### 기타&amp;#xD;
        &amp;#xD;
        리액트 관련해 흥미로웠던 글들을 소개합니다.&amp;#xD;
        &amp;#xD;
        * [Conceptual Model of React and RSC](https://ondrejvelisek.github.io/conceptual-model-of-react-and-rsc/)&amp;#xD;
        * [The two reacts](https://overreacted.io/the-two-reacts/)&amp;#xD;
        * [The anatomy of a React Island](https://swizec.com/blog/the-anatomy-of-a-react-island/)&amp;#xD;
        * [Locality of Behavior in React Components](https://alexkondov.com/locality-of-behavior-react/)&amp;#xD;
        * [React&apos;s evolution from Hooks to Concurrent React: From React 16 to 18, a long overview](https://tigerabrodi.blog/reacts-evolution-from-hooks-to-concurrent-react)&amp;#xD;
        * [Two ways to the two Reacts](https://bobaekang.com/blog/two-ways-to-the-two-reacts/)&amp;#xD;
        &amp;#xD;
        ## 개발 환경&amp;#xD;
        &amp;#xD;
        ### JavaScript Runtimes&amp;#xD;
        &amp;#xD;
        한때 크로스 브라우저 호환성이 웹 개발의 주요 화두였다면, 이제는 자바스크립트 런타임 전쟁이 서서히 다가오고 있습니다. 오랜 시간 표준처럼 여겨졌던 [Node.js](https://nodejs.org/en) 외에도 [Deno](https://deno.com/)와 [Bun](https://bun.sh/)이 놀라운 속도로 발전하며 도전장을 내밀고 있습니다. 여기에 [LLRT](https://github.com/awslabs/llrt)와 같은 새로운 도구들까지 등장하면서, 자바스크립트 런타임 생태계는 이제 다양한 선택지와 경쟁으로 가득 찬 새로운 시대를 맞이하고 있습니다. 한편, Node.js 또한 선두 주자로서 입지를 지키기 위해 새로운 기술을 적극 수용하며 생태계를 확장하고 우위를 유지하려는 노력을 이어가고 있습니다.&amp;#xD;
        &amp;#xD;
        * [JS Toolbox 2024: Runtime environments &amp;amp; package management](https://raygun.com/blog/js-toolbox-part-1/)&amp;#xD;
        * [Runtime Compatibility](https://runtime-compat.unjs.io/)&amp;#xD;
        * [Node Core PR module: add --experimental-transform-types](https://github.com/nodejs/node/pull/54283)&amp;#xD;
        * [Native Support for CJS/ESM Interoperability Begins in Node.js 22](https://webdeveloper.beehiiv.com/p/native-support-cjsesm-interoperability-begins-nodejs-22)&amp;#xD;
        * [Node.js takes steps towards removing corepack](https://socket.dev/blog/node-js-takes-steps-towards-removing-corepack)&amp;#xD;
        &amp;#xD;
        ### Package Managers&amp;#xD;
        &amp;#xD;
        다양한 JavaScript Runtime들만큼이나 프로젝트 의존성 관리와 패키지 설치를 돕는 Package Manager도 전통적인 npm과 Yarn을 넘어 더 빠르고 효율적인 대안들이 떠오르며 경쟁이 치열해지고 있습니다. &amp;#xD;
        &amp;#xD;
        [pnpm](https://pnpm.io/ko/)은 디스크 공간 효율성과 모노레포(monorepo) 지원을 강점으로, [Bun](https://bun.sh/)은 올인원 도구로 놀라운 설치 속도를 자랑합니다. 또한 [JSR](https://jsr.io/docs/with/deno)은 다양한 JavaScript Runtime과 호환되며 TypeScript와 ESM을 네이티브로 지원해 현대적인 모듈 관리를 선도하고 있습니다. npm의 개발자는 새롭게 팀을 만들어 [vlt](https://www.vlt.sh/client)라는 패키지 매니저와 레지스트리인 [vsr](https://www.vlt.sh/serverless-registry)까지 출시했습니다.&amp;#xD;
        &amp;#xD;
        각 Package Manager는 고유의 특징과 강점을 바탕으로 개발자들에게 더 나은 선택지를 제공하며, 자바스크립트 생태계를 넓히고 있습니다.&amp;#xD;
        &amp;#xD;
        * [Introducing Deno 2](https://deno.com/blog/v2.0)&amp;#xD;
        * [Bun 1.1](https://bun.sh/blog/bun-v1.1)&amp;#xD;
        * [vlt - Introducing our team, investors &amp;amp; more](https://blog.vlt.sh/blog/the-team)&amp;#xD;
        &amp;#xD;
        ### Build Tools&amp;#xD;
        &amp;#xD;
        현대적인 프론트엔드 개발을 위한 빌드 도구는 결국 [Vite](https://vite.dev/) 아래 모이게 되었습니다.  [Remix가 Vite로 마이그레이션](https://remix.run/blog/remix-vite-stable)되었고, [Angular의 툴 체인](https://v17.angular.io/guide/esbuild)에도 도입되었으며, 그 외 수많은 라이브러리와 프레임워크가 Vite 위에 구축되어 오고 있습니다. 이로 인해 Vite의 주간 npm 다운로드 수는 1,700만건 가까이 기록하고 있으며 Next.js를 제외한 주요 생태계를 점령했습니다.&amp;#xD;
        &amp;#xD;
        Evan You에 의하면 Vite팀은 소스 파일에서 AST를 만들고, Linting, Formatting, Testing 등 모든 과정을 네이티브 수준의 속도로 처리는 툴체인을 구축하려는 목표를 세웠고 실제로 [VoidZero](https://voidzero.dev/posts/announcing-voidzero-inc)라는 회사를 설립해 투자 유치에 성공했습니다.&amp;#xD;
        &amp;#xD;
        최근 릴리스된 Vite 6.0에선 [Environment API](https://main.vite.dev/guide/api-environment)를 추가해 단일 Vite 서버에서 필요한 만큼 환경을 생성해 앱이 동작하는 방식을 매핑할 수 있게 됩니다. Vite는 이제 Deno, Bun과 같은 JavaScript Runtime으로 실행과 번들링이 가능해지고, React Native나 Electron과 같은 특수한 Runtime도 지원합니다.&amp;#xD;
        &amp;#xD;
        * [Vite Dev Server 이해하기](https://1ilsang.dev/posts/js/dev-server)&amp;#xD;
        * [Increasing Vite&apos;s potential with the Environment API](https://green.sapphi.red/blog/increasing-vites-potential-with-the-environment-api)&amp;#xD;
        * ▶️ [Visual Guide to the Modern Frontend Toolchain - Vite](https://youtu.be/M_edImKoEt8?si=RL1VrRyD1BYhqEnA)&amp;#xD;
        &amp;#xD;
        ### Bundler&amp;#xD;
        &amp;#xD;
        현재 개발에 박차를 가하고 있는 차세대 번들러들은 Rust 기반으로 설계되어 기존의 Parcel, Rollup, Webpack에 비해 더욱 빠르고 강력한 성능을 자랑합니다. ⚡️ 물론, 일부 개발자들은 이런 장점을 위해 모든 것을 새로 만드는 것이 과연 필요한가에 대한 의문을 제기하기도 합니다.&amp;#xD;
        &amp;#xD;
        하지만, Go로 작성된 [esbuild](https://esbuild.github.io/)로 Dev Server를 제공하는 Vite를 경험하고 나니, 다시 Webpack으로 돌아가는 일이 무척 멀게만 느껴지는 것도 사실입니다.&amp;#xD;
        &amp;#xD;
        차세대 번들러를 간단히 살펴보자면, [Rspack](https://rspack.dev/)은 Webpack의 [Drop-in Replacement](https://rspack.dev/guide/migration/webpack#migrate-from-webpack)로 호환성과 성능을 강조하고 있으며, [Turbopack](https://nextjs.org/docs/architecture/turbopack)은 안정화 단계를 거쳐 [Next.js v15의 권장 도구](https://nextjs.org/blog/next-15#turbopack-dev)로 자리 잡으며 Webpack의 후계자를 자처하고 있습니다. 또한 Vite 진영에서도 [Rolldown](https://rolldown.rs/)이라는 차세대 도구를 개발 중입니다. 마지막으로 또 내가 제일 빠르다고 하는 [Mako](https://makojs.dev/)도 있네요.&amp;#xD;
        &amp;#xD;
        각 도구들이 경쟁하며 프론트엔드 생태계에 바람을 일으키고 있습니다. 앞으로 누가 이 씬을 주도하게 될지 기대가 됩니다. &amp;#xD;
        &amp;#xD;
        * [Rspack announcing 1.0](https://rspack.dev/blog/announcing-1-0)&amp;#xD;
        * [Rslib](https://github.com/web-infra-dev/rslib)&amp;#xD;
        * [Lessons learned switching to Rspack](https://birtles.blog/2024/08/14/lessons-learned-switching-to-rspack)&amp;#xD;
        * [Why does Vercel bother with Turbopack when Vite already exists?](https://github.com/vercel/next.js/issues/48748#issuecomment-2199941311)&amp;#xD;
        * [Why we are building Rolldown](https://rolldown.rs/about#why-we-are-building-rolldown)&amp;#xD;
        &amp;#xD;
        ## 프레임워크&amp;#xD;
        &amp;#xD;
        ### Vue&amp;#xD;
        &amp;#xD;
        Vue는 올해 [v3.5](https://blog.vuejs.org/posts/vue-3-5)의 첫 번째 마이너 릴리스를 발표하며 성능과 메모리 최적화, SSR 지원에 주력했습니다. 특히, 반응형 시스템을 리팩터링하여 메모리 사용량을 56% 감소시키고, 대규모 및 깊은 배열의 처리 성능을 눈에 띄게 향상시켰습니다. Vue 역시 SSR(Server-Side Rendering) 지원 강화를 목표로 새로운 기능들을 추가하며, 현대적인 웹 애플리케이션 개발을 위한 진화를 계속하고 있습니다.&amp;#xD;
        &amp;#xD;
        * [10 Years of Vue: the Past and the Future](https://www.youtube.com/watch?v=OmrwRrZitv4)&amp;#xD;
        &amp;#xD;
        ### Angular&amp;#xD;
        &amp;#xD;
        올해 Angular는 3월, 구글의 내부 프레임워크 [Wiz와의 통합](https://blog.angular.dev/angular-and-wiz-are-better-together-91e633d8cd5a)을 발표했습니다. Wiz는 Gmail, Search와 같은 성능이 중요한 서비스에서 사용되는 프레임워크로 이번 통합은 SSR 지원 강화를 목적으로 이루어졌습니다. 이어 5월에는 [v18](https://blog.angular.dev/angular-v18-is-now-available-e79d5ac0affe)을 발표하며 변경 감지 라이브러리 추출, 홈페이지 개편, 그리고 SSR 기능 강화를 진행했습니다. 11월에는 [v19](https://blog.angular.dev/meet-angular-v19-7b29dfd05b84)가 [릴리스](https://github.com/angular/angular/releases/tag/19.0.0)되며, 부족했던 렌더링 모드의 다양화, Hydration 제어 개선, 그리고 Signal 도입이라는 큰 변화를 선보였습니다.&amp;#xD;
        &amp;#xD;
        * [Google Angular Lead sees convergence in JavaScript Framework](https://thenewstack.io/google-angular-lead-sees-convergence-in-javascript-frameworks/)&amp;#xD;
        * [Managing Angular](https://blog.mgechev.com/2024/08/25/managing-angular/)&amp;#xD;
        &amp;#xD;
        ### Svelte&amp;#xD;
        &amp;#xD;
        Svelte는 [v5](https://svelte.dev/blog/svelte-5-is-alive) 릴리스를 통해 패러다임의 전환을 알렸습니다. 과거에는 `let count = 0`만 작성해도 Svelte의 컴파일러가 이를 상태로 처리했지만, 이제는 더 명시적으로 상태를 선언하는 방식으로 변화했습니다. 새로운 메커니즘인 [runes](https://svelte.dev/blog/runes)가 도입되었으며, `$state()`를 통해 반응형 상태를 명확히 관리할 수 있게 되었습니다.&amp;#xD;
        &amp;#xD;
        ### Astro&amp;#xD;
        &amp;#xD;
        Astro는 최근 [v5](https://astro.build/blog/astro-5/) 업데이트에서 타입 안전 환경 변수 설정을 지원하며 환경 변수의 가시성과 안전성을 강화했습니다. 또한 Content Layer API를 통해 다양한 데이터 소스에서 가져온 콘텐츠를 타입 안전하게 관리할 수 있게 되었으며, 렌더링 기능을 강화해 페이지와 컴포넌트별로 SSR과 정적 생성을 유연하게 선택할 수 있게 했습니다.&amp;#xD;
        &amp;#xD;
        ### Hono&amp;#xD;
        &amp;#xD;
        [Hono](https://hono.dev/)는 모든 JavaScript Runtime에서 실행 가능한 경량 웹 서버 프레임워크로, 빠른 라우팅을 지원하는 RegExpRouter 개념과 여러 헬퍼, 미들웨어로 주목 받고 있습니다.&amp;#xD;
        &amp;#xD;
        ### Waku&amp;#xD;
        &amp;#xD;
        [Waku](https://waku.gg/)는 Zustand와 Jotai 진영에서 개발한 리액트 기반 풀스택 프레임워크로, React의 Server Action API를 지원하는 [v0.21](https://waku.gg/blog/server-actions-are-here)을 출시했습니다. 중규모 애플리케이션 개발을 목표로 하며, Next.js의 대안으로 기대를 받고 있습니다.&amp;#xD;
        &amp;#xD;
        ## 다양한 렌더링 모드&amp;#xD;
        &amp;#xD;
        최근 프론트엔드 프레임워크들은 다양한 렌더링 방식을 제공하여 각자의 장점을 활용한 최적화 전략을 선보이고 있습니다.&amp;#xD;
        CSR(Client-Side Rendering)은 동적이고 빠른 사용자 경험(UX)을 제공하지만, 초기 HTML이 서버에서 제공되지 않기 때문에 SEO에는 다소 불리합니다.&amp;#xD;
        이에 반해, SSR(Server-Side Rendering)과 SSG(Static-Site Generation)은 초기 페이지 로드 시 완전한 HTML을 제공해 SEO와 성능 최적화에 유리합니다. Astro는 [Islands Architecture](https://jasonformat.com/islands-architecture)를 활용해 정적 콘텐츠와 인터랙티브 요소를 분리하고, Server Islands를 통해 필요한 동적 콘텐츠만 효율적으로 로드합니다. 또한, Next.js는 [PPR(Partial Pre-Rendering)](https://vercel.com/blog/partial-prerendering-with-next-js-creating-a-new-default-rendering-model)을 통해 동적 콘텐츠를 스트리밍 하며 점진적 렌더링 방식을 선도하고 있습니다.&amp;#xD;
        이런 다양한 렌더링 방식들은 앱의 목적과 성능 요구 사항에 따라 적절히 선택할 필요가 있습니다.&amp;#xD;
        &amp;#xD;
        * [Client-Side Rendering](https://github.com/theninthsky/client-side-rendering)&amp;#xD;
        * [What&apos;s a Single-Page App?](https://jakelazaroff.com/words/whats-a-single-page-app/)&amp;#xD;
        * [How to choose the best rendering strategy for your app](https://vercel.com/blog/how-to-choose-the-best-rendering-strategy-for-your-app)&amp;#xD;
        &amp;#xD;
        ## JavaScript&amp;#xD;
        &amp;#xD;
        ### Signals&amp;#xD;
        &amp;#xD;
        ECMAScript 위원회인 TC39에 [Signal 도입 제안](https://github.com/tc39/proposal-signals)이 올라왔습니다. [Preact](https://preactjs.com/), [Solid](https://solidjs.com/)와 같은 프레임워크는 이미 Signal을 활용하여 Reactivity를 구현하고 있습니다.&amp;#xD;
        Signal은 애플리케이션 상태를 관리하는 방식으로, 상태 변경 시 UI를 업데이트합니다. React의 상태 변경이 전체 리렌더링을 유발하는 반면, Signal은 세분화된 상태 변경을 처리하여 렌더링 없이 최소한의 작업으로 UI를 갱신합니다. 이 제안을 통해 여러 프레임워크에서 중복된 Reactivity 모델을 제거하고, 개발 도구에서도 Signal의 활용이 기대됩니다.&amp;#xD;
        &amp;#xD;
        * [How to Build Signals from Scratch](https://www.freecodecamp.org/news/learn-javascript-reactivity-build-signals-from-scratch/)&amp;#xD;
        &amp;#xD;
        ### Temporal&amp;#xD;
        &amp;#xD;
        새로운 날짜 및 시간 처리 기능인 Temporal이 [Stage 3](https://github.com/tc39/proposal-temporal)에 도달했습니다. 기존 JavaScript의 `Date` 객체가 가진 문제들을 해결하기 위해 설계되었으며, 개발자들이 보다 직관적이고 강력한 시간 처리를 할 수 있도록 도울 것입니다.&amp;#xD;
        &amp;#xD;
        * [JS Dates are about to be fixed](https://docs.timetime.in/blog/js-dates-finally-fixed)&amp;#xD;
        &amp;#xD;
        ### ECMAScript 2024&amp;#xD;
        &amp;#xD;
        2024년 ECMAScript 언어 사양이 승인되면서 새로운 기능들이 추가되었습니다. 주요 기능으로는 `Promise.withResolvers()`, `Object.groupBy`, `Map.groupBy` 등이 있습니다. 과거처럼 대규모 신규 개념이 추가되기보다는 기존 기능의 개선과 최적화가 주를 이루는 모습입니다.&amp;#xD;
        &amp;#xD;
        * [ECMAScript 2024](https://2ality.com/2024/06/ecmascript-2024.html)&amp;#xD;
        * [What&apos;s new in ECMAScript 2024](https://pawelgrzybek.com/whats-new-in-ecmascript-2024/)&amp;#xD;
        * [The state of ES5 on the web](https://philipwalton.com/articles/the-state-of-es5-on-the-web/)&amp;#xD;
        &amp;#xD;
        ### 기타&amp;#xD;
        &amp;#xD;
        TC39는 기존에 JavaScript 표준 프로세스에서 Stage를 [0~4 단계](https://tc39.es/process-document/)로 유지했으나, 최근 [Stage 2.7](https://thenewstack.io/inside-ecmascript-javascript-standard-gets-an-extra-stage/)을 추가했습니다. 이는 Stage 3에서 Stage 2로 롤백될 경우 발생하는 리소스를 줄이고, 작업 효율성을 향상시키기 위한 조치로 보입니다.&amp;#xD;
        &amp;#xD;
        ## CSS&amp;#xD;
        &amp;#xD;
        ### CSS-in-JS&amp;#xD;
        &amp;#xD;
        CSS-in-JS는 SSR의 활성화에 따라 한동안 인기를 잃어 왔지만 React v19의 새로운 기능으로 다시 주목을 받고 있습니다. 특히 스타일 호이스팅(precedence 속성을 사용해 `&amp;lt;style&amp;gt;` 태그를 문서 헤드로 이동)을 지원하며, 온디맨드 스타일 렌더링이 개선되었습니다. 이에 따라 [Restyle](https://www.restyle.dev/), [StyleX](https://stylexjs.com/blog/introducing-stylex), [PandaCSS](https://panda-css.com/)와 같은 라이브러리가 주목 받고 있으며, MUI는 [PigmentCSS의 도입](https://mui.com/blog/introducing-pigment-css)을 시도하고 있습니다. 또한 CSS-in-JS의 리소스 문제를 극복한 Compiled CSS-in-JS 방식도 주목을 받고 있습니다.&amp;#xD;
        &amp;#xD;
        * [CSS in React Server Components](https://www.joshwcomeau.com/react/css-in-rsc/)&amp;#xD;
        * [Why is CSS-in-JS slow?](https://playfulprogramming.com/posts/why-is-css-in-js-slow)&amp;#xD;
        * [A preview of Pigment CSS: the next generation of CSS-in-JS](https://mui.com/blog/introducing-pigment-css/)&amp;#xD;
        &amp;#xD;
        ### CSS 신규 기능&amp;#xD;
        &amp;#xD;
        2024년에는 CSS의 새로운 스펙들이 도입되며, 작년에 이어 [Baseline](https://web.dev/baseline)과 [Interop](https://web.dev/blog/interop-2024)과 같이 플랫폼 간 상호운용성에 대한 노력이 CSS 생태계가 더욱 강력하게 만들고 있습니다. `@property` 문법으로 CSS 변수를 선언하고 속성을 정의하거나, Popover API로 번거롭던 Modal 문제를 해결하는 등 개발자의 생산성을 크게 향상시키고 있습니다. 이번에는 2024년 도입된 주요 기능과 개인적으로 관심을 가졌던 최신 CSS 관련 링크를 공유 드립니다.&amp;#xD;
        &amp;#xD;
        * [CSS Container Queries](https://css-tricks.com/css-container-queries/)&amp;#xD;
        * [If CSS gets inline conditionals](https://css-tricks.com/if-css-gets-inline-conditionals/)&amp;#xD;
        * [CSS anchor API](https://developer.chrome.com/blog/anchor-positioning-api)&amp;#xD;
        * [Popover API lands in Baseline](https://web.dev/blog/popover-api)&amp;#xD;
        * [The undeniable utility of CSS `:has`](https://www.joshwcomeau.com/css/has/)&amp;#xD;
        * [CSS `@property` and the new style](https://ryanmulligan.dev/blog/css-property-new-style/)&amp;#xD;
        * [New CSS that can actually be used in 2024](https://thomasorus.com/new-css-that-can-actually-be-used-in-2024.html)&amp;#xD;
        * [Old Dogs, new CSS Tricks](https://mxb.dev/blog/old-dogs-new-css-tricks/)&amp;#xD;
        * [A Framework for evaluating browser support](https://www.joshwcomeau.com/css/browser-support/)&amp;#xD;
        * [CSS Wrapped 2024](https://chrome.dev/css-wrapped-2024/)&amp;#xD;
        * ▶️ [Amazing CSS in 2024](https://www.youtube.com/watch?v=D79TND9w_AY)&amp;#xD;
        &amp;#xD;
        ### Masonry 레이아웃&amp;#xD;
        &amp;#xD;
        오랫동안 웹 개발자들은 Pinterest 스타일의 Masonry 레이아웃을 구현하기 위해 JavaScript에 의존해왔습니다.  그러나 이제 CSS가 이 기능을 네이티브로 지원할 준비를 하고 있으며 이를 구현하는 방식에 대해 [Chrome팀](https://rachelandrew.co.uk/archives/2024/09/21/masonry-and-good-defaults)과 [Webkit팀](https://webkit.org/blog/15269/help-us-invent-masonry-layouts-for-css-grid-level-3/) 간에 새로운 `display`를 추가할지, `grid` 내에 통합할지 [논의](https://webkit.org/blog/16026/css-masonry-syntax/)가 이어지고 있습니다.&amp;#xD;
        &amp;#xD;
        ## Performance&amp;#xD;
        &amp;#xD;
        ### e18e&amp;#xD;
        &amp;#xD;
        JavaScript 생태계의 성능 향상을 목표로 한 커뮤니티 기반 이니셔티브 [e18e(Ecosystem Performance)](https://e18e.dev/)가 실질적인 진전을 이루며 주목 받고 있습니다. e18e는 다음 세 가지 목표를 중심으로 자바스크립트 생태계의 개선을 추진하고 있습니다.&amp;#xD;
        &amp;#xD;
        1. Clean up: 인기 있는 패키지의 중복된 의존성 제거 또는 대체를 통해 속도를 최적화.&amp;#xD;
        2. Speed up: 널리 사용되는 패키지와 프레임워크의 성능 개선.&amp;#xD;
        3. Level up: 오래된 패키지에 대해 더 가볍고 모던한 대안을 마련.&amp;#xD;
        &amp;#xD;
        이니셔티브의 노력은 커뮤니티의 참여로 이루어지며, 최신 패치 목록과 대안을 제안하는 [module-replacements](https://github.com/es-tooling/module-replacements)와 같은 결과물로 나타나고 있습니다.&amp;#xD;
        최근 공개된 [October Contribution Showcase](https://e18e.dev/blog/october-contributions-showcase.html)에서는 Storybook과 ESLint 같이 널리 사용되는 도구에 대한 기여가 눈에 띕니다. 이를 통해 e18e는 자바스크립트 생태계의 전반적인 성능을 끌어올리는 데 실질적인 역할을 하고 있으며, 커뮤니티와 오픈 소스 생태계의 협력을 이끌어내고 있습니다.&amp;#xD;
        e18e는 단순히 성능 개선을 넘어, 개발자들이 사용하는 핵심 도구를 더 가볍고 빠르게 만들어 생태계 전반에 긍정적인 영향을 미칠 것으로 기대됩니다.&amp;#xD;
        &amp;#xD;
        ### Web Vitals - INP&amp;#xD;
        &amp;#xD;
        [Web Vitals](https://web.dev/articles/vitals)는 Google이 정의한 웹 성능 지표로, 사용자 경험(UX)을 개선하기 위한 기준을 제공합니다. 2024년 정식으로 선정된 [INP(Interaction to Next Paint)](https://web.dev/articles/inp)는 사용자 상호작용 후 다음 화면이 렌더링될 때까지의 지연 시간과 응답성을 측정하며, 기존 FID(First Input Delay)보다 더 포괄적이고 실질적인 경험을 반영합니다. INP는 사용자 입력 이벤트를 우선 처리하고, 비동기 렌더링을 최적화하는 등 동시성 기술을 통해 개선할 수 있습니다.&amp;#xD;
        &amp;#xD;
        * [What is INP and why you should care](https://blog.sentry.io/what-is-inp/)&amp;#xD;
        * [Understanding Interaction to Next Paint(INP)](https://frontendmasters.com/blog/understanding-inp/)&amp;#xD;
        * [Investigating INP issues](https://www.stefanjudis.com/blog/investigating-inp-issues/)&amp;#xD;
        * [How to improve INP in React](https://kurtextrem.de/posts/improve-inp-react)&amp;#xD;
        * [How to Improve INP: Yield Patterns](https://kurtextrem.de/posts/improve-inp)&amp;#xD;
        * [Demystifying INP: New tools and actionable insights](https://vercel.com/blog/demystifying-inp-new-tools-and-actionable-insights)&amp;#xD;
        &amp;#xD;
        ## 기타&amp;#xD;
        &amp;#xD;
        ### Mobile&amp;#xD;
        &amp;#xD;
        모바일 개발 생태계에서도 큰 변화가 이어지고 있습니다. React Native는 [새로운 아키텍처](https://reactnative.dev/blog/2024/10/23/the-new-architecture-is-here)로 재작성되어 [v0.76](https://reactnative.dev/blog/2024/10/23/release-0.76-new-architecture)에서 동기식 네이티브 통신, 동시성 시스템, 새로운 이벤트 루프를 도입하며 성능과 안정성을 크게 개선했습니다. 한편, Google의 Flutter는 최근 조직 축소와 커뮤니티 대응 지연으로 비판을 받아 왔으며, 이에 대응해 커뮤니티가 Flutter를 포크한 [Flock](https://flutterfoundation.dev/blog/posts/we-are-forking-flutter-this-is-why)를 발표했습니다. Flock은 문제 해결에 집중하고 있지만 인원 부족으로 인상적인 성과를 내기 어려울 것이라는 전망도 있습니다. 이러한 변화들은 크로스 플랫폼 개발의 미래를 더욱 흥미롭게 만들고 있습니다.&amp;#xD;
        &amp;#xD;
        ### Architecture&amp;#xD;
        &amp;#xD;
        개인적으로 소프트웨어 설계 관련해 기억나는 포스트는 C4 모델을 응용한 Visualizing Frontend Architecture와 FSD(Feature-Sliced Design)입니다. C4 모델은 소프트웨어 시스템을 Context, Container, Component, Code의 4가지 레벨로 나누어 시각화하는 방식으로, 이를 프론트엔드에 적용해 아키텍처를 명확히 표현한 접근이 인상 깊었습니다.&amp;#xD;
        한편, FSD는 기능(feature) 단위로 관심사를 분리하여 모듈화된 폴더 구조를 설계하는 방식으로, 대규모 프로젝트의 유지 보수성과 확장성을 크게 향상시킵니다. 이러한 접근법들은 구조적 명확성을 제공하며, 올해 프론트엔드 설계에서 중요한 흐름으로 자리 잡았습니다.&amp;#xD;
        &amp;#xD;
        * ▶️ [Visualising software architecture with the C4 model](https://www.youtube.com/watch?v=x2-rSnhpw0g)&amp;#xD;
        * [Visualizing Frontend Architecture](https://frontendatscale.com/issues/17)&amp;#xD;
        * [Feature-Slided Design Pattern](https://feature-sliced.design/)&amp;#xD;
        * [FSD 관점으로 바라보는 코드 경계 찾기](https://velog.io/@teo/fsd)&amp;#xD;
        * [프론트엔드 개발자 관점으로 바라보는 관심사의 분리와 좋은 폴더 구조 (feat. FSD)](https://velog.io/@teo/separation-of-concerns-of-frontend)&amp;#xD;
        &amp;#xD;
        ### IT 뉴스&amp;#xD;
        &amp;#xD;
        * 미국 법무부는 2024년, ADA(미국 장애인 법)에 따라 웹 콘텐츠와 모바일 애플리케이션의 [접근성 규정을 발표](https://www.tpgi.com/the-ada-now-has-regulations-for-accessibility-of-web-content-and-mobile-apps/)했습니다. 이는 공공 기관이 운영하는 디지털 콘텐츠가 WCAG 2.1 레벨 AA 기준을 충족하도록 요구하며, 장애인을 위한 더 나은 디지털 접근성을 보장합니다. 민간 기업에도 간접적인 영향을 미칠 것으로 보입니다.&amp;#xD;
        * 영국 정부가 인도양 차고스 제도(British Indian Ocean Territory)의 주권을 포기하며 [.io 도메인이 사라질 예정](https://every.to/p/the-disappearance-of-an-internet-domain)입니다. 새로운 등록이 중단되며, 기존 도메인 역시 점진적으로 폐기 프로세스가 시작됩니다. .io 도메인을 사용하는 많은 기업에 영향을 미칠 것으로 보입니다. 하지만 .su 도메인이 특수 사례로 살아남은 적이 있어 어떤 식으로 흘러갈지는 지켜봐야 할 것 같습니다.&amp;#xD;
        * 올해 Sentry는 [페어 소스(Fair Source)](https://fair.io/)라는 새로운 소프트웨어 라이선스를 도입한다고 했습니다. 이는 오픈 소스와 유사하게 코드를 공개적으로 공유하지만, 제작자의 비즈니스 모델을 보호하기 위해 사용, 수정, 재배포에 제한을 두는 것이 특징입니다. 또한 지연된 오픈소스 퍼블리싱(DOSP) 방식으로 초기에는 독점 라이선스를 유지하다가 일정 계획에 따라 오픈소스화합니다.&amp;#xD;
        &amp;#xD;
        ***&amp;#xD;
        &amp;#xD;
        막상 정리를 해보겠다고 마음먹고 주요 기술 스택과 뉴스를 간추리는 데 생각보다 시간이 오래 걸렸습니다. 내년에도 같은 작업을 이어간다면 반기로 나누어 진행해야 할 것 같습니다. 처음에는 2~3개의 뉴스레터로 시작했지만, 욕심이 생겨 점점 범위를 넓히다 보니 지금은 UI/UX와 디자인까지 포함해 매주 약 19개의 뉴스레터를 훑고 있는 상황입니다. &amp;#xD;
        &amp;#xD;
        뉴스레터로 알게 된 내용을 실제로 제품에 적용해 볼 기회는 많지 않았지만, 프론트엔드 트렌드를 파악하고, 우리 팀과 조직에서 적용해 볼 만한 부분을 점검할 수 있었다는 점에서 개인적으로 만족스러웠습니다. 본 글에서 다루지 못했지만 JavaScript의 AI, Privacy 강화, WASM과 같은 주제에도 변화가 있었습니다. 관심 있는 분들은 관련 내용을 찾아보시는 것도 추천드립니다.&amp;#xD;
        &amp;#xD;
        앞으로도 새롭게 등장할 기술들 속에서 각자의 서비스에 적합한 것을 선택하고 도입하며, 변화에 유연하게 대응해 나가길 바랍니다. &amp;#xD;
        (혹시라도 깨진 링크나 잘못된 내용이 있다면 제보 부탁드립니다. )&amp;#xD;
        &amp;lt;br&amp;gt;&amp;#xD;
        [![NHN Cloud_meetup banner_footer_blue_202412_900.png](https://image.toast.com/aaaadh/real/2024/techblog/NHN%20Cloudmeetup%20bannerfooterblue202412900.png)](https://www.nhncloud.com/kr)
      </content:encoded>
    </item>
    <item>
      <title>Spring Boot 버전업 중 알게된 Java 버전별 캡슐화 정책 강화</title>
      <link>http://thefarmersfront.github.io/blog/75-java-module-with-gson-serialization/</link>
      <guid>http://thefarmersfront.github.io/blog/75-java-module-with-gson-serialization/</guid>
      <pubDate>Fri, 06 Dec 2024 14:00:00 GMT</pubDate>
      <content:encoded>자바 모듈 시스템의 변화로 인한 직렬화 문제를 분석하면서 알게된 내용을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>분산 시스템 환경에서 Kafka Consumer 오프셋 이동하기</title>
      <link>http://thefarmersfront.github.io/blog/2024-spring-kafka-consumer-offset-seeking/</link>
      <guid>http://thefarmersfront.github.io/blog/2024-spring-kafka-consumer-offset-seeking/</guid>
      <pubDate>Mon, 02 Dec 2024 10:30:00 GMT</pubDate>
      <content:encoded>Spring Kafka 활용한 오프셋 이동 및 메시지 재처리 방법</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드에서 테스트 데이터를 생성하는 방법 (feat. LLM)</title>
      <link>https://blog.banksalad.com/tech/how-banksalad-testdata/</link>
      <guid>https://blog.banksalad.com/tech/how-banksalad-testdata/</guid>
      <pubDate>Mon, 18 Nov 2024 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드 QA팀 Tech Lead Manager…</content:encoded>
    </item>
    <item>
      <title>OpenInfra Asia Summit 2024 돌아보기</title>
      <link>https://meetup.nhncloud.com/posts/389</link>
      <guid>https://meetup.nhncloud.com/posts/389</guid>
      <pubDate>Mon, 11 Nov 2024 02:16:45 GMT</pubDate>
      <content:encoded>
        ![1.jpg](https://image.toast.com/aaaadh/real/2024/techblog/1.jpg)&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        &amp;gt; 본 콘텐츠는 OpenInfra Foundation의 공식 블로그 [Superuser](https://superuser.openinfra.dev/articles/openinfra-asia-summit-2024-recap/)에 영문본이 게시되었습니다.&amp;#xD;
        &amp;lt;br/&amp;gt;&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        지난 9월 3일 개최된 오픈인프라 아시아 서밋(OpenInfra Summit Asia) 2024에 참여하는 좋은 기회를 얻게 되었습니다. 오픈인프라 아시아 서밋은 아시아 전역의 오픈소스 커뮤니티를 지원하기 위해 2023년 설립된 지역 허브인 [오픈인프라 아시아(OpenInfra Asia)](https://openinfraasia.org/)가 개최하는 첫 번째 서밋으로 그 자체로 매우 의미 있는 행사였습니다. 이번 서밋에는 앤트그룹, 화웨이 등 아시아 지역 유수의 기업이 참여했는데요. 그 중 [NHN Cloud](https://www.nhncloud.com/kr)도 아시아 지역의 핵심 클라우드 서비스 기업으로서 오픈스택 기술력과 그간의 커뮤니티 활동을 인정받아 오픈인프라 아시아의 창립 멤버로 초대되었다고 합니다.&amp;#xD;
        &amp;#xD;
        또한 이번 행사는 주요 오픈 소스 재단인 [Open Compute Project(OCP)](https://www.opencompute.org/) 재단과 공동으로 주최되어, 두 글로벌 오픈 소스 커뮤니티의 핵심 재단이 손잡은 만큼 규모도 크고 프로그램도 매우 다양하고 풍성하게 구성되었습니다.&amp;#xD;
        &amp;#xD;
        무려 240명이 넘는 연사가 190개 이상의 세션을 제공했으며, 리눅스, 오픈스택, Kubernetes 외 30개 이상의 오픈소스 프로젝트 등 다루는 주제도 무척 다양했습니다. 30개국이 넘는 국가에서 1500명이 넘는 참가자와 함께 저도 유익하고 인사이트가 풍부한 세션들을 들을 수 있었습니다.&amp;#xD;
        &amp;#xD;
        ![2.jpg](https://image.toast.com/aaaadh/real/2024/techblog/2%281%29.jpg)&amp;#xD;
        귀중한 정보를 담은 세션들이 주로 영어와 한국어로 제공되었으며 영어 세션이 다수를 이루었습니다. 따라서 영어가 익숙지 않은 청중들을 위해 flitto 동시번역 서비스로 16개국 이상의 언어를 제공한다는 점이 무척 흥미로웠습니다. 세션 룸에 동시번역 서비스 QR 코드가 배치되어있어, 앱 다운없이 손쉽고 간편하게 접근할 수 있었습니다.&amp;#xD;
        &amp;#xD;
        많은 참가자들이 자신의 패드에서 번역 서비스를 통해 연사가 말하자마자 매끄럽게 번역된 콘텐츠를 접할 수 있었습니다.&amp;#xD;
        &amp;#xD;
        ## Keynotes&amp;#xD;
        &amp;#xD;
        ![3.jpg](https://image.toast.com/aaaadh/real/2024/techblog/3%281%29.jpg)&amp;#xD;
        오픈 인프라 재단의 최고운영책임자(COO)인 Mark Collier의 키노트가 무척 인상 깊었는데요. 인프라 전반의 트렌드 4가지를 일목요연하고 간결하게 정리해 주었습니다.&amp;#xD;
        &amp;#xD;
        ### Digital Sovereignty&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        내 데이터가 어디에 저장되고 누가 접근 가능하며 어떤 법률의 지배를 받는지에 대한 관심이 커지고 중요한 사안이 되었는데요. 이는 개인에 국한되지 않고, 국가 기관 및 정부에게도 중요한 사안이 되었습니다.&amp;#xD;
        대표적인 예로 프랑스의 주요 은행들이 오픈 스택을 채택해 자신들의 데이터 위치와 접근 권한, 적용되는 법률을 직접 관리하고 있습니다. 이렇게 주요 기관들이 자신들의 데이터를 매우 독점적으로 처리하고 보유하고자하는 트렌드는 전 세계적으로 나타나고 있습니다.&amp;#xD;
        &amp;#xD;
        이 같은 트렌드는 하드웨어 영역에서도 나타나고 있는데요. RISC-V가 그 예입니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; RISC-V는 2010년 UC 버클리에서 개발한 오픈소스 RISC(Reduced Instruction Set Computer) 명령어 세트 아키텍처입니다. RISC-V 아키텍처를 통해 설계자는 최종 애플리케이션에 맞게 프로세서를 맞춤화하고 설계할 수 있습니다.&amp;#xD;
        &amp;#xD;
        즉 현재는 기존에 사용하는 프로그램이나 하드웨어에 대해 영구적인 접근 권한을 갖고 통제하고 원하는 대로 사용하고 싶어 하는 산업 전반의 트렌드가 있다고 합니다. 이 같은 트렌드로 오픈 소스 및 오픈 테크놀로지는 그 어느 때보다 중요해졌다고 합니다.&amp;#xD;
        &amp;#xD;
        ### License Changes&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        * Terraform&amp;#xD;
        예상치 못한 라이선스 변경으로 시장에 악영향을 미쳤지만, 오픈소스가 이에 대한 해결책을 제시해 줄 수 있습니다. Terraform의 라이선스 변경으로 인해 오픈 소스 프로젝트인 Open Tofu가 등장하여 테라폼을 대체하는 역할을 수행해 사용자들에게 신뢰를 제공하고 있습니다.&amp;#xD;
        &amp;#xD;
        * VMware&amp;#xD;
        VMware의 라이선스가 변경됨에 따라 많은 사용자들이 VMware에서 OpenStack으로 마이그레이션에 대한 관심이 커지고 있습니다. 대표적으로 미국의 주요 자동차 보험사인 GEICO가 최근 VMware를 버리고 오픈스택으로 대규모 클라우드 인프라를 구축해 큰 주목을 받았다고 합니다. Mark Collier는 마이그레이션에 관한 백서를 행사 당일에 [QR](https://www.openstack.org/vmware-migration-to-openstack-white-paper)로 공개하기도 했습니다.&amp;#xD;
        &amp;#xD;
        ### Security Concerns&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        최근 운영 중인 컨테이너 이미지의 87%에 치명적이거나 심각도가 높은 취약점이 있다는 사실이 밝혀져 우려를 자아내고 있습니다. 이 문제를 해결하기 위한 방안으로 오픈 인프라 재단에서 주최하는 카타 컨테이너(Kata Container) 프로젝트가 큰 주목을 받고 있습니다. 카타 컨테이너는 컨테이너의 속도와 가상 머신의 보안을 결합한 경량 가상화를 제공함으로써 속도와 보안 사이의 균형을 제공합니다. 컨테이너 환경 보안에 효과적이라는 점에서 Microsoft Azure, NVIDIA, AWS 등 주요 기업들이 카타 컨테이너에 투자하고 지원하고 있습니다.&amp;#xD;
        &amp;#xD;
        ### AI Redefining Infra&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        AI에 대한 기업들의 관심이 이례적입니다. 너도 나도 할 것 없이 기업들은 GPU를 최대한 많이 확보해 방대한 규모로 자신의 데이터 센터에 구축하고 있습니다. 기업이 AI 용량 구축에 거대한 투자하고 있으며, 오픈스택이 AI 워크 로드를 지원하는 데 중요한 역할을 하고 있습니다.&amp;#xD;
        &amp;#xD;
        이렇게 디지털 주권, 라이선스 변경, 보안 문제, 인공지능과 같이 네 가지 주요 트렌드로 오픈 소스에 대한 관심과 투자가 그 어느 때보다도 활발하게 일어나고 있고 오픈 소스 커뮤니티의 성장을 이끌고 있다고 합니다.&amp;#xD;
        &amp;#xD;
        ## Sessions&amp;#xD;
        &amp;#xD;
        오픈인프라 아시아 서밋 2024에는 다양한 주제에 대해 심도 있는 내용을 다루는 세션이 많았는데요. 저는 그중 NHN Cloud 인프라서비스개발랩 박성우 이사님이 발표하신 세션 **Openstack of NHN Cloud from a network perspective**을 통해 오픈스택의 실제 적용 사례와 오픈스택의 한계점을 어떻게 보완했는지를 배울 수 있었습니다. 아래는 세션 내용의 일부를 요약해 보았습니다.&amp;#xD;
        &amp;#xD;
        ### 1. Openstack of NHN Cloud from a network perspective&amp;#xD;
        &amp;#xD;
        ![4.jpg](https://image.toast.com/aaaadh/real/2024/techblog/4%281%29.jpg)&amp;#xD;
        &amp;#xD;
        &amp;gt;  [영상 보러 가기](https://www.youtube.com/watch?v=IgXJq8jmuJI&amp;amp;t=1s)&amp;#xD;
        &amp;#xD;
        NHN Cloud는 2015년에 OpenStack의 Neutron 모듈을 활용해 네트워크를 구축하고 서비스를 시작했습니다. 하지만 서비스 초기 단계에서는 Neutron 모듈의 기본 기능만으로는 NHN Cloud의 서비스들을 효율적이고 안정적으로 운영하는 데 한계가 있었다고 합니다. 이중화가 불가능하고 장애 조치 및 스케일업 기능이 지원되지 않았기 때문입니다. 이번 세션에서는 NHN Cloud가 이러한 문제를 어떻게 해결했는지에 대해 자세히 다뤘습니다.&amp;#xD;
        &amp;#xD;
        Neutron의 기본 구조는 컴퓨트 노드 안에 큐라우터와 OVS integration bridge, 그리고 그 사이에 위치한 리눅스 브릿지로 구성됩니다. 여기에 IP 테이블을 연결하여 보안 규칙(Security Rules)을 설정하게 됩니다. 하지만 이 구조는 보안 규칙이 많아질수록 코드 오류의 원인을 파악하고 문제를 분석하는 데 어려움을 겪게 되며 유지 보수에 큰 부담이 따랐다고 합니다. 더구나 리눅스 브릿지는 OSI 모델의 2계층에서만 동작하기 때문에, 라우팅이나 IP 주소를 기반으로 트래픽을 처리하는 데도 한계가 있었습니다.&amp;#xD;
        &amp;#xD;
        ![5.png](https://image.toast.com/aaaadh/real/2024/techblog/5.png)&amp;#xD;
        &amp;#xD;
        이러한 문제를 해결하기 위해 NHN Cloud는 컴퓨트 노드에서 리눅스 브릿지와 큐라우터를 제거하고, OVS integration bridge로 대체하는 방식을 채택했습니다. 네트워크를 VxLAN마다 나누고 각 브릿지와 연결하는 방식으로 구성하여 더 효율적인 네트워크 구조를 구현했습니다. 또한, NVIDIA의 SR-IOV Representer를 OVS bridge와 연결해 I/O 성능을 대폭 개선한 점이 인상 깊었습니다.&amp;#xD;
        &amp;#xD;
        &amp;gt; SR-IOV는 하나의 물리적 PCI Express 장치를 여러 가상 머신이 동시에 사용할 수 있게 해주는 기술로, 가상화 환경에서 매우 유용한 기술입니다.&amp;#xD;
        &amp;#xD;
        앞서 컴퓨트 노드에서 큐라우터를 제거했다고 했는데요. 이는 큐라우터의 한계를 극복하기 위한 조치로 랙 상단으로 이동시켰습니다. 즉, 각 랙이 하이퍼바이저처럼 작동하게 되어 컴퓨트 노드의 구조가 단순화되고, 최대 효율을 추구할 수 있었습니다.&amp;#xD;
        &amp;#xD;
        하지만 이러한 구조에서는 모든 트래픽이 랙 상단의 라우터로 집중되는 문제가 발생했습니다. 이를 해결하기 위해 NHN Cloud는 vSwitch를 개발했으며, 이 vSwitch는 5mpps(초당 500만 패킷)의 뛰어난 처리 속도를 자랑합니다.&amp;#xD;
        &amp;#xD;
        또한, VLAN에서 VxLAN으로 전환한 이유도 흥미로웠는데, 이는 고객 수가 증가함에 따라 퍼블릭 환경에서 여러 VPC(Virtual Private Cloud)를 생성해야 했기 때문입니다.&amp;#xD;
        &amp;#xD;
        ### 보안과 안정성&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        보안과 안정성 측면에서도 다양한 개선이 이루어졌습니다. 기본적으로 Neutron이 제공하는 Security Groups와 함께 Network ACL을 구성하여, 서버가 클라이언트 상태 정보를 저장하지 않아도 통신할 수 있는 환경을 구축했습니다. 또한, Internet Gateway 없이도 원격 호스트와 통신할 수 있도록 VPN Gateway를 연결해 네트워크 통신의 유연성을 강화했습니다.&amp;#xD;
        &amp;#xD;
        ![6.png](https://image.toast.com/aaaadh/real/2024/techblog/6.png)&amp;#xD;
        &amp;#xD;
        NHN Cloud는 2015년 서비스 시작 이후, OpenStack Neutron에 다양한 기능을 추가하기 위해 여러 플러그인과 자체 개발한 에이전트들을 도입해왔는데요. 세션을 통해 NHN Cloud가 기존의 네트워크 문제를 혁신적으로 해결하고, 서비스 안정성과 보안을 동시에 강화한 점을 직접 확인할 수 있었습니다. 동시에 클라우드 네트워크 관리의 복잡성을 체감할 수 있었습니다.&amp;#xD;
        &amp;#xD;
        &amp;lt;br/&amp;gt;&amp;#xD;
        &amp;#xD;
        ### 2. Bridging the Gap Between Community and Contributing Orgs&amp;#xD;
        &amp;#xD;
        ![7.jpg](https://image.toast.com/aaaadh/real/2024/techblog/7.jpg)&amp;#xD;
        &amp;#xD;
        강의 형태로 진행되는 세션이 아닌 참석자들과 함께 자유롭게 토의하는 포럼 형태의 세션도 제공되었습니다. 그중 하나인 **Bridging the Gap Between Community and Contributing Orgs**은 오픈 소스 커뮤니티를 더욱 활성화하기 위해 자유롭게 의견을 공유하는 자리였습니다. 주요 논의는 신규 기여자와 기존 기여자가 모두를 위한 커뮤니케이션을 활성화하고 기여자 경험을 개선하기 위한 방안을 논의하는 세션이었습니다.&amp;#xD;
        &amp;#xD;
        다양한 국가의 수많은 사람들이 오픈인프라 프로젝트에 기여하고 있는 만큼 커뮤니케이션의 한계를 극복하고 다양성을 높이고자하는 열정이 느껴지는 세션이었습니다.&amp;#xD;
        &amp;#xD;
        ## OpenStack’s Role in the Future&amp;#xD;
        &amp;#xD;
        &amp;#xD;
        오픈인프라 서밋 아시아 2024는 클라우드 산업의 혁신을 선도하는 오픈소스 커뮤니티의 중요성을 다시 한 번 강조한 행사였습니다.&amp;#xD;
        &amp;#xD;
        특히 VMware에서 오픈스택으로의 마이그레이션이 주목을 받으면서 확장 가능하고, 안전하며, 비용 효율적인 솔루션으로 오픈스택을 도입하려는 기업들이 전 세계적으로 많다는 것을 느낄 수 있었습니다.&amp;#xD;
        &amp;#xD;
        또한, AI와 같은 고성능 컴퓨팅을 위한 지속 가능한 인프라에 대한 요구가 커지고 있는 상황에서, 오픈인프라 커뮤니티는 혁신적이고 획기적인 솔루션으로 이러한 글로벌 과제에 대응할 준비가 충분히 갖추어져 있음을 확인할 수 있었습니다. 이번 서밋을 통해 오픈소스 기반 인프라의 미래와 지속 가능한 기술 개발에 대한 기대감이 더욱 커졌습니다.
      </content:encoded>
    </item>
    <item>
      <title>테크스펙은 문서가 아니다</title>
      <link>https://blog.banksalad.com/tech/techspec-is-not-doc/</link>
      <guid>https://blog.banksalad.com/tech/techspec-is-not-doc/</guid>
      <pubDate>Mon, 11 Nov 2024 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드에서 Tech Lead…</content:encoded>
    </item>
    <item>
      <title>컬리의 새로운 배송 시스템 구축 과정과 우리가 배운점</title>
      <link>http://thefarmersfront.github.io/blog/2023-delivery-system/</link>
      <guid>http://thefarmersfront.github.io/blog/2023-delivery-system/</guid>
      <pubDate>Fri, 25 Oct 2024 14:00:00 GMT</pubDate>
      <content:encoded>컬리의 새로운 배송 시스템 구축 과정과 프로젝트에서 얻은 교훈을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>엑셀 업로드 중 발생한 Zip Bomb 에러 파헤치기! 🥊</title>
      <link>http://thefarmersfront.github.io/blog/74-excel-upload-zip-bomb/</link>
      <guid>http://thefarmersfront.github.io/blog/74-excel-upload-zip-bomb/</guid>
      <pubDate>Wed, 23 Oct 2024 14:00:00 GMT</pubDate>
      <content:encoded>Zip Bomb 에러 소개 및 해결 방법 공유</content:encoded>
    </item>
    <item>
      <title>하이버네이트의 시간은 거꾸로 간다</title>
      <link>http://thefarmersfront.github.io/blog/fix-hibernate-localtime-bug/</link>
      <guid>http://thefarmersfront.github.io/blog/fix-hibernate-localtime-bug/</guid>
      <pubDate>Wed, 16 Oct 2024 10:00:00 GMT</pubDate>
      <content:encoded>스프링부트 버전을 업그레이드하는 과정에서 발견된 버그 해결기</content:encoded>
    </item>
    <item>
      <title>Accelerating Coupang’s AI Journey with LLMs</title>
      <link>https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3?source=rss----fb028911af07---4</guid>
      <pubDate>Mon, 14 Oct 2024 16:25:07 GMT</pubDate>
      <content:encoded>&lt;p&gt;&lt;em&gt;By ML Platform Team&lt;/em&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Z3QK3ZJPdvvWZ0mTTDyLTQ.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;In the last couple of years, Coupang has been using machine learning [ML/AI] heavily to improve customer experiences in areas like search, ads, catalog and recommendations. ML drives important decision making in pricing, transportation and logistics.&lt;/p&gt;&lt;p&gt;Coupang’s ML engineer’s toolkit has also grown significantly from simple classical ML techniques to deep learning and now, large language models (LLMs) for generative AI in this short time frame. Coupang’s ML platform has been at the forefront of this journey focused on enabling ML engineers to train and serve models in a resource efficient way. In this blog we write about LLM explorations at Coupang and the technical challenges it poses on the ML platform.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;ML at Coupang&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;There are three main types of ML models trained with Coupang’s ML infrastructure:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Recommendation system models:&lt;/strong&gt; This is primarily used in personalization and recommendation surfaces such as main home feed, search and ads across Coupang apps for shopping, eats and play. These are trained on large datasets of user interactions (clicks, views, purchases, add to cart) and human labeled relevance judgements.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Content understanding models:&lt;/strong&gt; Coupang has a huge dataset of product catalog data (text, image), user generated content (text — reviews, queries), user and merchant data (text, image). Various ML teams across product groups use deep learning techniques to understand product, customer and merchant representation and then use it to improve shopping experience.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Forecasting models:&lt;/strong&gt; Coupang has over 100+ unique fulfillment centers housing millions of products. Predictive modeling is crucial in the pricing, logistics, delivery, and pricing of these products for our customers. While these models are typically statistical in nature, deep learning techniques are now increasingly being incorporated.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Foundation models (FM) are large deep learning models trained on massive datasets. FM can adapt to multiple tasks unlike traditional ML models that are trained for specific tasks. FM are trained on text datasets (large language models) or multi-modal (combining multiple modalities such as text and image). These models can learn powerful representations and generate contextually relevant content. The LLMs (and multi-modal) models have been used in several ways to improve existing ML models and improve customer experiences.&lt;/p&gt;&lt;p&gt;Training and serving LLMs come with significant challenges on ML infrastructure — hardware resources (compute, storage and networking), efficiently scaling training and inference. In this article we describe the applications of LLMs and our key learnings from ML infrastructure related challenges.&lt;/p&gt;&lt;h4&gt;Applications&lt;/h4&gt;&lt;p&gt;Coupang’s largest presence is in South Korea and Taiwan. Training data for most ML tasks is relatively small in both Korean and Mandarin. Moreover, Coupang has a vibrant marketplace with global sellers from around the world selling to its customers. These pose unique challenges in several problem areas of e-commerce — especially seller and product understanding in different languages and images with embedded text, customer intent while they are searching. We describe three areas of application inside Coupang.&lt;/p&gt;&lt;h4&gt;Image &amp;amp; Language Understanding&lt;/h4&gt;&lt;p&gt;Coupang has large datasets of product and ads images along with corresponding metadata, which includes titles, descriptions, and user queries. The strategy of jointly modeling image and text data through vision and language transformer models yields superior embeddings, as opposed to learning embeddings separately. These embeddings are then used in various downstream models for more effective ad retrieval, similarity search, and serve as features in recommendation models.&lt;/p&gt;&lt;p&gt;Apart from these, there have been other successful applications of large models in content understanding inside Coupang:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Translating product titles from Korean to Mandarin&lt;/li&gt;&lt;li&gt;Improving image quality in shopping feed&lt;/li&gt;&lt;li&gt;User review summarization&lt;/li&gt;&lt;li&gt;Keyword generation for products and sellers&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Generating Weak Labels at Scale:&lt;/h4&gt;&lt;p&gt;Obtaining labels created by humans is often a challenging and costly task. This issue is magnified when dealing with multilingual content, such as English, Korean, and Mandarin in the context of Coupang.&lt;/p&gt;&lt;p&gt;However, LLMs present a solution to this problem. They have the ability to produce labels for text-based content on a large scale, with a quality that rivals that of human annotators. Once these generated labels pass some quality checks, they can serve as weak supervision labels for training various models.&lt;/p&gt;&lt;p&gt;The labels generated by LLMs are particularly useful when starting models for new segments where there’s a shortage of high-quality labels. Internal experiments have shown that these weak labels can enhance the quality of relevance models and have the potential of overcoming the challenges of label scarcity in under-resourced languages.&lt;/p&gt;&lt;h4&gt;Categorization &amp;amp; Attribute Extraction&lt;/h4&gt;&lt;p&gt;In the domain of product categorization and attribute extraction, the traditional approach involved deploying a single ML model for each category. This was necessitated by the fact that an unified or multi-class model often yielded noisy predictions for tail categories. However, this imposed an increased operational burden as teams were required to manage multiple models. LLMs provided a deeper understanding of product data (title, description, reviews, seller info). This resulted in a single LLM powered categorizer for all categories with gains in precision across most categories.&lt;/p&gt;&lt;h4&gt;Choice of Model Architectures&lt;/h4&gt;&lt;p&gt;This strategy of taking OSS model architectures and fine-tuning them with domain data provides an effective approach to apply LLMs to business problems. It allows ML teams to leverage state of the art pre-trained models and efficient architectures, saving both time and computational resources.&lt;/p&gt;&lt;p&gt;Naturally, the main interest has been around models which show strong multilingual performance specially in CJK (Chinese, Japanese and Korean) languages. Training can differ due to the unique characteristics of these languages. Key differences include the use of spacing, the character-based nature of these languages as opposed to the word-based structure of English, and the larger vocabulary sizes. Each of these factors influences the tokenizer, which in turn, affects the quality of the language model. For language/NLP tasks the most commonly used models have been based on Qwen [1.1], LLAMA [1.2], T5 [1.3], Phi[1.4] and Polyglot [1.5] amongst others. Parameter sizes ranging from 3B to 20B are favored because they strike a good balance between resource and compute efficiency and quality.&lt;/p&gt;&lt;p&gt;For image-text multi-modal models, CLIP [1.6] (Contrastive Language Image Pretraining) and TrOCR [1.7] (Transformer based OCR) were the model architectures of choice for their efficiency and performance.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/773/1*JgeMerV5mS4Ja_PlkJyseA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Patterns of Using LLMs&lt;/h4&gt;&lt;p&gt;There are a few commonly used patterns of using LLMs. We have arranged techniques in increasing order of resource requirements and complexity.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;In-context learning (ICL):&lt;/strong&gt; In this mode a pre-trained LLM is provided with a prompt or “context” to guide its answers for a specific task. This process does not involve any additional training, and the same model can be reused for different tasks with different prompts. This is very fast to set up and iterate, cheap — as it involves no training, and versatile as it can be used in several tasks. Internally, this remains one of the most popular ways of prototyping and evaluating usage of LLM in a product.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Retrieval Augmented Generation (RAG):&lt;/strong&gt; RAG is a technique where LLM generated responses are grounded with facts fetched from external sources (knowledge bases such as a corpus of documents, catalog of products, etc). Making the generation and retrieval components work seamlessly in real-time is nontrivial, leading to potential bottlenecks and errors.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Supervised fine-tuning (SFT):&lt;/strong&gt; This refers to further training an existing base LLM on small datasets to improve performance on a specific domain or task. A fine-tuned model on a high quality domain dataset often surpasses the base LLM performance.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Continued pre-training (CPT): &lt;/strong&gt;It refers to further pre-training of an existing base LLM on sizable datasets to improve generalized understanding of the model without focusing on any specific task. This is resource intensive but often produces the best results on downstream tasks like attribute extraction.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In-context learning and later supervised fine-tuning remains the most popular pattern of using LLM due to their flexibility and resource efficiency.&lt;/p&gt;&lt;h4&gt;Development Lifecycle &amp;amp; Challenges&lt;/h4&gt;&lt;p&gt;In this section we describe how developers write LLM training &amp;amp; inference pipelines.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Exploration phase:&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In the exploration phase, developers use small experiments to determine a list of model lines they want to try out further. Their main focus is on:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Model architecture&lt;/li&gt;&lt;li&gt;Model size — Return on Investment (ROI) of using larger sizes for example 70B+ variant vs &amp;lt; 10B parameter variant&lt;/li&gt;&lt;li&gt;Prompt templates — Changing prompt templates can change model outputs and therefore the performance&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ML infra components:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Most data preparation and processing is done with Apache Zeppelin [2.1] notebooks which delegate the tasks to underlying processing engines such as Spark [2.2] on Kubernetes.&lt;/li&gt;&lt;li&gt;Model architecture &amp;amp; prompt template explorations are done on GPU (or multi-GPU) containerized Jupyter notebooks [2.3].&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. Model training:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Based on the shortlist, developers use fine-tuning or pre-training from scratch depending on the compute budget, dataset size and comparing model performance.&lt;/li&gt;&lt;li&gt;Based on model performance on the application, developers finalize the model to put into production. There isn’t any process difference from the non-LLM model development lifecycle here. We will call it the source LLM model.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ML infra components:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We use Polyaxon [2.4] underneath for managing ML training lifecycle on Kubernetes.&lt;/li&gt;&lt;li&gt;LLM training at Coupang uses the model parallel training on Kubernetes distributed training operator for Pytorch (PytorchJob) [2.5].&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;3. Path to production:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For our workloads, we see developers use the following methods to go to production:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Distillation: Distill a smaller model from the trained source LLM. The smaller model is used in real-time inference.&lt;/li&gt;&lt;li&gt;Embedding: Embeddings can be exported from the LLMs and used in smaller models. We see this pattern being used in ranking problems.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ML infra components:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Batch and nearline inference on GPUs is the most popular way to extract predictions from the source LLMs at scale and then use it for distillation or as embeddings.&lt;/li&gt;&lt;li&gt;Developers use Ray + vLLM [2.6, 2.7] to write inference pipelines requiring both CPU and GPU processing.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/899/1*RT3-1cV7bDqVri74DELeNA.png&quot; /&gt;&lt;figcaption&gt;Figure 1: ML infra supporting LLM development workflows.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The key challenges in enabling our developers for LLM development workflows were:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Resource efficiency and management largely due to supply shortage and high cost of GPUs.&lt;/li&gt;&lt;li&gt;Capabilities of training and serving large models. Our training stack was not equipped for distributed training (especially model parallel). Before LLMs, our serving was entirely on CPUs which are too slow for the multi-billion parameter models.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We describe the key learnings and takeaways which enabled us to scale our ML stack for the challenges mentioned above.&lt;/p&gt;&lt;h4&gt;Choosing the Right Workhorse: Matching Appropriate GPU for the Workload&lt;/h4&gt;&lt;p&gt;Choice of GPUs: Large Language Models (LLMs) are both compute and memory intensive. When dealing with larger workloads for training and serving, we quickly realized that device memory constraints play a crucial role in both training and serving. The demand for large RAM GPUs, such as the Nvidia A100 &amp;amp; H100. GPUs offered by cloud vendors have a significant wait time. We conducted regular benchmarking with model-building teams to evaluate the price-to-performance ratio of different GPUs for each model line. For the training of models with more than 1 billion parameters in mixed precision mode, we utilized the A100–80 GB version. For testing and lightweight training purposes, we could employ a substantial quantity of A10G-24 [3.3] GB devices. Given that each LLM family is available in multiple parameter sizes, it is highly cost-effective to use a device with lower performance for testing smaller versions of the LLM.&lt;/p&gt;&lt;h4&gt;Hybrid &amp;amp; Multi-Region AI Clusters&lt;/h4&gt;&lt;p&gt;In response to the GPU supply shortage, we implemented a multi-region deployment strategy for our ML infrastructure. By leveraging cloud service clusters across various regions (Asia-Pacific &amp;amp; US), we ensure faster access to GPUs, mitigating the wait times that can disrupt execution plans. Additionally, we built an on-prem cluster to provision a significant portion of our computer, especially the higher-end Nvidia GPUs (such as A100/H100).&lt;/p&gt;&lt;p&gt;This hybrid arrangement has been instrumental in alleviating the shortage of GPUs from the cloud provider and reducing overall cost of training. However, it also presents its own set of challenges, such as ensuring consistent infrastructure (storage &amp;amp; networking) and developer experience.&lt;/p&gt;&lt;h4&gt;Embracing the Open Source - Frameworks &amp;amp; Tools:&lt;/h4&gt;&lt;p&gt;At Coupang, all ML training and inference is executed on managed containerized services. The clusters have access to distributed file system on both cloud and on-prem. This worked for LLM training and inference as well. For training and inference frameworks, we could leverage high-quality open-source projects to our advantage. We describe the key projects which helped in accelerating our journey below.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Model Parallel Training:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When it comes to LLM training, one of the key obstacles is the inability to fit the model into a single GPU RAM. As a result, the typical method of distributed training — data parallelism alone is insufficient. We support several training frameworks which implement the model sharding strategy, the most popular being DeepSpeed Zero [2.8] due to its quick setup time and availability of trainer recipes for the popular model architectures through hugging face hub. Developers internally experiment and share recipes with smart defaults for hyperparameters, such as the choice of optimizer, gradient accumulation, memory pinning, etc.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;GPU Inference:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Realtime model serving stack:&lt;/strong&gt; The compute-intensive nature of LLMs required the use of GPUs for serving. Our existing serving stack was not equipped for GPUs, prompting us to find an appropriate model serving engine. Nvidia Triton offers a containerized inference solution, complete with features such as dynamic batching, concurrent multi-model execution on GPUs, and compatibility with a broad range of backends. These features are vital for the efficient serving of large models. We do all realtime inference using Nvidia Triton [2.9] on AWS EKS.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Batch inference:&lt;/strong&gt; We also realized that batch inference plays a pivotal role in LLM explorations, as it is used to generate LLM responses for datasets post training. Batch inference often involves both GPU and CPU processing. For instance, text and image data preprocessing can be carried out in a distributed manner on CPU cores, while the primary model inference takes place on the GPU. After experimentation, we settled on Ray + vLLM which excels in managing this type of heterogeneous computing at scale.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Nearline inference: &lt;/strong&gt;Nearline inference combines the efficiency of batch inference (using small batches) and the responsiveness of being near real-time inference (within a certain time of event occurrence). ML systems in e-commerce applications have several content data streams (user and seller generated content, orders etc). Using LLMs in nearline inference mode helps teams to support diverse downstream applications with a smaller resource footprint.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Rapid Experimentation and Prototyping&lt;/h4&gt;&lt;p&gt;The LLM landscape is fast changing with frequent model releases, new state of the art techniques are introduced, and new performance benchmarks are broken. Changes are on all fronts — model architecture, training &amp;amp; inference frameworks, hardware, and optimization techniques. The best way to keep up is to experiment rapidly and learn from the failures.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Rapid experimentation with newer techniques gives you surprising wins and often a deeper understanding of existing toolkits. For example, through experimentation we observed that vLLM provided us nearly ~20x throughput improvement in multiple workloads with their kernel implementation.&lt;/li&gt;&lt;li&gt;Similarly, experimenting with techniques like offloading model parameters to CPU helped in creating recipes for fine tuning LLMs on more widely available GPU with less RAM. This unblocked developers to iterate on their training pipeline without being blocked on availability of high end GPUs.&lt;/li&gt;&lt;li&gt;With Nvidia H100s on block, we see significant opportunities with fp8 quantization and Nvidia’s transformer engine.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/691/1*qkwkfl2YVEvbYShoETjrGQ.png&quot; /&gt;&lt;figcaption&gt;Figure 2: Training and serving stack for rapid prototyping and experimentation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;Using LLMs have improved various production ML systems and shown promise in several areas including search &amp;amp; discovery, catalog, operations, and ads quality, amongst others. We expect more teams to use LLMs and similar model architectures in the coming quarters and ship wins for our customers.&lt;/p&gt;&lt;p&gt;We are continuously investing in our training to train larger models and improve resource efficiency of our GPU training and inference stacks. This involves optimization at all levels — hardware (compute, storage, networking), observability, frameworks (model and data sharded training, profiling, utilization).&lt;/p&gt;&lt;p&gt;If you are interested in working in ML infrastructure and product problems, do check out our ML positions at Coupang. (&lt;a href=&quot;https://www.linkedin.com/company/coupang/jobs/&quot;&gt;https://www.linkedin.com/company/coupang/jobs/&lt;/a&gt;).&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;We thank our partners in product ML teams, especially Search &amp;amp; Discovery for being the early adopters of LLMs in their applications and sharing the progress and pain-points.&lt;/p&gt;&lt;p&gt;We thank our Tech Infrastructure teams especially for their support in provisioning compute resources and cluster health.&lt;/p&gt;&lt;p&gt;&lt;em&gt;While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on &lt;/em&gt;&lt;a href=&quot;http://ir.aboutcoupang.com&quot;&gt;&lt;em&gt;ir.aboutcoupang.com&lt;/em&gt;&lt;/a&gt;&lt;em&gt; for information on our formal investment plans and product development strategies.&lt;/em&gt;&lt;/p&gt;&lt;h4&gt;References&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Model architectures:&lt;/em&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;QWEN: &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;https://huggingface.co/Qwen&lt;/a&gt;&lt;/li&gt;&lt;li&gt;LLAMA: &lt;a href=&quot;https://llama.meta.com/&quot;&gt;https://llama.meta.com/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;T5: &lt;a href=&quot;https://huggingface.co/docs/transformers/en/model_doc/t5&quot;&gt;https://huggingface.co/docs/transformers/en/model_doc/t5&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Phi: &lt;a href=&quot;https://huggingface.co/microsoft/phi-2&quot;&gt;https://huggingface.co/microsoft/phi-2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Polyglot: &lt;a href=&quot;https://github.com/EleutherAI/polyglot&quot;&gt;https://github.com/EleutherAI/polyglot&lt;/a&gt;&lt;/li&gt;&lt;li&gt;CLIP: &lt;a href=&quot;https://huggingface.co/docs/transformers/en/model_doc/clip&quot;&gt;https://huggingface.co/docs/transformers/en/model_doc/clip&lt;/a&gt;&lt;/li&gt;&lt;li&gt;TrOCR: &lt;a href=&quot;https://huggingface.co/docs/transformers/en/model_doc/trocr&quot;&gt;https://huggingface.co/docs/transformers/en/model_doc/trocr&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;Platform components:&lt;/em&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Zeppelin: &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;https://zeppelin.apache.org/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Spark: &lt;a href=&quot;https://spark.apache.org/&quot;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Jupyter notebook: &lt;a href=&quot;https://jupyter.org/&quot;&gt;https://jupyter.org/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Polyaxon: &lt;a href=&quot;https://polyaxon.com/&quot;&gt;https://polyaxon.com/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;PytorchJob: &lt;a href=&quot;https://www.kubeflow.org/docs/components/training/user-guides/pytorch/&quot;&gt;https://www.kubeflow.org/docs/components/training/user-guides/pytorch/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Ray: &lt;a href=&quot;https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html&quot;&gt;https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html&lt;/a&gt;&lt;/li&gt;&lt;li&gt;vLLM: &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;https://github.com/vllm-project/vllm&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Deepspeed zero: &lt;a href=&quot;https://www.deepspeed.ai/tutorials/zero/&quot;&gt;https://www.deepspeed.ai/tutorials/zero/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Nvidia Triton: &lt;a href=&quot;https://developer.nvidia.com/triton-inference-server&quot;&gt;https://developer.nvidia.com/triton-inference-server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2817d55004d3&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3&quot;&gt;Accelerating Coupang’s AI Journey with LLMs&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>컬리의 Virtual 물류 센터</title>
      <link>http://thefarmersfront.github.io/blog/picking-simulation/</link>
      <guid>http://thefarmersfront.github.io/blog/picking-simulation/</guid>
      <pubDate>Thu, 26 Sep 2024 10:00:00 GMT</pubDate>
      <content:encoded>Picking 공정 시뮬레이션의 구축부터 활용까지</content:encoded>
    </item>
    <item>
      <title>LLM Application 구축 도전기 (feat. 소중한 고객님들의 리뷰) - 1부</title>
      <link>http://thefarmersfront.github.io/blog/2024-review-llm-application/</link>
      <guid>http://thefarmersfront.github.io/blog/2024-review-llm-application/</guid>
      <pubDate>Wed, 25 Sep 2024 13:00:00 GMT</pubDate>
      <content:encoded>Prompt Engineering을 활용한 비정형 데이터 검수 실험</content:encoded>
    </item>
    <item>
      <title>이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 1편. BX</title>
      <link>https://blog.banksalad.com/tech/banksalad-saladgame-1/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-saladgame-1/</guid>
      <pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate>
      <content:encoded>“이걸 진짜 만든다고요? 🤯 ” 샐러드게임 탄생 배경 202…</content:encoded>
    </item>
    <item>
      <title>이걸 진짜 만든다고요? 세상에 없던 게임, 샐러드게임 | 2편. UX</title>
      <link>https://blog.banksalad.com/tech/banksalad-saladgame-2/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-saladgame-2/</guid>
      <pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>Vertex AI Search를 활용한 결과 없는 검색 개선하기</title>
      <link>http://thefarmersfront.github.io/blog/vertex-ai-search-NR/</link>
      <guid>http://thefarmersfront.github.io/blog/vertex-ai-search-NR/</guid>
      <pubDate>Wed, 07 Aug 2024 10:00:00 GMT</pubDate>
      <content:encoded>AI SaaS 도입으로 검색 경험을 개선한 사례 소개</content:encoded>
    </item>
    <item>
      <title>BigQuery와 Gemini로 리뷰 분석 업무 자동화하기</title>
      <link>http://thefarmersfront.github.io/blog/bigquery-gemini-review/</link>
      <guid>http://thefarmersfront.github.io/blog/bigquery-gemini-review/</guid>
      <pubDate>Thu, 25 Jul 2024 10:00:00 GMT</pubDate>
      <content:encoded>BigQuery에서 LLM 모델 Gemini Pro 활용법 소개</content:encoded>
    </item>
    <item>
      <title>서버리스에서 쿠버네티스로 - Airflow 운영 경험기</title>
      <link>http://thefarmersfront.github.io/blog/airflow-1/</link>
      <guid>http://thefarmersfront.github.io/blog/airflow-1/</guid>
      <pubDate>Tue, 09 Jul 2024 00:00:00 GMT</pubDate>
      <content:encoded>서버리스 Airflow를 쿠버네티스 환경으로 전환하며 경험한 삽질들</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 새로운 집(Home) 짓기 - 2편 | 완공편</title>
      <link>https://blog.banksalad.com/tech/building-brand-new-home-2/</link>
      <guid>https://blog.banksalad.com/tech/building-brand-new-home-2/</guid>
      <pubDate>Wed, 26 Jun 2024 00:00:00 GMT</pubDate>
      <content:encoded>4. 홈 어떤 과정으로 만들어졌나 4-…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 새로운 집(Home) 짓기 - 1편 | 기초공사</title>
      <link>https://blog.banksalad.com/tech/building-brand-new-home-1/</link>
      <guid>https://blog.banksalad.com/tech/building-brand-new-home-1/</guid>
      <pubDate>Wed, 26 Jun 2024 00:00:00 GMT</pubDate>
      <content:encoded>
        이제는 여러 사용자분들이 익숙해지셨을 뱅크샐러드의 홈 화면,
        
        그렇지만 2년 전의 뱅크샐러드에는 놀랍게도 ‘홈’이 없었다. 지금으로부터 2년 전인 202…
      </content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 새로운 집(Home) 짓기 - 3편 | 증축편</title>
      <link>https://blog.banksalad.com/tech/building-brand-new-home-3/</link>
      <guid>https://blog.banksalad.com/tech/building-brand-new-home-3/</guid>
      <pubDate>Wed, 26 Jun 2024 00:00:00 GMT</pubDate>
      <content:encoded>5. 출시 다음에는 ‘개선과 운영’ 홈탭을 출시하는 과정에서 팀 내에 많은 변동이 있었다. 커리어 개발・창업・이민 준비 등 다양한 이유로 팀원들은 회사를 떠났다. 초기 기획을 함께 이끌어주던 디자이너 동료도 떠나게 되었고, PM…</content:encoded>
    </item>
    <item>
      <title>데이터가 있었는데요, 아니 없어요</title>
      <link>http://thefarmersfront.github.io/blog/commit-mvcc-set-autocommit/</link>
      <guid>http://thefarmersfront.github.io/blog/commit-mvcc-set-autocommit/</guid>
      <pubDate>Thu, 13 Jun 2024 17:00:00 GMT</pubDate>
      <content:encoded>COMMIT, MVCC 그리고 SET AUTOCOMMIT</content:encoded>
    </item>
    <item>
      <title>함께 구매하면 좋은 상품이에요! - 장바구니 추천 개발기 2부</title>
      <link>http://thefarmersfront.github.io/blog/cart-recommend-model-development_second/</link>
      <guid>http://thefarmersfront.github.io/blog/cart-recommend-model-development_second/</guid>
      <pubDate>Mon, 27 May 2024 10:00:00 GMT</pubDate>
      <content:encoded>보완재 추천 모델을 서빙하기 위한 아키텍처 소개</content:encoded>
    </item>
    <item>
      <title>함께 구매하면 좋은 상품이에요! - 장바구니 추천 개발기 1부</title>
      <link>http://thefarmersfront.github.io/blog/cart-recommend-model-development/</link>
      <guid>http://thefarmersfront.github.io/blog/cart-recommend-model-development/</guid>
      <pubDate>Mon, 20 May 2024 13:00:00 GMT</pubDate>
      <content:encoded>보완재 추천 모델을 적용하고 성과를 거둔 사례 소개</content:encoded>
    </item>
    <item>
      <title>고객에게 뚜렷한 경험을: 컬리의 후기 이미지 처리 기술</title>
      <link>http://thefarmersfront.github.io/blog/kurly_review_image_detection/</link>
      <guid>http://thefarmersfront.github.io/blog/kurly_review_image_detection/</guid>
      <pubDate>Fri, 19 Jan 2024 09:20:48 GMT</pubDate>
      <content:encoded></content:encoded>
    </item>
    <item>
      <title>퀵메뉴로 비즈니스 널리 알리기 (feat. 전지적 디자이너 시점)</title>
      <link>http://thefarmersfront.github.io/blog/prod-design-quick-menu/</link>
      <guid>http://thefarmersfront.github.io/blog/prod-design-quick-menu/</guid>
      <pubDate>Thu, 21 Dec 2023 00:00:00 GMT</pubDate>
      <content:encoded>컬리에 이런 서비스도 있었다고?</content:encoded>
    </item>
    <item>
      <title>컬리가 상품을 고객에게 빠르게 전달하는 똑똑한 방법</title>
      <link>http://thefarmersfront.github.io/blog/tc-optimization/</link>
      <guid>http://thefarmersfront.github.io/blog/tc-optimization/</guid>
      <pubDate>Fri, 08 Dec 2023 11:00:00 GMT</pubDate>
      <content:encoded>최적화 기법을 활용한 배송 효율화 사례 소개</content:encoded>
    </item>
    <item>
      <title>안전제일! 뱅크샐러드가 모바일 앱을 안정적으로 배포하는 방법</title>
      <link>https://blog.banksalad.com/tech/how-banksalad-safely-deploys-mobile-app/</link>
      <guid>https://blog.banksalad.com/tech/how-banksalad-safely-deploys-mobile-app/</guid>
      <pubDate>Thu, 30 Nov 2023 00:00:00 GMT</pubDate>
      <content:encoded>지난 글에서는 뱅크샐러드 iOS…</content:encoded>
    </item>
    <item>
      <title>쿠팡의 머신러닝 플랫폼을 통한 ML 개발 가속화</title>
      <link>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%84-%ED%86%B5%ED%95%9C-ml-%EA%B0%9C%EB%B0%9C-%EA%B0%80%EC%86%8D%ED%99%94-de29804148bb?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%84-%ED%86%B5%ED%95%9C-ml-%EA%B0%9C%EB%B0%9C-%EA%B0%80%EC%86%8D%ED%99%94-de29804148bb?source=rss----fb028911af07---4</guid>
      <pubDate>Thu, 23 Nov 2023 05:07:33 GMT</pubDate>
      <content:encoded>&lt;h4&gt;쿠팡의 머신러닝 개발 속도를 높이는 쿠팡만의 ML 플랫폼에 대하여&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;By&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/hyunjung-baek/&quot;&gt;&lt;em&gt;Hyun Jung Baek&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/hara-ketha-ab88b111/&quot;&gt;&lt;em&gt;Hara Ketha&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/jaideepray/&quot;&gt;&lt;em&gt;Jaideep Ray&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/justina-min-649681112/&quot;&gt;&lt;em&gt;Justina Min&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/mohamed-sabbah-08bb5418/&quot;&gt;&lt;em&gt;Mohamed Sabbah&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/ronakpan/&quot;&gt;&lt;em&gt;Ronak Panchal&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/adunuthula/&quot;&gt;&lt;em&gt;Seshu Adunuthula&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/thimma-reddy-kalva-a27aa859/&quot;&gt;&lt;em&gt;Thimma Reddy Kalva&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, and &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/enhua&quot;&gt;&lt;em&gt;Enhua Tan&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*r-iwNRTZ7xDhcZJ0B1ERrg.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;본 포스트는 &lt;a href=&quot;https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172&quot;&gt;&lt;strong&gt;영문&lt;/strong&gt;&lt;/a&gt;으로도 제공됩니다.&lt;/blockquote&gt;&lt;h3&gt;&lt;strong&gt;소개&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;쿠팡은 고객이 앱을 켠 순간부터 주문한 상품이 문 앞에 도착하는 순간까지 최상의 쇼핑 및 배송 경험을 제공하기 위해 끊임없는 혁신을 이어가고 있습니다. 일반적인 전자 상거래 외에도, 쿠팡은 음식 배달 서비스인 쿠팡이츠, 비디오 스트리밍을 제공하는 쿠팡플레이, 결제 서비스 쿠팡페이, 신선 상품을 위한 쿠팡 로켓배송 등 다양한 소비자 서비스를 운영하고 있습니다.&lt;/p&gt;&lt;p&gt;머신러닝(이하 ML)은 쿠팡 고객의 전자 상거래 경험에 있어 상품 카탈로그, 검색, 가격 책정, 로보틱스, 재고 관리, 그리고 물류 처리와 같은 모든 요소에 영향을 미치고 있습니다. 또한 쿠팡이 새로운 시장으로 진출함에 따라, ML의 역할은 점점 더 중요해지고 있습니다.&lt;/p&gt;&lt;p&gt;ML은 쿠팡 웹사이트와 앱에서의 검색 기능 및 정보 탐색 기능을 향상시키며, 상품 및 서비스의 가격을 정하고, 물류와 배송을 더욱 효율적으로 이루어지게 합니다. 그리고 스트리밍을 위한 콘텐츠를 최적화하고, 광고의 순위를 정하는 등의 다양한 업무에서 중요한 역할을 수행합니다.&lt;/p&gt;&lt;p&gt;따라서, 저희는 애드혹(ad-hoc) 탐색부터 모델 훈련용 데이터(training data) 준비, 모델(model) 개발, 그리고 모델을 안정적으로 실서비스에 배포하는 단계까지 머신러닝 개발을 꾸준히 확장하고자 노력하고 있습니다.&lt;/p&gt;&lt;h4&gt;목차&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#ae2a&quot;&gt;배경 및 과제&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#bd3a&quot;&gt;1. 개발 적용 시간의 단축&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#45b8&quot;&gt;2. ML 개발 프로세스로 CI/CD 프로세스 통합&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#2a3c&quot;&gt;3. ML 컴퓨팅의 효율적 확장&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#c80b&quot;&gt;쿠팡 ML 플랫폼의 주요 서비스&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#c579&quot;&gt;1. Notebook 환경 제공 및 ML 파이프라인 구축&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#ccfa&quot;&gt;2. 피처 엔지니어링&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#1f82&quot;&gt;3. 모델 훈련&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#8d0e&quot;&gt;4. 모델 추론&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#95a6&quot;&gt;5. 모니터링 및 관찰&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#32e8&quot;&gt;6. 트레이닝 및 추론 클러스터&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#aa47&quot;&gt;성공 사례&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#73e8&quot;&gt;1. 코버트(Ko-BERT) 트레이닝을 통한 검색 쿼리 이해 개선&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#f04a&quot;&gt;2. 상품의 실시간 가격 예측&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;strong&gt;배경 및 과제&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;쿠팡 ML 플랫폼은 개발자의 작업 효율을 향상시킴으로써 ML 개발을 보다 가속화하기 위해 ‘바로 사용 가능한’ 서비스를 제공하고자 했습니다.&lt;/p&gt;&lt;p&gt;이 플랫폼에서 제공하는 핵심 서비스로는 관리형 주피터 노트북(Jupyter Notebook), 파이프라인 SDK, 피처 스토어, 모델 학습, 그리고 모델 추론이 있습니다. ML 팀들은 이러한 서비스를 각각 조합하여 ML 파이프라인을 구축할 수 있습니다. 저희의 주요 목표는 다음과 같습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;1. 개발 적용 시간의 단축&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡 ML 플랫폼이 도입되기 전에는, ML 모델을 설계하고 학습시키려면 데이터와 피처 준비, 그리고 모델 훈련 코드 작성에 대한 복잡한 설정 및 기본 코드 작성이 필요했습니다. 또한, 분산 훈련을 통해 모델 훈련을 확장하거나 GPU를 활용하는 데에는 고도의 엔지니어링 지식이 필요했고, 중복되는 작업도 종종 발생했습니다.&lt;/p&gt;&lt;p&gt;실시간 트래픽에 ML 모델을 배포하는 것은 성능 평가, 자동 스케일링, 보안, 롤백 등의 로직을 반복적으로 적용해야 하므로, 몇 주 동안 많은 시간과 자원이 투입되어야 했습니다. 이러한 이유로 많은 프로덕트 팀들이 ML의 대규모 적용을 미뤄왔습니다. 하지만 쿠팡 ML 플랫폼의 라이프 사이클을 활용하게 되면서, 간단한 모델부터 복잡한 모델까지 표준화된 방식으로 단 며칠 안에 학습, 디버그 및 배포가 가능해졌습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;2. ML 개발 프로세스로 CI/CD 프로세스 통합&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;ML 개발을 진행할 때, 복잡하고 해결하기 어려운 기술적 부채가 자주 발생할 수 있습니다. ML 팀들이 더 효율적으로 모델을 구축, 배포, 유지 관리할 수 있도록, 쿠팡 ML 플랫폼은 널리 쓰이고 있는 ML 라이브러리들과의 통합 테스트가 완료되어 있는 컨테이너들을 준비하여 제공합니다. 또한, 모델 검증을 위한 라이브러리, 카나리(Canary) 기반의 모델 배포, 그리고 서비스 중의 핵심 지표 모니터링 기능도 함께 제공하고 있습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;3. ML 컴퓨팅의 효율적 확장&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡에서는 딥러닝 훈련에 필요한 GPU, 대규모 데이터 세트 저장, 그리고 분산 훈련 시 필요한 네트워크 대역폭에 대한 컴퓨팅 요구가 급증하고 있습니다. 그리고 플랫폼 상에서 많은 모델들이 훈련되고 있어 클라우드 사용 비용도 증가하고 있습니다. 이를 해결하기 위해 쿠팡 ML 플랫폼 팀은 컴퓨팅과 스토리지 클러스터를 온프레미스(on-premise)와 AWS 상에 운영하는 하이브리드 구조를 차용하였습니다. 온프레미스는 더 저렴한 비용으로 강력한 GPU 클러스터와 다양한 사용자 맞춤 설정을 제공하고, 클라우드는 온프레미스 자원이 충분하지 않을 때 즉각적인 컴퓨팅 자원을 제공합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡의 머신러닝 플랫폼 개요&quot; src=&quot;https://cdn-images-1.medium.com/max/838/1*qGpaaTpPXgwaHZw9yldh7w.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 1. 쿠팡 ML 플랫폼 개요&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;&lt;strong&gt;쿠팡 ML 플랫폼의 주요 서비스&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;1.&lt;/strong&gt; &lt;strong&gt;Notebook 환경 제공 및 ML 파이프라인 구축&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;ML 플랫폼은 개발자들이 아이디어를 지속적으로 개선하고 테스트할 수 있도록 호스팅된 컨테이너 기반의 노트북(Jupyter Notebook) 환경 서비스를 제공합니다. 이 노트북 환경은 CPU나 GPU 상에서, 사용자가 지정한 컨테이너 또는 스탠다드 컨테이너를 통해 실행될 수 있습니다.&lt;/p&gt;&lt;p&gt;ML 플랫폼 팀은 텐서플로(Tensorflow), 파이토치(Pytorch), 스켈런(Sklearn), 허깅페이스(Huggingface), 트랜스포머스(Transformers) 등의 주요 ML 라이브러리가 포함된 표준 도커 컨테이너(Docker Container)를 관리하고 있습니다. 이 도커 컨테이너들로 패키지 간에 복잡하게 얽힌 의존성 문제를 효과적으로 해결하고, 안정적인 파이프라인을 구축할 수 있습니다.&lt;/p&gt;&lt;p&gt;파이프라인 구축과 관련해, ML 플랫폼은 데이터 추출, 피처 스토어, 학습, 그리고 추론에 사용할 수 있는 Python SDK 세트를 제공합니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;2. 피처 엔지니어링&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡 ML 플랫폼이 제공하는 피처 스토어를 통해 오프라인과 온라인 모드 모두에서 준비된 피처를 손쉽게 활용할 수 있습니다. 이 피처 스토어는 널리 알려진 오픈소스 프로젝트인 피스트(Feast)를 기반으로 구축되었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;오프라인 피처 스토어는 이미 만들어져 활용 중인 피처들을 공유하는 데 사용되며 모델 훈련에도 사용됩니다. 저희는 다양한 팀과 협력하여, 여러 팀에서도 활용할 수 있도록 고객 인사이트와 같은 핵심 피처들을 추가하고 있습니다.&lt;/li&gt;&lt;li&gt;추론 과정에서 온라인 피처 스토어를 통해 짧은 지연 시간으로 피처를 가져올 수 있습니다. 온라인 피처 스토어는 모델 피처 생성뿐만 아니라, 복잡한 계산을 필요로 하는 모델의 예측 결과를 임시 저장하는 역할도 합니다.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;3. 모델 훈련&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡의 ML 팀은 파이토치(Pytorch), 텐서플로(Tensorflow), 스켈런(Sklearn), 엑스지부스트(XGBoost)와 같은 널리 알려진 프레임워크부터, 예측 작업을 위한 프로펫(Prophet)과 같은 특수 목적의 프레임워크까지 다양하게 사용하고 있습니다.&lt;/p&gt;&lt;p&gt;훈련 스택은 프레임워크에 구애받지 않습니다. 사용자가 작성한 파이프라인은 컨테이너화되어 쿠버네티스(Kubernetes) 클러스터에서 실행됩니다. 배치 스케줄러는 원하는 하드웨어 설정으로 작업을 스케줄링합니다. 사용자는 클러스터 내에서 사용 가능한 모든 유형의 CPU나 GPU에서 작업이 실행될 수 있도록 설정할 수 있습니다. 이는 다양한 유형의 CPU와 GPU의 이점들을 작업의 특성에 맞게 활용함으로써 투자 대비 효과를 극대화할 수 있기 때문에 매우 유용합니다. 예를 들어, 사용자는 모델 훈련과 배치 추론을 다른 유형의 GPU에서 실행함으로써, GPU의 속도 향상과 비용 사이에서 최적화를 이룰 수 있습니다.&lt;/p&gt;&lt;p&gt;배치 스케줄러는 모든 리소스를 할당 또는 리소스를 전혀 할당하지 않는 전략을 따르도록 설정되어 있습니다. 훈련 스택은 대규모 모델 학습을 위한 분산 훈련 전략(분산 데이터 병렬 및 완전 분산 데이터 병렬)을 지원합니다. 쿠팡은 다중 GPU를 사용한 학습으로 모델 훈련 작업 속도를 크게 향상시켰습니다.&lt;/p&gt;&lt;p&gt;딥러닝 모델을 효과적으로 학습시키기 위해서는 트레이너 변수(trainer parameter)를 조정하는데 상당한 노력이 필요합니다. 저희 플랫폼 팀에서는 내부적으로 자주 사용되는 모델 구조에 대한 트레이너를 벤치마킹하고, 이를 통해 얻은 가장 효과적인 기법과 최선의 방법론을 이 플랫폼을 활용하는 모든 그룹에 공유하고 있습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;4. 모델 추론&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;훈련 후, 모델은 실험용 또는 실제 트래픽을 서빙하기 위한 프로덕션 환경에 배포됩니다. 쿠버네티스 상에서 모델 추론을 하기 위해 셀던(Seldon) 플랫폼이 이용됩니다. 셀던은 TFServing과 Triton 같은 서빙 라이브러리와 통합될 뿐만 아니라, 사용자 정의 Python 래퍼도 지원합니다. 이를 통해 다양한 모델 프레임워크, 런타임 및 하드웨어(CPU 및 GPU 서빙 포함)를 폭넓게 지원합니다.&lt;/p&gt;&lt;p&gt;각 ML 모델은 오토스케일링을 지원하는 독립 서비스로 배포할 수 있습니다. ML 모델을 서비스로 배포하면 모델별로 관리할 수 있고, 표준 CI/CD 인프라와의 원활한 통합이 가능합니다. 배포 전에는 모델 크기, 훈련과 예측 사이의 비대칭도 테스트 등 다양한 검증 테스트를 실행한 후 카나리 테스트 단계로 진행합니다. 카나리 테스트가 성공하면 모델은 점진적으로 전체 배포됩니다. 개발자는 이러한 간단한 작업을 통해 매우 손쉽고 안전하게 모델을 프로덕션 트래픽에 적용할 수 있습니다.&lt;/p&gt;&lt;p&gt;임베딩과 같은 높은 계산 부하를 필요로 하는 기능을 실시간으로 거의 지연 없이 제공하기 위해, 저희는 위에서 언급한 온라인 피처 스토어를 사용합니다. 특히 대규모 모델들(LLMs, 멀티모달 모델)을 위해, CPU 서빙보다 효율적인 처리량을 가진 GPU 기반의 실시간 및 배치 서빙 기술에 주력하고 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 ML 플랫폼의 훈련 워크플로&quot; src=&quot;https://cdn-images-1.medium.com/max/720/1*9LVdZNjIgG-5H-TDkeGJYg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 2. 훈련 워크플로&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 ML 플랫폼의 서빙 워크플로&quot; src=&quot;https://cdn-images-1.medium.com/max/950/1*DqM5JCopDlyJHsrD3zo9bQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 3. 서빙 워크플로&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;5. 모니터링 및 관찰&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡 ML 플랫폼의 모든 서비스에는 모니터링 기능이 탑재되어 있습니다. 훈련 클러스터에는 사용 중인 GPU, CPU, 메모리에 대한 리소스 및 작업 모니터링 대시보드가 구비되어 있습니다. 또한, 작업의 GPU 및 CPU 활용도에 관한 수치들을 확인할 수 있습니다.&lt;/p&gt;&lt;p&gt;추론 서비스는 메모리 사용량과 예측 점수에 대해 런타임 모니터링을 진행합니다. 향후에는 피처와 모델 서빙에 대한 데이터 품질 검사(이상 탐지, 드리프트 모니터링)를 도입할 예정입니다.&lt;/p&gt;&lt;p&gt;더불어, 개발자들은 대시보드를 통해 자원 할당과 스케줄링의 지연 상황을 쉽게 파악할 수 있습니다. 오류 발생 시, 해당 클러스터에서는 애플리케이션 및 자원 사용 로그를 수집하여 대시보드에서 확인할 수 있도록 합니다. 그 외에도 훈련 중인 작업이 정체되거나, 서빙 또는 메모리 상승과 같은 문제 상황에 대한 알림 설정도 마련되어 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 ML 플랫폼의 서빙 모니터링&quot; src=&quot;https://cdn-images-1.medium.com/max/548/1*iOlpbfMcpgfp1VFhNgm5ZA.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 ML 플랫폼의 서빙 모니터링&quot; src=&quot;https://cdn-images-1.medium.com/max/511/1*4C_fUiko3xcKPBrTlyDYEg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 4. 서빙 모니터링&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;6. 트레이닝 및 추론 클러스터&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;대용량 데이터와 딥러닝 모델의 시대에서, 하드웨어(특히 GPU 같은 가속기)는 ML 개발에서 필수적인 역할을 합니다. 쿠팡은 클라우드 인프라 엔지니어와의 적극적인 협업을 통해 온프레미스 데이터 센터와 AWS 클러스터에서 컴퓨팅 및 스토리지 클러스터를 제공합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 ML 플랫폼 트레이닝 클러스터의 GPU 사용률 모니터링&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*CQd2-FaoqZxYKC7wrqDKsw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 5. &lt;/strong&gt;트레이닝 클러스터의 GPU 사용률 모니터링&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;훈련에는 대용량 메모리가 있는 인스턴스, GPU와 같은 가속기, 분산 훈련을 위한 노드 간 고대역폭 연결, 그리고 훈련 데이터 및 모델 체크포인트와 같은 출력 아티팩트를 저장하기 위한 공유 스토리지 클러스터가 필요합니다.&lt;/p&gt;&lt;p&gt;서빙을 위해서는 성능과 가용성을 보장하는 높은 I/O 처리량을 가진 머신이 필수적입니다. 여러 가용성 영역에 최적화된 전용 머신들을 구비하고 있고, 오토스케일링 기능으로 클러스터가 트래픽의 급증에도 대응할 수 있습니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;성공 사례&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;쿠팡 ML 팀과의 협업 덕분에, 특정 도메인에서 입증된 해결 방안을 체계적으로 확장하고 일반적으로 적용할 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;아래는 쿠팡 ML 플랫폼의 지원을 받은 최근 성공 사례들 중 일부입니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;1. 코버트(Ko-BERT) 트레이닝을 통한 검색 쿼리 이해 개선&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;상품 검색 및 추천을 담당하는 ML 개발자들은 기존의 용어 매칭 기반 검색을 보완하기 위해 임베딩 기반 검색 방식을 적용했습니다. A100 GPU의 다중 GPU 분산 훈련을 통해 이전 세대의 GPU들과 훈련 전략에 비해 버트(BERT)의 훈련 속도가 10배 향상되었습니다.&lt;/p&gt;&lt;p&gt;버트의 성공 이후 개발자들은 다양한 경로로 유입되는 검색 요청들과 관련해 검색 품질을 향상시키기 위해 대규모 언어 모델(LLMs)을 세부적으로 조정하는 실험을 계속하고 있습니다. 이러한 세부 조정 작업은 ML 플랫폼의 여러 부분을 활용하게 되는데, 그 중에서도 효율적인 클러스터 이용, 분산 훈련 전략, 고처리량 GPU 기반 추론 등이 포함됩니다.&lt;/p&gt;&lt;p&gt;저희는 ML 플랫폼을 통해 쿠팡 개발자들이 최신 ML 기술들에 쉽게 접근하고 적용하는 데 있어 큰 성과를 거두고 있습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;2. 상품의 실시간 가격 예측&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;고객 및 성장을 위한 데이터 사이언스 팀은 가격, 수요, 페이지 조회 등을 예측하기 위해 다양한 시계열 데이터를 모델링합니다. 데이터 사이언스 팀은 사용자 정의 추론 스택에서 사용 중인 가격 모델을 저희 ML 플랫폼으로 전환하였습니다. 그 결과, 배포 클러스터를 유지 관리할 필요가 없어졌으며, 모델 개발에 완전히 집중할 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;저희의 여정은 아직 초기 단계이지만, 많은 사용자들이 저희 서비스를 ML 파이프라인 구성 요소로 사용하고 있으며 사용자들로부터 좋은 반응을 얻고 있습니다. 지난 1년 동안, ML 플랫폼에서 600개 이상의 ML 프로젝트에서 100,000건 이상의 워크플로가 실행되었습니다. 그리고 실험 중인 모델의 크기가 크게 증가함에 따라 고객들에게 제공되는 서비스들의 품질에서도 여러 가지 성과를 거두었습니다. 쿠팡의 모든 주요 ML 그룹에서는 하나 이상의 쿠팡 ML 플랫폼 서비스를 활용하고 있습니다.&lt;/p&gt;&lt;p&gt;쿠팡 개발자들은 저희 ML 플랫폼에서 언어 모델링 및 자동화된 머신러닝(AutoML) 같은 도메인 특화 도구를 구축하고 있습니다. 온라인 피처 스토어와 모니터링 등의 베스트 프랙티스(best practice)에서 CI/CD의 도입과 활용에 대한 관심이 크게 증가하고 있습니다.&lt;/p&gt;&lt;p&gt;다음 포스트에서는 쿠팡의 핵심 서비스와 이를 지원하는 애플리케이션들에 대해 더욱 상세히 소개하고자 합니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;머신러닝과 인프라와 관련된 도전들에 관심을 갖고 수많은 비즈니스 문제를 함께 해결하면서 고객 경험을 향상시키고 싶으시다면, 쿠팡 &lt;/em&gt;&lt;a href=&quot;https://www.coupang.jobs/kr/&quot;&gt;&lt;em&gt;채용 공고&lt;/em&gt;&lt;/a&gt;&lt;em&gt;를 확인해 보세요!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 &lt;a href=&quot;https://ir.aboutcoupang.com/English/home/default.aspx&quot;&gt;ir.aboutcoupang.com&lt;/a&gt; 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=de29804148bb&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%84-%ED%86%B5%ED%95%9C-ml-%EA%B0%9C%EB%B0%9C-%EA%B0%80%EC%86%8D%ED%99%94-de29804148bb&quot;&gt;쿠팡의 머신러닝 플랫폼을 통한 ML 개발 가속화&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>후기 서비스 AWS Opensearch 도입기</title>
      <link>http://thefarmersfront.github.io/blog/2023-review-opensearch/</link>
      <guid>http://thefarmersfront.github.io/blog/2023-review-opensearch/</guid>
      <pubDate>Thu, 23 Nov 2023 00:00:00 GMT</pubDate>
      <content:encoded>위기에서 기회를 만들어 낸 후기 서비스 이야기</content:encoded>
    </item>
    <item>
      <title>모듈 구조를 개선해 더 나은 뱅크샐러드 iOS 앱 개발하기</title>
      <link>https://blog.banksalad.com/tech/refactor-module-architecture-for-building-a-better-app/</link>
      <guid>https://blog.banksalad.com/tech/refactor-module-architecture-for-building-a-better-app/</guid>
      <pubDate>Wed, 22 Nov 2023 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드 iOS 챕터의 김봉균입니다. 최근 iOS 챕터는 뱅크샐러드 iOS…</content:encoded>
    </item>
    <item>
      <title>Web을 위한 gRPC Stub과 Runtime 생성하기 - Feat. Buf &amp; kubernetes</title>
      <link>https://blog.banksalad.com/tech/making-typescript-grpc-stub-generator/</link>
      <guid>https://blog.banksalad.com/tech/making-typescript-grpc-stub-generator/</guid>
      <pubDate>Tue, 31 Oct 2023 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 웹 프론트엔드 챕터의 민찬기입니다. gRPC…</content:encoded>
    </item>
    <item>
      <title>BULK 처리 Write에 집중해서 개선해보기</title>
      <link>http://thefarmersfront.github.io/blog/bulk-performance-tuning/</link>
      <guid>http://thefarmersfront.github.io/blog/bulk-performance-tuning/</guid>
      <pubDate>Thu, 21 Sep 2023 00:00:00 GMT</pubDate>
      <content:encoded>애플리케이션, DB 모두 행복한 BULK 처리</content:encoded>
    </item>
    <item>
      <title>Meet Coupang’s Machine Learning Platform</title>
      <link>https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172?source=rss----fb028911af07---4</guid>
      <pubDate>Fri, 08 Sep 2023 06:21:33 GMT</pubDate>
      <content:encoded>&lt;h4&gt;How Coupang’s ML Platform accelerates ML development for Coupang products&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;By&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/hyunjung-baek/&quot;&gt;&lt;em&gt;Hyun Jung Baek&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/hara-ketha-ab88b111/&quot;&gt;&lt;em&gt;Hara Ketha&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/jaideepray/&quot;&gt;&lt;em&gt;Jaideep Ray&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/justina-min-649681112/&quot;&gt;&lt;em&gt;Justina Min&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/mohamed-sabbah-08bb5418/&quot;&gt;&lt;em&gt;Mohamed Sabbah&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/ronakpan/&quot;&gt;&lt;em&gt;Ronak Panchal&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/adunuthula/&quot;&gt;&lt;em&gt;Seshu Adunuthula&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/thimma-reddy-kalva-a27aa859/&quot;&gt;&lt;em&gt;Thimma Reddy Kalva&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, and &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/enhua&quot;&gt;&lt;em&gt;Enhua Tan&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*r-iwNRTZ7xDhcZJ0B1ERrg.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;This post is also available in &lt;a href=&quot;https://medium.com/coupang-engineering/쿠팡의-머신러닝-플랫폼을-통한-ml-개발-가속화-de29804148bb&quot;&gt;&lt;strong&gt;Korean&lt;/strong&gt;&lt;/a&gt;.&lt;/blockquote&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;Coupang is reimagining the shopping and delivery experience to wow all the customers from the instant they open the Coupang app to the moment an order is delivered to their door. In addition to e-commerce, Coupang has various other consumer services ranging from Coupang Eats for food delivery, Coupang Play for video streaming, Coupang Pay for payments, and to Coupang Grocery for fresh products amongst others.&lt;/p&gt;&lt;p&gt;Machine Learning (ML) impacts every aspect of e-commerce experiences of Coupang customers: the product catalog, search, pricing, robotics, inventory, and fulfillment. As Coupang ventures into new markets, ML has continued to play an even more important role.&lt;/p&gt;&lt;p&gt;ML helps to power search and discovery across Coupang websites and apps, to price products and services, to streamline logistics and delivery, to optimize content for streaming, to rank ads, and to do many more jobs.&lt;/p&gt;&lt;p&gt;Therefore, we strive to scale machine learning development at all ML lifecycle stages, including ad-hoc exploration, training data preparation, model development, and robust production deployment of models.&lt;/p&gt;&lt;h4&gt;Table of Contents&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#5959&quot;&gt;ML @ Coupang&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#534f&quot;&gt;Motivation&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#79be&quot;&gt;1. Reduce time to production&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#14dc&quot;&gt;2. Incorporate CI/CD in ML development&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#bd5e&quot;&gt;3. Scale ML compute efficiently&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#46a6&quot;&gt;Core offerings of Coupang ML Platform&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#5cf8&quot;&gt;1. Notebooks &amp;amp; ML Pipeline Authoring&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#4adb&quot;&gt;2. Feature Engineering&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#a499&quot;&gt;3. Model Training&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#5b6c&quot;&gt;4. Model Inference&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#6536&quot;&gt;5. Monitoring &amp;amp; Observability&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#cf93&quot;&gt;6. Training &amp;amp; Inference Clusters&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#bcca&quot;&gt;Success Stories&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#671d&quot;&gt;1. Training Ko-BERT to understand search queries better&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#2284&quot;&gt;2. Real-time price forecasting of products&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;ML @ Coupang&lt;/h3&gt;&lt;p&gt;ML teams at Coupang are actively developing models in Natural Language Processing (NLP), Computer Vision (CV), Recommendations, and Forecasting. NLP is used to understand search queries, product listings, and ads content. Computer vision-enabled image understanding categorizes similar products and ads. Recommendation models rank content for product search, videos in Coupang Play, and product ads. Forecasting techniques help us understand supply, demand, and pricing for millions of products.&lt;/p&gt;&lt;p&gt;This post introduces Coupang’s internal ML platform and describes how the platform supports the increasing scale and diversity of workloads across ML frameworks, programming languages, different model architectures, and training &amp;amp; serving paradigms.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;The motivation behind Coupang ML Platform is to provide ‘batteries-included’ services to accelerate ML development through improved developer productivity.&lt;/p&gt;&lt;p&gt;Core services include managed notebooks (Jupyter), pipeline SDK, feature-store, model training, and model inference. ML teams can use the services independently to compose their ML pipeline. Our focus areas are as follow:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;1. Reduce time to production&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Before Coupang ML Platform, authoring and training a ML model required hours of non-trivial setup work and boilerplate code for preparing data, features and writing trainer code. Tasks like scaling model training through distributed training, using GPUs took deep engineering work resulting in duplicate stack. &lt;br&gt;Deploying the ML model for serving real-time traffic took weeks of effort, replicating logic for model benchmarking, auto-scaling, security and rollback. These were blockers for product groups to adopt ML at a larger scale. By leveraging ML Platform lifecycle services, one can train, debug and deploy simple to complex models in production within days in a standardized way.&lt;/p&gt;&lt;h4&gt;2. Incorporate CI/CD in ML development&lt;/h4&gt;&lt;p&gt;ML development can quickly incur heavy technical debt. To make it easier for ML teams to build, deploy and maintain models, we provide integration tested prepackaged containers with popular ML libraries. &lt;br&gt;Moreover, we provide libraries to validate model, add canary in model deployment and monitoring primary metrics during serving.&lt;/p&gt;&lt;h4&gt;3. Scale ML compute efficiently&lt;/h4&gt;&lt;p&gt;There is surging demand for compute in Coupang — GPUs for deep learning training, storage for large datasets, and network bandwidth for distributed training. Cloud costs are high, given the large fleet of models training on the platform. Coupang ML Platform team manages a hybrid setup with compute and storage clusters on on-prem and AWS. The on-prem setup provides more customization and a powerful GPU cluster at lower costs while the cloud setup can scale on demand if on-prem resources are insufficient.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;An overview of Coupang ML Platform&quot; src=&quot;https://cdn-images-1.medium.com/max/838/1*ki27OtAMLOA91Tqg_djvFw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 1. &lt;/strong&gt;Coupang ML Platform overview&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Core offerings of Coupang ML Platform&lt;/h3&gt;&lt;h4&gt;1. Notebooks &amp;amp; ML Pipeline Authoring&lt;/h4&gt;&lt;p&gt;ML platform provides a hosted, containerized notebook service for developers to iterate on their ideas. The notebook can be launched using custom or standard containers on CPUs or GPUs.&lt;/p&gt;&lt;p&gt;A set of standard docker containers are maintained by the platform team containing popular ML libraries such as Tensorflow, Pytorch, Sklearn, Huggingface, Transformers, etc. The docker containers help in avoiding dependency complexity and help in writing repeatable pipelines.&lt;/p&gt;&lt;p&gt;For pipeline authoring, the platform provides a set of Python SDKs for data fetching, feature-store, training, and inference.&lt;/p&gt;&lt;h4&gt;2. Feature Engineering&lt;/h4&gt;&lt;p&gt;Coupang ML Platform offers a feature-store built to access prepared features easily in both offline and online modes. The feature store is built on top of the popular open-source project Feast.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Offline feature stores are used to share prepared features and are also used for model training. We are working with teams to onboard fundamental features such as customer insights which can be consumed by multiple downstream teams.&lt;/li&gt;&lt;li&gt;Online feature store is used to fetch features with low latency during inference. This serves as a model feature generator as well as prediction response cache for compute-intensive models.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;3. Model Training&lt;/h4&gt;&lt;p&gt;ML teams at Coupang use different modeling frameworks, from the popular ones such as Pytorch, Tensorflow, Sklearn, XGBoost, to the niche ones such as Prophet for forecasting.&lt;/p&gt;&lt;p&gt;The training stack is agnostic of framework. User written pipelines are containerized and launched on the Kubernetes cluster. A batch scheduler schedules the jobs on the desired hardware setup. Users can configure their jobs to run on any CPU type or GPU type available in the cluster. This is very useful as jobs can benefit from various CPU and GPU types depending on their characteristics and can optimize the return on investment. For example, users can configure their model training and batch inference tp rim on different GPU types, optimizing itself for speedup vs. cost of GPU. &lt;br&gt;The scheduler is configured to follow all-or-nothing resource allocation strategy. The training stack supports distributed training strategies (distributed data parallel and fully sharded data parallel) to train large models. Multi-GPU training has sped up model training workloads significantly across Coupang.&lt;/p&gt;&lt;p&gt;It requires significant effort to tune trainer parameters to efficiently train deep learning models. As the platform team, we benchmark trainers for popular model architectures used internally and share the most effective techniques and best practices amongst all groups who use the platform.&lt;/p&gt;&lt;h4&gt;4. Model Inference&lt;/h4&gt;&lt;p&gt;Post training, a model is deployed for experimentation or production for serving real traffic. The Seldon platform is used on Kubernetes for model inference. Seldon has integrations with serving libraries such as TFServing and Triton while it can also support custom python wrappers. Through this, it can cover a wide range of model frameworks, runtimes and hardware (CPU &amp;amp; GPU Serving).&lt;/p&gt;&lt;p&gt;Each ML model can be deployed as a standalone service with autoscaling. Deploying each ML model as a service provides isolation and allows integration with standard CI/CD infrastructure. Model deployment jobs run multiple validation tests (model size, training-prediction skew tests, etc) before moving into a canary phase. If canary results are successful, the model can be gradually rolled out. Developers need minimum effort (adding hooks for model validation and canary results verification) to safely get their model serving production traffic. &lt;br&gt; &lt;br&gt;To serve compute-intensive features, such as embedding, in real time with low latency, we use the online feature store mentioned above. For very large models (LLMs, multimodal models), we are investing in batch and real-time GPU based serving which provides a high throughput compared to CPU serving.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang ML Platform’s training workflow&quot; src=&quot;https://cdn-images-1.medium.com/max/720/1*Fc2rm6L1FqWmAxSzbO-UpQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Training workflow&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang ML Platform’s serving workflow&quot; src=&quot;https://cdn-images-1.medium.com/max/950/1*G8BK042RU2PBvCLm9x4Y1g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Serving workflow&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;5. Monitoring &lt;strong&gt;&amp;amp; Observability&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;All Coupang ML Platform services have monitoring enabled. Training cluster has resource and job monitoring dashboards (GPUs, CPUs, Memory in use). There are GPU and CPU utilization metrics for workloads. &lt;br&gt;Inference service has runtime monitoring for memory usage, prediction scores. We have plans to introduce data quality checks (anomaly detection, drift monitoring) across feature and model serving. &lt;br&gt;Cluster usage dashboards are used by developers to understand resource allocations and scheduling delays. For error debugging, the application and resource usage logs are collected from clusters and made available to developers through dashboards. There also are alerts set up for various events such as stuck or idle training jobs, inability to launch instances for training, serving or memory spikes.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang ML Platform’s monitoring serving&quot; src=&quot;https://cdn-images-1.medium.com/max/548/1*JoMJ0x8dWJf_ORGd82Iheg.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang ML Platform’s monitoring serving&quot; src=&quot;https://cdn-images-1.medium.com/max/511/1*ps5ORx_eAKTUaFKOnrsdog.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Monitoring serving&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;6. Training &amp;amp; Inference Clusters&lt;/h4&gt;&lt;p&gt;In the era of large datasets and deep learning models, hardware (especially accelerators such as GPU) plays a crucial role in ML development. Through an active collaboration with the the cloud infrastructure engineers at Coupang, we provide compute and storage clusters in the on-prem data center and AWS cluster.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang ML Platform’s monitoring GPU utilization of its training cluster&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*_RtWkS0_mz_vv00dttkdGQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Monitoring GPU utilization of Training cluster&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Training requires instances with large memory, accelerators such as GPUs, high bandwidth connection between nodes for distributed training, and a shared storage cluster to store training data and output artifacts such as model checkpoints.&lt;/p&gt;&lt;p&gt;Serving requires high I/O throughput machines for performance and availability. We have a dedicated set of machines optimized for serving in multiple availability zones. Autoscaling ensures that the cluster can handle traffic spikes.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Success Stories&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Through our partnership with ML teams at Coupang, we are able to systematically scale solutions which have been proven in one domain and can be generalized.&lt;/p&gt;&lt;p&gt;The following is a couple of recent customer success stories supported by Coupang ML Platform:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;1. Training Ko-BERT to understand search queries better&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;ML developers working in search and recommendations launched embedding-based retrieval to augment classical term matching-based retrieval. Multi-GPU distributed training on A100 GPUs provided 10x speed up for BERT training compared to older generation GPUs and training strategy.&lt;/p&gt;&lt;p&gt;After success of BERT, the developers are experimenting with finetuned large language models (LLMs) to improve search quality across different surfaces. Large Language model finetuning exercises various parts of the ML platform — efficient cluster usage, distributed training strategies, high throughput GPU-based inference, etc. &lt;br&gt;We have been fairly successful in adapting and democratizing the new ML innovations through our platform.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;2. Real-time price forecasting of products&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Data science teams in Customer and Growth model various time series data for forecasting price, demand, page-view amongst others. The team onboarded their entire suite of pricing models from custom inference stack to our ML Platform serving. The team no longer has to maintain their deployment cluster. They can focus entirely on developing better models.&lt;/p&gt;&lt;p&gt;Even though we are still early in our journey, we see good traction in customers using the services as building blocks in their ML pipeline. Over the past year, there have been 100K+ workflow runs on the platform spanning 600+ ML projects. We saw massive increase in size of models being experimented on resulting in several wins in quality of Coupang services. All major ML groups at Coupang use one or more Coupang ML Platform services. &lt;br&gt;We see developers building domain-specific toolkits on the Coupang ML Platform, such as language modeling and AutoML. There has been strong interest in and adoption of CI/CD in best practice features such as online feature store and monitoring.&lt;/p&gt;&lt;p&gt;The coming posts will describe Coupang’s core services and applications supported by them in more detail. If you are interested in tackling Machine Learning and infra challenges that enable developers to solve hundreds of business problems and improve customer experience, consider applying for a &lt;a href=&quot;https://www.coupang.jobs/en/&quot;&gt;role&lt;/a&gt; on our team!&lt;/p&gt;&lt;p&gt;&lt;em&gt;While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on &lt;/em&gt;&lt;a href=&quot;http://ir.aboutcoupang.com/&quot;&gt;&lt;em&gt;ir.aboutcoupang.com&lt;/em&gt;&lt;/a&gt;&lt;em&gt; for information on our formal investment plans and product development strategies.&lt;/em&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cd00e9ccc172&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172&quot;&gt;Meet Coupang’s Machine Learning Platform&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 Go 코딩 컨벤션</title>
      <link>https://blog.banksalad.com/tech/go-best-practice-in-banksalad/</link>
      <guid>https://blog.banksalad.com/tech/go-best-practice-in-banksalad/</guid>
      <pubDate>Wed, 06 Sep 2023 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 코어 백엔드 팀의 정겨울입니다. 뱅크샐러드는 백엔드 서비스에 다양한 언어를 사용하고 있습니다. 특히 지난 4년간은 Go와 gRPC…</content:encoded>
    </item>
    <item>
      <title>컬리로그팀이 직접 고객을 만나고 리서치를 진행한 이야기: 컬리 푸드 페스타에서</title>
      <link>http://thefarmersfront.github.io/blog/kurlylog-user-research-in-kurly-food-festa-2023/</link>
      <guid>http://thefarmersfront.github.io/blog/kurlylog-user-research-in-kurly-food-festa-2023/</guid>
      <pubDate>Mon, 28 Aug 2023 00:00:00 GMT</pubDate>
      <content:encoded>프로덕트 조직의 엔지니어가 고객을 직접 만나고 인터뷰한 이유</content:encoded>
    </item>
    <item>
      <title>사용법과 함께 작성해본 좌충우돌 AWS DMS 사용기 - feat. RDS 통합 이야기</title>
      <link>https://blog.banksalad.com/tech/dms/</link>
      <guid>https://blog.banksalad.com/tech/dms/</guid>
      <pubDate>Wed, 19 Jul 2023 00:00:00 GMT</pubDate>
      <content:encoded>
        안녕하세요, 뱅크샐러드 Core Infra 팀의 DevOps Engineer 이재환 입니다.
        AWS DMS를 사용하면서 겪었던 경험들을 기반으로 기본적인 사용법에 대해 공유해보고자 합니다. RDS…
      </content:encoded>
    </item>
    <item>
      <title>서비스 기획자의 뷰티컬리로 확장하기</title>
      <link>http://thefarmersfront.github.io/blog/expand-to-Beauty-Kurly/</link>
      <guid>http://thefarmersfront.github.io/blog/expand-to-Beauty-Kurly/</guid>
      <pubDate>Thu, 06 Jul 2023 00:00:00 GMT</pubDate>
      <content:encoded>우선순위를 나눠 정복하고, 원팀으로 협업하기</content:encoded>
    </item>
    <item>
      <title>점점 커지는 RDB Table, S3로 귀양 보내고 Athena로 불러오기 - feat. Optimization with Spark Bucketing</title>
      <link>https://blog.banksalad.com/tech/data-optimization-with-bucketing/</link>
      <guid>https://blog.banksalad.com/tech/data-optimization-with-bucketing/</guid>
      <pubDate>Thu, 08 Jun 2023 00:00:00 GMT</pubDate>
      <content:encoded>
        안녕하세요, 뱅크샐러드 Core Infra 팀의 Data engineer 김문수 입니다.
        점점 커지는 이력성 데이터를 MySQL에서 더 저렴한 저장소인 S3로 옮기면서도,
        서비스에서 호출할 때 딱 필요한 데이터만 읽을 수 있도록 bucketing…
      </content:encoded>
    </item>
    <item>
      <title>분석 데이터를 프로덕션에서 쉽게 사용할 수 없을까?</title>
      <link>https://blog.banksalad.com/tech/dataserving/</link>
      <guid>https://blog.banksalad.com/tech/dataserving/</guid>
      <pubDate>Wed, 07 Jun 2023 00:00:00 GMT</pubDate>
      <content:encoded>이번 글에서는 분석 테이블을 API로 만들 수 있는 데이터서빙 서비스를 만들었던 과정을 소개 드리겠습니다. 뱅크샐러드에서는 S3, Airflow, Spark…</content:encoded>
    </item>
    <item>
      <title>데이터 분석가가 직접 정의, 배포, 관리하는 뱅크샐러드 데이터 파이프라인</title>
      <link>https://blog.banksalad.com/tech/datapipe/</link>
      <guid>https://blog.banksalad.com/tech/datapipe/</guid>
      <pubDate>Mon, 05 Jun 2023 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 저는 뱅크샐러드 Core Data팀 Tech Lead Manager…</content:encoded>
    </item>
    <item>
      <title>컬리 개발자들의 내돈내산 제품 찐추천 - 사실 컬리 개발자들은 회사에 월급을 반납하고 있습니다.</title>
      <link>http://thefarmersfront.github.io/blog/kurly-ft-product-recommendation/</link>
      <guid>http://thefarmersfront.github.io/blog/kurly-ft-product-recommendation/</guid>
      <pubDate>Wed, 31 May 2023 10:00:00 GMT</pubDate>
      <content:encoded>컬리 개발자들이 추천하는 컬리 제품</content:encoded>
    </item>
    <item>
      <title>헤이조이스 웨비나를 소개합니다</title>
      <link>http://thefarmersfront.github.io/blog/heyjoyce-webinar/</link>
      <guid>http://thefarmersfront.github.io/blog/heyjoyce-webinar/</guid>
      <pubDate>Thu, 18 May 2023 00:00:00 GMT</pubDate>
      <content:encoded>2주 만에 개발된 웨비나 시스템이 지금까지도 잘 사용되고 있다는 이야기 💬</content:encoded>
    </item>
    <item>
      <title>풀필먼트 입고 서비스팀에서 분산락을 사용하는 방법 - Spring Redisson</title>
      <link>http://thefarmersfront.github.io/blog/distributed-redisson-lock/</link>
      <guid>http://thefarmersfront.github.io/blog/distributed-redisson-lock/</guid>
      <pubDate>Thu, 18 May 2023 00:00:00 GMT</pubDate>
      <content:encoded>어노테이션 기반으로 분산락을 사용하는 방법에 대해 소개합니다.</content:encoded>
    </item>
    <item>
      <title>뷰티컬리에서 &lt;나의 컬리 스타일&gt;이 갖는 의미</title>
      <link>http://thefarmersfront.github.io/blog/my-kurly-style/</link>
      <guid>http://thefarmersfront.github.io/blog/my-kurly-style/</guid>
      <pubDate>Tue, 02 May 2023 00:00:00 GMT</pubDate>
      <content:encoded>뉴비가 뉴서비스를 만났을 때</content:encoded>
    </item>
    <item>
      <title>쿠팡 로켓배송: 공간 색인 기반의 새로운 배송 영역 관리 시스템</title>
      <link>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EB%B0%B0%EC%86%A1-%EA%B3%B5%EA%B0%84-%EC%83%89%EC%9D%B8-%EA%B8%B0%EB%B0%98%EC%9D%98-%EB%B0%B0%EC%86%A1-%EC%98%81%EC%97%AD-%EA%B4%80%EB%A6%AC-%EC%8B%9C%EC%8A%A4%ED%85%9C-a59006bc4b6e?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EB%B0%B0%EC%86%A1-%EA%B3%B5%EA%B0%84-%EC%83%89%EC%9D%B8-%EA%B8%B0%EB%B0%98%EC%9D%98-%EB%B0%B0%EC%86%A1-%EC%98%81%EC%97%AD-%EA%B4%80%EB%A6%AC-%EC%8B%9C%EC%8A%A4%ED%85%9C-a59006bc4b6e?source=rss----fb028911af07---4</guid>
      <pubDate>Tue, 18 Apr 2023 04:51:37 GMT</pubDate>
      <content:encoded>&lt;h4&gt;배송 영역 시각화 프로젝트를 통해 구축한 직관적이고 최적화된 배송 시스템에 대해&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/jinonearth/&quot;&gt;&lt;em&gt;Geo J Son&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &amp;amp; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/%ED%95%9C%EC%83%98-%EC%A0%84-a5a21994/&quot;&gt;&lt;em&gt;Sam HS Jeon&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*HXH-fv_TNeDAcCvGWXSGgw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;본 포스트는 &lt;a href=&quot;https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63&quot;&gt;&lt;strong&gt;영문&lt;/strong&gt;&lt;/a&gt;으로도 제공됩니다.&lt;/blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cZNRGjcfiKw&amp;amp;t=3s&quot;&gt;로켓배송&lt;/a&gt;은 쿠팡의 가장 중요하고 매력적인 서비스들 중 하나입니다. 고객들이 “쿠팡 없이 어떻게 살았을까?”라고 생각하게 만들고 고객들의 삶이 더 나아질 수 있도록, 저희는 최적화된 배송 시스템을 마련하고 배송 기사인 쿠팡친구(이하 쿠친)들과 함께 노력하고 있습니다.&lt;/p&gt;&lt;p&gt;쿠친이 택배 상자와 봉투들을 가장 효율적으로 배송하려면, 배송 시스템은 정확히 배송지 주소를 파악하고 이를 바탕으로 가장 적합한 배송 영역을 찾아낸 다음 쿠친에게 운전 경로를 안내해 주어야 합니다. 또한 시스템에 의해 택배 상자와 봉투들이 배송 영역으로 균등하게 할당되고 분배되려면, 경험 많은 캠프 작업자들이 배송 영역 정보를 수시로 업데이트할 수 있어야 합니다.&lt;/p&gt;&lt;p&gt;저희의 기존 시스템은 텍스트로 된 주소와 우편번호에 의존했고, 직관적이고 기능적인 부분도 부족하여 저희의 복잡한 운영 니즈를 만족시키지 못했습니다. 이에 저희는 배송 영역을 지도 위에 시각화하고, 직접 지도를 수정할 수 있는, 그리고 최적화에 필요한 수치 및 통계가 추가로 제공되는 시스템의 개발을 위한 프로젝트를 2021년에 시작했습니다.&lt;/p&gt;&lt;h4&gt;목차&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#2177&quot;&gt;과제: 텍스트 기반의 우편번호&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#9315&quot;&gt;배송 영역 관리 시스템의 새로운 목표&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#6c02&quot;&gt;시스템에 H3 도입하기&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#8da1&quot;&gt;지형공간 색인 시스템&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#fc70&quot;&gt;공간 색인 시스템&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#dadf&quot;&gt;새로운 기술들로 시스템 재설계하기&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#b46d&quot;&gt;격자 해상도&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#d6aa&quot;&gt;데이터 관리&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#6df3&quot;&gt;육각형 그룹을 육각형으로 채워진 다각형으로&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#f3ef&quot;&gt;시스템에 새로운 기술들 적용하기&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#db37&quot;&gt;향후 계획&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;과제: 텍스트 기반의 우편번호&lt;/h3&gt;&lt;p&gt;우편번호는 정부 및 각종 공공기관들이 보장하는 가장 잘 조직된 시스템 중 하나로, 특정 배송 영역에 할당된 코드를 가리킵니다. 지형적 요소와 배송 수요 측면에서 봤을 때 할당된 영역의 물리적 넓이는 제각각이긴 하지만 우편번호는 최적화된 실질 배송 효율을 제공합니다. 그래서 로켓배송은 처음부터 우편번호를 배송 영역의 기본 단위로 채택했습니다. 서비스 초기에는 잘 작동했지만, 쿠팡이 성장하면서 훨씬 더 많은 것들이 필요해졌습니다. 우편번호당 매일 처리해야 하는 배송 건수가 두 자릿수에서 세 자릿수로 증가했습니다. 한명의 쿠친이 하루에 처리하기에는 너무 많은 양이었습니다.&lt;/p&gt;&lt;p&gt;그러다보니 단일 우편번호를 분할해 처리 가능한 여러 영역으로 세분화해야 할 필요가 생겨났습니다. 처음에는 아파트 단지 몇개가 단일 영역이었지만, 분할되어 단지 하나가 단일 영역이 되었고, 결국엔 단지도 분할되어 아파트 동 하나가 단일 영역으로 세분화되었습니다. 그러나 텍스트로만 되어 있는 주소에서 공간 관련 정보를 식별해내기는 쉽지 않기 때문에 배송 영역의 특성을 잘 알고 있는 숙련된 캠프 리더와 쿠친 그룹 리더만이 세분화 작업을 수행할 수 있었습니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;배송 영역 관리 시스템의 새로운 목표&lt;/strong&gt;&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;지도 위에 배송 영역들을 좀 더 효율적이고 쉽게 시각화하기&lt;/li&gt;&lt;li&gt;관계자들 모두 쉽게 배송 영역들을 확인하고, 만들고, 수정하고, 다른 사람들과 공유하기&lt;/li&gt;&lt;li&gt;신축 및 철거 건물과 같은 변동 상황 발생 시에도 지속적으로 운영 가능하도록 배송 영역들을 우편번호 및 문자로 된 주소가 아닌 공간 데이터 기반으로 관리하기&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;&lt;strong&gt;시스템에 H3 도입하기&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;저희는 배송 영역 관리 시스템의 새로운 목표를 달성하기 위한 프로젝트를 시작했습니다. 지도 위에 그려진 다각형(polygon)들로 직관적인 공간 영역 관리가 가능하도록 육각형 격자(hexagon grid) 기반의 공간 색인 시스템(spatial indexing system)을 활용했습니다.&lt;/p&gt;&lt;p&gt;지도 위 영역들을 다각형으로 그리는 것이 간단해 보일 수도 있지만, 영역들 모두를 중복되고 생략되는 것 없이 깔끔하고 명확한 경계들로 그려내 빈틈없이 관리하는 것은 매우 어렵습니다. 이런 다각형 세트들이 상호배제와 전체포괄(mutually exclusive and collectively exhaustive, MECE) 원칙 하에 관리될 수 있게끔, 공간 단위를 정의하는 일부터 시작했습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;지형공간 색인 시스템&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;먼저 GIS 전문가들에 의해 개발된 다양한 지형공간 색인 시스템(geospatial indexing system)들을 검토했습니다. 지형공간 색인 시스템은 일종의 격자 세트(a set of grids)로, 고유 식별자를 갖는 격자들로 지리적 표면을 채워 관리합니다. 행정기관에 의해 유연하게 변경될 수 있는 우편번호와 행정구역 같은 경우에는 불규칙한 형태의 격자를 가집니다. 이렇게 공간 영역에 고유 식별자가 할당되면서도 공간의 경계는 유연한 시스템은 결합 시계열 분석(analysis of integrated time series) 시에 이점이 있습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;공간 색인 시스템&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://what3words.com/&quot;&gt;&lt;strong&gt;W3W (What3Words)&lt;/strong&gt;&lt;/a&gt;는 가장 널리 사용되는 공간 색인 시스템(spatial indexing system)들 중 하나이며, 전 세계를 3m² 정사각형으로 구분하여 각각의 정사각형을 3개의 단어로 표현합니다. 위도-경도 좌표계는 긴 숫자 배열로 구성되어 있어 말로 전달하기 어려운 반면, W3W는 3개 단어들의 고유한 조합만으로 위치를 나타낼 수 있습니다. 예를 들어 쿠팡의 사무실들 중 하나인 로켓 연구소의 위치는 위도-경도 좌표로는 [&lt;strong&gt;37.503819,&lt;/strong&gt; &lt;strong&gt;127.0481493&lt;/strong&gt;]이고 W3W로는 [&lt;a href=&quot;https://what3words.com/scoring.eager.patch&quot;&gt;&lt;strong&gt;scoring.eager.patch&lt;/strong&gt;&lt;/a&gt;]입니다. 하지만 아쉽게도 분석에 필요한 해상도 조정은 불가능합니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 로켓 연구소 사무실의 W3W 주소&quot; src=&quot;https://cdn-images-1.medium.com/max/484/1*lrtZxW5BP6ON5y8M2CCUTQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 1&lt;/strong&gt;. W3W 예시&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;a href=&quot;https://s2geometry.io/&quot;&gt;&lt;strong&gt;Google의 S2&lt;/strong&gt;&lt;/a&gt;와 &lt;a href=&quot;https://h3geo.org/&quot;&gt;&lt;strong&gt;Uber의 H3&lt;/strong&gt;&lt;/a&gt;는 잘 알려져 있는 불변 공간 색인 시스템으로 격자들이 부모-자식(parent-child)의 상하위 위계 구조를 갖습니다. N개의 하위 격자가 1:N 관계에 의해 하나의 상위 격자로 매핑됩니다. 하위 격자들을 합쳐 하나의 상위 격자를 만들 수 있고, 반대로 상위 격자를 하위 격자들로 분할할 수도 있습니다. 설정 가능한 격자의 크기는 다양하며 지리 좌표로 변환하려는 지도의 목적과 규모에 따라 유연하게 선택할 수 있습니다. 저희의 프로젝트 같은 경우 확장성 및 구현의 이점을 위해 H3를 채택했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Google S2와 Uber H3의 사양 비교&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*S2Mr01Zai2YMhdUovAUrVA.jpeg&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;표 1&lt;/strong&gt;. S2와 H3 비교 (출처: &lt;a href=&quot;https://www.uber.com/en-KR/blog/h3&quot;&gt;Uber blog&lt;/a&gt;)&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;왜곡도&lt;/strong&gt;: 위의 표에서 알 수 있듯이, S2와 H3의 가장 큰 차이점은 기본 격자의 모양입니다. H3는 육각형 격자로 구성되어 있어 공간 대부분에서 왜곡이 최소화됩니다. S2는 구에서 정방형으로 지구를 투영(projection)하는 과정에서 상당한 오류를 발생시킵니다. 따라서 같은 레벨의 격자들이라도 투영 중심(center of projection)으로부터의 거리에 따라 서로 크기가 다를 수도 있습니다. 한편, H3는 육각형마다 투영 중심점이 존재하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Fullerene&quot;&gt;풀러린&lt;/a&gt;이나 축구공 모양과 유사합니다. 이는 왜곡을 최소화하는 데 도움이 됩니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;이웃 격자&lt;/strong&gt;: H3는 동일한 영역을 더 적은 수의 격자들로 변환할 수 있습니다. 특정 영역을 나타낼 때, S2는 영역을 더 작은 사각형, 엄밀히 말해 마름모로 분할합니다. H3은 육각형을 이용하기에 서로 인접해 있는 격자들의 표면적이 더 넓어 영역이 상대적으로 더 큼직하게 분할됩니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;위계 구조 &amp;amp; 조밀도: &lt;/strong&gt;H3 같은 경우, 상위 격자들이 물리적으로 모든 하위 격자들을 포함하지는 않습니다. 바다처럼 배송지가 없는 곳에는 육각형 대신 예외적으로 오각형 격자로 분할합니다.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;새로운 기술들로 시스템 재설계하기&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;격자 해상도&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;쿠팡의 배송 서비스가 제공되는 영역들 전부를 육각형 격자로 정확히 표현해내려면 어떻게 해야 할까요? 기본적으로 최적의 크기를 갖는 육각형으로 대한민국 전체를 담아낼 수 있어야 합니다. 크기가 너무 크면 하나의 같은 육각형에 여러 건물 또는 주소들이 할당될 수 있습니다. 크기가 너무 작으면 하나의 건물이나 주소가 여러 다른 육각형들로 할당될 수 있습니다. &lt;br&gt; &lt;br&gt;아래 그림 2의 왼쪽 부분은 약 300m²의 해상도를 가진 12 레벨 육각형의 예를 보여줍니다. 그러나 이 크기의 육각형으로는 기본적으로 도로와 서로 이웃한 ​​건물들로 나누어지는 &lt;a href=&quot;https://en.wikipedia.org/wiki/City_block&quot;&gt;블록(block)들&lt;/a&gt;을 구분해낼 수가 없습니다. 블록을 배달 영역들로 분할하고 주변의 다른 블록들과 상호 배타적으로 만드는 것이 매우 중요하기 때문에 저희는 6.3m²의 해상도를 가진 14 레벨 육각형을 채택했습니다. 이를 통해 해상도로 입구와 우편함 같은 정확한 위치의 위도와 경도 좌표를 대체할 수 있었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;격자 해상도를 활용한 쿠팡 로켓배송 지도 상에서의 12 레벨 육각형 격자(왼쪽)와 14 레벨 육각형 격자(오른쪽) 비교&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*KMo1GFvRIUp3epfrnBJy7A.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 2&lt;/strong&gt;. 12 레벨 육각형 격자(왼쪽)와 14 레벨 육각형 격자(오른쪽) 비교&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;데이터 관리&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;서비스 목적에 맞는 육각형의 크기를 정의했으니, 그 다음으로는 해당 크기의 데이터를 확장성 있게 관리하는 방법을 정의해야 했습니다. 대한민국은 12 레벨의 해상도로 약 4억 3200만 개의 육각형으로 나눌 수 있습니다. 상위 육각형에는 7개의 하위 육각형이 있으므로 13 레벨 해상도는 31억 육각형으로, 14 레벨 해상도는 217억 육각형으로 변환됩니다. 확장성을 충분히 확보하면서 데이터를 관리할 수 있는 방법이 필요했습니다.&lt;/p&gt;&lt;p&gt;데이터를 관리하는 방법들 중 하나는 RDBMS를 사용하는 것입니다. 육각형의 ID를 키로 사용해 데이터를 저장할 수 있습니다. 그러나 RDBMS는 읽기/쓰기(read/write)의 성능 유지를 위해 일반적으로 테이블의 행(row)을 1억에서 10억개로 제한합니다. ID를 행 단위로 관리하면 12 레벨 해상도로 대한민국 전역을 하나의 테이블에 담아낼 수 있지만, 14 레벨 해상도를 적용하려면 지역별로 테이블을 나누거나 다른 방법을 채택해야 합니다. 그리고 데이터베이스 샤딩(database sharding)을 통해 어떻게든 데이터를 관리하더라도 국내외 더 많은 지역들에 서비스를 적용하게 될 경우 데이터 저장 공간이 늘어날 수밖에 없습니다. 따라서 저희는 RDBMS에 육각형 ID를 키로 해 특정 데이터를 저장하는 것은 저희의 유즈 케이스에 적합하지 않다는 결론을 내렸습니다.&lt;/p&gt;&lt;p&gt;이에 각 배송 영역이 영역 내 육각형들의 정보를 갖도록 설계하였습니다. 물론 하나의 매우 큰 영역의 경우 수없이 많은 14 레벨 육각형들의 정보를 갖고 있어야 하기 때문에 메모리 또는 데이터 크기와 관련된 문제가 발생할 수 있습니다. 이를 방지하기 위해 H3에서 제공하는 &lt;a href=&quot;https://h3geo.org/docs/api/hierarchy/&quot;&gt;압축(compaction)&lt;/a&gt; 관련 기능들을 사용하는 것을 고려하였습니다. 하위 육각형들이 모두 존재하는 경우 해당 기능들을 사용하면, 하위 육각형들(finer)과 상위 육각형(coarser)의 해상도를 오고 가면서 원하는 작업을 수행할 수 있습니다. 그리고 이를 통해 데이터 전체를 저장하는데 필요한 메모리 양도 줄일 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡이 배송 영역을 H3 육각형으로 채우고 압축하는 과정&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*LztkDevUVe7O3BPKhjeRrQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 3. &lt;/strong&gt;H3로 배송 영역을 육각형으로 채우고 압축하는 과정&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;이 디자인으로 배달 영역들을 효율적으로 저장하고, 많은 영역들을 저희가 원하는 만큼 14 레벨 해상도로 표현할 수 있습니다. 지역 제한이나 확장성에도 문제가 없습니다. 따라서 아래의 그림 4과 같이 특정 위도-경도 좌표인 [&lt;strong&gt;34.111, 127.111&lt;/strong&gt;]에 대한 영역을 검색하는 경우, 간단히 좌표를 14 단계 해상도를 갖는 육각형의 ID인 &lt;strong&gt;85283473fffffff&lt;/strong&gt;로 변환하고, 이 육각형 ID가 포함된 육각형 그룹을 찾습니다. &lt;a href=&quot;https://postgis.net/&quot;&gt;PostGIS&lt;/a&gt;나 다른 지오데이터베이스(Geodatabase) 모듈이 설치되어 있지 않은 환경에서도 육각형 ID와 육각형 그룹 사이의 포함 관계를 쉽게 분석할 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 로켓배송의 지도가 좌표 값에서 변환된 특정 육각형의 ID가 포함된 육각형 그룹을 조회하는 방법&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*SmgqFfuKWgdKrYJc_tE4lA.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 4&lt;/strong&gt;. 좌표 값에서 변환된 특정 육각형의 ID를 포함하고 있는 육각형 그룹 조회 방법&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;육각형 그룹을 육각형으로 채워진 다각형으로&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;시스템 개발의 첫 단계를 완료한 후 몇 가지 개선이 필요한 부분들을 발견했습니다. 예를 들어 육각형 ID 목록 사용 시 제한되는 것들이 몇 가지 있었습니다. 복잡한 공간을 분석하거나 특정 지점에서 멀리 떨어져 있는 배달 영역들 몇몇을 찾아내는 것이 어려웠습니다. 이를 개선하기 위해 영역의 기본 단위는 육각형 격자로 유지하되 실제 데이터는 다각형 포맷으로 저장했습니다. 어떤 다각형이든 &lt;a href=&quot;https://h3geo.org/docs/api/regions&quot;&gt;polyfill&lt;/a&gt; 기능을 이용해 14 레벨의 육각형들로 변환한 다음 경계를 다시 다각형으로 바꿔 저장할 수 있습니다. 이것은 픽셀 아트(pixel art)를 그리는 것과 비슷합니다. 벡터화된 .ai 파일을 .jpg 비트맵 파일로 변환한 다음, 변환된 파일을 분할해 다시 벡터화된 이미지를 만듭니다.&lt;/p&gt;&lt;p&gt;또한 위치 정보 검색 및 배송 영역 관리에 있어 육각형으로 채워진 다각형들을 활용하면 겹치고 누락되는 영역 없이 좀 더 쉽게 원하는 결과를 얻을 수 있습니다. 아래 그림 5와 같이 클라이언트 사용자가 지도 UI에 사각의 녹색 다각형을 그리면 서버는 이를 육각형 그룹으로 변환한 다음 외부와 접하는 경계만 다시 다각형으로 변환하여 육각형으로 채워진 적색 다각형을 만들어냅니다. 따라서 사용자가 그린 녹색 다각형과 육각형으로 채워진 적색 다각형으로 단일 영역을 표시할 수 있습니다.&lt;/p&gt;&lt;p&gt;이 방법의 유일한 단점은 클라이언트 측에서 매우 복잡한 다각형을 만들어내는 경우 육각형으로 변환하는 데 시간이 조금 더 걸릴 수 있다는 것입니다. 이를 해결하기 위해 사용자가 처음 생성한 다각형은 그대로 저장하고, 나중에 육각형으로 채워진 다각형을 비동기적으로 생성해 활용했습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 로켓배송 지도에서 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 변환하는 과정&quot; src=&quot;https://cdn-images-1.medium.com/max/985/1*euZiKJWYi28BDFbJNOxK6g.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 로켓배송 지도에서 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 처리하는 과정&quot; src=&quot;https://cdn-images-1.medium.com/max/739/1*Jsp4RdTbi5VcKZFazgRmPA.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 5.&lt;/strong&gt; 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 변환하는 과정&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;&lt;strong&gt;시스템에 새로운 기술들 적용하기&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;사용자가 어떻게 육각형 그룹들을 생성하고 관리할 수 있는지에 대해 다루었습니다. 이번 섹션에서는 이러한 기술들을 실제 배송 영역 관리 시스템에 적용한 방법을 소개하고자 합니다. 로켓배송 서비스는 앞에서 언급한 바와 같이 우편번호를 배송 영역의 기본 단위로 두고 필요에 따라 영역을 세분화해 적용할 수 있습니다. 따라서 실제 시스템에서는 목록에서 우편번호 선택 시 해당영역의 공간이 지도 위에 표시됩니다.&lt;/p&gt;&lt;p&gt;아래의 그림 6과 같이 실제 시스템에서 사용자는 우편번호가 가리키는 영역의 안쪽에 자유롭게 다각형을 그리고 영역에 대한 설명을 채워 넣을 수 있습니다. 저장 버튼을 누르면 영역 관련 데이터와 사용자가 그린 다각형이 서버로 전송되어 육각형으로 채워진 다각형으로 변환됩니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡의 배송 영역 관리 시스템 예시&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*pe-GLprJjJbWj98CY0xq4g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;그림 6. &lt;/strong&gt;쿠팡의 배송 영역 관리 시스템&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;예를 들어, 서울의 우편번호인 &lt;strong&gt;06961&lt;/strong&gt;을 선택하면 육각형 그룹들로 구성된 두 개의 영역이 지도 위에 나타납니다. 두 영역 모두 선택한 우편 번호 내에 위치한 개별 아파트 단지입니다. 지도 영역 상단에 제공되는 그리기 도구로 영역들을 그리면 각 영역은 육각형이 채워진 다각형으로 자동 변환됩니다. 각 영역의 테두리를 자세히 살펴보면 테두리가 매끄러운 직선이 아닌 육각형 모양으로 잘려져 있는 것을 볼 수 있습니다.&lt;/p&gt;&lt;p&gt;그림 6의 오른쪽은 &lt;strong&gt;Edit &lt;/strong&gt;버튼을 클릭할 경우 파란색 윤곽선으로 표시된 영역이 수정되는 걸 보여줍니다. 노란색 윤곽선 영역은 여전히 ​​육각형이 채워진 다각형으로 표시되는 반면, 파란색 윤곽선 영역은 사용자가 그린 다각형의 윤곽이 매끄러운 선으로 다시 그려졌습니다. 수정이 완료되면 다각형은 서버에서 처리된 다음 새로운 육각형이 채워진 다각형으로 업데이트됩니다.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;향후 계획&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;이번 포스트에서는 지오데이터베이스와의 일관성을 유지하면서 배송 영역을 효율적으로 관리하기 위해 시스템을 어떻게 설계했는지에 대해 소개했습니다. 새 시스템은 사용자에게 더 향상된 경험을 제공하기 위해 기존의 시스템을 개선한 것으로, 기존 시스템과 계속 함께 사용할 수 있습니다. 또한 배송 영역의 텍스트 주소 모두를 GIS 기반의 공간 데이터로 변환하면서, 시스템을 앞으로도 더 개선할 수 있는 기회들을 갖게 되었습니다.&lt;/p&gt;&lt;p&gt;배송 상태 및 관련 수치들을 육각형 격자에 기반해 분석하게 되면서, 더 이상 우편 번호나 주소로부터 영향받지 않고, 공간 기반의 절대(absolute) 통계를 생성할 수 있게 되었습니다. 어떤 이유로 배송 영역에 변화가 일어나더라도 이전과 동일한 기준으로 통계들을 비교할 수 있습니다. 배송 영역에 대해 정확하게 평가할 수 있으며, 비효율적인 배송 영역을 빠르게 식별해 낼 수 있습니다. 이를 통해 저희 팀은 배송 속도의 저하 요인을 심층적으로 분석하고 문제를 해결하기 위한 대책을 찾을 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;또한 배송 난이도 평가, 배송 영역간 거리 측정, 이동시간 등 배송 영역의 상황도 분석 가능해졌습니다. 덕분에 1) 배송 영역의 난이도를 보다 정확하게 평가하고, 2) 배송 물량에 따라 배송 영역을 동적으로 관리하며, 3) 쿠친이 보다 안전하고 빠르게 배송할 수 있도록 최적화된 이동 경로를 추천할 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;로켓배송 서비스가 처음 시작됐을 때, 주문한 상품이 하루 만에 배송되는 일은 불가능해 보인다는 의구심도 있었습니다. 하지만 지금 익일 배송은 대한민국 쇼핑 문화의 새로운 표준이 되었습니다. 쿠팡은 앞으로도 더 많은 기술 혁신으로 배송 시스템을 발전시켜 고객들을 더욱 더 감동시킬 것입니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;비전을 공유하고 저희와 함께 기술 혁신을 이끌 새로운 팀원을 적극적으로 찾고 있습니다. 쿠팡의 &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3bw6NjT&quot;&gt;&lt;em&gt;채용 웹사이트&lt;/em&gt;&lt;/a&gt;&lt;em&gt;를 확인해보세요!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 &lt;a href=&quot;https://ir.aboutcoupang.com/English/home/default.aspx&quot;&gt;ir.aboutcoupang.com&lt;/a&gt; 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a59006bc4b6e&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EB%B0%B0%EC%86%A1-%EA%B3%B5%EA%B0%84-%EC%83%89%EC%9D%B8-%EA%B8%B0%EB%B0%98%EC%9D%98-%EB%B0%B0%EC%86%A1-%EC%98%81%EC%97%AD-%EA%B4%80%EB%A6%AC-%EC%8B%9C%EC%8A%A4%ED%85%9C-a59006bc4b6e&quot;&gt;쿠팡 로켓배송: 공간 색인 기반의 새로운 배송 영역 관리 시스템&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Coupang Rocket Delivery’s spatial index-based delivery management system</title>
      <link>https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63?source=rss----fb028911af07---4</guid>
      <pubDate>Mon, 17 Apr 2023 00:10:08 GMT</pubDate>
      <content:encoded>&lt;h4&gt;Our initiative to visualize delivery areas for an optimized and intuitive delivery system&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/jinonearth/&quot;&gt;&lt;em&gt;Geo J Son&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &amp;amp; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/%ED%95%9C%EC%83%98-%EC%A0%84-a5a21994/&quot;&gt;&lt;em&gt;Sam HS Jeon&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*HXH-fv_TNeDAcCvGWXSGgw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;This post is also available in &lt;a href=&quot;https://medium.com/coupang-engineering/쿠팡-로켓배송-공간-색인-기반의-배송-영역-관리-시스템-a59006bc4b6e&quot;&gt;&lt;strong&gt;Korean&lt;/strong&gt;&lt;/a&gt;.&lt;/blockquote&gt;&lt;p&gt;Coupang’s &lt;a href=&quot;https://www.youtube.com/watch?v=0nTc6M8asao&quot;&gt;&lt;strong&gt;Rocket Delivery&lt;/strong&gt;&lt;/a&gt; service is the most crucial and attractive feature to WOW the customers. This is achieved by the combined efforts of our delivery drivers, referred to as Coupang Friends (CPFs), and the optimized delivery system.&lt;/p&gt;&lt;p&gt;To enable CPFs to deliver the parcels in the most efficient way possible, the delivery system must have a correct understanding of the delivery address and then direct the drive to the best delivery area for the delivery address. Also, to evenly assign and distribute parcels to each area for optimization, experienced camp workers must be able to frequently update the area information.&lt;/p&gt;&lt;p&gt;Our existing system was dependent on text-based addresses and postal codes, meaning that the system was not as intuitive, functional, or utilitarian as we needed for our complex operations.&lt;/p&gt;&lt;p&gt;Therefore, we started a project in 2021 to provide the foundation for visualizing the delivery areas on the map, enabling direct modification to the map, and providing further metrics and statistics for optimization.&lt;/p&gt;&lt;h4&gt;Table of Contents&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#9f67&quot;&gt;Challenge: Text-based postal code&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#7fc4&quot;&gt;What did we need?&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#1a50&quot;&gt;Adopting H3 into delivery area management system&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#0225&quot;&gt;Geospatial indexing systems&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#fc24&quot;&gt;Spatial indexing systems&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#1e61&quot;&gt;Redesigning the system with new techniques&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#13c9&quot;&gt;Grid resolution&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#6277&quot;&gt;Data management&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#d1f4&quot;&gt;From a hexagon group to a hexagonized polygon&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#6ee1&quot;&gt;Applying new techniques to the system&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#2bfc&quot;&gt;What’s next?&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;strong&gt;Challenge: Text-based postal code&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Postal codes are one of the most well-organized systems guaranteed by the government and authorities that assign a code to a specific delivery area. The physical areas of postal codes vary in terms of geometric element and delivery demands, but the efficiency of deliveries they provide is highly optimized. That is why Rocket Delivery adopted the use of postal codes in the beginning as the basic unit for assigning delivery areas. It worked for us at first, but the growth of Coupang demanded much more. The number of deliveries that needed to be made each day per postal code increased from double digits to triple digits. That was too much for a single CPF to cover in one day.&lt;/p&gt;&lt;p&gt;Therefore, we needed to segment a single postal code and break it down into multiple processable areas. For segmentation, for example, we started by perceiving the postal area as multiple apartment complexes. Then, we segmented that into a single complex, and then further into individual buildings. However, because addresses were solely in text with no or little spatial information, only experienced camp leaders and CPF group leaders were able to manage segmentation since they were familiar with the characteristics of the delivery areas.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;What did we need?&lt;/strong&gt;&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;A more efficient and easier way to visualize delivery areas on a map.&lt;/li&gt;&lt;li&gt;A way to allow all related personnel to easily view, create, and modify delivery areas on the map and share them with others.&lt;/li&gt;&lt;li&gt;Including and utilizing consistent spatial data of delivery areas, not just postal codes and text addresses that require to be updated when buildings were newly built or demolished.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;&lt;strong&gt;Adopting H3 into delivery area management system&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;To address our needs, we started an initiative. We used a hexagon grid-based spatial indexing system that provides an intuitive way to manage spatial areas by drawing polygons on a map.&lt;/p&gt;&lt;p&gt;Drawing polygons on a map might seem simple, but it is difficult to meticulously cover all areas by clear and definite boundaries without duplicating or omitting areas. To manage mutually exclusive and collectively exhaustive (MECE) polygon sets, we started by defining the spatial units.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Geospatial indexing systems&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;We first reviewed various domain geospatial indexing systems created by GIS professionals. A geospatial indexing system is a set of grids which fully cover the specified geographic surface with unique identifiers. Postal codes and administrative divisions are good examples of irregular-shaped grids, and flexibly editable by authorities. For an analysis of integrated time series, mutable spatial boundaries, of which identifiers do not change for areas that they are assigned to, have more advantages.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Spatial indexing systems&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://what3words.com/&quot;&gt;&lt;strong&gt;W3W (What3Words)&lt;/strong&gt;&lt;/a&gt; is one of the most popular spatial indexing systems, which splits the world into 3-meter square grids and gives each grid a unique combination of three words. Latitude-longitude coordination system has difficulties in delivering on verbal messages as it is made up of a long sequence of numbers, while W3W can place the location with the combination of only unique 3 words. For example, &lt;a href=&quot;https://www.coupang.jobs/en/locations/seoul/&quot;&gt;Rocket Laboratory&lt;/a&gt;, one of the Coupang offices, is located at [&lt;strong&gt;37.503819, 127.0481493&lt;/strong&gt;] in latitude-longitude coordinates and the W3W is [&lt;a href=&quot;https://what3words.com/scoring.eager.patch&quot;&gt;&lt;strong&gt;scoring.eager.patch&lt;/strong&gt;&lt;/a&gt;]. Unfortunately, the size of the resolution cannot be adjusted for analysis.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Coupang Rocket Laboratory’s W3W address&quot; src=&quot;https://cdn-images-1.medium.com/max/484/1*lrtZxW5BP6ON5y8M2CCUTQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. Example of W3W&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;a href=&quot;https://s2geometry.io/&quot;&gt;&lt;strong&gt;Google’s S2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;and&lt;strong&gt; &lt;/strong&gt;&lt;a href=&quot;https://h3geo.org/&quot;&gt;&lt;strong&gt;Uber’s H3&lt;/strong&gt;&lt;/a&gt; are popular immutable spatial indexing systems with parent-child hierarchies. N number of child grids are mapped to a single parent grid by 1:N relation. Child grids can be accumulated into their parent, and in reverse, parent grid can be divided into child grids. The level of grid size varies and can be flexibly selected according to the purpose and scale. For our project, we adopted H3 for benefits in scalability and implementation.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Specification comparison of Google’s S2 and Uber’s H3&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*e-IH2DaNqMIQcLjmYvFTYA.jpeg&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Table 1&lt;/strong&gt;. Comparison of S2 and H3 (Source: &lt;a href=&quot;https://www.uber.com/blog/h3/&quot;&gt;Uber blog&lt;/a&gt;)&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Distortion:&lt;/strong&gt; As shown in the table above, the biggest difference between S2 and H3 is the shape of the basic grid. H3 is made up of hexagon grids, which have minimal distortion in most spaces. S2 has significant errors when projecting the Earth from a sphere to a cubic square. Therefore, even grids of the same level can vary in terms of size depending on the distance from the center of projection. On the other hand, H3 is similar to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fullerene&quot;&gt;fullerene&lt;/a&gt; or soccer ball shape, of which the projection center point exists for each hexagon. This helps to minimize distortion.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Neighborhood:&lt;/strong&gt; H3 can convert the same area into a fewer number of grids. When representing an area, S2 divides it into much smaller squares — strictly, diamond squares. Whereas H3 is hexagonal, the surface extent of the neighboring grids is larger, which divides areas into relatively coarse sections.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Hierarchy &amp;amp; Compactness:&lt;/strong&gt; When using H3, note that parent grids do not physically cover all child grids, and there are exceptional pentagonal grids instead of hexagons, mainly in the sea, which has no delivery address.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;Redesigning the system with new techniques&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Grid resolution&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;How can we accurately express delivery areas in hexagon grids for Coupang, having in mind that we need to cover all areas of the delivery service? The size of the hexagon needs to be optimal as we cover the entire area within South Korea. If the size is too big, multiple buildings or addresses can be assigned to the same single hexagon. If the size is too small, a single building or address can be assigned to the different multiple hexagons.&lt;/p&gt;&lt;p&gt;In Figure 2 below, the image on the left shows a 12-level hexagon example with a resolution of about 300-square meters. However, the size of the hexagon in the image cannot distinguish &lt;a href=&quot;https://en.wikipedia.org/wiki/City_block&quot;&gt;blocks&lt;/a&gt; which are divided by road and neighboring buildings. It is critical to segment a block into delivery areas and make it mutually exclusive to other blocks surrounding it. From this perspective, we adopted a 14-level hexagon with a resolution of 6.3-square meters. The resolution means that it can replace the latitude and longitude coordinates of precise locations like entrance and mailboxes.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Comparison of 12-level hexagon (left) and 14-level hexagon (right) grids, using grid resolution on Coupang Rocket Delivery map&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*KMo1GFvRIUp3epfrnBJy7A.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. Comparison of 12-level hexagon (left) and 14-level hexagon (right) grids&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Data management&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Once we have defined the size of the hexagon for our service, we then needed to define how to manage data with scalability. South Korea can be divided into approximately 432 million hexagons in 12-level resolution. A parent hexagon has seven child hexagons, so a 13-level resolution would be converted to 3.1 billion hexagons and a 14-level resolution to 21.7 billion hexagons. We needed a way to manage data while ensuring sufficient scalability.&lt;/p&gt;&lt;p&gt;One way to manage data is to use RDBMS. We can use a hexagon ID as a key and put data into it. However, for general usage RDBMS restricts &lt;strong&gt;read/write &lt;/strong&gt;function from 0.1 to 1 billion counts of table rows. If we manage the IDs row-by-row, we may be able to control the entire area of Korea in 12-level resolution, but it will be impossible to do so in 14-level without dividing the tables according to regions or adopting other standards. And even if we do somehow manage data by database sharing, we would not be able to avoid increasing the storage for data once we apply the service to more regions outside Korea. We concluded that it is not suitable for our use case to put specific data of which keys are individual hexagon IDs into RDBMS.&lt;/p&gt;&lt;p&gt;Therefore, we designed each delivery area to have information of hexagons. This means that a single large area can have a large number of 14-level hexagons, encountering potential memory or data size problems. To avoid this, we considered using the &lt;a href=&quot;https://h3geo.org/docs/api/hierarchy/&quot;&gt;compaction functions&lt;/a&gt; provided by H3. It allows to convert between resolutions of child hexagons (finer) and their parent hexagon (coarser) when all child hexagons exist. We can also reduce the amount of memory for whole records.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Overview of how Coupang Rocket Delivery map compacts H3 hexagons&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*j_HtbkISZ1c0HyZcwzUVgQ.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Overview of how to compact H3 hexagons&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This design can store delivery areas efficiently and represents as many areas as required in the 14-level resolution. There are no regional restrictions or scalability issues. As shown in Figure 4 below, in order to search an area for a specific latitude-longitude coordinate as [&lt;strong&gt;34.111, 127.111]&lt;/strong&gt;, we simply convert the coordinate to an ID of a hexagon grid with 14-level resolution as &lt;strong&gt;85283473fffffff&lt;/strong&gt; and look up which hexagon group contains this hexagon ID. The inclusion relationship between hexagon ID and hexagon groups can be easily analyzed even in the environment that is not installed with &lt;a href=&quot;https://postgis.net/&quot;&gt;PostGIS&lt;/a&gt; or other geodatabase modules.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Overview of how Coupang Rocket Delivery map looks up hexagon groups including a specific hexagon ID converted from a coordinate value&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*F2tBKbCIVpVGgbwsigljqw.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. Overview of how to look up hexagon groups including a specific hexagon ID converted from a coordinate value&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;From a hexagon group to a hexagonized polygon&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Once we completed the first phase of the system, we found some areas to be improved. For example, there were limitations with using hexagon ID lists. It was difficult to perform complex spatial analysis or find certain delivery areas that are distant from a specific point. To improve this, we kept the basic unit of the areas as hexagon grids but stored the actual data in a polygon format. Whatever polygon you draw, you can convert it to 14-level hexagon by the &lt;a href=&quot;https://h3geo.org/docs/api/regions&quot;&gt;polyfill&lt;/a&gt; function and then store the boundaries back into polygon. This resembles drawing a pixel art. Convert a vectorized .ai file to a .jpg bitmap file, and segmentize it back into a vectorized image again.&lt;/p&gt;&lt;p&gt;Hexagonized polygons also allow easy geosearching and managing delivery areas without overlapping or omitted areas. As shown in Figure 5 below, when a client draws a green square polygon on a map UI, the server converts it to a group of hexagons, and then generates only the outer boundary back into the polygon as a red hexagonized polygon. Therefore, a single area can be displayed with both user-drawn green polygon and the red hexagonized polygon.&lt;/p&gt;&lt;p&gt;The only one flaw of this method is, if the client side creates highly complex polygons, it may take a little longer to convert them into hexagons. We resolved to first store the polygon as drawn by users, and then later asynchronously generate and utilize the hexagonized polygon from it.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Process of how Coupang Rocket Delivery map converts a user-drawn square polygon into a hexagonized polygon&quot; src=&quot;https://cdn-images-1.medium.com/max/977/1*OCXQ95SlwWvNdWRrxh8IpQ.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;Overview of how Coupang Rocket Delivery map handles a user-drawn square polygon and converts it to a hexagonized polygon&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*J39jV91lg5FTlROBZeogSg.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; How a user-drawn square polygon is converted to a hexagonized polygon&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;&lt;strong&gt;Applying new techniques to the system&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;We have covered the concept of how users can generate and manage hexagon groups. This section introduces how to apply these techniques to the actual delivery area management system. As mentioned in the introduction, Rocket Delivery service considers the postal code as the basic unit of delivery areas, and subdivisions can be applied if necessary. Therefore, in the actual system, when the postal code is selected from the list, the spatial extent is displayed on the map.&lt;/p&gt;&lt;p&gt;As shown in Figure 6 below, users can freely draw polygons within the postal code area and add area description together in the actual system. When the &lt;strong&gt;Save &lt;/strong&gt;button is pressed, the area data and polygon drawn by the user are transmitted to the server and converted into hexagonized polygons.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Example of Coupang’s delivery area management system&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*pe-GLprJjJbWj98CY0xq4g.png&quot; /&gt;&lt;figcaption&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Coupang’s delivery area management system&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For example, the number 06961, a post code in Seoul, is selected and displayed on the map, which is then divided into two areas of hexagon groups. Both areas are individual apartment complexes located within this postal code. Each area is drawn with a drawing tool provided at the top of the map section, and the conversion to hexagonized polygon is automatically carried out. If you focus on the border of each area closely, you will see that the border is cut into hexagonal shapes and not a smooth straight line.&lt;/p&gt;&lt;p&gt;On the right side of Figure 6, the &lt;strong&gt;Edit &lt;/strong&gt;button is pressed to make corrections to the area outlined in blue. The area outlined in yellow is still represented by hexagonized polygon, while the area outlined in blue is redrawn by smooth lines with user-drawn polygons. When the modification is completed, the modified polygon is processed by the server and updated to the new hexagonized polygon.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;What’s next?&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;This post introduced how we designed our system to manage the delivery area efficiently by maintaining consistency with geodatabase. The new system is an improvement of our old system to provide better experiences for users, and it can still be used with the legacy system. Also, we have new further opportunities to improve the system as we utilize GIS-based spatial data as we have converted all textual addresses of delivery areas into spatial data.&lt;/p&gt;&lt;p&gt;The status and metrics of each delivery can be analyzed for each hexagon grid, which can produce spatially absolute statistics that are no longer bound to postal codes or addresses. Even if there is a change in the delivery area for some reason, the metrics can be compared on the same basis as before. Delivery areas can be accurately evaluated, and the less efficient delivery area can be identified quickly. This leads our team to be able to deep-dive the slowing factors and find the countermeasures to solve the problem.&lt;/p&gt;&lt;p&gt;Furthermore, we can analyze the conditions of delivery areas, from rating delivery difficulties, measuring distances to moving time between delivery areas and so on. These achievements lead to 1) evaluating the difficulty of the delivery area more precisely, 2) managing the delivery area dynamically according to the volume of the shipment, and 3) recommending the optimized trip routes for CPFs to deliver safer and faster.&lt;/p&gt;&lt;p&gt;Some were skeptical about our Rocket Delivery service when it was first released, as it seemed impossible&lt;em&gt; &lt;/em&gt;to guarantee overnight delivery of goods. But now, it is a new standard of the Korean online shopping culture. Coupang will continue to develop the delivery system through technological innovation to continue to wow the customer.&lt;/p&gt;&lt;p&gt;&lt;em&gt;We’re actively looking for new teammates who share our vision and will accomplish great things together. Interested? Check out our &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3Q7AKWS&quot;&gt;&lt;em&gt;career website&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on &lt;/em&gt;&lt;a href=&quot;http://ir.aboutcoupang.com/&quot;&gt;&lt;em&gt;ir.aboutcoupang.com&lt;/em&gt;&lt;/a&gt;&lt;em&gt; for information on our formal investment plans and product development strategies.&lt;/em&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=26940eaaee63&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63&quot;&gt;Coupang Rocket Delivery’s spatial index-based delivery management system&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>뷰티 필터를 개발하며 얻은 새로운 경험</title>
      <link>http://thefarmersfront.github.io/blog/2022-5th-tech-meetup-web-frontend/</link>
      <guid>http://thefarmersfront.github.io/blog/2022-5th-tech-meetup-web-frontend/</guid>
      <pubDate>Thu, 13 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded>처음 만나는 React Query, 그리고...</content:encoded>
    </item>
    <item>
      <title>우당탕탕 beauty 풀한, 컬리 앱 서비스 런칭기</title>
      <link>http://thefarmersfront.github.io/blog/beauty-kurly-launching-app/</link>
      <guid>http://thefarmersfront.github.io/blog/beauty-kurly-launching-app/</guid>
      <pubDate>Tue, 11 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded>iOS 개발자 관점에서 뷰티 컬리를 어떻게 오픈했을까?</content:encoded>
    </item>
    <item>
      <title>Dataflow로 컬리의 준실시간 수요 예측모델 파이프라인 구축하기 - 1편</title>
      <link>http://thefarmersfront.github.io/blog/dataflow-pipeline-1/</link>
      <guid>http://thefarmersfront.github.io/blog/dataflow-pipeline-1/</guid>
      <pubDate>Mon, 10 Apr 2023 10:00:00 GMT</pubDate>
      <content:encoded>Dataflow 서비스 잘 이해하기</content:encoded>
    </item>
    <item>
      <title>지난 9개월간의 딜리버리프로덕트팀의 경험과 변화에 대한 이야기: 애자일</title>
      <link>http://thefarmersfront.github.io/blog/teamsoftwaredevelopment-changes-deliveryproduct/</link>
      <guid>http://thefarmersfront.github.io/blog/teamsoftwaredevelopment-changes-deliveryproduct/</guid>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded>팀 소프트웨어 개발 (애자일에 기반한 팀 플레이) 여정을 애자일 코치와 함께 한 우리는 어떤 일들을 겪었고, 무엇을 얻었을까?</content:encoded>
    </item>
    <item>
      <title>6시간 동안 단체로 해외 콘퍼런스 참여하기: 프로덕트콘 같이 보기 이벤트 - ProductCon London 2023</title>
      <link>http://thefarmersfront.github.io/blog/event-watch-productcon-london2023/</link>
      <guid>http://thefarmersfront.github.io/blog/event-watch-productcon-london2023/</guid>
      <pubDate>Thu, 30 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded>프로덕트를 다양한 관점에서 바라보고 더 깊게 이해할 수 있는 콘퍼런스를 원격으로 함께 보는 이벤트를 하였습니다.</content:encoded>
    </item>
    <item>
      <title>클라우드 서비스 사용량 관리를 통한 운영 비용 최적화</title>
      <link>https://medium.com/coupang-engineering/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%82%AC%EC%9A%A9%EB%9F%89-%EA%B4%80%EB%A6%AC%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9A%B4%EC%98%81-%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%A0%81%ED%99%94-1521565c64ec?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%82%AC%EC%9A%A9%EB%9F%89-%EA%B4%80%EB%A6%AC%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9A%B4%EC%98%81-%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%A0%81%ED%99%94-1521565c64ec?source=rss----fb028911af07---4</guid>
      <pubDate>Mon, 27 Mar 2023 01:40:09 GMT</pubDate>
      <content:encoded>&lt;h4&gt;쿠팡 엔지니어링 조직들이 클라우드 비용을 줄이기 위해 들인 노력과 그 결과에 대하여&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/luketravers&quot;&gt;&lt;em&gt;Luke Travers&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &amp;amp; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/amitarora58&quot;&gt;&lt;em&gt;Amit Arora&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*9Z5qFMsAjOUCQowytGnyCA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;본 포스트는 &lt;a href=&quot;https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b&quot;&gt;&lt;strong&gt;영문&lt;/strong&gt;&lt;/a&gt;으로도 제공됩니다.&lt;/blockquote&gt;&lt;p&gt;쿠팡의 파이낸스 및 엔지니어링 팀들은 지난 몇 분기 동안 로드맵을 바탕으로 서로 협력하며 클라우드 서비스에 지출되는 온디맨드(on-demand) 비용을 관리하고 최적화해왔습니다. 여러 엔지니어링 조직들이 하나의 팀이 되어 노력했기에 가능한 일이었습니다.&lt;/p&gt;&lt;p&gt;하나의 팀으로서 저희는 다음과 같은 세 가지 핵심 원칙 하에 비용 최적화 작업을 수행했고, 이 포스트를 통해 어떤 작업이 이루어졌는지 공유드리고자 합니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;예산 할당 및 준수&lt;/li&gt;&lt;li&gt;목표 절감액&lt;/li&gt;&lt;li&gt;쿠팡 리더십 원칙인&lt;strong&gt; Hate Waste&lt;/strong&gt;에 기반해 서비스 신뢰성, 지속 가능성 및 서비스 사용량 통제 부분에 집중&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;목차&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#421f&quot;&gt;배경 및 과제&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#cf9e&quot;&gt;1 단계: 최적화 프로젝트 팀 구성하기&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#4a2d&quot;&gt;2 단계: 더 적게 쓰고 더 적게 지불하기&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#2642&quot;&gt;인스턴스 세대 조정&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#5843&quot;&gt;EMR&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#56a5&quot;&gt;스토리지&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#97ba&quot;&gt;결론&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;배경 및 과제&lt;/h3&gt;&lt;p&gt;비용 최적화 작업을 시작할 당시 쿠팡은 다음과 같은 상황에 처해있었습니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;클라우드 서비스를 효율적으로 사용하는 방법에 대한 엔지니어링 팀들은 이해도가 그리 높지 않은 상태였습니다. 그로 인해 필요 이상으로 서비스를 더 많이 사용하면서 불필요한 비용을 지출하고 있었습니다.&lt;/li&gt;&lt;li&gt;파이낸스 팀들은 어떤 팀이 클라우드 서비스 사용료를 지출하고 있는지, 그리고 성장하는 비즈니스에 영향을 주지 않으면서도 지출을 억제할 수 있는 방법이 무엇인지 이해하고자 고군분투 중이었습니다.&lt;/li&gt;&lt;li&gt;리더십 팀은 클라우드 서비스 비용 지출에 대해 충분히 분석하고 있지 못했습니다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이에 리더십 팀은 클라우드 서비스의 가변 비용 모델에 대한 재정적 책임을 명확히 해야 한다는 판단을 내렸습니다. 리더십 팀의 지원 하에 클라우드와 관련해 전문성을 지닌 동료들이 엔지니어링 팀들에 합류하게 되었고, 모두 다 함께 클라우드 서비스를 효율적으로 사용하면서 동시에 비용도 절감할 수 있는 방법을 찾기 시작했습니다.&lt;/p&gt;&lt;h3&gt;1 단계: 최적화 프로젝트 팀 구성하기&lt;/h3&gt;&lt;p&gt;클라우드 인프라 엔지니어와 테크니컬 프로그램 관리자(Technical Program Manager, TPM)를 주축으로 한 최적화 프로젝트 팀이 만들어졌습니다. 팀 구성원들은 서로 협력하며 클라우드 서비스 비용을 효율적으로 지출할 수 있는 방법을 찾아냈습니다.&lt;/p&gt;&lt;p&gt;프로젝트 팀은 각 도메인 팀과의 협업을 통해, 클라우드 서비스를 소유하고 사용할 때에는 서비스의 가변 비용 모델을 활용해야 한다는 점을 도메인 팀이 이해할 수 있도록 도왔습니다. 예를 들어, 저희는 Amazon S3에 저장된 데이터를 이해하고 유휴 상태(at rest)에서 스토리지 구조를 최적화할 수 있도록 도메인 팀과 협업했습니다. 또한 AWS Spot Instances 및 ARM 기반 AWS Graviton과 같은 서비스를 활용해 데이터 저장 및 처리 비용을 획기적으로 줄일 수 있는 방법을 찾아냈습니다. 그리고 프로젝트 팀은 정확한 분석이 제공되는 환경을 만들었고, 이에 도메인 팀들이 데이터에 기반해 의사 결정을 내릴 수 있게 되면서 클라우드 서비스를 더 잘 활용할 수 있게 되었습니다.&lt;/p&gt;&lt;p&gt;프로젝트 팀은 도메인 팀들 전체가 클라우드 서비스의 효율적인 사용을 하나의 엔지니어링 문화로 받아들일 수 있게끔, 팀들이 필요로 하는 애널리틱스(analytics), 도구 및 프로세스를 만들어 제공했습니다. 가령 Amazon Athena를 통해 처리된 Amazon CloudWatch 데이터를 사용해 맞춤형 대시보드를 개발했고 AWS CUR(Cost &amp;amp; Usage Reports) 데이터 처리를 위해 저희의 BI(Business Intelligence) 대시보드도 활용했습니다. 다른 한편으로는 파이낸스 팀이 저희와 함께 도메인 팀들을 지원하였고, 그 과정에서 도메인 팀들은 팀에 할당된 월별 및 분기별 예산을 관리하는 것이 얼마나 중요한지를 이해하게 되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*M12QIbXip5i44vJKSPL1tA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;2 단계: 더 적게 쓰고 더 적게 지불하기&lt;/h3&gt;&lt;p&gt;프로젝트 팀은 도메인 팀들이 필요로 하는 애널리틱스와 도구들을 개발할 때 다음과 같이 서로 연관되어 있지만 대조되는 두 가지 방법에 중점을 두고 필요한 작업들을 진행했습니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;더 적게 쓰기(사용량을 줄여 비용 절감):&lt;/strong&gt; 비-프로덕션(non-prod) 환경에서 필요 시에만 AWS 리소스가 자동 시작되게끔 하였습니다. 이를 통해 쿠팡은 비-프로덕션 환경에서 25%의 비용을 절감할 수 있었습니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;더 적게 지불하기(조정을 통해 사용량을 적정 수준으로 줄이기):&lt;/strong&gt; 관련 데이터를 분석해 파악한 클라우드 서비스 사용 패턴을 바탕으로 프로젝트 팀은 쿠팡 내 전체 도메인 팀들과 긴밀히 협력하며 사용되지 않는 EC2 리소스를 수동으로 제거했습니다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;사용량 최적화 및 비용 절감을 주요 목표로 잡고, 다음과 같은 방법들을 통해 AWS 클라우드 서비스의 2021년 사용료를 수백만 달러(온디맨드 비용) 이상 절감했습니다.&lt;/p&gt;&lt;p&gt;저희가 채택한 최적화 기술은 비용 절감에 도움이 되었을 뿐만 아니라 클라우드 리소스를 더 효율적으로 활용할 수 있게 해주었습니다. AWS의 모범 사례(best practices) 및 권장 사항을 참고해 다음과 같은 작업들을 수행했습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;인스턴스 세대 조정&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;저희는 성능 향상, 비용 절감 및 가용성 향상을 위해 쿠팡의 모든 단일 인스턴스를 최신 세대로 조정하고 싶었습니다. 이를 위해서는 AMD 및 ARM과 같은 다양한 칩 아키텍처를 탐색할 뿐만 아니라 각각의 모든 인스턴스 유형을 테스트해야 했고 도메인 팀들과의 광범위한 협업이 필요했습니다. 힘든 테스트 과정을 거쳐 저희는 내부에서 사용되는 제품 전체를 AMD CPU 기반 클라우드 서비스로 성공적으로 이전할 수 있었고, 이전 버전에 비해 가격 대비 성능을 20% 더 향상시킬 수 있었습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;EMR&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;저희는 EMR에 Spot Instances를 사용하는 것을 선호하지만, 잘 알려져 있듯이 피크 시간대에 Spot을 사용하면 원하는 만큼의 용량을 확보하기 어려울 수도 있습니다. 복잡하게 통합되어 있는 EMR 시스템으로 서비스 중단 없이 툴체인(toolchain) 각 부분의 업데이트를 확실히 보장해야만 했습니다. 이를 위해 소프트웨어 버전을 업그레이드하여 AWS EMR의 &lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html&quot;&gt;인스턴스 플릿 기능&lt;/a&gt;을 활용하였고, 이 업그레이드를 통해 전체 EMR 비용을 25% 절감할 수 있었습니다.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;스토리지&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;EBS 및 S3의 AWS 스토리지 비용을 다음과 같이 절감할 수 있었습니다.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Amazon EBS: 스토리지의 경우 차세대 EBS인 GP3가 저희의 요구 사항들을 앞으로도 계속 충족시키면서 동시에 내부 구성원들의 신뢰도 얻을 수 있을 것이라는 판단 하에, GP3에 대한 광범위한 테스트를 수행했습니다. 내부 구성원들의 신뢰를 얻기 위해, 다양한 도구들로 폭넓은 성능 테스트를 수행했습니다. 먼저 개발용 계정에서 모든 테스트를 수행한 결과, 별다른 영향 없이 한 번에 500–1000개의 라이브 볼륨을 수월하게 마이그레이션할 수 있음을 확인했습니다.&lt;/li&gt;&lt;li&gt;Amazon S3: 50 페타바이트(PB) 이상을 Intelligent-Tiering(IT)으로 옮겼습니다. 그 과정에서 저희는 모든 워크로드가 IT와 원활하게 작동하는 것은 아니며 개체 크기에 매우 주의해야 한다는 비싼 교훈을 얻었습니다. 평균 크기가 매우 작은 오브젝트들(object)이 수십억 개가 있는 경우, 해당 워크로드에 대한 전체 S3 비용은 크게 증가할 수 있습니다. 이 경우 관련 정책을 조정하려면 S3 수명 주기 필터(lifecycle filter)를 사용해야 합니다. S3의 복잡한 과금 패턴과 문제를 일으키지 않으려면, 이를 ‘일회성으로 수행되는(one and done)’ 프로세스(process)가 아니라 ‘계속 이어지는(ongoing)’ 주기(cycle)로 이해하고 긴 시간 동안 많은 주의를 기울여야 합니다.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;클라우드 사용 비용을 최적화하기 위한 쿠팡의 노력으로, Amazon S3의 사용량이 증가함에도 AWS 사용 금액은 절감됨&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*eKqrpaDIcrlhPe0mVp7WOQ.png&quot; /&gt;&lt;figcaption&gt;그림 1. 증가하는 Amazon S3 사용량 대비 줄어드는 저장 용량 단위당 AWS 비용 추이&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;결론&lt;/h3&gt;&lt;p&gt;위와 같은 방법들을 통해 저희는 2021년에 수백만 달러의 온디맨드 비용을 절감함으로써 AWS 클라우드 서비스로의 지출을 최소화할 수 있었습니다. 최적화할 영역을 정확히 파악해내고 최적화 달성에 집중하면서 다양한 프로세스를 직접 수행해냈습니다. 저희는 비용 절감과 효율성 향상을 위해 여전히 최적화가 필요한 영역을 최선을 다해 찾고 있습니다.&lt;/p&gt;&lt;p&gt;클라우드 서비스 비용을 수백만 달러 절약했지만 아직 저희의 여정은 끝나지 않았습니다. 다음 단계로 보다 복잡한 분석 도구들에 대한 투자를 늘려 클라우드 핀옵스(Cloud FinOps) 사고방식이 쿠팡에 자리 잡을 수 있도록 노력할 것입니다. 또한 클라우드 서비스 비용의 최적화에 필요한 모니터링 및 분석 프로세스들 중 일부를 자동화할 예정입니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;클라우드 최적화와 비용 효율화에 대해 깊이 이해하고 열정을 가지고 계시다면, 쿠팡 &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3bw6NjT&quot;&gt;&lt;em&gt;채용 공고&lt;/em&gt;&lt;/a&gt;&lt;em&gt;를 확인해 보세요!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 &lt;a href=&quot;https://ir.aboutcoupang.com/English/home/default.aspx&quot;&gt;ir.aboutcoupang.com&lt;/a&gt; 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1521565c64ec&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%82%AC%EC%9A%A9%EB%9F%89-%EA%B4%80%EB%A6%AC%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9A%B4%EC%98%81-%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%A0%81%ED%99%94-1521565c64ec&quot;&gt;클라우드 서비스 사용량 관리를 통한 운영 비용 최적화&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Cloud expenditure optimization for cost efficiency</title>
      <link>https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b?source=rss----fb028911af07---4</guid>
      <pubDate>Tue, 21 Mar 2023 06:03:32 GMT</pubDate>
      <content:encoded>&lt;h4&gt;The roadmap and execution for cutting down cloud spending across Coupang engineering organizations&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/luketravers&quot;&gt;&lt;em&gt;Luke Travers&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &amp;amp; &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/amitarora58&quot;&gt;&lt;em&gt;Amit Arora&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*9Z5qFMsAjOUCQowytGnyCA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;This post is also available in &lt;a href=&quot;https://medium.com/coupang-engineering/클라우드-서비스-사용량-관리를-통한-운영-비용-최적화-1521565c64ec&quot;&gt;&lt;strong&gt;Korean&lt;/strong&gt;&lt;/a&gt;.&lt;/blockquote&gt;&lt;p&gt;In this post, we share how the finance and engineering teams at Coupang have partnered together over the past few quarters to provide a roadmap to manage and optimize cloud expenditure. We will also detail how multiple engineering teams formed a Central team to further optimize the cloud spending for on-demand cost.&lt;/p&gt;&lt;p&gt;The Central team’s efforts were narrowed down to the following three principles:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Budget allocation and its compliance&lt;/li&gt;&lt;li&gt;Savings as the goal&lt;/li&gt;&lt;li&gt;Focus areas of credibility, sustainability, and control in line with the company’s leadership principle of “&lt;strong&gt;Hate Waste&lt;/strong&gt;”&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Table of Contents&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#2d73&quot;&gt;Background &amp;amp; Challenges&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#dc48&quot;&gt;Stage 1: Forming a Central team&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#8d88&quot;&gt;Stage 2: Spending Less &amp;amp; Paying Less&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#9946&quot;&gt;Instance generation alignment&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#2e5f&quot;&gt;EMR&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#3ad5&quot;&gt;Storage&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#f7a1&quot;&gt;Conclusion&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;Background &amp;amp; Challenges&lt;/h3&gt;&lt;p&gt;As a company we were in a classic situation where:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;engineering teams were spending more than they needed on cloud, with little understanding of cloud efficiency.&lt;/li&gt;&lt;li&gt;finance teams were struggling to understand what teams were spending on, and how to curb expenditures without impacting business growth.&lt;/li&gt;&lt;li&gt;the leadership team did not have enough analytics into cloud spending.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To bring financial accountability to the variable spending models of cloud, the leadership team aided the engineering teams by engaging the right people to find the opportunities for efficiency and cost savings.&lt;/p&gt;&lt;h3&gt;Stage 1: Forming a Central team&lt;/h3&gt;&lt;p&gt;Our cloud infrastructure engineers and technical program managers collaborated as a Central team to identify a few initiatives for cloud spending efficiency.&lt;/p&gt;&lt;p&gt;The Central team collaborated with each domain team and helped them understand that while they are the owners of cloud usage, they must also take advantage of the cloud’s variable cost models. For instance, we helped one of our domain teams to understand their data stored in Amazon S3 and how the storage structure could be optimized at rest. Also, we shed light on how we could use tools such as AWS Spot Instances and ARM-based AWS Graviton, resulting in the dramatic cost reduction on storing and processing data. The Central team made sure that the right analytics was available, helping teams to take data-driven decisions based on the value of cloud.&lt;/p&gt;&lt;p&gt;The Central team understood the importance of the right analytics, tools, and processes to derive cloud efficiency as a culture across the domain teams. In that sense, we created custom dashboards using Amazon CloudWatch data processed through Amazon Athena, and we also utilized our BI (Business Intelligence) dashboards for processing AWS CUR (Cost &amp;amp; Usage Reports) data. The finance team also partnered with us from the other end and helped us to push forward the importance of managing the domain teams to their assigned monthly and quarterly budgets.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*M12QIbXip5i44vJKSPL1tA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;Stage 2: Spending Less &amp;amp; Paying Less&lt;/h3&gt;&lt;p&gt;The Central team equipped with the right analytics and tools focused on optimizing in the following two interrelated yet contrasting methods:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Spending Less (Expenditure Reduction by Using Less):&lt;/strong&gt; Automating the launch of AWS resources on non-production environment on a need basis. This helped the company save 25% in costs on non-prod environments.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Paying Less (Usage Reduction by Rightsizing):&lt;/strong&gt; With the right data to analyze the usage patterns, the Central team worked closely with the domain teams across the company to manually eliminate unutilized and underutilized EC2 resources.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;With usage optimization and cost savings as our main goals, the following initiatives helped save millions of dollars (On-Demand cost) in 2021 on AWS Cloud.&lt;/p&gt;&lt;p&gt;The optimization techniques we adopted not only helped us save in costs, but also unlocked more efficient cloud resources. Based on the best practices and recommendations from AWS, we implemented the following initiatives.&lt;/p&gt;&lt;h4&gt;Instance generation alignment&lt;/h4&gt;&lt;p&gt;We wanted to bring every single instance in Coupang up to the current generation for improved performance, lower cost, and higher availability. This required extensive collaboration with the domain teams to test on each and every instance type for us, as well as exploring different chip architecture such as AMD and ARM. After this arduous testing process, we successfully moved entire families of internal products onto AMD CPUs, gaining 20% in better price performance in comparison to the older versions.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;EMR&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;We love using Spot Instances for EMR, but as we all know, it can be difficult to get ideal capacity at peak times using Spot. With our intricate and integrated EMR systems, we had to carefully ensure that each part of our toolchain was updated without causing an interruption to our service. Therefore, we had to upgrade our software versions to take advantage of the &lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html&quot;&gt;instance fleets feature&lt;/a&gt; of AWS EMRs. This upgrade helped us to get 25% cost reduction on total EMR costs.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;In this section, we discuss how we managed to cut our AWS storage costs for EBS and S3.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Amazon EBS: &lt;/strong&gt;For storage we found that there was extensive testing required to gain internal customer trust that the newer generation of EBS, GP3 would continue to meet our needs. To gain this trust, extensive performance testing was conducted using various tools. With all tests done in our development account first, we found that we could comfortably migrate 500–1000 live volumes at a time in parallel without any tangible impact.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Amazon S3: &lt;/strong&gt;We moved 50+PB to Intelligent-Tiering (IT). During the process, we learned the hard way that not all workloads work well with IT, and you need to be very careful with object size. If the average object size is too low and you have multiple billions of objects, you can end up drastically increasing your overall S3 costs for that workload. In that case, the usage of S3 lifecycle filters is required to tune the policy. This is not a ‘one and done’ process but an ongoing cycle that requires extensive time, care and attention to not fall afoul of S3’s complex billing patterns.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;Result of Coupang’s effort to optimize the cloud expenditure shows the reduced AWS cost against the increasing amount of Amazon S3 usage&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*szPiCc1YDKeK_E1EkpLzFg.png&quot; /&gt;&lt;figcaption&gt;Figure 1. Crossing trend of increasing Amazon S3 usage versus decreasing AWS cost per size&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;By adopting the methods above, we were able to minimize our AWS costs by millions of dollars (On-Demand) in 2021. A lot of the process was manually done, focusing on identifying the right optimization areas and achieving them. We are still working hard to identify additional areas for cost savings and improved efficiency.&lt;/p&gt;&lt;p&gt;Although we managed to save the company millions in cloud costs, we are not done yet. As next steps, we wish to invest in more complex analytic tools to drive the Cloud FinOps mindset at Coupang. Additionally, we will be automating some of the monitoring and analytics processes required for cost optimization in cloud.&lt;/p&gt;&lt;p&gt;&lt;em&gt;If you are a passionate engineer with a deep understanding of cloud optimization and cost efficiency, &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3Q7AKWS&quot;&gt;&lt;em&gt;join our talented team&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on &lt;/em&gt;&lt;a href=&quot;http://ir.aboutcoupang.com/&quot;&gt;&lt;em&gt;ir.aboutcoupang.com&lt;/em&gt;&lt;/a&gt;&lt;em&gt; for information on our formal investment plans and product development strategies.&lt;/em&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=44e9bea3d91b&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b&quot;&gt;Cloud expenditure optimization for cost efficiency&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>TestContainers로 유저시나리오와 비슷한 통합테스트 만들어 보기</title>
      <link>http://thefarmersfront.github.io/blog/delivery-testContainer-apply/</link>
      <guid>http://thefarmersfront.github.io/blog/delivery-testContainer-apply/</guid>
      <pubDate>Wed, 15 Mar 2023 10:00:00 GMT</pubDate>
      <content:encoded>진정한 통테를...</content:encoded>
    </item>
    <item>
      <title>기계 학습 모델을 활용한 물류 입고 프로세스 최적화</title>
      <link>https://medium.com/coupang-engineering/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AC%BC%EB%A5%98-%EC%9E%85%EA%B3%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%B5%9C%EC%A0%81%ED%99%94-fe4490e44514?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AC%BC%EB%A5%98-%EC%9E%85%EA%B3%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%B5%9C%EC%A0%81%ED%99%94-fe4490e44514?source=rss----fb028911af07---4</guid>
      <pubDate>Mon, 13 Mar 2023 01:33:09 GMT</pubDate>
      <content:encoded>&lt;h4&gt;쿠팡 풀필먼트 센터로의 제품 입고 시 필요한 운송 트럭의 적정 수량을 데이터에 기반해 예측하기&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By Austin Yang &amp;amp; JY Cho&lt;/em&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*b9zku0Izrt8kvjRPLNbLzw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;본 포스트는 &lt;a href=&quot;https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304&quot;&gt;&lt;strong&gt;영문&lt;/strong&gt;&lt;/a&gt;으로도 제공됩니다.&lt;/blockquote&gt;&lt;p&gt;쿠팡은 물류 입고 프로세스의 최적화를 위해 끊임없이 노력하고 있습니다. 풀필먼트 센터로의 제품 입고과정에서 낭비되는 자원을 최소화하면, 적기에 제품을 판매하고 더 많은 고객들에게 더 빠르게 배송할 수 있습니다. 이를 위해 쿠팡은 기계 학습을 통해 직매입 제품들의 풀필먼트 센터 입고 프로세스를 효율적으로 개선해 왔습니다. 본문을 통해 어떤 개선이 있었는지를 살펴보겠습니다.&lt;/p&gt;&lt;h4&gt;목차&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#f0aa&quot;&gt;배경 및 과제&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#dac3&quot;&gt;트럭 수량 예측 모델 학습하기&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#601a&quot;&gt;특징 추출&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#11f1&quot;&gt;모델 학습: LightGBM 알고리즘&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#eba0&quot;&gt;모델 하이퍼 파라미터 탐색 : 베이지안 최적화&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#6d8f&quot;&gt;입고 예약 시스템과 모델 연계 구성&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#45d4&quot;&gt;과소 예측과 과대 예측 사이의 트레이드 오프&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#66f4&quot;&gt;모델 적용 결과&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#00f9&quot;&gt;향후 계획&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;배경 및 과제&lt;/h3&gt;&lt;p&gt;매일 각지의 수많은 업체들이 트럭에 다양한 종류의 제품을 적재해 쿠팡 풀필먼트 센터로 제품을 반입합니다. 각 풀필먼트 센터에는 트럭을 세워두고 물건을 하역하는 도크(dock)가 있습니다. 센터마다 시간대별로 사용 가능한 도크의 최대 개수는 정해져 있습니다. 제품 하역 작업 시 한 대의 트럭이 하나의 도크를 정해진 시간 동안 사용하게 되는데, 이 시간을 슬롯(slot)이라고 부릅니다.&lt;/p&gt;&lt;p&gt;정해진 개수의 슬롯들로 여러 업체들이 납품하는 제품들이 효율적으로 입고되려면 각 입고에 필요한 슬롯 개수가 사전에 정확히 예측되어야 합니다. 만약 사전에 예측한 필요 슬롯 개수가 실제 필요 슬롯보다 적을 경우 입고 과정에서 지연이 발생할 수 있으며, 실제 필요 슬롯보다 많은 경우 한정된 자원인 슬롯을 불필요하게 낭비하게 됩니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;벤더에서 쿠팡 풀필먼트 센터로의 납품 과정에서 발생할 수 있는 자원 낭비&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*O9LCAZ4eN38TbJvqvpy-2Q.png&quot; /&gt;&lt;figcaption&gt;그림 1. 납품 과정에서 발생할 수 있는 자원 낭비&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;이에 저희는 업체 특성과 풀필먼트 센터로 입고되는 제품의 특성을 바탕으로, 입고 예약 신청을 하는 업체들에게 적정한 필요 슬롯 개수를 예측해 제공하는 시스템을 개발하기 시작했습니다. 목표는 낭비되는 슬롯의 개수를 줄이고, 슬롯이 부족해 입고가 지연되는 문제를 최소화하는 것이었습니다. 이 목표를 달성하기 위해 저희가 활용한 기술들을 다음 섹션에서 자세히 소개하겠습니다.&lt;/p&gt;&lt;h3&gt;트럭 수량 예측 모델 학습하기&lt;/h3&gt;&lt;p&gt;저희는 데이터로 문제를 풀고자 했고 이를 위해:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;그동안 축적해온 물류 데이터 및 입고 신청 정보로부터 트럭 대수에 영향을 미치는 특징들(feature)을 도출하고, 각 물류 입고에 실제 사용된 트럭 대수 데이터를 결합해 학습 데이터를 구성했습니다.&lt;/li&gt;&lt;li&gt;적정 트럭 입차 수량을 예측하는 머신 러닝(ML) 모델을 학습시켰습니다.&lt;/li&gt;&lt;li&gt;학습된 모델을 입고 예약 시스템과 연결해 업체가 입고 신청을 하면 시스템으로 바로 적정 트럭 대수를 확인할 수 있게끔 설계했습니다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이렇게 저희는 입고 예약 시스템에 새로운 예측 자동화 기능을 더해 프로젝트의 목표였던 슬롯 운영 효율을 달성할 수 있었습니다.&lt;/p&gt;&lt;h4&gt;특징 추출&lt;/h4&gt;&lt;p&gt;쿠팡에 축적된 대량의 물류 데이터들로부터 트럭 수량 예측 모델에 적합한 특징들을 찾아내기 위해 탐색적 데이터 분석(EDA: Exploratory Data Analysis) 과정을 거쳤습니다. 하지만 데이터 사이에 숨겨져 있는 의미를 찾아내기 위해서는 도메인 전문가들의 조언이 필요했습니다. 물류 담당자들과 심층 인터뷰를 진행하면서 물류 입고 과정에서 나타나는 의미있는 패턴들을 다양하게 파악할 수 있었으며, 이러한 과정을 통해 트럭 수량 예측에 유용하게 활용할 수 있는 다수의 특징들을 발견하였습니다. 그리고 이렇게 도출된 특징들을 피처 엔지니어링(Feature Engineering)으로 가공해 최종적인 피처 셋(Feature Set)을 확정했습니다.&lt;/p&gt;&lt;h4&gt;모델 학습: LightGBM 알고리즘&lt;/h4&gt;&lt;p&gt;약 2년여의 기간 동안 수집된 입고 신청 데이터로부터 약 80만 건의 학습 데이터를 추출했습니다. 데이터 세트의 크기가 작지 않다 보니 빠른 학습과 튜닝이 가능한 알고리즘을 찾게 되었습니다. 또한 확정된 특징들 중 많은 것들이 범주형(categorical) 특징이었습니다. 이러한 특성을 갖는 데이터 세트에서 높은 예측 성능을 보이는 LightGBM 알고리즘을 사용하기로 결정했습니다.&lt;/p&gt;&lt;p&gt;LightGBM은 트리 기반 부스팅 모델이며, 많은 기계 학습 문제에서 성능을 입증한 알고리즘입니다. 다른 트리 기반 알고리즘들은 트리를 수평적으로 확장하는 level-wise tree growth 방식을 사용하는 반면, LightGBM은 트리를 수직적으로 확장하는 leaf-wise tree growth 방식을 사용해 학습 속도가 빠르다는 것이 특징입니다. Level-wise tree growth 방식은 각 트리 깊이가 모두 확장될 때까지 기다려야 하지만, leaf-wise 방식은 이를 기다리지 않고 가장 손실이 높은 리프 노드를 분할해 나가면서 수직으로 확장합니다. 이러한 방식 덕분에 LightGBM은 다른 알고리즘들보다 빠른 속도로 학습이 수행할 수 있습니다.&lt;/p&gt;&lt;p&gt;또한 다른 대부분의 알고리즘들과 달리 LightGBM은 범주형 특징에 대해 별도의 원핫 인코딩(one-hot encoding)을 해주지 않아도 됩니다. LightGBM이 &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479&quot;&gt;Fisher&lt;/a&gt; 알고리즘을 적용해 데이터의 클래스를 최적으로 분할하는 학습을 수행해 나가기 때문입니다. 이로 인해 LightGBM은 일반적으로 보다 빠른 학습 속도와 함께 높은 예측 성능을 보여줍니다. 검증(validation) 데이터 세트로 다른 주요 트리 기반 알고리즘들과 비교 평가한 결과, LightGBM은 저희가 원하는 학습 속도와 예측 성능을 보여주었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 풀필먼트 센터로 인입되는 입고 요청들의 검증(validation) 데이터 셋에 대한 주요 트리(tree) 기반 알고리즘들의 예측 성능 확인 결과&quot; src=&quot;https://cdn-images-1.medium.com/max/819/1*8ZgbAZ-e4Fl8KjdtPOmDaA.png&quot; /&gt;&lt;figcaption&gt;그림 2. 주요 트리(tree) 기반 알고리즘들의 검증(validation) 데이터에 대한 예측 성능 확인 결과&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;모델 하이퍼 파라미터 탐색 : 베이지안 최적화&lt;/h4&gt;&lt;p&gt;모델의 하이퍼 파라미터 탐색은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization&quot;&gt;베이지안 최적화&lt;/a&gt;(Bayesian Optimization)를 활용해 자동으로 진행되도록 구성하였습니다. 베이지안 최적화는 목적 함수 &lt;em&gt;f&lt;/em&gt;의 함수 값 &lt;em&gt;f(x)&lt;/em&gt;를 최대로 만드는 입력 값 &lt;em&gt;x&lt;/em&gt;의 전역 최적해(global optimization) &lt;em&gt;x*&lt;/em&gt;를 찾기 위한 방법이며, 목적 함수가 명시적이지 않고 함수 값 &lt;em&gt;f(x)&lt;/em&gt;의 계산 비용이 클 때 최적해를 효과적으로 찾아낼 수 있습니다. 머신 러닝에서는 최적의 하이퍼 파라미터 조합을 효율적으로 찾아내는 방법 중 하나로 많이 사용됩니다.&lt;/p&gt;&lt;p&gt;베이지안 최적화가 이루어지는 과정을 개략적으로 설명하면 다음과 같습니다.&lt;/p&gt;&lt;p&gt;(1) 미리 지정된 하이퍼 파라미터의 탐색 범위 내에서 n개의 값들을 랜덤하게 선택하여 모델을 학습하고, 학습된 모델의 함수 값을 계산&lt;/p&gt;&lt;p&gt;(2) 입력 값과 함수 값 쌍으로 집합을 구성하고, 이를 바탕으로 Gaussian Process 등의 방법을 사용하여 미지의 목적 함수 &lt;em&gt;f&lt;/em&gt; 를 확률적인 방법으로 추정&lt;/p&gt;&lt;p&gt;(3) 현재까지 목적 함수에 대해 추정한 결과를 바탕으로, &lt;em&gt;x*&lt;/em&gt;를 찾을 수 있을 것으로 예상되는 다음 입력 값 후보를 선택하여 모델을 학습하고, 학습된 모델의 함수 값을 계산하여 입력 값과 함수 값 쌍의 집합에 추가&lt;/p&gt;&lt;p&gt;(4) 위 (2)~(3)의 과정을 정해진 횟수만큼 반복하면서 추정 함수를 갱신한 후, 추정 함수의 함수 값을 최대로 만드는 최적 하이퍼 파라미터 &lt;em&gt;x*&lt;/em&gt;를 선택&lt;/p&gt;&lt;p&gt;이 과정은 한 달 주기로 새로운 데이터들을 추가 반영하여 반복되면서 모델을 업데이트해 나가게 됩니다.&lt;/p&gt;&lt;h4&gt;입고 예약 시스템과 모델 연계 구성&lt;/h4&gt;&lt;p&gt;모델은 &lt;a href=&quot;https://aws.amazon.com/ko/sagemaker/&quot;&gt;SageMaker&lt;/a&gt;에서 학습됩니다. 업체가 입고 예약 시스템으로 슬롯 예약 신청을 하면, 예약 시스템은 SageMaker 엔드포인트를 호출해 모델 예측 결과 값을 반환 받아 업체에게 적정 트럭 대수를 알려줍니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡의 입고 예약 시스템과 머신 러닝 모델 간의 연계 구성&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*OBO6u162sg5ENJo5eYcJOw.png&quot; /&gt;&lt;figcaption&gt;그림 3. 입고 예약 시스템과 모델 연계 구성&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;과소 예측과 과대 예측 사이의 트레이드 오프&lt;/h4&gt;&lt;p&gt;기계 학습 예측 모델은 필연적으로 오차를 동반합니다. 저희가 해결하고자 하는 문제의 경우 실제 적정 슬롯 개수보다 적은 개수를 예측하면 과소 예측이, 많은 개수를 예측하면 과대 예측이 발생합니다. 과소 예측과 과대 예측 사이에는 트레이드 오프가 있는데, 같은 수준의 오차율 내에서 예측 모델이 적정 트럭 개수를 가급적 적게 예측하는 경향을 띄면 과대 예측은 줄어들지만 과소 예측이 늘어나고, 트럭 개수를 가급적 많게 예측하는 경향을 갖게 되면 과소 예측은 줄어들지만 과대 예측이 늘어납니다.&lt;/p&gt;&lt;p&gt;저희의 기본적인 목표는 과대 예측을 줄여 불필요하게 예약되는 슬롯들을 최소화하는 것이지만, 과대 예측을 지나치게 최소화하는 방향으로 모델의 예측 슬롯 개수를 결정하면, 업체들 입장에서는 너무 적은 슬롯들이 배정된다고 느끼게 되는 의도치 않은 상황이 발생할 수도 있습니다.&lt;/p&gt;&lt;p&gt;따라서 저희는 관련 부서들과 논의 끝에 적정한 수준의 과소 예측 비율을 유지하면서 과도 예측을 최대한 줄이는 것이 쿠팡과 업체 모두에게 이로운 방식임을 확인하였으며, 이를 반영해 최종 모델을 도출하였습니다.&lt;/p&gt;&lt;h3&gt;모델 적용 결과&lt;/h3&gt;&lt;p&gt;최종 학습된 모델의 적정 트럭 대수를 과소 예측하는 비율은 2.53% 수준이며, 과대 예측하는 비율은 5.04% 수준입니다. 이는 이전에 업체들이 직접 적정 트럭 대수를 예측해 입고 예약 시스템에 등록할 때의 과소 예측 비율 8.71%와 과대 예측 비율 44.45%로부터 크게 개선된 수치입니다.&lt;/p&gt;&lt;p&gt;이 학습 모델을 활용한 결과, 슬롯이 부족해 업체가 입고일을 변경하는 사례가 67.9% 감소했고, 결과적으로 업체는 원하는 일정으로 제품을 풀필먼트 센터로 입고할 수 있게 되었고 쿠팡은 불필요하게 낭비되는 비용을 줄이고 필요한 수량의 물품들을 원하는 일정에 납품 받을 수 있게 되었습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 풀필먼트 센터로의 상품 입고에 필요한 벤더들의 트럭 수를 예측하는 머신러닝 모델의 적용 후 결과&quot; src=&quot;https://cdn-images-1.medium.com/max/654/1*Bmd09fvTtGJJMUhMpD8baA.png&quot; /&gt;&lt;figcaption&gt;표 1. 트럭 수량 예측 모델 적용 결과&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;향후 계획&lt;/h3&gt;&lt;p&gt;쿠팡의 비즈니스 영역이 점점 더 확대될수록, 쿠팡이 취급하는 제품군도 다양해지고 있습니다. 최근에는 가전 제품을 전문적으로 취급하는 풀필먼트 센터들도 생겨나는 등, 쿠팡의 작업 방식도 변해가면서 이에 맞는 정확한 적정 트럭 대수 예측이 필요해진 상황입니다. 따라서 이런 새로운 유형의 제품과 작업 방식의 변화들에 대해서도 모델이 적절한 예측 성능을 보일 수 있도록, 저희는 앞으로도 파생 특징들을 발굴하고 데이터를 추가하면서 지속적으로 모델을 개선해 나갈 예정입니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;물류 프로세스 혁신에 관심이 있으시다면, 쿠팡 &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3bw6NjT&quot;&gt;&lt;em&gt;채용 공고&lt;/em&gt;&lt;/a&gt;&lt;em&gt;를 확인해 보세요!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 &lt;a href=&quot;https://ir.aboutcoupang.com/English/home/default.aspx&quot;&gt;ir.aboutcoupang.com&lt;/a&gt; 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fe4490e44514&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AC%BC%EB%A5%98-%EC%9E%85%EA%B3%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%B5%9C%EC%A0%81%ED%99%94-fe4490e44514&quot;&gt;기계 학습 모델을 활용한 물류 입고 프로세스 최적화&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>Optimizing the inbound process with a machine learning model</title>
      <link>https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304?source=rss----fb028911af07---4</guid>
      <pubDate>Thu, 09 Mar 2023 01:37:49 GMT</pubDate>
      <content:encoded>&lt;h4&gt;How we predict the adequate number of delivery trucks needed for vendors to send their products to Coupang’s fulfillment centers&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By Austin Yang &amp;amp; JY Cho&lt;/em&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*b9zku0Izrt8kvjRPLNbLzw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;This post is also available in &lt;a href=&quot;https://medium.com/coupang-engineering/기계-학습-모델을-활용한-물류-입고-프로세스-최적화-fe4490e44514&quot;&gt;&lt;strong&gt;Korean&lt;/strong&gt;&lt;/a&gt;.&lt;/blockquote&gt;&lt;p&gt;Coupang continuously strives to optimize the inbound logistics process. By minimizing the resources wasted while receiving products at fulfillment centers, we can sell products in a more timely fashion and deliver them to more customers faster. To this end, Coupang has been efficiently improving the process of receiving products at fulfillment center that Coupang directly purchases from vendors through machine learning. Let’s look at what improvements we have made so far in this post.&lt;/p&gt;&lt;h4&gt;Table of Contents&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#13ce&quot;&gt;Background and challenges&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#2b55&quot;&gt;Training a model to predict the number of trucks&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#a118&quot;&gt;Feature extraction&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#bf13&quot;&gt;Model learning: LightGBM algorithm&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#817c&quot;&gt;Model hyper parameter search: Bayesian optimization&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#d283&quot;&gt;Inbound reservation system integrated with the model&lt;/a&gt;&lt;br&gt; ∘ &lt;a href=&quot;#b050&quot;&gt;Trade-off between underprediction and overprediction&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#8b3b&quot;&gt;Result of applying the model&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#49a2&quot;&gt;Future plan&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;Background and challenges&lt;/h3&gt;&lt;p&gt;Every day, thousands of vendors all over the place load different types of products onto truck to send them to Coupang’s fulfillment centers. Each fulfillment center has docks where trucks are parked and goods are unloaded. The number of docks at each center that can be used per hour is fixed. To unload goods, one truck uses one dock for a certain period of time, and this is called a slot.&lt;/p&gt;&lt;p&gt;The number of required slots for each inbound must be precisely predicted so that the products from various vendors can be efficiently unloaded to the set number of slots. If the predicted number of slots is smaller than the actual number of required slots, it could cause a delay in the inbound process. On the other hand, if the predicted number of slots turns out to be bigger than necessary, it would end up wasting our limited resource.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Potential resource waste in the supply process from vendors to Coupang fulfillment centers&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*jCJozrpL1x9-mwI1nBAW0A.png&quot; /&gt;&lt;figcaption&gt;Figure 1. Potential resource waste in the supply process&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To address this issue, we have worked on developing a system that predicts the appropriate number of slots based on the characteristics of goods to be supplied and the characteristics of vendors show when reserving slots. The system aimed to reduce the number of wasted slots and prevent IB delay resulting from a lack of slots. In the next section, we will explain in more detail what techniques were used to achieve this goal.&lt;/p&gt;&lt;h3&gt;Training a model to predict the number of trucks&lt;/h3&gt;&lt;p&gt;We decided to solve the problem by using data and:&lt;/p&gt;&lt;p&gt;1. Extracted features that had an impact on the number of trucks based on the logistics data and inbound requests accumulated over the years, and then prepared training data by incorporating them with the data on the number of trucks that were actually used for inbound.&lt;/p&gt;&lt;p&gt;2. Trained a machine learning model to predict an adequate number of trucks that should arrive at a dock.&lt;/p&gt;&lt;p&gt;3. Integrated the trained model with the reservation system, and made the adequate number of trucks displayed on the system right away when vendors make a request for inbound.&lt;/p&gt;&lt;p&gt;By adding this new automated prediction to the inbound reservation system, we have achieved our project’s goal which is the efficiency in our slot operations.&lt;/p&gt;&lt;h4&gt;Feature extraction&lt;/h4&gt;&lt;p&gt;We went through an exploratory data analysis (EDA) process to find the right features for the model which predicts the number of trucks to be used for inbound, utilizing the massive logistics data accumulated at Coupang. However, we soon learned that we needed knowledge from the domain experts to read between the lines among those data. Through a series of interviews with the Coupang logistics managers, we found certain patterns from the inbound process. Based on the findings, we came to discover multiple useful features for predicting the number of trucks. After processing those discovered features via feature engineering, we were able to define the final feature set.&lt;/p&gt;&lt;h4&gt;Model learning: LightGBM algorithm&lt;/h4&gt;&lt;p&gt;We extracted about 800,000 training data sets from the inbound request data collected for over two years. Since the size of the data sets wasn’t small, we looked for an algorithm which could be trained fast and tuned. In addition, many of the identified features were categorical features. We decided to use the LightGBM algorithm, which has a high predictive accuracy for data sets which have such features.&lt;/p&gt;&lt;p&gt;LightGBM is a tree-based boosting model as well as an algorithm that has demonstrated effective performance in many machine learning problems. Because LightGBM applies leaf-wise tree growth where trees are grown vertically while other tree-based algorithms apply level-wise tree growth where trees are grown horizontally, LightGBM enables fast training. In level-wise tree growth, we have to wait until each tree is fully grown, but the leaf-wise growth approach grows the tree vertically by splitting the data at the leaf nodes with the highest loss change. Because of this, LightGBM is faster in learning than other algorithms.&lt;/p&gt;&lt;p&gt;In addition, unlike most of the other algorithms, LightGBM does not require separate one-hot encoding for categorical features, because it applies &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479&quot;&gt;Fisher&lt;/a&gt; algorithm to find the optimal split of data classes. Thanks to this, LightGBM generally shows a high predictive accuracy and a higher learning rate. When compared with other major tree-based algorithms based on the validation data set, LightGBM showed the learning rate and predictive performance we desired.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Major tree-based algorithms’ predictive performance on the validation data set of the inbound requests to Coupang’s fulfillment centers&quot; src=&quot;https://cdn-images-1.medium.com/max/819/1*ukrx0gxBz2S5BwxViVIxuA.png&quot; /&gt;&lt;figcaption&gt;Figure 2. Major tree-based algorithms’ predictive performance on the validation set&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Model hyper parameter search: Bayesian optimization&lt;/h4&gt;&lt;p&gt;We configured a set of hyperparameters of this model to be automatically chosen using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization&quot;&gt;Bayesian Optimization&lt;/a&gt;. Bayesian Optimization is a method to find the global optimization &lt;em&gt;x*&lt;/em&gt; out of &lt;em&gt;x&lt;/em&gt; that maximizes the &lt;em&gt;f(x)&lt;/em&gt; value of the objective function, and shows a high efficiency for acquiring the optimal solution when the objective function is not specified and getting the &lt;em&gt;f(x)&lt;/em&gt; is computationally expensive. It is also one of the commonly used methods in machine learning when it comes to figuring out the optimal combinations of hyperparameters.&lt;/p&gt;&lt;p&gt;The Bayesian optimization process is as follows.&lt;/p&gt;&lt;p&gt;(1) Train a model selecting N values randomly within the set hyperparameter bandwidth, and calculate the function value from the model.&lt;/p&gt;&lt;p&gt;(2) Configure the group which consists of the sets of input values and function values in order to assume the objective function, utilizing a probabilistic method such as Gaussian Process.&lt;/p&gt;&lt;p&gt;(3) Based on the assumptions of the objective function so far, train the model to select the input value candidates that are expected for finding the optimal &lt;em&gt;x*&lt;/em&gt;. Calculate the function from the trained model and then add the set of input values and function values to the group.&lt;/p&gt;&lt;p&gt;(4) Repeat the mentioned (2) and (3) for an assigned number of rounds, renewing the assumed function. Select the optimal hyperparameter &lt;em&gt;x* &lt;/em&gt;which maximizes the value of the function.&lt;/p&gt;&lt;p&gt;We run this process on a monthly basis with new data, continuously updating the model.&lt;/p&gt;&lt;h4&gt;Inbound reservation system integrated with the model&lt;/h4&gt;&lt;p&gt;The model is deployed on &lt;a href=&quot;https://aws.amazon.com/ko/sagemaker/&quot;&gt;SageMaker&lt;/a&gt;. When a vendor requests a slot reservation on the reservation system, the reservation system calls SageMaker endpoint, receives the result predicted by the model and informs the vendor of the appropriate number of trucks.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;How the Coupang’s inbound reservation system and the ML model are integrated&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*V41Hm0JNBqHLonDpRSYlrA.png&quot; /&gt;&lt;figcaption&gt;Figure 3. How the IB reservation system and the model are integrated&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Trade-off between underprediction and overprediction&lt;/h4&gt;&lt;p&gt;Errors are inevitably present in the machine learning predictive model. As for the issue we’re trying to resolve, predicting fewer slots than the actual number of needed slots leads to underprediction, and predicting more slots leads to overprediction. There is a trade-off between underprediction and overprediction. Within the same range of error rates, if a predictive model tends to predict fewer trucks as the adequate number of trucks, overprediction decreases but underprediction increases. On the other hand, if a predictive model tends to predict more trucks as the adequate number of trucks, underprediction decreases but overprediction increases.&lt;/p&gt;&lt;p&gt;Basically, our goal is to minimize the number of slots that are unnecessarily reserved by reducing overprediction. But if we train a model to overly minimize overprediction, vendors might feel that too few slots are allocated to them resulting in unintended inconvenience.&lt;/p&gt;&lt;p&gt;After discussing with relevant departments, we agreed that it would be beneficial for both Coupang and vendors to reduce overprediction as much as possible while maintaining the ratio of underprediction at an appropriate level at the same time and came up with a final model reflecting this consensus.&lt;/p&gt;&lt;h3&gt;Result of applying the model&lt;/h3&gt;&lt;p&gt;The final model underpredicts the number of trucks by 2.53% and overpredicts it by 5.04%. This is a significant improvement from the underprediction rate of 8.71% and the overprediction rate of 44.45% we had when vendors had predicted the appropriate number of trucks by themselves and registered it on the reservation system.&lt;/p&gt;&lt;p&gt;As a result of using this learning model, the number of cases where a vendor changes the delivery date due to a lack of slots has decreased by 67.9%. Now they can supply their products to fulfillment centers according to a schedule they want. Coupang can reduce unnecessary expenses and receive as many products as needed on a desired date.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;Result of applying a ML model which predicts the number of trucks that a vendor needs to supply their products to Coupang fulfillment centers&quot; src=&quot;https://cdn-images-1.medium.com/max/711/1*Ikm7Y1TLNj5sy4VhXbJn1Q.png&quot; /&gt;&lt;figcaption&gt;Table 1. Result of applying a model which predicts the number of trucks&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Future plan&lt;/h3&gt;&lt;p&gt;As Coupang is expanding its business, we are handing a wider variety of products and are also operating recently built fulfillment centers that only handle home appliances. Thus, we need more accurate prediction on the number of trucks that meets Coupang’s new needs. In order to make the model show an adequate performance in predicting the number of trucks for these new types of products in a new environment, we will continuously improve the model by identifying derived features and adding data.&lt;/p&gt;&lt;p&gt;&lt;em&gt;If the innovation in fulfillment process interests you, come and check out our &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3Q7AKWS&quot;&gt;&lt;em&gt;open positions&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on &lt;/em&gt;&lt;a href=&quot;http://ir.aboutcoupang.com/&quot;&gt;&lt;em&gt;ir.aboutcoupang.com&lt;/em&gt;&lt;/a&gt;&lt;em&gt; for information on our formal investment plans and product development strategies.&lt;/em&gt;&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2db48bbbc304&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304&quot;&gt;Optimizing the inbound process with a machine learning model&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>컬리의 BigQuery 도입기 - 2부</title>
      <link>http://thefarmersfront.github.io/blog/bigquery-2/</link>
      <guid>http://thefarmersfront.github.io/blog/bigquery-2/</guid>
      <pubDate>Fri, 17 Feb 2023 10:00:00 GMT</pubDate>
      <content:encoded>컬리 데이터 파이프라인의 BigQuery 도입 결과 및 효과</content:encoded>
    </item>
    <item>
      <title>컬리 커머스플랫폼의 개발문화를 만들어가는 사람들</title>
      <link>http://thefarmersfront.github.io/blog/commerce-platform-facilitators-2022/</link>
      <guid>http://thefarmersfront.github.io/blog/commerce-platform-facilitators-2022/</guid>
      <pubDate>Mon, 13 Feb 2023 10:00:00 GMT</pubDate>
      <content:encoded>커머스플랫폼 퍼실리테이터 2022년 활동 결산 인터뷰</content:encoded>
    </item>
    <item>
      <title>컬리의 BigQuery 도입기 - 1부</title>
      <link>http://thefarmersfront.github.io/blog/bigquery-1/</link>
      <guid>http://thefarmersfront.github.io/blog/bigquery-1/</guid>
      <pubDate>Tue, 07 Feb 2023 10:00:00 GMT</pubDate>
      <content:encoded>컬리 데이터 파이프라인의 BigQuery 도입 배경과 그 주안점</content:encoded>
    </item>
    <item>
      <title>3년마다 제공되는 뱅크샐러드의 안식휴가. 2주 이상은 푹-쉬고 돌아오세요!</title>
      <link>https://blog.banksalad.com/pnc/vacation/</link>
      <guid>https://blog.banksalad.com/pnc/vacation/</guid>
      <pubDate>Fri, 27 Jan 2023 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>AWS MLOps 분산학습 워크숍 방문기</title>
      <link>http://thefarmersfront.github.io/blog/aws-mlops-workshop-2022/</link>
      <guid>http://thefarmersfront.github.io/blog/aws-mlops-workshop-2022/</guid>
      <pubDate>Tue, 10 Jan 2023 10:00:00 GMT</pubDate>
      <content:encoded>워크숍에 방문하여 경험한 내용들을 공유합니다.</content:encoded>
    </item>
    <item>
      <title>데이터사이언스팀이 예측모델을 개발하고 운영하는 방법을 소개합니다.</title>
      <link>http://thefarmersfront.github.io/blog/introduce_datascience_team/</link>
      <guid>http://thefarmersfront.github.io/blog/introduce_datascience_team/</guid>
      <pubDate>Sat, 07 Jan 2023 16:00:00 GMT</pubDate>
      <content:encoded>데이터사이언스팀에 대한 소개와 예측모델을 개발하고 운영하는 방법을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>&quot;검색엔진 가이드&quot;를 금융쇼핑 서비스에 SEO로 활용하기</title>
      <link>https://blog.banksalad.com/tech/how-banksalad-seo/</link>
      <guid>https://blog.banksalad.com/tech/how-banksalad-seo/</guid>
      <pubDate>Thu, 05 Jan 2023 00:00:00 GMT</pubDate>
      <content:encoded>이 글은 2022년 12월 기준으로 제공된 검색엔진 가이드를 기반으로 작성되었습니다. 가이드 및 정책은 변경될 수 있습니다. 안녕하세요. 뱅크샐러드 웹 프론트엔드 엔지니어 김희찬입니다. 최근 금융쇼핑 PA…</content:encoded>
    </item>
    <item>
      <title>Spark on Kubernetes로 가자!</title>
      <link>https://blog.banksalad.com/tech/spark-on-kubernetes/</link>
      <guid>https://blog.banksalad.com/tech/spark-on-kubernetes/</guid>
      <pubDate>Thu, 05 Jan 2023 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 저희는 뱅크샐러드 Data Platform 팀 김민수, 김태일 입니다. 이번 글에서는 뱅크샐러드 데이터 분석환경 컴퓨팅을 EMR, YARN 기반 Spark에서 Self-hosted Kubernetes…</content:encoded>
    </item>
    <item>
      <title>컬리, IEEM 2022에서 물류센터 생산 계획 최적화 논문을 발표하다</title>
      <link>http://thefarmersfront.github.io/blog/ieem2022/</link>
      <guid>http://thefarmersfront.github.io/blog/ieem2022/</guid>
      <pubDate>Wed, 28 Dec 2022 10:00:00 GMT</pubDate>
      <content:encoded>A Fast Metaheuristic Optimizer for Large-scale Batch Fulfillment Planning</content:encoded>
    </item>
    <item>
      <title>데일리 스크럼 : &apos;데일리 스크럼&apos;을 더 잘하기 위한 생각</title>
      <link>http://thefarmersfront.github.io/blog/daily-scrum-thinking/</link>
      <guid>http://thefarmersfront.github.io/blog/daily-scrum-thinking/</guid>
      <pubDate>Tue, 27 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded>우리는 데일리 스크럼에 대해 더 자세히 알아보고, 어떻게 하면 더 잘할 수 있을지 같이 이야기 해보았습니다.</content:encoded>
    </item>
    <item>
      <title>Datadog Dash 2022 컨퍼런스 방문기</title>
      <link>http://thefarmersfront.github.io/blog/datadog-dash-2022/</link>
      <guid>http://thefarmersfront.github.io/blog/datadog-dash-2022/</guid>
      <pubDate>Thu, 15 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded>우리회사 미국도 보내줍니다</content:encoded>
    </item>
    <item>
      <title>후기 개선 프로젝트가 끝이 아닌 시작인 이유</title>
      <link>http://thefarmersfront.github.io/blog/review-renewal/</link>
      <guid>http://thefarmersfront.github.io/blog/review-renewal/</guid>
      <pubDate>Tue, 13 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded>후기의 진화는 지금부터</content:encoded>
    </item>
    <item>
      <title>지연 시간 없이 웹폰트 서빙하기 - Feat. Safari &amp; Edge functions</title>
      <link>https://blog.banksalad.com/tech/font-preload-on-safari/</link>
      <guid>https://blog.banksalad.com/tech/font-preload-on-safari/</guid>
      <pubDate>Tue, 13 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요 뱅크샐러드 Web Front-End Engineer 민찬기입니다. 여러분들은 운영하시는 서비스의 폰트를 바꾸신 적이 있으신가요? 바꾸시는 과정에서 어떤 어려움을 겪으셨나요? 눈썰미가 남다르신 분들은 눈치채셨겠지만, 뱅크샐러드는…</content:encoded>
    </item>
    <item>
      <title>쿠팡 SCM 워크플로우: 효율적이고 확장 가능한 low-code, no-code 플랫폼 개발</title>
      <link>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-scm-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B4%EA%B3%A0-%ED%99%95%EC%9E%A5-%EA%B0%80%EB%8A%A5%ED%95%9C-low-code-no-code-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C-7d997644d14?source=rss----fb028911af07---4</link>
      <guid>https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-scm-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B4%EA%B3%A0-%ED%99%95%EC%9E%A5-%EA%B0%80%EB%8A%A5%ED%95%9C-low-code-no-code-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C-7d997644d14?source=rss----fb028911af07---4</guid>
      <pubDate>Mon, 12 Dec 2022 00:45:29 GMT</pubDate>
      <content:encoded>&lt;h4&gt;개발자, 비개발자 모두 쉽게 사용할 수 있는 데이터 구축 및 서비스 엔지니어링 플랫폼에 대하여&lt;/h4&gt;&lt;p&gt;&lt;em&gt;By &lt;/em&gt;&lt;a href=&quot;https://www.linkedin.com/in/atinjin/&quot;&gt;&lt;em&gt;Ryan Donghyun Jin&lt;/em&gt;&lt;/a&gt;&lt;em&gt; (&lt;/em&gt;&lt;a href=&quot;https://medium.com/@atinjin&quot;&gt;&lt;em&gt;@atinjin&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/836/1*sQCIlFHe8A9G0Yuy4lxW9Q.jpeg&quot; /&gt;&lt;/figure&gt;&lt;blockquote&gt;본 포스트는 &lt;a href=&quot;https://medium.com/coupang-engineering/coupang-scm-workflow-building-a-low-code-no-code-platform-3d035c1fa37a&quot;&gt;&lt;strong&gt;영문&lt;/strong&gt;&lt;/a&gt;으로도 제공됩니다.&lt;/blockquote&gt;&lt;p&gt;SCM(Supply Chain Management)은 상품 공급자, 운송 업체, 배송 서비스, 제품 등으로 이루어진 복잡한 네트워크를 데이터 흐름에 기반해 관리합니다. 쿠팡은 로켓 배송을 포함한 다양한 종류의 배송 서비스들을 제공하기 위해 전국 곳곳에 자체 풀필먼트 센터(fulfillment center, FC)를 구축했습니다. SCM 팀은 상품 수요를 예측하고, 구매 주문이행에 필요한 파이프라인을 만들고 관리합니다. 주요 업무는 구매 주문 및 센터 간 물류를 효율적으로 분배하기 위해 다양한 알고리즘 및 모델을 개발하고, 운영팀과 협업하는 것입니다. 팀에서는 물류 이동 정보 등 다양한 데이터가 다뤄지고 있습니다.&lt;/p&gt;&lt;p&gt;쿠팡 SCM 팀은 업무 특성상 운영팀과 개발자, 비즈니스 분석가(Business Analyst, BA), 데이터 사이언티스트(Data Scientist) 등 다양한 직군의 구성원들이 모여 함께 시스템을 만들고 운영합니다. 그렇기 때문에 모두들 시스템의 데이터를 활용하고, 그 데이터를 다시 시스템에 피드백으로 주면서, 팀의 플랫폼을 개발해나가고 있습니다.&lt;/p&gt;&lt;p&gt;이렇게 정보 및 아이디어 교환이 많은 팀에서 개발자로 일하면서 다음과 같은 이슈들을 자주 접하게 되었습니다.&lt;/p&gt;&lt;blockquote&gt;“누구나 웹을 통해 데이터를 생성 또는 수정하고 생산한 데이터를 프로덕션 시스템까지 쉽게 연동할 수 있으면 좋을텐데…”&lt;/blockquote&gt;&lt;blockquote&gt;“새로운 시스템을 구축하지 않고, 도메인을 확장할 수 있는 방법은 없을까?”&lt;/blockquote&gt;&lt;blockquote&gt;“사용 중인 파이프인의 코드를 변경하지 않고도 바뀐 요구사항을 반영할 수는 없을까?”&lt;/blockquote&gt;&lt;blockquote&gt;“Low-code, no-code 시스템 개발?”&lt;/blockquote&gt;&lt;p&gt;문제 해결 과정에서 점점 더 많은 주목을 받고 있는 기술 트렌드인 low-code, no-code가 큰 인사이트를 주었습니다. 개발자는 알고리즘 개발에 집중할 수 있고, 비개발자도 개발자 못지않은 결과물을 만들어 낼 수 있는 플랫폼이 필요하다는 생각에 no-code 데이터 빌더 및 low-code 서비스 빌더를 제공하는 SCM 워크플로우 플랫폼(SCM Workflow platform)을 개발했습니다.&lt;/p&gt;&lt;p&gt;저희는 앞으로 SCM 워크플로우를 통해 기존의 &lt;a href=&quot;https://www.jenkins.io/&quot;&gt;Jenkins&lt;/a&gt;, &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Airflow&lt;/a&gt;, &lt;a href=&quot;https://jupyter.org/&quot;&gt;Notebook&lt;/a&gt; 같은 툴이 가진 복잡성 및 연결성을 개선하여 완전히 대체하는 것은 물론, 한발 더 나아가 누구나 간단한 조작만으로도 데이터를 만들어내고 시각화해 간단한 서비스까지 론칭할 수 있는 플랫폼을 만들어나가려고 합니다.&lt;/p&gt;&lt;h4&gt;목차&lt;/h4&gt;&lt;blockquote&gt;· &lt;a href=&quot;#caae&quot;&gt;No-code 데이터 빌더&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#71a4&quot;&gt;Low-code 서비스 빌더&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#443a&quot;&gt;기타 특징 및 기능&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#a6ee&quot;&gt;쿠팡 워크플로우 웹 인터페이스&lt;/a&gt;&lt;br&gt;· &lt;a href=&quot;#07dc&quot;&gt;향후 계획&lt;/a&gt;&lt;/blockquote&gt;&lt;h3&gt;No-code 데이터 빌더&lt;/h3&gt;&lt;p&gt;쿠팡은 데이터에 기반해 모든 의사결정을 내립니다. 대량의 데이터가 마이크로서비스 아키텍처(MSA) 내의 도메인 단위로 나누어지고, 나누어진 데이터는 해당 도메인을 담당하는 팀에 의해 관리됩니다. 각 도메인 팀은 ODS, EDW, API 서비스 등 다양한 인터페이스를 통해 다른 팀들에게 데이터를 제공합니다.&lt;/p&gt;&lt;p&gt;하지만 비즈니스가 성장하고 다양해지면서 요구사항 또한 빠르게 바뀌고 많아졌습니다. 데이터 소스와 분석 쿼리 또한 수시로 변하게 되었고 이를 시스템에 빠르게 반영하고 프로덕션 결과까지 연동해야 했습니다. 간단한 데이터 검색 및 분석 작업도 여러 툴에 접속해야만 가능했기 때문에 담당 개발자가 아닌 사용자 입장에서는 작업 수행에 어려움이 있었습니다.&lt;/p&gt;&lt;p&gt;예를 들어 비즈니스 분석가 및 데이터 사이언티스트가 새 모델을 만들고 적용하려면, 담당 개발자에게 비즈니스 요구사항에 맞는 새로운 데이터 파이프라인을 개발해달라고 요청해야 했습니다. 정형화된 프로세스로 이루어진 작업이지만 막상 관련 툴을 개발하기도 애매한 일이라 개발자는 개발자대로, 관련 작업자는 작업자대로 매번 요청하고 기다리는 식의 프로세스 아닌 프로세스가 반복되고 있었습니다. 이와 같은 작업들로 인해 팀원들 간에 많은 커뮤니케이션 비용이 발생했습니다.&lt;/p&gt;&lt;p&gt;이런 문제를 해결하기 위해 SCM 워크플로우의 no-code 데이터 빌더(data builder) 기능을 개발했습니다. 데이버 빌더를 통해 데이터 소스 접근, 데이터 질의(query) 및 추출(export), 그리고 시스템 연동을 쉽게 수행할 수 있습니다. 개발자는 물론 비즈니스 분석가, 데이터 사이언티스트, 실무 운영진까지 코드 없이 안전하고 쉽게 데이터에 접근하고 데이터를 처리하고 시스템 연동까지 해낼 수 있습니다. 데이터 프로세서(data processor)로서 SCM 워크플로우는 팀 내에서 사용 중인 다음과 같은 공용 데이터 소스로 접근 가능합니다: &lt;a href=&quot;https://aws.amazon.com/redshift/&quot;&gt;Redshift&lt;/a&gt;, &lt;a href=&quot;https://hive.apache.org/&quot;&gt;Hive&lt;/a&gt;, &lt;a href=&quot;https://prestodb.io/&quot;&gt;Presto&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Aurora&lt;/a&gt;, &lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;SCM 워크플로우의 데이터 빌더를 사용하는 과정은 다음과 같습니다. 예를 들어 풀필먼트 센터 간 상품 이관이 필요한 상황 에서 어떤 상품을 얼마만큼 언제 이관해야 하는지를 결정해야 할 경우, 데이터 빌더 사용자는 일종의 데이터 생산자로서 상품 이관 워크플로우를 다음과 같이 만들 수 있습니다.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Hive, Redshift, Aurora 등 다양한 데이터 소스에 저장되어 있는 상품 재고 및 주문 데이터를 추출하는 노드를 생성합니다.&lt;/li&gt;&lt;li&gt;각 노드에 데이터를 추출할 수 있는 쿼리를 작성해 넣습니다.&lt;/li&gt;&lt;li&gt;데이터 추출이 완료되면 이를 입력 삼아 이관 상품수량을 계산하는 노드를 생성해 앞서 생성된 노드들과 연결합니다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이런 간단한 과정을 통해 만든 워크플로우로 사용자는 이관이 필요한 상품의 수량을 계산할 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;이관 상품 수량 계산을 위한 쿠팡 SCM 워크플로우&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*tLme86SZaYTNGy7PApM8Pg.png&quot; /&gt;&lt;figcaption&gt;그림 1. 이관 상품 수량 계산을 위한 워크플로우&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;또한 이 과정에서 복잡한 쿼리를 분할해 다수의 쿼리들로 만들 경우, 원하는 만큼 병렬 처리가 가능합니다. 순차 실행보다 더 빨리 결과를 얻을 수 있습니다. 질의의 결과는 사용자가 따로 저장하지 않고, 데이터 소비자가 될 개발자 또는 다른 팀에 워크플로우 ID 또는 URL, 파일 경로를 전달해 주면 됩니다. 콜백(callback) 기능을 사용해 다른 시스템과 연동도 쉽게 할 수 있습니다.&lt;/p&gt;&lt;p&gt;워크플로우의 스케줄링 기능을 통해 매일 정해진 시간에 데이터를 생성할 수 있습니다. 또한 시, 일, 주 단위 등으로 반복 처리되는 작업이 있다면 스케줄링을 통해 해당 작업을 자동화할 수도 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;다수의 분할된 쿼리들을 병렬로 처리하는 쿠팡 SCM 워크플로우&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*zzrSwPZam0XwZr3LHByy8A.png&quot; /&gt;&lt;figcaption&gt;그림 2. 다수의 분할된 쿼리들을 병렬로 처리&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Low-code 서비스 빌더&lt;/h3&gt;&lt;p&gt;전국 쿠팡 풀필먼트 센터의 입고 및 출고는 상품 수요 예측 시스템, 풀필먼트 센터 인력 관리 시스템, 구매 주문 분배 시스템, 구매 주문 시스템, 센터 간 물류 이동 시스템 등 다양한 SCM 관련 시스템들에 의해 관리되고 있으며 해당 시스템들은 쿠팡의 공급망 관리(SCM)를 책임지고 있습니다. 각 도메인 팀은 담당하고 있는 시스템의 운영을 위해 서버를 구축하고 수많은 배치 시스템과 설정을 관리하고 있습니다.&lt;/p&gt;&lt;p&gt;문제는 비즈니스 도메인이 확장하면서 비슷한 시스템들이 계속 생겨나고 유지 및 보수 관련 공수가 계속 늘어나는 것이었습니다. 특히 수요 예측, 주문량 분배, 주문, 풀필먼트 센터 간 물류 이관 같은 서비스들은 시스템에서 생성된 데이터를 서로 주고받으면서 사용 및 운영되기 때문에, 각각의 시스템에는 비슷한 기능을 수행하는 코드들이 존재합니다. 핵심 알고리즘을 제외하면 대부분 데이터를 받고, 처리하고, 전달하는 과정들의 반복입니다. 이런 반복적인 일을 간편하게 처리할 수 있도록 SCM 워크플로우는 low-code 서비스 빌더를 제공합니다. 해당 서비스를 통해 개발자는 주요 알고리즘과 데이터 처리 부분에만 집중해 개발할 수 있게 되며 개발 효율성 및 시스템 안정성을 개선할 수 있습니다.&lt;/p&gt;&lt;p&gt;SCM 워크플로우의 low-code 서비스 빌더의 주요 기능은 다음과 같습니다.&lt;/p&gt;&lt;h4&gt;개발 플랫폼 제공&lt;/h4&gt;&lt;p&gt;서비스 개발을 위해 필수적으로 필요한 버저닝(versioning), 설정(configuration), 에러 로그 뷰어를 제공합니다. 그래서 개발자는 웹을 통해 디버깅이 가능하며 별도의 버전 관리 툴이 필요하지 않습니다. 또한 운영상 필요한 알람 기능, 워크플로우의 상태를 실시간으로 모니터링할 수 있는 대시보드(dashboard)를 제공하여 시스템 운영을 보조합니다.&lt;/p&gt;&lt;h4&gt;공용 서비스 컴포넌트 제공&lt;/h4&gt;&lt;p&gt;시스템 구축에 필수적인 서비스 컴포넌트를 제공하여 손쉬운 서비스 구성이 가능합니다. 메신저 알림(notification), REST 콜백(callback), 요청 재시도(retry) 등을 프로그래밍 관점의 기본 컴포넌트로 제공합니다. 또한 파일 업로드, 다운로드, 복사, 큐 메시지 발송 및 수신 등과 AWS 인프라를 쉽게 사용할 수 있는 서비스 블록을 활용해, 데이터와 메시지를 주고받는 시스템 구축이 가능합니다. 또한 사용자들의 피드백을 바탕으로 새로운 공용 서비스 컴포넌트를 계속 추가하고 있습니다.&lt;/p&gt;&lt;h4&gt;커스텀 서비스 컴포넌트 등록 및 사용&lt;/h4&gt;&lt;p&gt;커스텀(custom) 서비스 컴포넌트를 등록해 SCM 워크플로우의 확장성 및 유연성을 강화할 수 있습니다. 도메인 개발자는 핵심 알고리즘을 커스텀 서비스 컴포넌트로 만들고 이를 시스템에 등록하면 SCM 워크플로우는 이를 하나의 블록으로 만들 수 있게 됩니다. 등록한 블록을 통해 해당 도메인의 특화 기능을 워크플로우로 쉽게 통합할 수 있습니다. 이를 통해 머신러닝 모델의 동작도 가능한 복잡도가 높은 서비스도 구축 가능합니다. 커스텀 서비스 컴포넌트를 수정하거나 재사용해 보다 쉽게 시스템을 업그레이드하고 도메인을 확장할 수 있습니다.&lt;/p&gt;&lt;h4&gt;그래픽 워크플로우 모델링&lt;/h4&gt;&lt;p&gt;기존의 상용 워크플로우와 비교해 SCM 워크플로우가 가진 가장 큰 장점 중 하나는 캔버스 에디터(canvas editor)와 서비스 컴포넌트(블록) 간의 입출력 데이터 연동 방식입니다. 블록과 캔버스를 사용해 모델을 그리는 것은 Low code no-code 개발 플랫폼의 영향이 컸습니다. 블록화된 SCM 워크플로우의 서비스 컴포넌트들을 드래그 앤 드랍(drag &amp;amp; drop)으로 캔버스 에디터로 손쉽게 배치하고 연결 및 조합할 수 있습니다. 또한 워크플로우 내 모든 컴포넌트들은 런타임(runtime) 중에 다른 컴포넌트의 입출력 값을 참조해 각 컴포넌트의 입력 변수(input parameter)로 사용할 수 있습니다. 이를 통해 워크플로우 모델은 하나의 프로그램으로 동작할 수 있게 됩니다. 여기에 조건 분기(conditional statement), 반복(iteration), 병렬 처리(parallelism)와 같은 프로그래밍적 요소들도 추가되었고, 사용자는 이 요소들을 활용해 좀 더 복잡한 프로그램을 작성하고 고도화된 내부 애플리케이션을 만들어낼 수도 있습니다.&lt;/p&gt;&lt;h3&gt;기타 특징 및 기능&lt;/h3&gt;&lt;p&gt;No-code, low-code 외에도 SCM 워크플로우는 다음과 같은 특징과 기능을 갖고 있습니다.&lt;/p&gt;&lt;h4&gt;확장성&lt;/h4&gt;&lt;p&gt;SCM 워크플로우의 아키텍처는 확장성과 안정성에 주안점을 두고 설계되었습니다. 다양한 데이터 소스들에 대한 데이터 쿼리와 프로세싱이 안정적으로 이뤄져야 함은 물론 개별 사용자 및 각종 시스템에서 하고 있던 작업이 시스템 내로 통합될 수 있어야 하기 때문입니다. 모든 작업을 떠안지만 단일 장애점(Single Point of Failure, SPOF)은 되지 않도록, 각각의 서비스를 3개의 레이어로 구성되게끔 설계했습니다. 각 레이어를 클러스터링(clustering)해 처리량이 늘면 각 레이어의 클러스터가 스케일 아웃(scale out)될 수 있도록 하였습니다.&lt;/p&gt;&lt;h4&gt;작업 간 독립성 및 안정성&lt;/h4&gt;&lt;p&gt;SCM 워크플로우 엔진은 모델을 생성하고 관리하는 것은 물론 각각의 모델로부터 생성되는 인스턴스를 실행하고 작동시키는 일도 합니다. 워크플로우 인스턴스는 실행 중 자신만의 고유 상태를 가지며 엔진은 모델링된 상태 정보를 이용해 사용자가 정의한 작업을 순차, 병렬, 반복 처리하며 완료하게 합니다. 클러스터 내에서 작동할 수 있는 스케줄러를 통해 사용자가 원하는 시간에 원하는 개수만큼 인스턴스를 생성해 작업을 수행할 수 있습니다. 일련의 작업 수행 시 특정 작업에 오랜 시간이 걸리거나 특정 작업의 과도한 리소스 사용으로 인해 다른 작업이 받는 영향을 줄이기 위해, 각 작업은 SCM 워크플로우 시스템 내 클러스터에서 비동기 요청을 통해 독립적으로 실행됩니다. 워크플로우 엔진은 각 작업의 성공 및 실패를 모니터링하고 사용자의 설정에 따라 워크플로우를 진행시키는 역할만 합니다. 수많은 워크플로우의 계속되는 요청들을 안정적으로 처리할 수 있도록 워크플로우 엔진과 워커(worker)를 독립적으로 설계했습니다.&lt;/p&gt;&lt;h4&gt;공용 데이터 처리 서비스&lt;/h4&gt;&lt;p&gt;SCM 워크플로우 프로세서는 자체 데이터 처리(data processing) 서비스를 가지고 있습니다. 기본적으로 팀 내에서 서비스되는 각종 데이터 소스에 대한 정보를 공용 서비스 컴포넌트 형태로 엔진에 제공 가능하며, 사용자는 별도의 설정 없이도 데이터 소스에 대한 작업을 수행할 수 있습니다. 또한 데이터 프로세싱 유틸리티 기능과 파일 업로드, 다운로드, 병합(Merge) 등 실제 서비스에 쓰이는 기능을 제공해 사용자의 서비스 구현을 돕고 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우 아키텍처&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*q8oLd6R8kfdF4LcYX8ilKw.jpeg&quot; /&gt;&lt;figcaption&gt;그림 3. SCM 워크플로우 아키텍처&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;쿠팡 워크플로우 웹 인터페이스&lt;/h3&gt;&lt;p&gt;SCM 워크플로우 웹은 사용자 웹 인터페이스를 제공합니다. 워크플로우 모델 및 스케줄과 실행 히스토리를 생성, 편집, 관리할 수 있습니다. 워크플로우 웹 캔버스를 이용해 사용자는 등록된 서비스 컴포넌트들을 직관적으로 캔버스 위에 배치하고 컴포넌트들을 의존성에 따라 연결하고 컴포넌트들의 실행 순서를 지정할 수 있습니다. 코딩에 익숙하지 않은 사용자도 이 툴로 손쉽게 워크플로우를 만들 수 있습니다.&lt;/p&gt;&lt;h4&gt;워크플로우 모델러&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우의 모델링 캔버스&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*yucG-AXV69VCRSLB-M-HcQ.png&quot; /&gt;&lt;figcaption&gt;그림 4. 워크플로우 모델링 캔버스&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;인스턴스 실행에 대한 성공/실패 히스토리를 확인할 수 있습니다. 어떤 부분에서 얼마큼 시간이 걸렸는지, 실패했을 경우 에러 로그를 확인할 수도 있어 디버깅에 도움을 주고 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우의 인스턴스 히스토리&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*Q90809jZobfHZ2TzqVqRQw.png&quot; /&gt;&lt;figcaption&gt;그림 5. 워크플로우 인스턴스 히스토리&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;워크플로우 매니저&lt;/h4&gt;&lt;p&gt;워크플로우 매니저를 이용해 각 팀은 프로젝트 또는 팀 별로 그룹을 생성하고 그룹 폴더 내에서 모델들을 관리할 수 있습니다. 그룹 폴더의 권한 설정으로 모델의 생성, 수정, 삭제 등을 제한할 수도 있습니다. 향후에는 개별 사용자 플레이그라운드(playground)와 같은 개인 워크스페이스(workspace)를 제공하여 프로덕션(production) 워크플로우 외에도 실험적인 작업이 가능하도록 지원할 계획입니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우의 모델 매니저&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*8j1gK5JotMln2Tg0gsQEEw.png&quot; /&gt;&lt;figcaption&gt;그림 6. 모델 그룹 관리&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;워크플로우 스케줄러&lt;/h4&gt;&lt;p&gt;모델 별로 스케줄을 설정해 사용자가 원하는 시간에 원하는 작업을 자동으로 실행하고 완료할 수 있도록 지원합니다. 비개발자를 위해 Cron Expression을 생성할 수 있는 사용자 인터페이스도 별도로 제공하고 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우의 작업 스케줄러&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*SDLXxmcTIY9wVq2Ivw-PsQ.png&quot; /&gt;&lt;figcaption&gt;그림 7. 작업 스케줄 관리하기&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;워크플로우 대시보드&lt;/h4&gt;&lt;p&gt;대시보드를 통해 워크플로우 엔진의 상태를 모니터링할 수 있습니다. 사용자는 개별 인스턴스 설정에 따라 메신저 알림을 수신할 수도 있지만, 대시보드를 통해 현재 팀 내에서 관리 중인 서비스 워크플로우의 상태를 확인하고 실시간으로 대응할 수 있습니다.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;쿠팡 SCM 워크플로우 대시보드 내 인스턴스 모니터링 페이지&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*axZ8y6geUYJri5wyI5gBRQ.png&quot; /&gt;&lt;figcaption&gt;그림 8. 대시보드 내 인스턴스 모니터링 페이지&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;향후 계획&lt;/h3&gt;&lt;p&gt;SCM 워크플로우로 새로운 컴포넌트와 시스템들이 계속 추가되고 있습니다. 이를 뒷받침하기 위해 다양한 플랫폼 기능들 또한 함께 개발되고 업그레이드되고 있습니다.&lt;/p&gt;&lt;p&gt;저희의 최종 목표는 SCM 워크플로우를 통해 팀 내 작은 프로젝트들을 마이그레이션해 관리 가능한 형태로 바꾸는 것입니다. 또한 데이터 처리 부분을 더 강화해 간단한 엑셀 작업부터 대용량 데이터 처리 작업까지 각각의 용도에 맞는 적정 데이터 기술을 제공하려고 합니다. 또 다른 목표는 일반 사용자를 위한 애플리케이션 생성 플랫폼으로의 성장입니다. 사용자가 입출력 인터페이스를 직접 구성하고 다른 사용자가 만든 애플리케이션을 활용해 시스템과 상호작용할 수 있게 만드는 것입니다.&lt;/p&gt;&lt;p&gt;앞으로도 계속 SCM 워크플로우를 개발자 및 비개발자 모두 함께 사용할 수 있는, 사용하기 쉽고 편리하면서 동시에 확장성을 가진 플랫폼으로 발전시켜 나가겠습니다.&lt;/p&gt;&lt;p&gt;&lt;em&gt;SCM 워크플로우를 더 발전된 플랫폼으로 만들어 나가는데 관심이 있으시다면, 쿠팡 &lt;/em&gt;&lt;a href=&quot;https://bit.ly/3bw6NjT&quot;&gt;&lt;em&gt;채용 공고&lt;/em&gt;&lt;/a&gt;&lt;em&gt;를 확인해 보세요!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 &lt;a href=&quot;https://ir.aboutcoupang.com/English/home/default.aspx&quot;&gt;ir.aboutcoupang.com&lt;/a&gt; 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.&lt;/p&gt;&lt;img src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7d997644d14&quot; width=&quot;1&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-scm-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B4%EA%B3%A0-%ED%99%95%EC%9E%A5-%EA%B0%80%EB%8A%A5%ED%95%9C-low-code-no-code-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C-7d997644d14&quot;&gt;쿠팡 SCM 워크플로우: 효율적이고 확장 가능한 low-code, no-code 플랫폼 개발&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/coupang-engineering&quot;&gt;Coupang Engineering Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
    </item>
    <item>
      <title>컬리는 물류 최적화 문제를 어떻게 풀고 있을까? - 2부</title>
      <link>http://thefarmersfront.github.io/blog/logistics-optimization-2/</link>
      <guid>http://thefarmersfront.github.io/blog/logistics-optimization-2/</guid>
      <pubDate>Mon, 14 Nov 2022 10:00:00 GMT</pubDate>
      <content:encoded>Digital Twin을 구축해 최적화 알고리즘을 검증하기</content:encoded>
    </item>
    <item>
      <title>Kurly만의 MLOps 구축하기 - 쿠브플로우 도입기</title>
      <link>http://thefarmersfront.github.io/blog/second-mlops/</link>
      <guid>http://thefarmersfront.github.io/blog/second-mlops/</guid>
      <pubDate>Thu, 03 Nov 2022 00:00:00 GMT</pubDate>
      <content:encoded>쿠브플로우를 도입한 이유와 유용한 팁</content:encoded>
    </item>
    <item>
      <title>KURLY 현직자가 직접 뽑은 직무별 핵심 키워드를 소개합니다!</title>
      <link>http://thefarmersfront.github.io/blog/kurly-live-job-talkshow/</link>
      <guid>http://thefarmersfront.github.io/blog/kurly-live-job-talkshow/</guid>
      <pubDate>Wed, 26 Oct 2022 01:00:00 GMT</pubDate>
      <content:encoded>2022 LIVE 직무 토크쇼 핵심 정리</content:encoded>
    </item>
    <item>
      <title>Kurly만의 MLOps 구축하기 - 초석 다지기</title>
      <link>http://thefarmersfront.github.io/blog/first-mlops/</link>
      <guid>http://thefarmersfront.github.io/blog/first-mlops/</guid>
      <pubDate>Wed, 26 Oct 2022 00:00:00 GMT</pubDate>
      <content:encoded>GPU사용환경 만들기</content:encoded>
    </item>
    <item>
      <title>컬리는 물류 최적화 문제를 어떻게 풀고 있을까? - 1부</title>
      <link>http://thefarmersfront.github.io/blog/logistics-optimization-1/</link>
      <guid>http://thefarmersfront.github.io/blog/logistics-optimization-1/</guid>
      <pubDate>Thu, 13 Oct 2022 10:00:00 GMT</pubDate>
      <content:encoded>유전 알고리즘 적용을 통한 최적화 사례 소개</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 사내 해커톤을 소개합니다!</title>
      <link>https://blog.banksalad.com/pnc/hackathon_2022/</link>
      <guid>https://blog.banksalad.com/pnc/hackathon_2022/</guid>
      <pubDate>Thu, 13 Oct 2022 00:00:00 GMT</pubDate>
      <content:encoded>꾸준한 인기를 누리고 있는 ‘뱅크샐러드의 주택청약 알림🔔’ 서비스! 이 서비스는 사실 뱅크샐러드 사내 해커톤을 통해 런칭된 기능이에요. 지난해 사내 해커톤에서 발제된 10개의 아이디어 중 무려…</content:encoded>
    </item>
    <item>
      <title>내가 만든 API를 널리 알리기 - Spring REST Docs 가이드편</title>
      <link>http://thefarmersfront.github.io/blog/spring-rest-docs-guide/</link>
      <guid>http://thefarmersfront.github.io/blog/spring-rest-docs-guide/</guid>
      <pubDate>Wed, 28 Sep 2022 00:00:00 GMT</pubDate>
      <content:encoded>&amp;apos;추석맞이 선물하기 재개발&amp;apos;에 차출되어 API 문서화를 위해 도입한 Spring REST Docs 를 소개합니다.</content:encoded>
    </item>
    <item>
      <title>Kurly Design Principle</title>
      <link>http://thefarmersfront.github.io/blog/kurly-design-principle/</link>
      <guid>http://thefarmersfront.github.io/blog/kurly-design-principle/</guid>
      <pubDate>Tue, 27 Sep 2022 17:00:00 GMT</pubDate>
      <content:encoded>컬리 프로덕트 디자인 원칙을 소개합니다</content:encoded>
    </item>
    <item>
      <title>우리가 실행해 본 팀원 모두 함께 할 수 있는 팀 빌딩 게임 3가지</title>
      <link>http://thefarmersfront.github.io/blog/team-building-game/</link>
      <guid>http://thefarmersfront.github.io/blog/team-building-game/</guid>
      <pubDate>Tue, 27 Sep 2022 00:00:00 GMT</pubDate>
      <content:encoded>원팀을 만드는데에 도움이 되는 팀 빌딩 게임을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>컬리 공통 암호화 모듈의 동시성 이슈 해결하기</title>
      <link>http://thefarmersfront.github.io/blog/concurrency-issue-solving/</link>
      <guid>http://thefarmersfront.github.io/blog/concurrency-issue-solving/</guid>
      <pubDate>Mon, 26 Sep 2022 00:00:00 GMT</pubDate>
      <content:encoded>공통 모듈에서 발생한 동시성 이슈의 해결 과정을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>KURLY HACK FESTA 2022 개최기를 소개합니다.</title>
      <link>http://thefarmersfront.github.io/blog/kurly-hack-festa-2022/</link>
      <guid>http://thefarmersfront.github.io/blog/kurly-hack-festa-2022/</guid>
      <pubDate>Fri, 16 Sep 2022 00:00:00 GMT</pubDate>
      <content:encoded>컬리의 입사 연계 특전 온라인 해커톤 그 첫번째 이야기</content:encoded>
    </item>
    <item>
      <title>새로운 컬리몰 NX를 소개합니다.</title>
      <link>http://thefarmersfront.github.io/blog/introduction_new_kurlymall_web/</link>
      <guid>http://thefarmersfront.github.io/blog/introduction_new_kurlymall_web/</guid>
      <pubDate>Thu, 01 Sep 2022 00:00:00 GMT</pubDate>
      <content:encoded>새로운 컬리몰을 만드는 과정을 소개합니다.</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 Web chapter에서 GitHub Action 기반의 CI 속도를 개선한 방법</title>
      <link>https://blog.banksalad.com/tech/github-action-npm-cache/</link>
      <guid>https://blog.banksalad.com/tech/github-action-npm-cache/</guid>
      <pubDate>Mon, 29 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded>이 글은 대부분의 웹 프론트엔드에서 사용하는 Node.js와 Npm 을 사용하는 사례를 바탕으로 작성되었습니다. 안녕하세요 뱅크샐러드 웹 프론트엔드 엔지니어 조성동입니다. Web chapter 내 개발 과정의 많은 부분에서 GitHub Action…</content:encoded>
    </item>
    <item>
      <title>컬리 검색이 카프카를 들여다본 이야기 2</title>
      <link>http://thefarmersfront.github.io/blog/search-system-with-kafka-2/</link>
      <guid>http://thefarmersfront.github.io/blog/search-system-with-kafka-2/</guid>
      <pubDate>Thu, 25 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded>카프카 스트림즈를 추가하다</content:encoded>
    </item>
    <item>
      <title>제1회 뱅크샐러드 디자인 드레싱을 소개합니다</title>
      <link>https://blog.banksalad.com/pnc/design_dressing/</link>
      <guid>https://blog.banksalad.com/pnc/design_dressing/</guid>
      <pubDate>Mon, 22 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded>2022년도 이제 반을 훌쩍 지나고 있습니다. 여러분의 상반기는 어떠셨나요? 뱅크샐러드의 디자인 챕터는 지난…</content:encoded>
    </item>
    <item>
      <title>컬리 검색이 카프카를 들여다본 이야기 1</title>
      <link>http://thefarmersfront.github.io/blog/search-system-with-kafka-1/</link>
      <guid>http://thefarmersfront.github.io/blog/search-system-with-kafka-1/</guid>
      <pubDate>Mon, 08 Aug 2022 09:00:00 GMT</pubDate>
      <content:encoded>카프카 설정 튜닝만으로 색인 속도를 개선하다</content:encoded>
    </item>
    <item>
      <title>스크럼, 입고팀이 애자일하게 일하는 법 2부</title>
      <link>http://thefarmersfront.github.io/blog/inbound-squad-sprint-2/</link>
      <guid>http://thefarmersfront.github.io/blog/inbound-squad-sprint-2/</guid>
      <pubDate>Mon, 11 Jul 2022 09:00:00 GMT</pubDate>
      <content:encoded>개발자 입장에서 쓴 스크럼 운영 후기</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] Data Engineer 에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-de/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-de/</guid>
      <pubDate>Thu, 30 Jun 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 Data Engineer는 무슨 일을 하나요? 뱅크샐러드 Data Engineer…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] Data Scientist &amp; Analyst 에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-dsa/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-dsa/</guid>
      <pubDate>Thu, 30 Jun 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 Data Scientist &amp; Analyst 는 무슨 일을 하나요? 뱅크샐러드 Data Scientist 와 Data Analyst…</content:encoded>
    </item>
    <item>
      <title>스크럼, 입고팀이 애자일하게 일하는 법 1부</title>
      <link>http://thefarmersfront.github.io/blog/inbound-squad-sprint-1/</link>
      <guid>http://thefarmersfront.github.io/blog/inbound-squad-sprint-1/</guid>
      <pubDate>Mon, 27 Jun 2022 09:00:00 GMT</pubDate>
      <content:encoded>스크럼 운영 방안 맛보기</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 iOS팀이 숨쉬듯이 테스트코드 짜는 방식 3편 - 스펙별 단위 테스트</title>
      <link>https://blog.banksalad.com/tech/test-in-banksalad-ios-3/</link>
      <guid>https://blog.banksalad.com/tech/test-in-banksalad-ios-3/</guid>
      <pubDate>Mon, 30 May 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요! 뱅크샐러드에서 iOS…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] VP of Engineering에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-vpe/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-vpe/</guid>
      <pubDate>Fri, 27 May 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 VP of Engineering 정채상 님을 소개합니다! 뱅크샐러드는 실리콘밸리의 구글 본사에서 근무하시던 정채상 님을 VP of Engineering으로 모셨습니다! 1…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] CTO에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-cto/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-cto/</guid>
      <pubDate>Mon, 16 May 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 CTO 김문규님을 소개합니다! 지난 10월, 뱅크샐러드는 실리콘밸리의 구글 본사에서 근무하시던 김문규님을 CTO로 모셨습니다! 구글, 아마존 등 미국의 빅테크 기업에서 오랜기간 엔지니어로 근무하신 문규님은, 구글 안드로이드 운영체제(OS…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] DevOps팀에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-devops/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-devops/</guid>
      <pubDate>Wed, 04 May 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 DevOps팀은 무슨 일을 하나요? 뱅크샐러드 DevOps…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 iOS팀이 숨쉬듯이 테스트코드 짜는 방식 2편 - 화면 단위 통합 테스트</title>
      <link>https://blog.banksalad.com/tech/test-in-banksalad-ios-2/</link>
      <guid>https://blog.banksalad.com/tech/test-in-banksalad-ios-2/</guid>
      <pubDate>Wed, 27 Apr 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요! 뱅크샐러드에서 iOS…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 iOS팀이 숨쉬듯이 테스트코드 짜는 방식 1편 - 통합 UI테스트</title>
      <link>https://blog.banksalad.com/tech/test-in-banksalad-ios-1/</link>
      <guid>https://blog.banksalad.com/tech/test-in-banksalad-ios-1/</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요! 뱅크샐러드에서 iOS…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 안드로이드 앱에서 Koin 걷어내고 Hilt로 마이그레이션하기</title>
      <link>https://blog.banksalad.com/tech/migrate-from-koin-to-hilt/</link>
      <guid>https://blog.banksalad.com/tech/migrate-from-koin-to-hilt/</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] Android 팀에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-android/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-android/</guid>
      <pubDate>Wed, 30 Mar 2022 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] iOS팀에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-ios/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-ios/</guid>
      <pubDate>Wed, 23 Mar 2022 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 iOS 개발자는 무슨 일을 하나요? 뱅크샐러드 iOS…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 채용 여정 함께하기</title>
      <link>https://blog.banksalad.com/pnc/recruiting-process/</link>
      <guid>https://blog.banksalad.com/pnc/recruiting-process/</guid>
      <pubDate>Tue, 15 Mar 2022 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>팀을 더욱 유연하게 만들어가는 힘</title>
      <link>http://thefarmersfront.github.io/blog/squad-b-team-building/</link>
      <guid>http://thefarmersfront.github.io/blog/squad-b-team-building/</guid>
      <pubDate>Mon, 28 Feb 2022 09:00:00 GMT</pubDate>
      <content:encoded>체계라 쓰지만 보이지 않는 환상속 기린 같은 단어를 팀에 어울리게 써보자</content:encoded>
    </item>
    <item>
      <title>2022년 2월 마켓컬리 개발자 밋업 후기</title>
      <link>http://thefarmersfront.github.io/blog/review-20220224-tech-meetup/</link>
      <guid>http://thefarmersfront.github.io/blog/review-20220224-tech-meetup/</guid>
      <pubDate>Mon, 28 Feb 2022 00:00:00 GMT</pubDate>
      <content:encoded></content:encoded>
    </item>
    <item>
      <title>주니어 엔지니어의 눈으로 본 AWS re:Invent</title>
      <link>https://blog.banksalad.com/tech/aws-reinvent-for-junior-engineer/</link>
      <guid>https://blog.banksalad.com/tech/aws-reinvent-for-junior-engineer/</guid>
      <pubDate>Tue, 22 Feb 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, Data Platform팀 Data Engineer 이은지입니다.   코로나로 인해 2020년에 온라인으로 진행된 AWS re:Invent가, 2021년에는 오프라인 행사로 열렸습니다! 2021년 11월 29일부터 12월 0…</content:encoded>
    </item>
    <item>
      <title>컬리에서 데이터 분석가로 일한다는 것</title>
      <link>http://thefarmersfront.github.io/blog/how-to-work-da/</link>
      <guid>http://thefarmersfront.github.io/blog/how-to-work-da/</guid>
      <pubDate>Tue, 15 Feb 2022 13:10:12 GMT</pubDate>
      <content:encoded>맛있는 데이터를 만드는 방법</content:encoded>
    </item>
    <item>
      <title>마이데이터 프로젝트에서 배운 것들</title>
      <link>https://blog.banksalad.com/tech/what-i-learned-from-mydata-project/</link>
      <guid>https://blog.banksalad.com/tech/what-i-learned-from-mydata-project/</guid>
      <pubDate>Mon, 07 Feb 2022 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 인증 스쿼드(Authentication Squad)의 엔지니어링 매니저(Engineering Manager…</content:encoded>
    </item>
    <item>
      <title>분기마다 돌아오는 뱅크샐러드 Pit Stop을 소개합니다.</title>
      <link>https://blog.banksalad.com/pnc/pit-stop/</link>
      <guid>https://blog.banksalad.com/pnc/pit-stop/</guid>
      <pubDate>Fri, 28 Jan 2022 00:00:00 GMT</pubDate>
      <content:encoded>지난달 뱅크샐러드에서는 4 Lap Pit Stop : We Wish Your Merry Mydata🎄가 개최되었습니다! 매 분기 말에 진행되는 “Pit Stop…</content:encoded>
    </item>
    <item>
      <title>우리는 왜 공통 라이브러리를 만들기 시작했나</title>
      <link>http://thefarmersfront.github.io/blog/why-we-make-common-library/</link>
      <guid>http://thefarmersfront.github.io/blog/why-we-make-common-library/</guid>
      <pubDate>Tue, 04 Jan 2022 15:00:00 GMT</pubDate>
      <content:encoded></content:encoded>
    </item>
    <item>
      <title>주니어 개발자의 뱅샐 성장기!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-engineer/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-engineer/</guid>
      <pubDate>Fri, 31 Dec 2021 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드에는 서비스 개발을 담당하는 Squad 소속 개발자와, 스쿼드를 지원하고 앱 전반을 담당하는 Engineering Foundation 소속 개발자분들이 계시는데요. 각자의 위치에서 고객에게 Impact…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] Web팀에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-web/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-web/</guid>
      <pubDate>Tue, 28 Dec 2021 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 웹 개발자는 무슨 일을 하나요? 뱅크샐러드 Web Front-End Engineer는 뱅크샐러드 앱 내에 들어가는 웹뷰부터 데스크탑 환경의 웹사이트 그리고 다양한 Internal Product…</content:encoded>
    </item>
    <item>
      <title>일하는 것만큼 쉬는 것도 중요합니다.</title>
      <link>https://blog.banksalad.com/pnc/work-and-rest/</link>
      <guid>https://blog.banksalad.com/pnc/work-and-rest/</guid>
      <pubDate>Mon, 20 Dec 2021 00:00:00 GMT</pubDate>
      <content:encoded>학창시절 쉬는시간과 점심시간은 언제나 힘이 나는 순간이었죠. 직장인에게도 적절한 휴식은 업무 효율을 높여주는 원동력이 되곤 합니다. 뱅크샐러드는 개개인의 업무리듬을 존중하고 주기적인 Refresh…</content:encoded>
    </item>
    <item>
      <title>[여의도 43층 사람들] BX팀에게 무엇이든 물어보세요!</title>
      <link>https://blog.banksalad.com/pnc/team-interview-bx/</link>
      <guid>https://blog.banksalad.com/pnc/team-interview-bx/</guid>
      <pubDate>Fri, 26 Nov 2021 00:00:00 GMT</pubDate>
      <content:encoded>‘뱅크샐러드’하면 무엇이 떠오르나요? ‘뱅크샐러드’하면 떠오르는 모든 이미지와 경험을 책임지는 분들은 바로 뱅크샐러드 Brand eXperience…</content:encoded>
    </item>
    <item>
      <title>팀과 함께 성장하는 Engineering Manager의 역할</title>
      <link>https://blog.banksalad.com/tech/engineering-manager-role-growth/</link>
      <guid>https://blog.banksalad.com/tech/engineering-manager-role-growth/</guid>
      <pubDate>Tue, 12 Oct 2021 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드 UX Strategy 팀 Product Manager 이승민 입니다. 저는 2019년 11월부터 2021년 6월까지 Android, iOS, Web, QA 4개의 Frontend 플랫폼의 Engineering Manager…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 Data Discovery Platform의 시작</title>
      <link>https://blog.banksalad.com/tech/the-starting-of-datadiscoveryplatform-era-in-banksalad/</link>
      <guid>https://blog.banksalad.com/tech/the-starting-of-datadiscoveryplatform-era-in-banksalad/</guid>
      <pubDate>Wed, 15 Sep 2021 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 Data Foundation의 Data Engineering팀 Data Engineer Jensen…</content:encoded>
    </item>
    <item>
      <title>DevOps팀의 Terraform 모험</title>
      <link>http://thefarmersfront.github.io/blog/terraform-adventure/</link>
      <guid>http://thefarmersfront.github.io/blog/terraform-adventure/</guid>
      <pubDate>Thu, 26 Aug 2021 00:00:00 GMT</pubDate>
      <content:encoded>사실은 낭만이 아닌 헬이었다</content:encoded>
    </item>
    <item>
      <title>마이데이터 맵과 비즈니스 확장성</title>
      <link>https://blog.banksalad.com/pnc/mydata-handbook-202108/</link>
      <guid>https://blog.banksalad.com/pnc/mydata-handbook-202108/</guid>
      <pubDate>Tue, 24 Aug 2021 00:00:00 GMT</pubDate>
      <content:encoded>데이터…</content:encoded>
    </item>
    <item>
      <title>카트 개발 연대기</title>
      <link>http://thefarmersfront.github.io/blog/my-cart-development-history/</link>
      <guid>http://thefarmersfront.github.io/blog/my-cart-development-history/</guid>
      <pubDate>Wed, 04 Aug 2021 09:00:00 GMT</pubDate>
      <content:encoded>컬리 카트는 어떤 모습으로 성장하고 있을까?</content:encoded>
    </item>
    <item>
      <title>구성원이 재충전의 시간을 갖는 다양한 방법</title>
      <link>https://blog.banksalad.com/pnc/refresh-day-202107/</link>
      <guid>https://blog.banksalad.com/pnc/refresh-day-202107/</guid>
      <pubDate>Fri, 30 Jul 2021 00:00:00 GMT</pubDate>
      <content:encoded>직장인이라면 한 번쯤 걸린다는 ‘월요병’. 출근하는 사람들로 북적이는 월요일 오전, 평화롭게 늦잠을 자는 상상을 하게 하죠. 6월의 마지막 주만큼은 뱅크샐러드 구성원 모두 월요병에서 벗어날 수 있었습니다. 바로 지난 6월 2…</content:encoded>
    </item>
    <item>
      <title>Tech All Hands, 뱅크샐러드 개발 아고라</title>
      <link>https://blog.banksalad.com/pnc/tech-all-hands-202107/</link>
      <guid>https://blog.banksalad.com/pnc/tech-all-hands-202107/</guid>
      <pubDate>Thu, 08 Jul 2021 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드 컬쳐 블로그에 방문해주신 여러분, 안녕하세요! 혹시 Tech All Hands에 대해서 들어보신 분 계신가요? Tech All Hands…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 오피스 투어 [업무공간 편]</title>
      <link>https://blog.banksalad.com/pnc/office-tour/</link>
      <guid>https://blog.banksalad.com/pnc/office-tour/</guid>
      <pubDate>Mon, 05 Jul 2021 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 채용, 무엇이든 물어보세요</title>
      <link>https://blog.banksalad.com/pnc/culture-faq/</link>
      <guid>https://blog.banksalad.com/pnc/culture-faq/</guid>
      <pubDate>Fri, 04 Jun 2021 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>DDD와 MSA 기반으로 좋은 서비스 개발하기</title>
      <link>http://thefarmersfront.github.io/blog/ddd-msa-service-development/</link>
      <guid>http://thefarmersfront.github.io/blog/ddd-msa-service-development/</guid>
      <pubDate>Tue, 11 May 2021 00:00:00 GMT</pubDate>
      <content:encoded>컬리의 서비스 개발 원칙</content:encoded>
    </item>
    <item>
      <title>왜 폴 그레이엄은 회사 이름을 Y Combinator라고 지었을까</title>
      <link>http://thefarmersfront.github.io/blog/y-combinator/</link>
      <guid>http://thefarmersfront.github.io/blog/y-combinator/</guid>
      <pubDate>Fri, 30 Apr 2021 00:00:00 GMT</pubDate>
      <content:encoded></content:encoded>
    </item>
    <item>
      <title>두려움 없이 성장하는 뱅크샐러드</title>
      <link>https://blog.banksalad.com/tech/grow-fearlessly/</link>
      <guid>https://blog.banksalad.com/tech/grow-fearlessly/</guid>
      <pubDate>Mon, 15 Feb 2021 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드 보험 스쿼드의 테크 리드(Tech Lead…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 실험플랫폼 분석 인프라 살펴보기</title>
      <link>https://blog.banksalad.com/tech/experiment-platform-analysis-architecture/</link>
      <guid>https://blog.banksalad.com/tech/experiment-platform-analysis-architecture/</guid>
      <pubDate>Wed, 10 Feb 2021 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 실험플랫폼 팀 Data Scientist 이수형입니다. 뱅크샐러드는 실험을 통해 제품을 개선하는 노력이 실제로 사용자에게 더 좋은 경험으로 이어지는지 매일매일 데이터를 통해 검증해나가고 있습니다. 여기서 말하는 실험(A/B Test…</content:encoded>
    </item>
    <item>
      <title>두근두근 컬리의 면접, 팀에서 성장하기</title>
      <link>http://thefarmersfront.github.io/blog/experience-the-kurly-interview-process/</link>
      <guid>http://thefarmersfront.github.io/blog/experience-the-kurly-interview-process/</guid>
      <pubDate>Tue, 12 Jan 2021 14:40:00 GMT</pubDate>
      <content:encoded>컬리 입사 과정과 합격 이후 일어난 일들을 소개합니다</content:encoded>
    </item>
    <item>
      <title>Banksalad Product Language는 어떻게 디자인되었나요?</title>
      <link>https://blog.banksalad.com/tech/banksalad-product-language-design/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-product-language-design/</guid>
      <pubDate>Thu, 03 Dec 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드 Product Language 팀 김성민입니다. 뱅크샐러드에 합류 한 후부터 지금까지 주변 디자이너분들에게 Product Language에 관한 질문을 많이 받아 왔는데요, 아직도 국내에서는 Product Language…</content:encoded>
    </item>
    <item>
      <title>코드 리뷰 in 뱅크샐러드 개발 문화</title>
      <link>https://blog.banksalad.com/tech/banksalad-code-review-culture/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-code-review-culture/</guid>
      <pubDate>Tue, 24 Nov 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 BanksaladX iOS Engineer…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드의 특별한 스펙, &apos;테크 스펙&apos;</title>
      <link>https://blog.banksalad.com/tech/we-work-by-tech-spec/</link>
      <guid>https://blog.banksalad.com/tech/we-work-by-tech-spec/</guid>
      <pubDate>Mon, 26 Oct 2020 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>진정한 실험 조직의 탄생</title>
      <link>https://blog.banksalad.com/tech/birth-of-a-genuine-experiment-organization/</link>
      <guid>https://blog.banksalad.com/tech/birth-of-a-genuine-experiment-organization/</guid>
      <pubDate>Wed, 07 Oct 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요 뱅크샐러드 데이터 파운데이션의 실험 플랫폼 팀 Product Manager…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드는 어떻게 레거시 서비스를 박살 내는가</title>
      <link>https://blog.banksalad.com/tech/how-banksalald-decomposes-legacy-services/</link>
      <guid>https://blog.banksalad.com/tech/how-banksalald-decomposes-legacy-services/</guid>
      <pubDate>Mon, 21 Sep 2020 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드는 앱 출시 이후 약…</content:encoded>
    </item>
    <item>
      <title>신규 서비스 배포 전에 실험과 개선을 반복한 이야기</title>
      <link>http://thefarmersfront.github.io/blog/vsms-performance-experiment/</link>
      <guid>http://thefarmersfront.github.io/blog/vsms-performance-experiment/</guid>
      <pubDate>Mon, 07 Sep 2020 00:00:00 GMT</pubDate>
      <content:encoded>성능 테스트로 데드락을 찾아 없애고 TPS를 끌어올리자!</content:encoded>
    </item>
    <item>
      <title>컬리에서 선물하기를 개발하며 회고</title>
      <link>http://thefarmersfront.github.io/blog/gift-order-development/</link>
      <guid>http://thefarmersfront.github.io/blog/gift-order-development/</guid>
      <pubDate>Thu, 03 Sep 2020 00:00:00 GMT</pubDate>
      <content:encoded>주문서비스 개발팀이 선물하기 서비스를 오픈하기 까지 여정</content:encoded>
    </item>
    <item>
      <title>Banksalad Product Language를 소개합니다</title>
      <link>https://blog.banksalad.com/tech/banksalad-product-language-ios/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-product-language-ios/</guid>
      <pubDate>Fri, 14 Aug 2020 00:00:00 GMT</pubDate>
      <content:encoded>Banksalad Product Language를 소개합니다 안녕하세요 뱅크샐러드의 iOS개발자 류성두입니다. 오늘은 뱅크샐러드가 UI를 디자인하고 구현하는 방식의 큰 기둥인 BPL, 즉 Banksalad Product Language…</content:encoded>
    </item>
    <item>
      <title>React 이해하기</title>
      <link>http://thefarmersfront.github.io/blog/thinking-in-react/</link>
      <guid>http://thefarmersfront.github.io/blog/thinking-in-react/</guid>
      <pubDate>Fri, 10 Jul 2020 00:00:00 GMT</pubDate>
      <content:encoded>React의 기본 개념에 대해 알아봅시다</content:encoded>
    </item>
    <item>
      <title>JPA 덕분에 DB에서 삽질한 이야기</title>
      <link>http://thefarmersfront.github.io/blog/jpa-uuid-sapjil/</link>
      <guid>http://thefarmersfront.github.io/blog/jpa-uuid-sapjil/</guid>
      <pubDate>Mon, 06 Jul 2020 00:00:00 GMT</pubDate>
      <content:encoded>DB에 저장을 했는데, 조회가 안 돼요</content:encoded>
    </item>
    <item>
      <title>Lambda Calculus에 대해 알아보자</title>
      <link>http://thefarmersfront.github.io/blog/lambda-calculus-1/</link>
      <guid>http://thefarmersfront.github.io/blog/lambda-calculus-1/</guid>
      <pubDate>Wed, 17 Jun 2020 00:00:00 GMT</pubDate>
      <content:encoded>그만 알아보자</content:encoded>
    </item>
    <item>
      <title>코드 악취를 맡는 후각 훈련의 시간</title>
      <link>http://thefarmersfront.github.io/blog/rms-refactoring/</link>
      <guid>http://thefarmersfront.github.io/blog/rms-refactoring/</guid>
      <pubDate>Tue, 09 Jun 2020 00:00:00 GMT</pubDate>
      <content:encoded>한 달, 각 잡고 리팩토링하기 좋은 시간, 그 기록의 공유</content:encoded>
    </item>
    <item>
      <title>Database Driven Development에서 진짜 DDD로의 선회, 이벤트 스토밍 -2-</title>
      <link>http://thefarmersfront.github.io/blog/event-storming/</link>
      <guid>http://thefarmersfront.github.io/blog/event-storming/</guid>
      <pubDate>Fri, 22 May 2020 00:00:00 GMT</pubDate>
      <content:encoded>DDD를 위한 첫걸음. Event Storming</content:encoded>
    </item>
    <item>
      <title>PHP Development Roadmap In 2020 At Kurly</title>
      <link>http://thefarmersfront.github.io/blog/cpd-developer-roadmap-2020/</link>
      <guid>http://thefarmersfront.github.io/blog/cpd-developer-roadmap-2020/</guid>
      <pubDate>Wed, 06 May 2020 00:00:00 GMT</pubDate>
      <content:encoded>2020 컬리 PHP 개발자 로드맵</content:encoded>
    </item>
    <item>
      <title>마켓컬리에서의 PHP 이야기</title>
      <link>http://thefarmersfront.github.io/blog/php-development-in-kurly/</link>
      <guid>http://thefarmersfront.github.io/blog/php-development-in-kurly/</guid>
      <pubDate>Thu, 30 Apr 2020 10:00:00 GMT</pubDate>
      <content:encoded>마켓컬리에서의 PHP 이야기</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 데이터 조직과 플래닝샵 떠나기</title>
      <link>https://blog.banksalad.com/tech/data-foundation-planningshop/</link>
      <guid>https://blog.banksalad.com/tech/data-foundation-planningshop/</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요 뱅크샐러드 데이터 파운데이션의 Product Manager…</content:encoded>
    </item>
    <item>
      <title>React밖에 모르는 당신에게. GatsbyJS한 잔, &apos;채용~&apos;</title>
      <link>https://blog.banksalad.com/tech/build-a-website-with-gatsby/</link>
      <guid>https://blog.banksalad.com/tech/build-a-website-with-gatsby/</guid>
      <pubDate>Sun, 05 Apr 2020 00:00:00 GMT</pubDate>
      <content:encoded>혹시 다들 뱅크샐러드 채용 사이트 보셨나요? 🙋🏻‍♂️ 안녕하세요, 이번에 뱅크샐러드 채용 사이트와 기술 블로그를 새롭게 개발한 Web Engineer…</content:encoded>
    </item>
    <item>
      <title>집에서 일했을 뿐인데 생산성이 폭발했다</title>
      <link>https://blog.banksalad.com/pnc/work-from-home/</link>
      <guid>https://blog.banksalad.com/pnc/work-from-home/</guid>
      <pubDate>Sat, 14 Mar 2020 00:00:00 GMT</pubDate>
      <content:encoded>전면 재택근무 하는 오늘 일어나 이 닦고 세수한 후 출근까지 걸린 시간…</content:encoded>
    </item>
    <item>
      <title>매출 손실을 줄여주는 외부링크 관제 Bot, &apos;URL Checker&apos; 개발기</title>
      <link>https://blog.banksalad.com/tech/url-status-checker/</link>
      <guid>https://blog.banksalad.com/tech/url-status-checker/</guid>
      <pubDate>Fri, 13 Mar 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 Engineering Foundation의 Front End Team 소속 Web Engineer…</content:encoded>
    </item>
    <item>
      <title>하루에 1000번 배포하는 조직 되기</title>
      <link>https://blog.banksalad.com/tech/become-an-organization-that-deploys-1000-times-a-day/</link>
      <guid>https://blog.banksalad.com/tech/become-an-organization-that-deploys-1000-times-a-day/</guid>
      <pubDate>Sat, 29 Feb 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 Engineering Foundation의 Framework Team 소속 Server Engineer…</content:encoded>
    </item>
    <item>
      <title>프로덕션 환경에서 사용하는 golang과 gRPC</title>
      <link>https://blog.banksalad.com/tech/production-ready-grpc-in-golang/</link>
      <guid>https://blog.banksalad.com/tech/production-ready-grpc-in-golang/</guid>
      <pubDate>Thu, 27 Feb 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 엔지니어링 파운데이션의 정겨울입니다. 뱅크샐러드는 마이크로 서비스 환경에서 다양한 언어와 프로토콜을 활용해 서비스를 운영하고 있습니다. 하나의 서비스는 요청을 처리하기 위해 다른 서비스의 API…</content:encoded>
    </item>
    <item>
      <title>폐쇄망 환경의 배포 시스템 개발기</title>
      <link>https://blog.banksalad.com/tech/how-we-have-built-alice/</link>
      <guid>https://blog.banksalad.com/tech/how-we-have-built-alice/</guid>
      <pubDate>Tue, 25 Feb 2020 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요. 뱅크샐러드, Advanced Financial Infra Team(이하 AFIT) 서지원 입니다. AFIT 은 최고의 개발환경을 구축한다 는 Vision 아래, IDC 에 Nutanix 를 올린 Private Cloud…</content:encoded>
    </item>
    <item>
      <title>테스트 코드, 안드로이드에서는 어떻게 작성해야 할까?</title>
      <link>https://blog.banksalad.com/tech/test-in-banksalad-android/</link>
      <guid>https://blog.banksalad.com/tech/test-in-banksalad-android/</guid>
      <pubDate>Thu, 20 Feb 2020 00:00:00 GMT</pubDate>
      <content:encoded>…</content:encoded>
    </item>
    <item>
      <title>AWS re:Invent 2019 견문록</title>
      <link>https://blog.banksalad.com/tech/aws-reinvent-2019/</link>
      <guid>https://blog.banksalad.com/tech/aws-reinvent-2019/</guid>
      <pubDate>Tue, 10 Dec 2019 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드의 서버 엔지니어 1명과 데이터 엔지니어 1명이 한국시간으로 2019년 11월 29일 금요일부터 2019년 12월 7일 토요일까지 미국 라스베가스에서 열린 AWS re:Invent 201…</content:encoded>
    </item>
    <item>
      <title>PyCon KR 2019 뱅크샐러드 돌아보기</title>
      <link>https://blog.banksalad.com/tech/pycon19/</link>
      <guid>https://blog.banksalad.com/tech/pycon19/</guid>
      <pubDate>Mon, 26 Aug 2019 00:00:00 GMT</pubDate>
      <content:encoded>뱅크샐러드가 작년에 다이아몬드 후원사로 참여한 데 이어 올해 [파이콘 한국 2019]에도 키스톤 후원사로 참여했습니다. 이번 글을 통해 파이콘에 참여한 뱅크샐러드의 이모저모를 돌아보려 합니다.  Talks…</content:encoded>
    </item>
    <item>
      <title>뱅크샐러드 at Apple Store</title>
      <link>https://blog.banksalad.com/tech/banksalad-at-apple-store/</link>
      <guid>https://blog.banksalad.com/tech/banksalad-at-apple-store/</guid>
      <pubDate>Fri, 28 Dec 2018 00:00:00 GMT</pubDate>
      <content:encoded>안녕하세요, 뱅크샐러드 iOS 엔지니어 김찬울입니다. 오늘은 iOS 개발자라면 누구나 듣고 싶어 할 내용이지만, 많은 분이 경험하지 못하셨을 경험을 공유해보고자 합니다. 기존에 뱅크샐러드 제품팀은 Apple로부터 몇 번 연락을 받아 Artwork…</content:encoded>
    </item>
    <item>
      <title>Typescript로 Local Storage 안전하게 사용하기</title>
      <link>https://blog.banksalad.com/tech/typescript-local-storage/</link>
      <guid>https://blog.banksalad.com/tech/typescript-local-storage/</guid>
      <pubDate>Thu, 03 Aug 2017 00:00:00 GMT</pubDate>
      <content:encoded>최근…</content:encoded>
    </item>
  </channel>
</rss>