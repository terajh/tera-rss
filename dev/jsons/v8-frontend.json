[
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "How we made JSON.stringify more than twice as fast",
    "partialText": "<p><code>JSON.stringify</code> is a core JavaScript function for serializing data. Its performance directly affects common operations across the web, from serializing data for a network request to saving data to <code>localStorage</code>. A faster <code>JSON.stringify</code> translates to quicker page interactions and more responsive applications. That’s why we’re excited to share that a recent engineering effort has made <code>JSON.stringify</code> in V8 <strong>more than twice as fast</strong>. This post breaks down the technical optimizations that made this improvement possible.</p>\n<h1 id=\"a-side-effect-free-fast-path\" tabindex=\"-1\">A Side-Effect-Free Fast Path <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#a-side-effect-free-fast-path\">#</a></h1>\n<p>The foundation of this optimization is a new fast path built on a simple premise: if we can guarantee that serializing an object will not trigger any side effects, we can use a much faster, specialized implementation. A &quot;side effect&quot; in this context is anything that breaks the simple, streamlined traversal of an object.<br />\nThis includes not only the obvious cases like executing user-defined code during serialization, but also more subtle internal operations that might trigger a garbage collection cycle. For more details on what exactly can cause side effects and how you can avoid them, see <a href=\"https://v8.dev/blog/json-stringify#limitations\">Limitations</a>.</p>\n<p>As long as V8 can determine that serialization will be free from these effects, it can stay on this highly-optimized path. This allows it to bypass many expensive checks and defensive logic required by the general-purpose serializer, resulting in a significant speedup for the most common types of JavaScript objects that represent plain data.</p>\n<p>Furthermore, the new fast path is iterative, in contrast to the recursive general-purpose serializer. This architectural choice not only eliminates the need for stack overflow checks and allows us to quickly resume after <a href=\"https://v8.dev/blog/json-stringify#handling-different-string-representations\">encoding changes</a>, but also allows developers to serialize significantly deeper nested object graphs than was previously possible.</p>\n<h1 id=\"handling-different-string-representations\" tabindex=\"-1\">Handling different String Representations <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#handling-different-string-representations\">#</a></h1>\n<p>Strings in V8 can be represented with either one-byte or two-byte characters. If a string contains only ASCII characters, they are stored as a one-byte string in V8 that uses 1 byte per character. However if a string contains just a single character outside of the ASCII range, all characters of the string use a 2 byte representation, essentially doubling the memory utilization.</p>\n<p>To avoid the constant branching and type checks of a unified implementation, the entire stringifier is now templatized on the character type. This means we compile two distinct, specialized versions of the serializer: one completely optimized for one-byte strings and another for two-byte strings. This has an impact on binary size, but we think the increased performance is definitely worth it.</p>\n<p>The implementation handles mixed encodings efficiently. During serialization, we must already inspect each string's instance type to detect representations we can’t handle on the fast path (like <a href=\"https://crsrc.org/c/v8/src/objects/string.h;drc=9768251f3e8f598d82420259a940d2057ed56b42;l=1013\"><code>ConsString</code></a>, which might trigger a GC during flattening) that require a fallback to the slow path. This necessary check also reveals whether a string uses one-byte or two-byte encoding.</p>\n<p>Because of this, the decision to switch from our optimistic one-byte stringifier to the two-byte version is essentially free. When this existing check reveals a two-byte string, a new two-byte stringifier is created, inheriting the current state. At the end, the final result is constructed by simply concatenating the output from the initial one-byte stringifier with the output from the two-byte one. This strategy ensures we stay on a highly-optimized path for the common case, while the transition to handling two-byte characters is lightweight and efficient.</p>\n<h1 id=\"optimizing-string-serialization-with-simd\" tabindex=\"-1\">Optimizing String Serialization with SIMD <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#optimizing-string-serialization-with-simd\">#</a></h1>\n<p>Any string in JavaScript can contain characters that require escaping when serializing to JSON (e.g. <code>&quot;</code> or <code>\\</code>). A traditional character-by-character loop to find them is slow.</p>\n<p>To accelerate this, we employ a two-level strategy based on the string's length:</p>\n<ul>\n<li>For longer strings, we switch to dedicated hardware SIMD instructions (e.g., ARM64 Neon). This allows us to load a much larger chunk of the string into a wide SIMD register and check multiple bytes for any escapable characters at once in just a few instructions. (<a href=\"https://crsrc.org/c/v8/src/json/json-stringifier.cc;drc=1645281bbd1b183a252835d376166bd210135bbe;l=3369\">source</a>)</li>\n<li>For shorter strings, where the setup cost of hardware instructions would be too high, we use a technique called SWAR (SIMD Within A Register). This approach uses clever bitwise logic on standard general-purpose registers to process multiple characters at once with very low overhead. (<a href=\"https://crsrc.org/c/v8/src/json/json-stringifier.cc;drc=1645281bbd1b183a252835d376166bd210135bbe;l=3353\">source</a>)</li>\n</ul>\n<p>Regardless of the method, the process is highly efficient: we rapidly scan through the string chunk by chunk. If no chunk contains any special characters (the common case), we can simply copy the whole string.</p>\n<h1 id=\"the-express-lane-on-the-fast-path\" tabindex=\"-1\">The Express Lane on the Fast Path <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#the-express-lane-on-the-fast-path\">#</a></h1>\n<p>Even within the main fast path, we found an opportunity for another, even faster 'express lane'. By default, the fast path must still iterate over an object's properties and, for each key, perform a series of checks: confirm the key is not a <code>Symbol</code>, ensure it's enumerable, and finally, scan the string for characters that require escaping (e.g. <code>&quot;</code> or <code>\\</code>).</p>\n<p>To eliminate this, we introduce a flag on an object's <a href=\"https://v8.dev/docs/hidden-classes\">hidden class</a>. Once we have serialized all properties of an object, we mark its hidden class as fast-json-iterable if no property key is a <code>Symbol</code>, all properties are enumerable, and no property key contains characters that require escaping.</p>\n<p>When we serialize an object that has the same hidden class as an object we serialized before (which is quite common, e.g. an array of objects which all have the same shape) and it is fast-json-iterable, we can simply copy all the keys to the string buffer without any further checks.</p>\n<p>We also added this optimization to <code>JSON.parse</code>, where we can utilize it for fast key comparisons while parsing an array, assuming that objects in the array often have the same hidden classes.</p>\n<h1 id=\"a-faster-double-to-string-algorithm\" tabindex=\"-1\">A faster double-to-string algorithm <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#a-faster-double-to-string-algorithm\">#</a></h1>\n<p>Converting numbers to their string representation is a surprisingly complex and performance-critical task. As part of our work on <code>JSON.stringify</code>, we identified an opportunity to significantly speed up this process by upgrading our core <code>DoubleToString</code> algorithm. We have now replaced the long-serving Grisu3 algorithm with <a href=\"https://github.com/jk-jeon/dragonbox\">Dragonbox</a> for shortest length number to string conversions.</p>\n<p>While this optimization was driven by our <code>JSON.stringify</code> profiling, the new Dragonbox implementation benefits <strong>all</strong> calls to <code>Number.prototype.toString()</code> throughout V8. This means any code that converts numbers to strings, not just JSON serialization, will see this performance boost for free.</p>\n<h1 id=\"optimizing-the-underlying-temporary-buffer\" tabindex=\"-1\">Optimizing the underlying temporary buffer <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#optimizing-the-underlying-temporary-buffer\">#</a></h1>\n<p>A significant source of overhead in any string-building operation is how memory is managed. Previously, our stringifier built the output in a single, contiguous buffer on the C++ heap. While simple, this approach has a significant drawback: whenever the buffer ran out of space, we had to allocate a larger one and copy the entire existing content over. For large JSON objects, this cycle of re-allocation and copying created major performance overhead.</p>\n<p>The crucial insight was that forcing this temporary buffer to be contiguous offered no real benefit, as the final result is assembled into a single string only at the very end.</p>\n<p>With this in mind, we replaced the old system with a segmented buffer. Instead of one large, growing block of memory, we now use a list of smaller buffers (or &quot;segments&quot;), allocated in V8's Zone memory. When a segment is full, we simply allocate a new one and continue writing there, completely eliminating the expensive copy operations.</p>\n<h1 id=\"limitations\" tabindex=\"-1\">Limitations <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#limitations\">#</a></h1>\n<p>The new fast path achieves its speed by specializing for common, simple cases. If the data being serialized doesn't meet these criteria, V8 falls back to the general-purpose serializer to ensure correctness. To get the full performance benefit, the <code>JSON.stringify</code> call must adhere to the following conditions.</p>\n<ul>\n<li>No <code>replacer</code> or <code>space</code> arguments: Providing a <code>replacer</code> function or a <code>space</code>/<code>gap</code> argument for pretty-printing are features handled exclusively by the general-purpose path. The fast path is designed for compact, non-transformed serialization.</li>\n<li>Plain data objects and arrays: The objects being serialized should be simple data containers. This means they, and their prototypes, must not have a custom <code>.toJSON()</code> method. The fast path assumes standard prototypes (like <code>Object.prototype</code> or <code>Array.prototype</code>) that don't have custom serialization logic.</li>\n<li>No indexed properties on objects: The fast path is optimized for objects with regular, string-based keys. If an object contains array-like indexed properties (e.g., <code>'0', '1', ...</code>), it will be handled by the slower, more general serializer.</li>\n<li>Simple string types: Some internal V8 string representations (like <code>ConsString</code>) can require memory allocation to be flattened before they can be serialized. The fast path avoids any operation that might trigger such allocations and works best with simple, sequential strings. This is something that’s hard to influence as a web developer. But don’t worry, it should just work in most cases.</li>\n</ul>\n<p>For the vast majority of use cases, such as serializing data for API responses or caching configuration objects, these conditions are naturally met, allowing developers to benefit from the performance improvements automatically.</p>\n<h1 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"bookmark\" href=\"https://v8.dev/blog/json-stringify#conclusion\">#</a></h1>\n<p>By rethinking <code>JSON.stringify</code> from the ground up, from its high-level logic down to its core memory and character-handling operations, we've delivered a more than 2x performance improvement measured on the JetStream2 json-stringify-inspector benchmark. See the figure below for results on different platforms. These optimizations are available in V8 starting with version 13.8 (Chrome 138).</p>\n<figure><img src=\"https://v8.dev/_img/json-stringify/results-jetstream2.svg\" alt=\"\" width=\"600\" height=\"371\" loading=\"lazy\" /><figcaption>JetStream2 Results</figcaption></figure>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/json-stringify"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Speculative Optimizations for WebAssembly using Deopts and Inlining",
    "partialText": "<p>In this blog post, we explain two optimizations for WebAssembly that we recently implemented in V8 and that shipped with Google Chrome M137, namely <em>speculative call_indirect inlining</em> and <em>deoptimization</em> support for WebAssembly. In combination, they allow us to generate better machine code by making assumptions based on runtime feedback. This speeds up WebAssembly execution, in particular for <a href=\"https://v8.dev/blog/wasm-gc-porting\">WasmGC</a> programs. On a set of Dart microbenchmarks for example, the speedup by the combination of both optimizations is more than 50% on average, and on larger, realistic applications and benchmarks shown below the speedup is between 1% and 8%. Deoptimizations are also an important building block for further optimizations in the future.</p>\n<h1 id=\"background\" tabindex=\"-1\">Background <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#background\">#</a></h1>\n<p>Fast execution of JavaScript relies heavily on <a href=\"https://archive.is/PUmGn\"><em>speculative optimizations</em></a>. That is, JIT-compilers make assumptions when generating machine code based on feedback that was collected during earlier executions. For example, given the expression <code>a + b</code>, the compiler can generate machine code for an integer addition if past feedback indicates that <code>a</code> and <code>b</code> are integers (and not strings, floating point numbers, or other objects). Without making such assumptions, the compiler would have to emit generic code that handles the <a href=\"https://tc39.es/ecma262/multipage/ecmascript-language-expressions.html#sec-applystringornumericbinaryoperator\">full behavior of the + operator in JavaScript</a>, which is complex and thus much slower. If the program later behaves differently and thus violates assumptions made when generating the optimized code, V8 performs a <em>deoptimization</em> (or deopt, for short). That means throwing away the optimized code and continuing execution in unoptimized code (and collecting more feedback to possibly tier-up again later).</p>\n<p>In contrast to JavaScript, fast execution of WebAssembly hasn’t required speculative optimizations and deopts. One reason is that WebAssembly programs can already be optimized quite well because more information is statically available as e.g., functions, instructions, and variables are all statically typed. Another reason is that WebAssembly binaries are often compiled from C, C++, or Rust. These source languages are also more amenable to static analysis than JavaScript, and thus toolchains such as <a href=\"https://emscripten.org/index.html\">Emscripten</a> (based on LLVM) or <a href=\"https://github.com/WebAssembly/binaryen\">Binaryen</a> can already optimize the program ahead-of-time. This results in fairly well-optimized binaries, at least when targeting WebAssembly 1.0, which <a href=\"https://webassembly.org/features/\">launched in 2017</a>.</p>\n<h1 id=\"motivation\" tabindex=\"-1\">Motivation <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#motivation\">#</a></h1>\n<p>Then why do we employ more speculative optimizations for WebAssembly in V8 now? One reason is that <a href=\"https://v8.dev/blog/wasm-gc-porting\">WebAssembly has evolved</a> with the introduction of <a href=\"https://github.com/WebAssembly/gc\"><em>WasmGC</em>, the WebAssembly Garbage Collection proposal</a>. It better supports compiling “managed” languages such as <a href=\"https://j2cl.io/\">Java</a>, <a href=\"https://kotlinlang.org/docs/wasm-overview.html\">Kotlin</a>, or <a href=\"https://docs.flutter.dev/platform-integration/web/wasm\">Dart</a> to WebAssembly. The resulting WasmGC bytecode is more high-level than Wasm 1.0, e.g., it supports rich types, such as structs and arrays, subtyping, and operations on such types. The generated machine code for WasmGC thus benefits more from speculative optimizations.</p>\n<p>One particularly important optimization is <em>inlining</em>, that is, replacing a call instruction with the body of the callee function. Not only does this get rid of the administrative overhead associated with the call itself (which might be higher than the actual work for very small functions), but it also enables many other, subsequent optimizations to “see through” the function call, even if those optimizations are not inter-procedural. No wonder inlining was already recognized in 1971 as one of the most important optimizations in <a href=\"https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf\">Frances Allen’s seminal “Catalogue of Optimizing Transformations”</a>.</p>\n<p>One complication for inlining are <em>indirect calls</em>, i.e., calls where the callee is only known at runtime and can be one of many potential targets. This is particularly true for the languages that compile to WasmGC. Consider Java or Kotlin where methods are <code>virtual</code> by default, whereas in C++ one has to opt-in by marking them explicitly. If there is not a single statically known callee, inlining is obviously not as straightforward.</p>\n<h1 id=\"speculative-inlining\" tabindex=\"-1\">Speculative Inlining <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#speculative-inlining\">#</a></h1>\n<p>This is where <em>speculative inlining</em> comes into play. In theory, indirect calls can target many different functions, but in practice, they still often go to a single target (called a <em>monomorphic</em> function call) or a few select cases (called <em>polymorphic</em>). We record those targets when executing the unoptimized code, and then inline up to four target functions when generating the optimized code.</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/speculative-inlining-overview.svg\" alt=\"\" width=\"966\" height=\"416\" loading=\"lazy\" /><figcaption>Speculative inlining overview</figcaption></figure>\n<p>The figure above shows the high-level picture. We start at the top left, with the unoptimized code for function <code>func_a</code>, generated by <a href=\"https://v8.dev/blog/liftoff\">Liftoff</a>, our baseline compiler for WebAssembly. At each call site Liftoff also emits code to update the <em>feedback vector</em>. This metadata array exists once per function, and it contains one entry per call instruction in the given function. Each entry records the call targets and counts for this particular call site. The example feedback at the bottom of the figure shows a monomorphic entry for the <code>call_indirect</code> in <code>func_a</code>; here the call target was 1337 times <code>func_b</code>.</p>\n<p>When a function is hot enough to <em>tier-up</em> to <a href=\"https://v8.dev/docs/turbofan\">TurboFan</a>, i.e., gets compiled with our optimizing compiler, we come to the second step. TurboFan reads the corresponding feedback vector and decides whether and which targets to inline at each call site. Whether it is worthwhile inlining one or multiple callees depends on a <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/inlining-tree.h\">variety of heuristics</a>. E.g., large functions are never inlined, tiny functions are almost always inlined, and generally there is a maximum <em>inlining budget</em> after which no more inlining into a function happens, as that also has costs in terms of compile time and generated machine code size. As in many places in compilers, particularly multi-tier JITs, these trade-offs are quite complex and get tuned over time. In this example, TurboFan decides to inline <code>func_b</code> into <code>func_a</code>.</p>\n<p>On the upper right hand side of the figure, we see the result of speculative inlining in the generated optimized code. Instead of an indirect call, the code first checks if the target at runtime matches what we have assumed during compilation. If that’s the case, we continue executing the inlined body of the corresponding function. Subsequent optimizations can also transform the inlined code further, given the now available surrounding context. E.g., <a href=\"https://en.wikipedia.org/wiki/Constant_folding\">constant propagation and constant folding</a> could specialize the code to this particular call site or <a href=\"https://en.wikipedia.org/wiki/Value_numbering\">GVN</a> could hoist out repeated computations. In the case of polymorphic feedback, TurboFan can also emit a series of target checks and inlined bodies, not just one as in this example.</p>\n<h2 id=\"technical-deep-dive\" tabindex=\"-1\">Technical Deep Dive <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#technical-deep-dive\">#</a></h2>\n<p>So much for the high-level picture. For readers interested in the implementation, we look at some more details and concrete code in this section.</p>\n<p>In the figure above, the feedback vector is only shown conceptually as an array of entries and only one kind of entry is shown. Below, we see that each entry can go through four stages over the course of the execution: Initially, all entries are <em>uninitialized</em> (all call counts are zero), potentially transitioning to <em>monomorphic</em> (a single call target was recorded), <em>polymorphic</em> (up to four call targets), and finally <em>megamorphic</em> (more than four targets, where we don’t inline any more and thus don’t need to record call counts either). Each entry is actually a <em>pair</em> of objects such that the most common monomorphic case can store both the call count and the target inline in the vector, i.e., without an additional allocation. For the polymorphic case, feedback is stored in an out-of-line array, as shown below. The <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/wasm.tq;drc=a7b14a0d53b39fb9e651fc02ea68633aa8cea51b;l=697\">builtin</a> that updates a feedback entry (corresponding to <code>update_feedback()</code> in the first figure) is written in <a href=\"https://v8.dev/docs/torque-builtins\">Torque</a>. (It’s quite easy to read, give it a try!) It first checks for monomorphic or polymorphic “hits”, where just the count has to be incremented. This is again because they are the most common cases and thus performance-sensitive. The feedback vector and its entries are JavaScript objects (e.g., the call counts are <a href=\"https://v8.dev/blog/pointer-compression#value-tagging-in-v8\">Smis</a>), so they live on the managed heap. As such, it is part of the <a href=\"https://v8.dev/blog/sandbox\">V8 sandbox</a> and is automatically cleaned up if the corresponding Wasm instance (see below) is no longer reachable.</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/speculative-inlining-feedback-vector.svg\" alt=\"\" width=\"954\" height=\"283\" loading=\"lazy\" /><figcaption>Details of the feedback vector</figcaption></figure>\n<p>Next, let’s look at the effect of inlining on an actual WebAssembly program. The <code>example</code> function below does 200M indirect calls in a loop to a single target <code>inlinee</code> that contains an addition. Obviously, this is a somewhat simplified microbenchmark, but it demonstrates the benefits of speculative inlining well.</p>\n<pre class=\"language-wasm\"><code class=\"language-wasm\"><span class=\"token punctuation\">(</span><span class=\"token keyword\">func</span> <span class=\"token variable\">$example</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">param</span> <span class=\"token variable\">$iterations</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">result</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span><br />  <span class=\"token punctuation\">(</span><span class=\"token keyword\">local</span> <span class=\"token variable\">$sum</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span><br />  <span class=\"token keyword\">block</span> $<span class=\"token keyword\">block</span><br />    <span class=\"token keyword\">loop</span> $<span class=\"token keyword\">loop</span><br />      <span class=\"token keyword\">local</span>.get <span class=\"token variable\">$iterations</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>eqz</span><br />      <span class=\"token keyword\">br_if</span> $<span class=\"token keyword\">block</span> <span class=\"token comment\">;; terminate loop</span><br />      <span class=\"token keyword\">local</span>.get <span class=\"token variable\">$iterations</span> <span class=\"token comment\">;; update loop counter</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>const</span> <span class=\"token number\">1</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>sub</span><br />      <span class=\"token keyword\">local</span>.set <span class=\"token variable\">$iterations</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>const</span> <span class=\"token number\">7</span> <span class=\"token comment\">;; argument for the function call</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>const</span> <span class=\"token number\">1</span> <span class=\"token comment\">;; table index, refering to $inlinee</span><br />      <span class=\"token keyword\">call_indirect</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">param</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">result</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span><br />      <span class=\"token keyword\">local</span>.get <span class=\"token variable\">$sum</span><br />      <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>add</span><br />      <span class=\"token keyword\">local</span>.set <span class=\"token variable\">$sum</span><br />      <span class=\"token keyword\">br</span> $<span class=\"token keyword\">loop</span> <span class=\"token comment\">;; repeat</span><br />    <span class=\"token keyword\">end</span><br />  <span class=\"token keyword\">end</span><br />  <span class=\"token keyword\">local</span>.get <span class=\"token variable\">$sum</span><br /><span class=\"token punctuation\">)</span><br />...<br /><span class=\"token punctuation\">(</span><span class=\"token keyword\">func</span> <span class=\"token variable\">$inlinee</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">param</span> <span class=\"token variable\">$x</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">result</span> <span class=\"token keyword\">i32</span><span class=\"token punctuation\">)</span><br />  <span class=\"token keyword\">local</span>.get <span class=\"token variable\">$x</span><br />  <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>const</span> <span class=\"token number\">37</span><br />  <span class=\"token keyword\">i32<span class=\"token punctuation\">.</span>add</span><br /><span class=\"token punctuation\">)</span></code></pre>\n<p>For readers not familiar with the WebAssembly text format, here is a rough C equivalent of the above program:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">int</span> <span class=\"token function\">inlinee</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> x <span class=\"token operator\">+</span> <span class=\"token number\">37</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">int</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>func_ptr<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> inlinee<span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">int</span> <span class=\"token function\">example</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> iterations<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> sum <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>iterations <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    iterations<span class=\"token operator\">--</span><span class=\"token punctuation\">;</span><br />    sum <span class=\"token operator\">+=</span> <span class=\"token function\">func_ptr</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br />  <span class=\"token keyword\">return</span> sum<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The next figure shows excerpts of <a href=\"https://v8.dev/blog/leaving-the-sea-of-nodes\">TurboFan’s intermediate representation</a> for the <code>example</code> function, visualized with <a href=\"https://v8.github.io/tools/head/turbolizer/\">Turbolizer</a>: Speculative inlining and Wasm deopts are enabled on the right, and disabled on the left. In both versions, we have to check whether the table index argument to the <code>call_indirect</code> instruction is in-bounds, as per <a href=\"https://webassembly.github.io/spec/core/exec/instructions.html#xref-syntax-instructions-syntax-instr-control-mathsf-call-indirect-x-y\">the WebAssembly semantics</a> (first red box in both cases). Without inlining, we also have to check whether the function at this index has the correct signature before actually calling it (second red box on the left). Finally, the first green box on the left is the indirect call, and the second green box is the addition of the result of said call. In the green box on the right, we see that after inlining and further optimizations, the call is completely gone and the addition in <code>inlinee</code> and the addition in <code>example</code> were constant-folded into a single addition with a constant. Altogether, on this particular microbenchmark, inlining, deopts, and subsequent optimizations speed up the program from around 675 ms to 90 ms execution time on an x64 workstation. In this case, the optimized machine code with inlining is even smaller than without (968 vs. 1572 bytes), although that certainly need not be.</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/example-turbolizer-inlining.svg\" alt=\"\" width=\"1132\" height=\"604\" loading=\"lazy\" /><figcaption>TurboFan IR without and with inlining</figcaption></figure>\n<p>Finally, we want to briefly explain the Wasm instance check and target check that the code with speculative inlining does on the right. Semantically, Wasm functions are closures over a Wasm instance (which “holds” the current state of globals, tables, imports from the host, etc.). Correctly inlining functions that belong to a different instance (e.g., which are called via an imported table) would hence require additional compiler machinery as well as solving a few obstacles in our general handling of generated code. Luckily, most calls are within a single instance anyway, so for the time being we check that the call target’s instance matches the current instance, which lets the compiler make the simplifying assumption that both instances are the same. If not, we deoptimize in block 8 (due to wrong instance) or block 6 (due to wrong target).</p>\n<p>This additional Wasm instance check was specifically introduced for the new <code>call_indirect</code> inlining. WebAssembly also has another kind of indirect call, <code>call_ref</code>, for which we already added inlining support when launching <a href=\"https://v8.dev/blog/wasm-gc-porting#optimizing-wasmgc\">our WasmGC implementation</a>. The fast path for <code>call_ref</code> inlining doesn’t require an explicit instance check, since the <code>WasmFuncRef</code> object that is the <code>call_ref</code> input already <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/wasm-objects.tq;drc=736622ed7d9cf605750afa417b3f4e681eef686c;l=80\">includes the instance</a> the function closes over, so comparing the target for equality subsumes both checks.</p>\n<p>With the new <code>call_indirect</code> inlining, V8 now supports inlining Wasm-to-Wasm calls for all types of call instructions: direct <code>call</code>s, <code>call_ref</code>, <code>call_indirect</code>, and their respective tail-call variants <code>return_call</code>, <code>return_call_ref</code>, and <code>return_call_indirect</code>.</p>\n<h1 id=\"deoptimization\" tabindex=\"-1\">Deoptimization <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#deoptimization\">#</a></h1>\n<p>So far, we have focused on inlining and how that improves the optimized code. But what happens if we <em>cannot</em> stay on a fast path, i.e., if one of the assumptions made during optimization turns out to be false at runtime? This is where deopts come into play.</p>\n<p>The very first figure of this post already shows the high-level idea of a deopt: We cannot continue execution in the optimized code because it has made some assumptions that are now invalidated. So instead, we “go back” to the unoptimized baseline code. Crucially, this transition to unoptimized code happens <em>in the middle of executing the current function</em>, i.e., when the optimized code has already performed operations with side-effects (say, called the underlying operating system), which we cannot undo, and while it is holding intermediate values in registers and on the stack. So a deopt cannot just jump to the beginning of the unoptimized function, but instead does something much more interesting:</p>\n<ol>\n<li>\n<p>First, we <strong>save the current program state</strong>. We do this by calling into the runtime from optimized code. The <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/deoptimizer/deoptimizer.h\"><code>Deoptimizer</code></a> then serializes the current program state into an internal data structure, the <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/deoptimizer/frame-description.h\"><code>FrameDescription</code></a>. This entails reading out CPU registers and inspecting the stack frame of the function to be deoptimized.</p>\n</li>\n<li>\n<p>Next, we <strong>transform this state such that it fits the unoptimized code</strong>, that is, placing values in the correct registers and stack slots that the Liftoff-generated code expects at the deoptimization point. E.g., a value that TurboFan code has put in a register may now end up in a stack slot. The stack layout and expectations for the baseline code (its “calling convention” for the deopt point, if you will) are read out of metadata generated by Liftoff during compilation.</p>\n</li>\n<li>\n<p>And finally, we <strong>replace the optimized stack frame</strong> with the corresponding unoptimized stack frame(s) and <strong>jump to the middle of the baseline code</strong>, corresponding to the point in the optimized code where the deopt was triggered.</p>\n</li>\n</ol>\n<p>Obviously, this is quite complex machinery, which begs the question: Why do we go through the hassle of it all and not just generate a slow path in optimized code? Let’s compare the intermediate representation for the earlier example WebAssembly code, left without and right with deopts. The three red boxes (table bounds / Wasm instance / target check) are fundamentally the same. The difference is in the code for the slow path. Without deopts, we don’t have the option of returning to baseline code, so the optimized code must handle the full, generic behavior of an indirect call, as shown in the yellow box on the left. This unfortunately impedes further optimizations and thus results in slower execution.</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/example-turbolizer-deopts.svg\" alt=\"\" width=\"1233\" height=\"770\" loading=\"lazy\" /><figcaption>TurboFan IR without and with deopts</figcaption></figure>\n<p>There are two reasons why the non-deopt slow path impedes optimizations. First, it contains more operations, in particular a call, which we have to assume goes anywhere and has arbitrary side-effects. Second, note how the <code>Deoptimize</code> operation in block 8 on the right has no successor in the control-flow graph, whereas the yellow slow path on the left has a successor. In particular for loops, the deoptimization node/block will not produce data-flow facts (in the sense of data-flow analyses and optimizations, such as <a href=\"https://en.wikipedia.org/wiki/Live-variable_analysis\">live variables</a>, load elimination, or <a href=\"https://en.wikipedia.org/wiki/Escape_analysis\">escape analysis</a>) that propagate to the next iteration of the loop. In essence, the deopt point “just” terminates the execution of the function, without much effect on the surrounding code, which can be utilized nicely by subsequent optimizations.</p>\n<p>Finally, this also explains why the combination of speculative optimizations (e.g., inlining) and deoptimization is so useful: The first adds a fast path based on speculative assumptions, and deopts allow the compiler to not worry much about the cases where the assumptions turn out to be false. Concretely, for the earlier microbenchmark with 200M indirect calls, performing just speculative inlining without deopts speeds up the program “only” to about 180 ms, compared to 90 ms with both inlining and deopts (and 675 ms without either).</p>\n<h2 id=\"technical-deep-dive-1\" tabindex=\"-1\">Technical Deep Dive <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#technical-deep-dive-1\">#</a></h2>\n<p>For the interested reader, we now again look at a concrete example and technical details, this time for when we deopt.</p>\n<p>Let's assume we execute the optimized code from above but the function stored at table index 1 has changed in the meantime. The table bounds check and Wasm instance check will pass, but the inlined target will be different from the one in the table, so we need to deoptimize the program in its current state. For that the code contains so-called <em>deoptimization exits</em>. The target check conditionally jumps to such an exit, which itself is a call to the <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc;l=4831;drc=6ec26945cdb9064a6c6835a7c638092f344e6a89\"><code>DeoptimizationEntry</code></a> builtin. The builtin first saves all register values by spilling them to the stack<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fn1\" id=\"fnref1\">[1]</a></sup>. Then it allocates the C++ <code>Deoptimizer</code> object and its <code>FrameDescription</code> input object. The builtin copies the spilled registers and all other stack slots from the optimized frame into the <code>FrameDescription</code> on the heap, and pops these values from the stack in the process. (Note that the execution is still in the builtin while it has already removed its own return address from the stack and started unwinding the calling frame!) Then the builtin computes the output frames. To do that, the deoptimizer loads the <code>DeoptimizationData</code> for the optimized frame, extracts the information for the deopt point, and recompiles each inlined function at this call site with Liftoff. Due to nested inlining there can be more than one inlined function, and with inlined tail calls the optimized function to which the optimized stack frame nominally belonged might not even be part of the unoptimized stack frames to be constructed.<br />\nWhile compiling, Liftoff calculates the expected stack frame layout and the deoptimizer transforms the optimized frame description into the desired layouts reported by Liftoff. It returns to the builtin which then reads these output <code>FrameDescription</code> objects and pushes their values onto the stack. Finally the builtin fills in the registers from the top output <code>FrameDescription</code>.</p>\n<p>For the example above, our internal tracing with <code>--trace-deopt-verbose</code> shows the following:</p>\n<pre><code>[bailout (kind: deopt-eager, reason: wrong call target, type: Wasm): begin. deoptimizing example, function index 2, bytecode offset 134, deopt exit 0, FP to SP delta 32, pc 0x14886e50cbb4]\n  reading input for Liftoff frame =&gt; bailout_id=134, height=4, function_id=2 ; inputs:\n     0: 4 ; rdx (int32)\n     1: 0 ; rbx (int32)\n     2: (wasm int32 literal 7)\n     3: (wasm int32 literal 1)\n  Liftoff stack &amp; register state for function index 2, frame size 48, total frame size 64\n     0: i32:rax\n     1: i32:s0x28\n     2: i32:c7\n     3: i32:c1\n[bailout end. took 0.082 ms]\n</code></pre>\n<p>First we can see that a deopt is triggered because of a wrong call target in Wasm for the function <code>example</code>. The trace then shows information about the input (optimized) frame, which has four values: The <code>iterations</code> parameter of the function (value <code>4</code>, stored in register <code>rdx</code>), the <code>sum</code> local (<code>0</code> stored in <code>rbx</code>), the literal 7 and the literal 1, which are the two arguments to the <code>call_indirect</code> instruction. In this simple example, there is only one unoptimized output frame, so there is a 1:1 mapping between the two frames: The <code>iterations</code> value has to be stored in register <code>rax</code> while the <code>sum</code> value needs to end up in the stack slot <code>s0x28</code>. The two constants are also recognized by Liftoff as constants and don’t need to be transferred into stack slots or registers.<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fn2\" id=\"fnref2\">[2]</a></sup></p>\n<p>After these transformations have been done the builtin “returns” to the inner-most unoptimized frame which calls a final builtin to clean-up the <code>Deoptimizer</code> object and perform any needed allocations on the managed heap.<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fn3\" id=\"fnref3\">[3]</a></sup> Finally, execution continues in the unoptimized code, in this case executing the <code>call_indirect</code>, which will also directly record the new call target in its feedback vector, so that any later tier-up is aware of this new target.</p>\n<h1 id=\"results\" tabindex=\"-1\">Results <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#results\">#</a></h1>\n<p>Besides the technical description and examples, we also want to demonstrate the usefulness of <code>call_indirect</code> inlining and Wasm deopt support with some measurements.<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fn4\" id=\"fnref4\">[4]</a></sup></p>\n<p>We first look at a collection of <a href=\"https://github.com/mkustermann/wasm_gc_benchmarks/blob/main/micro-benchmarks/non_devirtualized_typed_data_access.dart\">Dart microbenchmarks</a> in the figure below. It compares three configurations against each other: All given numbers are speedups relative to V8 and Chrome’s behavior before <code>call_indirect</code> inlining and Wasm deopts (i.e., a speedup of 2x means the runtime is half of that of the baseline). The blue bars show the configuration with <code>call_indirect</code> inlining enabled but no Wasm deopts, i.e., where the optimized code contains a generic slow path. On several of these microbenchmarks this already yields (sometimes substantial) speedups.<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fn5\" id=\"fnref5\">[5]</a></sup> On average across all items, <code>call_indirect</code> inlining speeds up execution by 1.19x compared to the baseline without. Finally, the red bars show the configuration we actually ship, where both Wasm deopts and <code>call_indirect</code> inlining are enabled. With an average speedup of 1.59x over the baseline, this shows that in particular the combination of speculative optimizations and deoptimization support is highly beneficial.</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/results-microbenchmarks.png\" alt=\"\" width=\"1643\" height=\"979\" loading=\"lazy\" /></figure>\n<p>Naturally, microbenchmarks isolate and emphasize the effects of an optimization quite a bit. This is useful during development or to get a strong signal with noisy measurements. However, more realistic are results on larger applications and benchmarks, as shown in the following figure. To the very left, we see a 2% speedup in terms of runtime for <code>richards-wasm</code>, a workload from the <a href=\"https://github.com/WebKit/JetStream\">JetStream benchmark suite</a>. Next, we see a 1% speedup for a <a href=\"https://sqlite.org/wasm/doc/trunk/about.md\">Wasm build</a> of the widely-used SQLite 3 database, and 8% speedup for <a href=\"https://github.com/dart-lang/flute\">Dart Flute</a>, a WasmGC benchmark that emulates a UI workload similar to <a href=\"https://flutter.dev/\">Flutter</a>. The final two results are from an internal benchmark for the <a href=\"https://v8.dev/blog/wasm-gc-porting#v8-optimizations\">Google Sheets calc engine</a>, which is powered by WasmGC, with speedups due to deopts of up to 7% (only deopts matter here as this last application only uses <code>call_ref</code>s for runtime dispatch, i.e., it has no <code>call_indirect</code>s).</p>\n<figure><img src=\"https://v8.dev/_img/wasm-speculative-optimizations/results-applications.png\" alt=\"\" width=\"1300\" height=\"844\" loading=\"lazy\" /></figure>\n<h1 id=\"conclusion-and-outlook\" tabindex=\"-1\">Conclusion and Outlook <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#conclusion-and-outlook\">#</a></h1>\n<p>This concludes our post about two new optimizations in the V8 engine for WebAssembly. To summarize, we have seen:</p>\n<ul>\n<li>How speculative inlining can inline functions even in the presence of indirect calls,</li>\n<li>what feedback is and how it is used and updated,</li>\n<li>what to do when assumptions made during optimizations are invalid at runtime,</li>\n<li>how a deoptimization can exit optimized code and enters baseline code in the middle of executing a function, and finally</li>\n<li>how this significantly improves the execution of real-world workloads.</li>\n</ul>\n<p>In the future, we plan on adding more speculative optimizations based on deopt support for WebAssembly, e.g. bounds-check elimination or more extensive load elimination for WasmGC objects. And also in terms of inlining, there is more to be done: While we now have Wasm-into-Wasm inlining for all kinds of call instructions, we can still extend inlining across the language boundary, e.g., for JavaScript-to-Wasm calls. Check back on our V8 blog for exciting updates in the future!</p>\n<!-- Footnotes themselves at the bottom. -->\n<h2 id=\"footnotes\" tabindex=\"-1\">Footnotes <a class=\"bookmark\" href=\"https://v8.dev/blog/wasm-speculative-optimizations#footnotes\">#</a></h2>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Due to the SIMD extension for Wasm this also includes all 128-bit vector registers used by TurboFan. <a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>In other cases values might be constant in the optimized version after constant folding but need to be materialized into stack slots or registers for Liftoff, so the deoptimization data needs to store these constant values. <a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>During the deoptimization itself we cannot allocate on the heap since allocations can trigger the garbage collector (GC) and the stack is not in a state that the GC can inspect. (The GC needs to visit all heap-references on the stack and potentially update them when moving an object.) <a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>The measurements were taken on an x64 workstation; the figures show the median of N=21 repetitions. <a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fnref4\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>For the three <code>Matrix4Benchmark</code> items that slightly regress, enabling <code>call_indirect</code> inlining causes our heuristic to prefer 16 indirect call sites for inlining over other direct call sites. This leads to exhausting the inlining budget (i.e., stop inlining because the resulting code grows too large), so that fewer of the direct calls are inlined compared with before. In this particular case, the heuristic doesn’t predict perfectly how beneficial one inlining decision is over the other and leads to a suboptimal result. Improving this heuristics is interesting future work. <a href=\"https://v8.dev/blog/wasm-speculative-optimizations#fnref5\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/wasm-speculative-optimizations"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Giving V8 a Heads-Up: Faster JavaScript Startup with Explicit Compile Hints",
    "partialText": "<p>Getting JavaScript running fast is key for a responsive web app. Even with V8's advanced optimizations, parsing and compiling critical JavaScript during startup can still create performance bottlenecks. Knowing which JavaScript functions to compile during the initial script compilation can speed up web page loading.</p>\n<p>When processing a script loaded from the network, V8 has to choose for each function: either compile it immediately (&quot;eagerly&quot;) or defer this process. If a function that hasn't been compiled is later called, V8 must then compile it on demand.</p>\n<p>If a JavaScript function ends up being called during page load, compiling it eagerly is beneficial, because:</p>\n<ul>\n<li>During the initial processing of the script, we need to do at least a lightweight parse to find the function end. In JavaScript, finding the function end requires parsing the full syntax (there are no shortcuts where we could count the curly braces - the grammar is too complex). Doing the lightweight parsing first and the actual parsing afterwards is duplicate work.</li>\n<li>If we decide to compile a function eagerly, the work happens on a background thread, and parts of it are interleaved with loading the script from the network. If we instead compile the function only when it's being called, it's too late to parallelize work, since the main thread cannot proceed until the function is compiled.</li>\n</ul>\n<p>You can read more about how V8 parses and compiles JavaScript in <a href=\"https://v8.dev/blog/preparser\">here</a>.</p>\n<p>Many web pages would benefit from selecting the correct functions for eager compilation. For example, in our experiment with popular web pages, 17 out of 20 showed improvements, and the average foreground parse and compile times reduction was 630 ms.</p>\n<p>We're developing a feature, <a href=\"https://github.com/WICG/explicit-javascript-compile-hints-file-based\">Explicit Compile Hints</a>, which allows web developers to control which JavaScript files and functions are compiled eagerly. Chrome 136 is now shipping a version where you can select individual files for eager compilation.</p>\n<p>This version is particularly useful if you have a &quot;core file&quot; which you can select for eager compilation, or if you're able to move code between source files to create such a core file.</p>\n<p>You can trigger eager compilation for the whole file by inserting the magic comment</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token comment\">//# allFunctionsCalledOnLoad</span></code></pre>\n<p>at the top of the file.</p>\n<p>This feature should be used sparingly though - compiling too much will consume time and memory!</p>\n<h2 id=\"see-for-yourself---compile-hints-in-action\" tabindex=\"-1\">See for yourself - compile hints in action <a class=\"bookmark\" href=\"https://v8.dev/blog/explicit-compile-hints#see-for-yourself---compile-hints-in-action\">#</a></h2>\n<p>You can observe compile hints working by telling v8 to log the function events. For example, you can use the following files to set up a minimal test.</p>\n<p>index.html:</p>\n<pre class=\"language-html\"><code class=\"language-html\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>script</span> <span class=\"token attr-name\">src</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>script1.js<span class=\"token punctuation\">\"</span></span><span class=\"token punctuation\">></span></span><span class=\"token script\"></span><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>script</span><span class=\"token punctuation\">></span></span><br /><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>script</span> <span class=\"token attr-name\">src</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>script2.js<span class=\"token punctuation\">\"</span></span><span class=\"token punctuation\">></span></span><span class=\"token script\"></span><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;/</span>script</span><span class=\"token punctuation\">></span></span></code></pre>\n<p>script1.js:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">function</span> <span class=\"token function\">testfunc1</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span><span class=\"token string\">'testfunc1 called!'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token function\">testfunc1</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>script2.js:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token comment\">//# allFunctionsCalledOnLoad</span><br /><br /><span class=\"token keyword\">function</span> <span class=\"token function\">testfunc2</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span><span class=\"token string\">'testfunc2 called!'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token function\">testfunc2</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Remember to run Chrome with a clean user data directory, so that code caching won't mess with your experiment. An example command line would be:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\"><span class=\"token function\">rm</span> <span class=\"token parameter variable\">-rf</span> /tmp/chromedata <span class=\"token operator\">&amp;&amp;</span> google-chrome --no-first-run --user-data-dir<span class=\"token operator\">=</span>/tmp/chromedata --js-flags<span class=\"token operator\">=</span>--log-function_events <span class=\"token operator\">></span> log.txt</code></pre>\n<p>After you've navigated to your test page, you can see the following function events in the log:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">$ <span class=\"token function\">grep</span> testfunc log.txt<br />function,preparse-no-resolution,5,18,60,0.036,179993,testfunc1<br />function,full-parse,5,18,60,0.003,181178,testfunc1<br />function,parse-function,5,18,60,0.014,181186,testfunc1<br />function,interpreter,5,18,60,0.005,181205,testfunc1<br />function,full-parse,6,48,90,0.005,184024,testfunc2<br />function,interpreter,6,48,90,0.005,184822,testfunc2</code></pre>\n<p>Since <code>testfunc1</code> was compiled lazily, we see the <code>parse-function</code> event when it's eventually called:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">function,parse-function,5,18,60,0.014,181186,testfunc1</code></pre>\n<p>For <code>testfunc2</code>, we don't see a corresponding event, since the compile hint forced it to be parsed and compiled eagerly.</p>\n<h2 id=\"future-of-explicit-compile-hints\" tabindex=\"-1\">Future of Explicit Compile Hints <a class=\"bookmark\" href=\"https://v8.dev/blog/explicit-compile-hints#future-of-explicit-compile-hints\">#</a></h2>\n<p>In the long term, we want to move towards selecting individual functions for eager compilation. This empowers web developers to control exactly which functions they want to compile, and squeeze out the last bits of compilation performance to optimize their web pages. Stay tuned!</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/explicit-compile-hints"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Land ahoy: leaving the Sea of Nodes",
    "partialText": "<p>V8’s end-tier optimizing compiler, Turbofan, is famously one of the few large-scale production compilers to use <a href=\"https://en.wikipedia.org/wiki/Sea_of_nodes\">Sea of Nodes</a> (SoN). However, since almost 3 years ago, we’ve started to get rid of Sea of Nodes and fall back to a more traditional <a href=\"https://en.wikipedia.org/wiki/Control-flow_graph\">Control-Flow Graph</a> (CFG) <a href=\"https://en.wikipedia.org/wiki/Intermediate_representation\">Intermediate Representation</a> (IR), which we named Turboshaft. By now, the whole JavaScript backend of Turbofan uses Turboshaft instead, and WebAssembly uses Turboshaft throughout its whole pipeline. Two parts of Turbofan still use some Sea of Nodes: the builtin pipeline, which we’re slowly replacing by Turboshaft, and the frontend of the JavaScript pipeline, which we’re replacing by Maglev, another CFG-based IR. This blog post explains the reasons that led us to move away from Sea of Nodes.</p>\n<h1 id=\"the-birth-of-turbofan-and-sea-of-nodes\" tabindex=\"-1\">The birth of Turbofan and Sea of Nodes <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#the-birth-of-turbofan-and-sea-of-nodes\">#</a></h1>\n<p>12 years ago, in 2013, V8 had a single optimizing compiler: <a href=\"https://blog.chromium.org/2010/12/new-crankshaft-for-v8.html\">Crankshaft</a>. It was using a Control-Flow Graph based Intermediate Representation. The initial version of Crankshaft provided significant performance improvements despite still being quite limited in what it supported. Over the next few years, the team kept improving it to generate even faster code in ever more situations. However, technical debt was starting to stack up and a number of issues were arising with Crankshaft:</p>\n<ol>\n<li>\n<p>It contained too much hand-written assembly code. Every time a new operator was added to the IR, its translation to assembly had to be manually written for the four architectures officially supported by V8 (x64, ia32, arm, arm64).</p>\n</li>\n<li>\n<p>It struggled with optimizing <a href=\"https://en.wikipedia.org/wiki/Asm.js\">asm.js</a>, which was back then seen as an important step towards high-performance JavaScript.</p>\n</li>\n<li>\n<p>It didn’t allow introducing control flow in lowerings. Put otherwise, control flow was created at graph building time, and was then final. This was a major limitation, given that a common thing to do when writing compilers is to start with high-level operations, and then lower them to low-level operations, often by introducing additional control flow. Consider for instance a high-level operation <code>JSAdd(x,y)</code>, it could make sense to later lower it to something like <code>if (x is String and y is String) { StringAdd(x, y) } else { … }</code>. Well, that wasn’t possible in Crankshaft.</p>\n</li>\n<li>\n<p>Try-catches were not supported, and supporting them was very challenging: multiple engineers had spent months trying to support them, without success.</p>\n</li>\n<li>\n<p>It suffered from many performance cliffs and bailouts. Using a specific feature or instruction, or running into a specific edge case of a feature, could cause performance to drop by a factor 100. This made it hard for JavaScript developers to write efficient code and to anticipate the performance of their applications.</p>\n</li>\n<li>\n<p>It contained many <em>deoptimization loops</em>: Crankshaft would optimize a function using some speculative assumptions, then the function would get deoptimized when those assumptions didn’t hold, but too often, Crankshaft would reoptimize the function with the same assumptions, leading to endless optimization-deoptimization loops.</p>\n</li>\n</ol>\n<p>Individually, each of these issues could have probably been overcome. However, combined all together, they seemed like too much. So, the decision was made to replace Crankshaft with a new compiler written from scratch: <a href=\"https://v8.dev/docs/turbofan\">Turbofan</a>. And, rather than using a traditional CFG IR, Turbofan would use a supposedly more powerful IR: Sea of Nodes. At the time, this IR had already been used for more than 10 years in C2, the JIT compiler of the Java HotSpot Virtual Machine.</p>\n<h1 id=\"but-what-is-sea-of-nodes%2C-really%3F\" tabindex=\"-1\">But what is Sea of Nodes, really? <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#but-what-is-sea-of-nodes%2C-really%3F\">#</a></h1>\n<p>First, a small reminder about control-flow graph (CFG): a CFG is a representation of a program as a graph where nodes of the graph represent <a href=\"https://en.wikipedia.org/wiki/Basic_block\">basic blocks</a> of the program (that is, sequence of instructions without incoming or outgoing branches or jumps), and edges represent the control flow of the program. Here is a simple example:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-example-1.svg\" alt=\"\" width=\"925\" height=\"355\" loading=\"lazy\" /><figcaption>Simple CFG graph</figcaption></figure>\n<p>Instructions within a basic block are implicitly ordered: the first instruction should be executed before the second one, and the second one before the third, etc. In the small example above, it feels very natural: <code>v1 == 0</code> can’t be computed before <code>x % 2</code> has been computed anyways. However, consider</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-example-2.svg\" alt=\"\" width=\"809\" height=\"292\" loading=\"lazy\" /><figcaption>CFG graph with arithmetic operations that could be reordered</figcaption></figure>\n<p>Here, the CFG seemingly imposes that <code>a * 2</code> be computed before <code>b * 2</code>, even though we could very well compute them the other way around.<br />\nThat’s where Sea of Nodes comes in: Sea of Nodes does not represent basic blocks, but rather only true dependencies between the instructions. Nodes in Sea of Nodes are single instructions (rather than basic blocks), and edges represent value uses (meaning: an edge from <code>a</code> to <code>b</code> represents the fact that <code>a</code> uses <code>b</code>). So, here is how this last example would be represented with Sea of Nodes:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-arith.svg\" alt=\"\" width=\"928\" height=\"400\" loading=\"lazy\" /><figcaption>Simple Sea of Nodes graph with arithmetic operations</figcaption></figure>\n<p>Eventually, the compiler will need to generate assembly and thus will sequentially schedule these two multiplications, but until then, there is no more dependency between them.</p>\n<p>Now let’s add control flow in the mix. Control nodes (e.g. <code>branch</code>, <code>goto</code>, <code>return</code>) typically don’t have value dependencies between each other that would force a particular schedule, even though they definitely have to be scheduled in a particular order. Thus, in order to represent control-flow, we need a new kind of edge, <em>control edges</em>, which impose some ordering on nodes that don’t have value dependency:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-control.svg\" alt=\"\" width=\"995\" height=\"530\" loading=\"lazy\" /><figcaption>Sea of Nodes graph with control flow</figcaption></figure>\n<p>In this example, without control edges, nothing would prevent the <code>return</code>s from being executed before the <code>branch</code>, which would obviously be wrong.<br />\nThe crucial thing here is that the control edges only impose an order of the operations that have such incoming or outgoing edges, but not on other operations such as the arithmetic operations. This is the main difference between Sea of Nodes and Control flow graphs.</p>\n<p>Let’s now add effectful operations (eg, loads and stores from and to memory) in the mix. Similarly to control nodes, effectful operations often have no value dependencies, but still cannot run in a random order. For instance, <code>a[0] += 42; x = a[0]</code> and <code>x = a[0]; a[0] += 42</code> are not equivalent. So, we need a way to impose an order (= a schedule) on effectful operations. We could reuse the control chain for this purpose, but this would be stricter than required. For instance, consider this small snippet:</p>\n<pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">let</span> v <span class=\"token operator\">=</span> a<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>c<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> v<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>By putting <code>a[2]</code> (which reads memory) on the control chain, we would force it to happen before the branch on <code>c</code>, even though, in practice, this load could easily happen after the branch if its result is only used inside the body of the then-branch. Having lots of nodes in the program on the control chain would defeat the goal of Sea of Nodes, since we would basically end up with a CFG-like IR where only pure operations float around.</p>\n<p>So, to enjoy more freedom and actually benefit from Sea of Nodes, Turbofan has another kind of edge, <em>effect edges</em>, which impose some ordering on nodes that have side effects. Let’s ignore control flow for now and look at a small example:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-effects.svg\" alt=\"\" width=\"995\" height=\"530\" loading=\"lazy\" /><figcaption>Sea of Nodes graph with effectful operations</figcaption></figure>\n<p>In this example, <code>arr[0] = 42</code> and <code>let x = arr[a]</code> have no value dependency (ie, the former is not an input of the latter, and vice versa) . However, because <code>a</code> could be <code>0</code>,  <code>arr[0] = 42</code> should be executed before <code>x = arr[a]</code> in order for the latter to always load the correct value from the array.<br />\n<em>Note that while Turbofan has a single effect chain (which splits on branches, and merges back when the control flow merges) which is used for all effectful operations, it’s possible to have multiple effect chains, where operations that have no dependencies could be on different effect chains, thus relaxing how they can be scheduled (see <a href=\"https://github.com/SeaOfNodes/Simple/blob/main/chapter10/README.md\">Chapter 10 of SeaOfNodes/Simple</a> for more details). However, as we’ll explain later, maintaining a single effect chain is already very error prone, so we did not attempt in Turbofan to have multiple ones.</em></p>\n<p>And, of course, most real programs will contain both control flow and effectful operations.</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-control-and-effects.svg\" alt=\"\" width=\"1019\" height=\"530\" loading=\"lazy\" /><figcaption>Sea of Nodes graph with control flow and effectful operations</figcaption></figure>\n<p>Note that <code>store</code> and <code>load</code> need control inputs, since they could be protected by various checks (such as type checks or bound checks).<br />\nThis example is a good showcase of the power of Sea of Nodes compared to CFG: <code>y = x * c</code> is only used in the <code>else</code> branch thus will freely float to after the <code>branch</code> rather than being computed before as was written in the original JavaScript code. This is similar for <code>arr[0]</code>, which is only used in the <code>else</code> branch, and <em>could</em> thus float after the <code>branch</code> (although, in practice, Turbofan will not move down <code>arr[0]</code>, for reasons that I’ll explain later).<br />\nFor comparison, here is what the corresponding CFG would look like:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-control-and-effects.svg\" alt=\"\" width=\"931\" height=\"404\" loading=\"lazy\" /><figcaption>CFG graph with control flow and effectful operations</figcaption></figure>\n<p>Already, we start seeing the main issue with SoN: it’s much further away from both the input (source code) and the output (assembly) of the compiler than CFG is, which makes it less intuitive to understand. Additionally, having effect and control dependencies always explicit makes it hard to quickly reason about the graph, and to write lowerings (since lowerings always have to explicitly maintain the control and effect chain, which are implicit in a CFG).</p>\n<h1 id=\"and-the-troubles-begin%E2%80%A6\" tabindex=\"-1\">And the troubles begin… <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#and-the-troubles-begin%E2%80%A6\">#</a></h1>\n<p>After more than a decade of dealing with Sea of Nodes, we think that it has more downsides than upsides, at least as far as JavaScript and WebAssembly are concerned.  We’ll go into details in a few of the issues below.</p>\n<h2 id=\"manually%2Fvisually-inspecting-and-understanding-a-sea-of-nodes-graph-is-hard\" tabindex=\"-1\">Manually/visually inspecting and understanding a Sea of Nodes graph is hard <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#manually%2Fvisually-inspecting-and-understanding-a-sea-of-nodes-graph-is-hard\">#</a></h2>\n<p>We’ve already seen that on small programs CFG is easier to read, as it is closer to the original source code, which is what developers (including Compiler Engineers!) are used to write. For the unconvinced readers, let me offer a slightly larger example, so that you understand the issue better. Consider the following JavaScript function, which concatenates an array of strings:</p>\n<pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">function</span> <span class=\"token function\">concat</span><span class=\"token punctuation\">(</span><span class=\"token parameter\">arr</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">let</span> res <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">let</span> i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> arr<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    res <span class=\"token operator\">+=</span> arr<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br />  <span class=\"token keyword\">return</span> res<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Here is the corresponding Sea of Node graph, in the middle of the Turbofan compilation pipeline (which means that some lowerings have already happened):</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-array-concat.png\" alt=\"\" width=\"1073\" height=\"863\" loading=\"lazy\" /><figcaption>Sea of Nodes graph for a simple array concatenation function</figcaption></figure>\n<p>Already, this starts looking like a messy soup of nodes. And, as a compiler engineer, a big part of my job is looking at Turbofan graphs to either understand bugs, or to find optimization opportunities. Well, it’s not easy to do when the graph looks like this. After all, the input of a compiler is the source code, which is CFG-like (instructions all have a fixed position in a given block), and the output of the compiler is assembly, which is also CFG-like (instructions also all have a fixed position in a given block). Having a CFG-like IR thus makes it easier for compiler engineers to match elements or the IR to either the source or the generated assembly.</p>\n<p>For comparison, here is the corresponding CFG graph (which we have available because we’ve already started the process of replacing sea of nodes with CFG):</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-array-concat.png\" alt=\"\" width=\"538\" height=\"749\" loading=\"lazy\" /><figcaption>CFG graph for the same simple array concatenation function</figcaption></figure>\n<p>Among other things, with the CFG, it’s clear where the loop is, it’s clear what the exit condition of the loop is, and it’s easy to find some instructions in the CFG based on where we expect them to be: for instance <code>arr.length</code> can be found in the loop header (it’s <code>v22 = [v0 + 12]</code>), the string concatenation can be found towards the end of the loop (<code>v47 StringConcat(...)</code>).<br />\nArguably, value use-chains are harder to follow in the CFG version, but I would argue that more often than not, it’s better to clearly see the control-flow structure of the graph rather than a soup of value nodes.</p>\n<h2 id=\"too-many-nodes-are-on-the-effect-chain-and%2For-have-a-control-input\" tabindex=\"-1\">Too many nodes are on the effect chain and/or have a control input <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#too-many-nodes-are-on-the-effect-chain-and%2For-have-a-control-input\">#</a></h2>\n<p>In order to benefit from Sea of Nodes, most nodes in the graph should float freely around, without control or effect chain. Unfortunately, that’s not really the case in the typical JavaScript graph, because almost all generic JS operations can have arbitrary side effects. They should be rare in Turbofan though, since we have <a href=\"https://www.youtube.com/watch?v=u7zRSm8jzvA\">feedback</a> that should allow to lower them to more specific operations.</p>\n<p>Still, every memory operation needs both an effect input (since a Load should not float past Stores and vise-versa) and a control input (since there might be a type-check or bound-check before the operation). And even some pure operations like division need control inputs because they might have special cases that are protected by checks.</p>\n<p>Let’s have a look at a concrete example, and start from the following JavaScript function:</p>\n<pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">function</span> <span class=\"token function\">foo</span><span class=\"token punctuation\">(</span><span class=\"token parameter\">a<span class=\"token punctuation\">,</span> b</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token comment\">// assuming that `a.str` and `b.str` are strings</span><br />  <span class=\"token keyword\">return</span> a<span class=\"token punctuation\">.</span>str <span class=\"token operator\">+</span> b<span class=\"token punctuation\">.</span>str<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Here is the corresponding Turbofan graph. To make things clearer, I’ve highlighted part of the effect chain with dashed red lines, and annotated a few nodes with numbers so that I can discuss them below.</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-string-add.png\" alt=\"\" width=\"1024\" height=\"768\" loading=\"lazy\" /><figcaption>Sea of Nodes graph for a simple string concatenation function</figcaption></figure>\n<p>The first observation is that almost all nodes are on the effect chain. Let’s go over a few of them, and see if they really need to be:</p>\n<ul>\n<li><code>1</code> (<code>CheckedTaggedToTaggedPointer</code>): this checks that the 1st input of the function is a pointer and not a “small integer” (see <a href=\"https://v8.dev/blog/pointer-compression\">Pointer Compression in V8</a>). On its own, it wouldn’t really <em>need</em> an effect input, but in practice, it still needs to be on the effect chain, because it guards the following nodes.</li>\n<li><code>2</code> (<code>CheckMaps</code>): now that we know that the 1st input is a pointer, this node loads its “map” (see <a href=\"https://v8.dev/docs/hidden-classes\">Maps (Hidden Classes) in V8</a>), and checks that it matches what the feedback recorded for this object.</li>\n<li><code>3</code> (<code>LoadField</code>): now that we know that the 1st object is a pointer with the right map, we can load its <code>.str</code> field.</li>\n<li><code>4</code>, <code>5</code> and <code>6</code> are a repeat for the second input.</li>\n<li><code>7</code> (<code>CheckString</code>): now that we’ve loaded <code>a.str</code>, this node checks that it’s indeed a string.</li>\n<li><code>8</code>: repeat for the second input.</li>\n<li><code>9</code>: checks that the combined length of <code>a.str</code> and <code>b.str</code> is less than the maximum size of a String in V8.</li>\n<li><code>10</code> (<code>StringConcat</code>): finally concatenates the 2 strings.</li>\n</ul>\n<p>This graph is very typical of Turbofan graphs for JavaScript programs: checking maps, loading values, checking the maps of the loaded values, and so on, and eventually doing a few calculations on those values. And like in this example, in a lot of cases, most instructions end up being on the effect or control chain, which imposes a strict order on the operations, and completely defeats the purpose of Sea of Nodes.</p>\n<h2 id=\"memory-operations-do-not-float-easily\" tabindex=\"-1\">Memory operations do not float easily <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#memory-operations-do-not-float-easily\">#</a></h2>\n<p>Let’s consider the following JavaScript program:</p>\n<pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">let</span> x <span class=\"token operator\">=</span> arr<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token keyword\">let</span> y <span class=\"token operator\">=</span> arr<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /><span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>c<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> x<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> y<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Given that <code>x</code> and <code>y</code> are each only used in a single side of the <code>if</code>-<code>else</code>, we may hope that SoN would allow them to freely float down to inside the “then” and the “else” branches. However, in practice, making this happen in SoN would not be easier than in a CFG. Let’s have a look at the SoN graph to understand why:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-mirror-control-effect.svg\" alt=\"\" width=\"1008\" height=\"527\" loading=\"lazy\" /><figcaption>Sea of Nodes graph where the effect chain mirrors the control chain, leading to effectful operations not floating as freely as one may hope</figcaption></figure>\n<p>When we build the SoN graph, we create the effect chain as we go along, and thus the second <code>Load</code> ends up being right after the first one, after which the effect chain has to split to reach both <code>return</code>s (if you’re wondering why <code>return</code>s are even on the effect chain, it’s because there could be operations with side-effects before, such as <code>Store</code>s, which have to be executed before returning from the function). Given that the second <code>Load</code> is a predecessor to both <code>return</code>s, it has to be scheduled before the <code>branch</code>, and SoN thus doesn’t allow any of the two <code>Load</code>s to float down freely.<br />\nIn order to move the <code>Load</code>s down the “then” and “else” branches, we would have to compute that there are no side effects in between them, and that there are no side effects in between the second <code>Load</code> and the <code>return</code>s, then we could split the effect chain at the beginning instead of after the second <code>Load</code>. Doing this analysis on a SoN graph or on a CFG is extremely similar.</p>\n<p>Now that we’ve mentioned that a lot of nodes end up on the effect chain, and that effectful nodes often don’t freely float very far, it’s a good time to realize that in a way, <strong>SoN is just CFG where pure nodes are floating</strong>. Indeed, in practice, the control nodes and control chain always mirror the structure of the equivalent CFG. And, when both destinations of a branch have side effects (which is frequent in JavaScript), the effect chain splits and merges exactly where the control chain does (as is the case in the example above: the control chain splits on the <code>branch</code>, and the effect chain mirrors this by splitting on the <code>Load</code>; and if the program would continue after the <code>if</code>-<code>else</code>, both chains would merge around the same place). Effectful nodes thus typically end up being constrained to be scheduled in between two control nodes, a.k.a., in a basic block. And within this basic block, the effect chain will constrain effectful nodes to be in the same order as they were in the source code. In the end, only pure nodes actually float freely.</p>\n<p>One way to get more floating nodes is to use multiple effect chains, as mentioned earlier, but this comes at a price: first, managing a single effect chain is already hard; managing multiple ones will be much harder. Second, in a dynamic language like JavaScript, we end up with a lot of memory accesses that could alias, which means that the multiple effect chains would have to all merge very often, thus negating part of the advantages of having multiple effect chains.</p>\n<h2 id=\"managing-the-effect-and-control-chains-manually-is-hard\" tabindex=\"-1\">Managing the effect and control chains manually is hard <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#managing-the-effect-and-control-chains-manually-is-hard\">#</a></h2>\n<p>As mentioned in the previous section, while the effect and control chain are somewhat distinct, in practice, the effect chain typically has the same “shape” as the control chain: if the destinations of a branch contain effectful operations (and it’s often the case), then the effect chain will split on the branch and merge back when the control flow merges back.<br />\nBecause we’re dealing with JavaScript, a lot of nodes have side effects, and we have a lot of branches (typically branching on the type of some objects), which leads to having to keep track of both the effect and control chain in parallel, whereas with a CFG, we would only have to keep track of the control chain.</p>\n<p>History has shown that managing both the effect and control chains manually is error prone, hard to read and hard to maintain. Take this sample of code from the <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/js-native-context-specialization.cc;l=1482;drc=22629fc9a7e45cf5e4c691db371f69f176318f11\">JSNativeContextSpecialization</a> phase:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token class-name\">JSNativeContextSpecialization</span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">ReduceNamedAccess</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  Effect effect<span class=\"token punctuation\">{</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><br />  Node<span class=\"token operator\">*</span> receiverissmi_effect <span class=\"token operator\">=</span> effect<span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><br />  Effect this_effect <span class=\"token operator\">=</span> effect<span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><br />  this_effect <span class=\"token operator\">=</span> <span class=\"token function\">graph</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">NewNode</span><span class=\"token punctuation\">(</span><span class=\"token function\">common</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">EffectPhi</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> this_effect<span class=\"token punctuation\">,</span><br />                                 receiverissmi_effect<span class=\"token punctuation\">,</span> this_control<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  receiverissmi_effect <span class=\"token operator\">=</span> receiverissmi_control <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><br />  effect <span class=\"token operator\">=</span> <span class=\"token function\">graph</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">NewNode</span><span class=\"token punctuation\">(</span><span class=\"token function\">common</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-></span><span class=\"token function\">EffectPhi</span><span class=\"token punctuation\">(</span>control_count<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Because of the various branches and cases that have to be handled here, we end up managing 3 different effect chains. It’s easy to get it wrong and use one effect chain instead of the other. So easy that we indeed <a href=\"https://crbug.com/41470351\">got it wrong initially</a>, and only <a href=\"https://crrev.com/c/1749902\">realized our mistake</a> after a few months:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-effects-fix.png\" alt=\"\" width=\"707\" height=\"74\" loading=\"lazy\" /></figure>\n<p>For this issue, I would place the blame on both Turbofan and Sea of Nodes, rather than only on the latter. Better helpers in Turbofan could have simplified managing the effect and control chains, but this would not have been an issue in a CFG.</p>\n<h2 id=\"the-scheduler-is-too-complex\" tabindex=\"-1\">The scheduler is too complex <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#the-scheduler-is-too-complex\">#</a></h2>\n<p>Eventually, all instructions must be scheduled in order to generate assembly code. The theory to schedule instructions is simple enough: each instruction should be scheduled after its value, control and effect inputs (ignoring loops).</p>\n<p>Let’s have a look at an interesting example:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-switch-case.svg\" alt=\"\" width=\"1028\" height=\"502\" loading=\"lazy\" /><figcaption>Sea of Nodes graph for a simple switch-case</figcaption></figure>\n<p>You’ll notice that while the source JavaScript program has two identical divisions, the Sea of Node graph only has one. In reality, Sea of Nodes would start with two divisions, but since this is a pure operation (assuming double inputs), redundancy elimination would easily deduplicate them into one.<br />\nThen when reaching the scheduling phase, we would have to find a place to schedule this division. Clearly, it cannot go after <code>case 1</code> or <code>case 2</code>, since it’s used in the other one. Instead, it would have to be scheduled before the <code>switch</code>. The downside is that, now, <code>a / b</code> will be computed even when <code>c</code> is <code>3</code>, where it doesn’t really need to be computed. This is a real issue that can lead to many deduplicated instructions floating to the common dominator of their users, slowing down many paths that don’t need them.<br />\nThere is a fix though: Turbofan’s scheduler will try to identify these cases and duplicate the instructions so that they are only computed on the paths that need them. The downside is that this makes the scheduler more complex, requiring additional logic to figure out which nodes could and should be duplicated, and how to duplicate them.<br />\nSo, basically, we started with 2 divisions, then “optimized” to a single division, and then optimized further to 2 divisions again. And this doesn’t happen just for division: a lot of other operations will go through similar cycles.</p>\n<h2 id=\"finding-a-good-order-to-visit-the-graph-is-difficult\" tabindex=\"-1\">Finding a good order to visit the graph is difficult <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#finding-a-good-order-to-visit-the-graph-is-difficult\">#</a></h2>\n<p>All passes of a compiler need to visit the graph, be it to lower nodes, to apply local optimizations, or to run analysis over the whole graph. In a CFG, the order in which to visit nodes is usually straightforward: start from the first block (assuming a single-entry function), and iterate through each node of the block, and then move on to the successors and so on. In a <a href=\"https://en.wikipedia.org/wiki/Peephole_optimization\">peephole optimization</a> phase (such as <a href=\"https://en.wikipedia.org/wiki/Strength_reduction\">strength reduction</a>), a nice property of processing the graph in this order is that inputs are always optimized before a node is processed, and visiting each node exactly once is thus enough to apply most peephole optimizations. Consider for instance the following sequence of reductions:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-peepholes.svg\" alt=\"\" width=\"723\" height=\"163\" loading=\"lazy\" /></figure>\n<p>In total, it took three steps to optimize the whole sequence, and each step did useful work. After which, dead code elimination would remove <code>v1</code> and <code>v2</code>, resulting in one less instruction than in the initial sequence.</p>\n<p>With Sea of Nodes, it’s not possible to process pure instructions from start to end, since they aren’t on any control or effect chain, and thus there is no pointer to pure roots or anything like that. Instead, the usual way to process a Sea of Nodes graph for peephole optimizations is to start from the end (e.g., <code>return</code> instructions), and go up the value, effect and control inputs. This has the nice property that we won’t visit any unused instruction, but the upsides stop about there, because for peephole optimization, this is about the worst visitation order you could get. On the example above, here are the steps we would take:</p>\n<ul>\n<li>Start by visiting <code>v3</code>, but can’t lower it at this point, then move on to its inputs\n<ul>\n<li>Visit <code>v1</code>, lower it to <code>a &lt;&lt; 3</code>, then move on to its uses, in case the lowering of <code>v1</code> enables them to be optimized.\n<ul>\n<li>Visit <code>v3</code> again, but can’t lower it yet (this time, we wouldn’t visit its inputs again though)</li>\n</ul>\n</li>\n<li>Visit <code>v2</code>, lower it to <code>b &lt;&lt; 3</code>, then move on to its uses, in case this lowering enables them to be optimized.\n<ul>\n<li>Visit <code>v3</code> again, lower it to <code>(a &amp; b) &lt;&lt; 3</code>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>So, in total, <code>v3</code> was visited 3 times but only lowered once.</p>\n<p>We measured this effect on typical JavaScript programs a while ago, and realized that, on average, nodes are changed only once every 20 visits!</p>\n<p>Another consequence of the difficulty to find a good visitation order of the graph is that <strong>state tracking is hard and expensive.</strong> A lot of optimizations require tracking some state along the graph, like Load Elimination or Escape Analysis. However, this is hard to do with Sea of Nodes, because at a given point, it’s hard to know if a given state needs to be kept alive or not, because it’s hard to figure out if unprocessed nodes would need this state to be processed.<br />\nAs a consequence of this, Turbofan’s Load Elimination phase has a bailout on large graphs to avoid taking too long to finish and consuming too much memory. By comparison, we wrote a <a href=\"https://docs.google.com/document/d/1AEl4dATNLu8GlLyUBQFXJoCxoAT5BeG7RCWxoEtIBJE/edit?usp=sharing\">new Load elimination phase for our new CFG compiler</a>, which we’ve benchmarked to be up to 190 times faster (it has better worst-case complexity, so this kind of speedup is easy to achieve on large graphs), while using way less memory.</p>\n<h2 id=\"cache-unfriendliness\" tabindex=\"-1\">Cache unfriendliness <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#cache-unfriendliness\">#</a></h2>\n<p>Almost all phases in Turbofan mutate the graph in-place. Given that nodes are fairly large in memory (mostly because each node has pointers to both its inputs and its uses), we try to reuse nodes as much as possible. However, inevitably, when we lower nodes to sequences of multiple nodes, we have to introduce new nodes, which will necessarily not be allocated close to the original node in memory. As a result, the deeper we go through the Turbofan pipeline and the more phases we run, the less cache friendly the graph is. Here is an illustration of this phenomenon:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/Sea-of-Nodes-cache-unfriendliness.svg\" alt=\"\" width=\"773\" height=\"417\" loading=\"lazy\" /></figure>\n<p>It’s hard to estimate the exact impact of this cache unfriendliness on memory. Still, now that we have our new CFG compiler, we can compare the number of cache misses between the two: Sea of Nodes suffers on average from about 3 times more L1 dcache misses compared to our new CFG IR, and up to 7 times more in some phases. We estimate that this costs up to 5% of compile time, although this number is a bit handwavy. Still, keep in mind that in a JIT compiler, compiling fast is essential.</p>\n<h2 id=\"control-flow-dependent-typing-is-limited\" tabindex=\"-1\">Control-flow dependent typing is limited <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#control-flow-dependent-typing-is-limited\">#</a></h2>\n<p>Let’s consider the following JavaScript function:</p>\n<pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">function</span> <span class=\"token function\">foo</span><span class=\"token punctuation\">(</span><span class=\"token parameter\">x</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x <span class=\"token operator\">&lt;</span> <span class=\"token number\">42</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token keyword\">return</span> x <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br />  <span class=\"token keyword\">return</span> x<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>If so far we’ve only seen small integers for <code>x</code> and for the result of <code>x+1</code> (where “small integers” are 31-bit integers, cf. <a href=\"https://v8.dev/blog/pointer-compression#value-tagging-in-v8\">Value tagging in V8</a>), then we’ll speculate that this will remain the case. If we ever see <code>x</code> being larger than a 31-bit integer, then we will deoptimize. Similarly, if <code>x+1</code> produces a result that is larger than 31 bits, we will also deoptimize. This means that we need to check whether <code>x+1</code> is less or more than the maximum value that fits in 31 bits. Let’s have a look at corresponding the CFG and SoN graphs:</p>\n<figure><img src=\"https://v8.dev/_img/leaving-the-sea-of-nodes/CFG-vs-SoN-control-flow-typing.svg\" alt=\"\" width=\"982\" height=\"484\" loading=\"lazy\" /></figure>\n<p>(assuming a <code>CheckedAdd</code> operation that adds its inputs and deoptimizes if the result overflows 31-bits)<br />\nWith a CFG, it’s easy to realize that when <code>CheckedAdd(v1, 1)</code> is executed, <code>v1</code> is guaranteed to be less than <code>42</code>, and that there is therefore no need to check for 31-bit overflow. We would thus easily replace the <code>CheckedAdd</code> by a regular <code>Add</code>, which would execute faster, and would not require a deoptimization state (which is otherwise required to know how to resume execution after deoptimizing).<br />\nHowever, with a SoN graph, <code>CheckedAdd</code>, being a pure operation, will flow freely in the graph, and there is thus no way to remove the check until we’ve computed a schedule and decided that we will compute it after the branch (and at this point, we are back to a CFG, so this is not a SoN optimization anymore).</p>\n<p>Such checked operations are frequent in V8 due to this 31-bit small integer optimization, and the ability to replace checked operations by unchecked operations can have a significant impact on quality of the code generated by Turbofan. So, Turbofan’s SoN <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/simplified-operator.cc;l=966;drc=0a1fae9e77c6d8e85d8197b4f4396815ec9194b9\">puts a control-input on <code>CheckedAdd</code></a>, which can enable this optimization, but also means introducing a scheduling constraint on a pure node, a.k.a., going back to a CFG.</p>\n<h2 id=\"and-many-other-issues%E2%80%A6\" tabindex=\"-1\">And many other issues… <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#and-many-other-issues%E2%80%A6\">#</a></h2>\n<p><strong>Propagating deadness is hard.</strong> Frequently, during some lowering, we realize that the current node is actually unreachable. In a CFG, we could just cut the current basic block here, and the following blocks would automatically become obviously unreachable since they would have no predecessors anymore. In Sea of Nodes, it’s harder, because one has to patch both the control and effect chain. So, when a node on the effect chain is dead, we have to walk forward the effect chain until the next merge, killing everything along the way, and carefully handling nodes that are on the control chain.</p>\n<p><strong>It’s hard to introduce new control flow.</strong>  Because control flow nodes have to be on the control chain, it’s not possible to introduce new control flow during regular lowerings. So, if there is a pure node in the graph, such as <code>Int32Max</code>, which returns the maximum of 2 integers, and which we would eventually like to lower to <code>if (x &gt; y) { x } else { y }</code>, this is not easily doable in Sea of Nodes, because we would need a way to figure out where on the control chain to plug this subgraph. One way to implement this would be to put <code>Int32Max</code> on the control chain from the beginning, but this feels wasteful: the node is pure and should be allowed to move around freely. So, the canonical Sea of Nodes way to solve this, used both in Turbofan, and also by Cliff Click (Sea of Nodes’ inventor), as mentioned in this <a href=\"https://youtu.be/Vu372dnk2Ak?t=3037\">Coffee Compiler Club</a> chat, is to delay this kind of lowerings until we have a schedule (and thus a CFG). As a result, we have a phase around the middle of the pipeline that computes a schedule and lowers the graph, where a lot of random optimizations are packed together because they all require a schedule. By comparison, with a CFG, we would be free to do these optimizations earlier or later in the pipeline.<br />\nAlso, remember from the introduction that one of the issues of Crankshaft (Turbofan’s predecessor) was that it was virtually impossible to introduce control flow after having built the graph. Turbofan is a slight improvement over this, since lowering of nodes on the control chain can introduce new control flow, but this is still limited.</p>\n<p><strong>It’s hard to figure out what is inside of a loop.</strong> Because a lot of nodes are floating outside of the control chain, it’s hard to figure out what is inside each loop. As a result, basic optimizations such as loop peeling and loop unrolling are hard to implement.</p>\n<p><strong>Compiling is slow.</strong> This is a direct consequence of multiple issues that I’ve already mentioned: it’s hard to find a good visitation order for nodes, which leads to many useless revisitation, state tracking is expensive, memory usage is bad, cache locality is bad… This might not be a big deal for an ahead of time compiler, but in a JIT compiler, compiling slowly means that we keep executing slow unoptimized code until the optimized code is ready, while taking away resources from other tasks (eg, other compilation jobs, or the Garbage Collector). One consequence of this is that we are forced to think very carefully about the compile time - speedup tradeoff of new optimizations, often erring towards the side of optimizing less to keep optimizing fast.</p>\n<p><strong>Sea of Nodes destroys any prior scheduling, by construction.</strong> JavaScript source code is typically not manually optimized with CPU microarchitecture in mind. However, WebAssembly code can be, either at the source level (C++ for instance), or by an <a href=\"https://en.wikipedia.org/wiki/Ahead-of-time_compilation\">ahead-of-time (AOT)</a> compilation toolchain (like <a href=\"https://github.com/WebAssembly/binaryen\">Binaryen/Emscripten</a>). As a result, a WebAssembly code could be scheduled in a way that should be good on most architectures (for instance, reducing the need for <a href=\"https://en.wikipedia.org/wiki/Register_allocation#Components_of_register_allocation\">spilling</a>, assuming 16 registers). However, SoN always discards the initial schedule, and needs to rely on its own scheduler only, which, because of the time constraints of JIT compilation, can easily be worse than what an AOT compiler (or a C++ developer carefully thinking about the scheduling of their code) could do. We have seen cases where WebAssembly was suffering from this. And, unfortunately, using a CFG compiler for WebAssembly and a SoN compiler for JavaScript in Turbofan was not an option either, since using the same compiler for both enables inlining across both languages.</p>\n<h1 id=\"sea-of-nodes%3A-elegant-but-impractical-for-javascript\" tabindex=\"-1\">Sea of Nodes: elegant but impractical for JavaScript <a class=\"bookmark\" href=\"https://v8.dev/blog/leaving-the-sea-of-nodes#sea-of-nodes%3A-elegant-but-impractical-for-javascript\">#</a></h1>\n<p>So, to recapitulate, here are the main problems we have with Sea of Nodes and Turbofan:</p>\n<ol>\n<li>\n<p>It’s <strong>too complex</strong>. Effect and control chains are hard to understand, leading to many subtle bugs. Graphs are hard to read and analyze, making new optimizations hard to implement and refine.</p>\n</li>\n<li>\n<p>It’s <strong>too limited</strong>. Too many nodes are on the effect and control chain (because we’re compiling JavaScript code), thus not providing many benefits over a traditional CFG. Additionally, because it’s hard to introduce new control-flow in lowerings, even basic optimizations end up being hard to implement.</p>\n</li>\n<li>\n<p>Compiling is <strong>too slow</strong>. State-tracking is expensive, because it’s hard to find a good order in which to visit graphs. Cache locality is bad. And reaching fixpoints during reduction phases takes too long.</p>\n</li>\n</ol>\n<p>So, after ten years of dealing with Turbofan and battling Sea of Nodes, we’ve finally decided to get rid of it, and instead go back to a more traditional CFG IR. Our experience with our new IR has been extremely positive so far, and we are very happy to have gone back to a CFG: compile time got divided by 2 compared to SoN, the code of the compiler is a lot simpler and shorter, investigating bugs is usually much easier, etc.<br />\nStill, this post is already quite long, so I’ll stop here. Stay tuned for an upcoming blog post that will explain the design of our new CFG IR, Turboshaft.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/leaving-the-sea-of-nodes"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Turbocharging V8 with mutable heap numbers",
    "partialText": "<p>At V8, we're constantly striving to improve JavaScript performance. As part of this effort, we recently revisited the <a href=\"https://browserbench.org/JetStream2.1/\">JetStream2</a> benchmark suite to eliminate performance cliffs. This post details a specific optimization we made that yielded a significant <code>2.5x</code> improvement in the <code>async-fs</code> benchmark, contributing to a noticeable boost in the overall score. The optimization was inspired by the benchmark, but such patterns do appear in <a href=\"https://github.com/WebAssembly/binaryen/blob/3339c1f38da5b68ce8bf410773fe4b5eee451ab8/scripts/fuzz_shell.js#L248\">real-world code</a>.</p>\n<h1 id=\"the-target-async-fs-and-a-peculiar-math.random\" tabindex=\"-1\">The target <code>async-fs</code> and a peculiar <code>Math.random</code> <a class=\"bookmark\" href=\"https://v8.dev/blog/mutable-heap-number#the-target-async-fs-and-a-peculiar-math.random\">#</a></h1>\n<p>The <code>async-fs</code> benchmark, as its name suggests, is a JavaScript file system implementation, focusing on asynchronous operations. However, a surprising performance bottleneck exists: the implementation of <code>Math.random</code>. It uses a custom, deterministic implementation of <code>Math.random</code> for consistent results across runs. The implementation is:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">let</span> seed<span class=\"token punctuation\">;</span><br />Math<span class=\"token punctuation\">.</span>random <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">function</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> <span class=\"token keyword\">function</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">+</span> <span class=\"token number\">0x7ed55d16</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">&lt;&lt;</span> <span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">^</span> <span class=\"token number\">0xc761c23c</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">^</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">>>></span> <span class=\"token number\">19</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">+</span> <span class=\"token number\">0x165667b1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">&lt;&lt;</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">+</span> <span class=\"token number\">0xd3a2646c</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">^</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">&lt;&lt;</span> <span class=\"token number\">9</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">+</span> <span class=\"token number\">0xfd7046c5</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">&lt;&lt;</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    seed <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>seed <span class=\"token operator\">^</span> <span class=\"token number\">0xb55a4f09</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">^</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">>>></span> <span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xffffffff</span><span class=\"token punctuation\">;</span><br />    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>seed <span class=\"token operator\">&amp;</span> <span class=\"token number\">0xfffffff</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">0x10000000</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>The key variable here is <code>seed</code>. It's updated on every call to <code>Math.random</code>, generating the pseudo-random sequence. Crucially, here <code>seed</code> is stored in a <code>ScriptContext</code>.</p>\n<p>A <code>ScriptContext</code> serves as a storage location for values accessible within a particular script. Internally, this context is represented as an array of V8's tagged values. On the default V8 configuration for 64-bit systems, each of these tagged values occupies 32 bits. The least significant bit of each value acts as a tag. A <code>0</code> indicates a 31-bit <em>Small Integer</em> (<code>SMI</code>). The actual integer value is stored directly, left-shifted by one bit. A <code>1</code> indicates a <a href=\"https://v8.dev/blog/pointer-compression\">compressed pointer</a> to a heap object, where the compressed pointer value is incremented by one.</p>\n<figure><img src=\"https://v8.dev/_img/mutable-heap-number/script-context.svg\" alt=\"\" width=\"1205\" height=\"980\" loading=\"lazy\" /><figcaption><code>ScriptContext</code> layout: blue slots are pointers to the context metadata and to the global object (<code>NativeContext</code>). The yellow slot indicates an untagged double-precision floating-point value.</figcaption></figure>\n<p>This tagging differentiates how numbers are stored. <code>SMIs</code> reside directly in the <code>ScriptContext</code>. Larger numbers or those with decimal parts are stored indirectly as immutable <code>HeapNumber</code> objects on the heap (a 64-bit double), with the <code>ScriptContext</code> holding a compressed pointer to them. This approach efficiently handles various numeric types while optimizing for the common <code>SMI</code> case.</p>\n<h1 id=\"the-bottleneck\" tabindex=\"-1\">The bottleneck <a class=\"bookmark\" href=\"https://v8.dev/blog/mutable-heap-number#the-bottleneck\">#</a></h1>\n<p>Profiling <code>Math.random</code> revealed two major performance issues:</p>\n<ul>\n<li>\n<p><strong><code>HeapNumber</code> allocation:</strong> The slot dedicated to the <code>seed</code> variable in the script context points to a standard, immutable <code>HeapNumber</code>. Each time the <code>Math.random</code> function updates <code>seed</code>, a new <code>HeapNumber</code> object has to be allocated on the heap resulting in  significant allocation and garbage collection pressure.</p>\n</li>\n<li>\n<p><strong>Floating-point arithmetic:</strong> Even though the calculations within <code>Math.random</code> are fundamentally integer operations (using bitwise shifts and additions), the compiler can't take full advantage of this. Because <code>seed</code> is stored as a generic <code>HeapNumber</code>, the generated code uses slower floating-point instructions. The compiler can't prove that <code>seed</code> will always hold a value representable as an integer. While the compiler could potentially speculate about 32-bit integer ranges, V8 primarily focuses on <code>SMIs</code>. Even with 32-bit integer speculation, a potentially costly conversion from 64-bit floating-point to 32-bit integer, along with a lossless check, would still be necessary.</p>\n</li>\n</ul>\n<h1 id=\"the-solution\" tabindex=\"-1\">The solution <a class=\"bookmark\" href=\"https://v8.dev/blog/mutable-heap-number#the-solution\">#</a></h1>\n<p>To address these issues, we implemented a two-part optimization:</p>\n<ul>\n<li>\n<p><strong>Slot type tracking / mutable heap number slots:</strong> We extended <a href=\"https://issues.chromium.org/u/2/issues/42203515\">script context const value tracking</a> (let-variables that were initialized but never modified) to include type information. We track whether that slot value is constant, a <code>SMI</code>, a <code>HeapNumber</code> or a generic tagged value. We also introduced the concept of mutable heap number slots within script contexts, similar to <a href=\"https://v8.dev/blog/react-cliff#smi-heapnumber-mutableheapnumber\">mutable heap number fields</a> for <code>JSObjects</code>. Instead of pointing to an immutable <code>HeapNumber</code>, the script context slot owns the <code>HeapNumber</code>, and it should not leak its address. This eliminates the need to allocate a new <code>HeapNumber</code> on every update for optimized code. The owned <code>HeapNumber</code> itself is modified in-place.</p>\n</li>\n<li>\n<p><strong>Mutable heap <code>Int32</code>:</strong> We enhance the script context slot types to track whether a numeric value falls within the <code>Int32</code> range. If it does, the mutable <code>HeapNumber</code> stores the value as a raw <code>Int32</code>. If needed, the transition to a <code>double</code> carries the added benefit of not requiring <code>HeapNumber</code> reallocation. In the case of <code>Math.random</code>, the compiler can now observe that <code>seed</code> is consistently being updated with integer operations and mark the slot as containing a mutable <code>Int32</code>.</p>\n</li>\n</ul>\n<figure><img src=\"https://v8.dev/_img/mutable-heap-number/transitions.svg\" alt=\"\" width=\"1225\" height=\"980\" loading=\"lazy\" /><figcaption>Slot type state machine.  A green arrow indicates a transition triggered by storing an <code>SMI</code> value.  Blue arrows represent transitions by storing an <code>Int32</code> value, and red arrows, a double-precision floating-point value. The <code>Other</code> state acts as a sink state, preventing further transitions.</figcaption></figure>\n<p>It's important to note that these optimizations introduce a code dependency on the type of the value stored in the context slot. The optimized code generated by the JIT compiler relies on the slot containing a specific type (here, an <code>Int32</code>). If any code writes a value to the <code>seed</code> slot that changes its type (e.g., writing a floating-point number or a string), the optimized code will need to deoptimize. This deoptimization is necessary to ensure correctness. Therefore, the stability of the type stored in the slot is crucial for maintaining peak performance. In the case of <code>Math.random</code>, the bitmasking in the algorithm ensures that the seed variable always holds an <code>Int32</code> value.</p>\n<h1 id=\"the-results\" tabindex=\"-1\">The results <a class=\"bookmark\" href=\"https://v8.dev/blog/mutable-heap-number#the-results\">#</a></h1>\n<p>These changes significantly speed up the peculiar <code>Math.random</code> function:</p>\n<ul>\n<li>\n<p><strong>No allocation / fast in-place updates:</strong> The <code>seed</code> value is updated directly within its mutable slot in the script context. No new objects are allocated during the <code>Math.random</code> execution.</p>\n</li>\n<li>\n<p><strong>Integer operations:</strong> The compiler, armed with the knowledge that the slot contains an <code>Int32</code>, can generate highly optimized integer instructions (shifts, adds, etc.). This avoids the overhead of floating-point arithmetic.</p>\n</li>\n</ul>\n<figure><img src=\"https://v8.dev/_img/mutable-heap-number/result.png\" alt=\"\" width=\"874\" height=\"608\" loading=\"lazy\" /><figcaption><code>async-fs</code> benchmark results on a Mac M1. Higher scores are better.</figcaption></figure>\n<p>The combined effect of these optimizations is a remarkable <code>~2.5x</code> speedup on the <code>async-fs</code> benchmark. This, in turn, contributes to a <code>~1.6%</code> improvement in the overall JetStream2 score. This demonstrates that seemingly simple code can create unexpected performance bottlenecks, and that small, targeted optimizations can have large impact not just for the benchmark.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/mutable-heap-number"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Introducing the WebAssembly JavaScript Promise Integration API",
    "partialText": "<p>The JavaScript Promise Integration (JSPI) API allows WebAssembly applications that were written assuming <em>synchronous</em> access to external functionality to operate smoothly in an environment where the functionality is actually <em>asynchronous</em>.</p>\n<p>This note outlines what the core capabilities of the JSPI API are, how to access it, how to develop software for it and offers some examples to try out.</p>\n<h2 id=\"what-is-%E2%80%98jspi%E2%80%99-for%3F\" tabindex=\"-1\">What is ‘JSPI’ for? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#what-is-%E2%80%98jspi%E2%80%99-for%3F\">#</a></h2>\n<p>Asynchronous APIs operate by separating the <em>initiation</em> of the operation from its <em>resolution</em>; with the latter coming some time after the first. Most importantly, the application continues execution after kicking off the operation; and is then notified when the operation completes.</p>\n<p>For example, using the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\"><code>fetch</code> API</a>, Web applications can access the contents associated with a URL; however, the <code>fetch</code> function does not directly return the results of the fetch; instead it returns a <code>Promise</code> object. The connection between the fetch response and the original request is reestablished by attaching a <em>callback</em> to that <code>Promise</code> object. The callback function can inspect the response and collect the data (if it is there of course).</p>\n<p>On the other hand, many cases C/C++ (and many other languages) applications are originally written against a <em>synchronous</em> API. For example, the Posix <code>read</code> function does not complete until the I/O operation is complete: the <code>read</code> function <em>blocks</em> until the read is complete.</p>\n<p>However, it is not permitted to block the browser’s main thread; and many environments are not supportive of synchronous programming. The result is a mismatch between the desires of the application programmer for a simple to use API and the wider ecosystem that requires I/O to be crafted with asynchronous code. This is especially a problem for existing legacy applications that would be expensive to port.</p>\n<p>The JSPI is an API that bridges the gap between synchronous applications and asynchronous Web APIs. It works by intercepting <code>Promise</code> objects returned by asynchronous Web API functions and <em>suspending</em> the WebAssembly application. When the asynchronous I/O operation is completed, the WebAssembly application is <em>resumed</em>. This allows the WebAssembly application to use straight-line code to perform asynchronous operations and to process their results.</p>\n<p>Crucially, using JSPI requires very few changes to the WebAssembly application itself.</p>\n<h3 id=\"how-does-jspi-work%3F\" tabindex=\"-1\">How does JSPI work? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#how-does-jspi-work%3F\">#</a></h3>\n<p>The JSPI works by intercepting the <code>Promise</code> object returned from  calls to JavaScript and suspending the main logic of the WebAssembly application. A callback is attached to this <code>Promise</code> object which will resume the suspended WebAssembly code when invoked by the browser's event loop task runner.</p>\n<p>In addition, the WebAssembly export is refactored to return a <code>Promise</code> object — instead of the original returned value from the export. This <code>Promise</code> object becomes the value returned by the WebAssembly application: when the WebAssembly code is suspended,<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn1\" id=\"fnref1\">[1]</a></sup> the exported <code>Promise</code> object is returned as the value of the call into WebAssembly.</p>\n<p>The export Promise is resolved when the original call completes: if the original WebAssembly function returns a normal value the export <code>Promise</code> object is resolved with that value (converted to a JavaScript object); if an exception is thrown then the export <code>Promise</code> object is rejected.</p>\n<h4 id=\"wrapping-imports-and-exports\" tabindex=\"-1\">Wrapping imports and exports <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#wrapping-imports-and-exports\">#</a></h4>\n<p>This is enabled by <em>wrapping</em> imports and exports during the WebAssembly module instantiation phase. The function wrappers add the suspending behavior to the normal asynchronous imports and route suspensions to <code>Promise</code> object callbacks.</p>\n<p>It is not necessary to wrap all the exports and imports of a WebAssembly module. Some exports whose execution paths don’t involve calling asynchronous APIs are better left unwrapped. Similarly, not all of a WebAssembly module’s imports are to asynchronous API functions; those imports too should not be wrapped.</p>\n<p>Of course, there is a significant amount of internal mechanisms that allow this to happen;<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn2\" id=\"fnref2\">[2]</a></sup> but neither the JavaScript language nor WebAssembly itself are changed by the JSPI. Its operations are confined to the boundary between JavaScript and WebAssembly.</p>\n<p>From the perspective of a Web application developer, the result is a body of code that participates in the JavaScript world of async functions and Promises in an analogous way that other async functions written in JavaScript work. From the perspective of the WebAssembly developer, this allows them to craft applications using synchronous APIs and yet participate in the Web’s asynchronous ecosystem.</p>\n<h3 id=\"expected-performance\" tabindex=\"-1\">Expected performance <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#expected-performance\">#</a></h3>\n<p>Because the mechanisms used when suspending and resuming WebAssembly modules are essentially constant time, we don’t anticipate high costs in using JSPI — especially compared to other transformation based approaches.</p>\n<p>There is a constant amount of work needed to propagate the <code>Promise</code> object returned by the asynchronous API call to the WebAssembly. Similarly, when a Promise is resolved, the WebAssembly application can be resumed with constant-time overhead.</p>\n<p>However, as with other Promise-style APIs in the browser, any time the WebAssembly application suspends it will not be ‘woken up’ again except by the browser’s task runner. This requires that the execution of the JavaScript code that started the WebAssembly computation itself returns to the browser.</p>\n<h3 id=\"can-i-use-jspi-to-suspend-javascript-programs%3F\" tabindex=\"-1\">Can I use JSPI to suspend JavaScript programs? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#can-i-use-jspi-to-suspend-javascript-programs%3F\">#</a></h3>\n<p>JavaScript already has a well developed mechanism for representing asynchronous computations: the <code>Promise</code> object and the <code>async</code> function notation. The JSPI is designed to integrate well with this but not to replace it.</p>\n<p>In particular, it is <em>not</em> permitted to cause JavaScript code to be suspended by using JSPI.</p>\n<h3 id=\"how-can-i-use-jspi-today%3F\" tabindex=\"-1\">How can I use JSPI today? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#how-can-i-use-jspi-today%3F\">#</a></h3>\n<p>JSPI is currently <a href=\"https://github.com/WebAssembly/meetings/blob/main/process/phases.md#4-standardize-the-feature-working-group\">phase 4</a> in the W3C WebAssembly WG. This means that the specification has been voted on by the W3C Wasm CG -- it is effectively standardized. In addition, it is available in Chrome 137, and in Firefox 139.</p>\n<p>JSPI is available for Chrome on Linux, MacOS, Windows and ChromeOS, on Intel and Arm platforms, both 64 bit and 32 bit.</p>\n<p>Below we show how you can use Emscripten to generate a WebAssembly module in C/C++ that uses JSPI. If your application involves a different language, not using Emscripten for example, then we suggest that you look at how the API works by reading the <a href=\"https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md\">proposal</a>.</p>\n<h2 id=\"a-small-demo\" tabindex=\"-1\">A small demo <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#a-small-demo\">#</a></h2>\n<p>To see all this working, let’s try a simple example. This C program computes Fibonacci in a spectacularly bad way: by asking JavaScript to do the addition, even worse by using JavaScript <code>Promise</code> objects to do it:<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn3\" id=\"fnref3\">[3]</a></sup></p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">long</span> <span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br /> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br />   <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br /> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><br />   <span class=\"token keyword\">return</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br /> <span class=\"token keyword\">return</span> <span class=\"token function\">promiseAdd</span><span class=\"token punctuation\">(</span><span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><span class=\"token comment\">// promise an addition</span><br /><span class=\"token function\">EM_ASYNC_JS</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span><span class=\"token punctuation\">,</span> promiseAdd<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">long</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> Promise<span class=\"token punctuation\">.</span><span class=\"token function\">resolve</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>The <code>promiseFib</code> function itself is a straightforward recursive version of the Fibonacci function. The intriguing part (from our point of view) is the definition of <code>promiseAdd</code> which does the addition of the two Fibonacci halves — using JSPI!.</p>\n<p>We use the <code>EM_ASYNC_JS</code> Emscripten macro to write down the <code>promiseFib</code> function as a JavaScript function within the body of our C program. Since addition does not normally involve Promises in JavaScript, we have to force it by constructing a <code>Promise</code>.</p>\n<p>The <code>EM_ASYNC_JS</code> macro generates all the necessary glue code so that we can use JSPI to access the Promise’s result as though it were a normal function.</p>\n<p>To compile our small demo, we use Emscripten’s <code>emcc</code> compiler:<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn4\" id=\"fnref4\">[4]</a></sup></p>\n<pre class=\"language-sh\"><code class=\"language-sh\">emcc <span class=\"token parameter variable\">-O3</span> badfib.c <span class=\"token parameter variable\">-o</span> b.html <span class=\"token parameter variable\">-s</span> JSPI</code></pre>\n<p>This compiles our program, creating a loadable HTML file (<code>b.html</code>). The most special command line option here is <code>-s JSPI</code>. This invokes the option to generate code that uses JSPI to interface with JavaScript imports that return Promises.</p>\n<p>If you load the generated <code>b.html</code> file into Chrome, then you should see output that approximates to:</p>\n<pre><code>fib(0) 0μs 0μs 0μs\nfib(1) 0μs 0μs 0μs\nfib(2) 0μs 0μs 3μs\nfib(3) 0μs 0μs 4μs\n…\nfib(15) 0μs 13μs 1225μs\n</code></pre>\n<p>This is simply a list of the first 15 Fibonacci numbers followed by the average time in microseconds it took to compute a single Fibonacci number. The three time values on each line refer to the time taken for a pure WebAssembly computation, for a mixed JavaScript/WebAssembly computation and the third number gives the time for a suspending version of the computation.</p>\n<p>Note that <code>fib(2)</code> is the smallest calculation that involves accessing a Promise, and, by the time <code>fib(15)</code> is computed, approximately 1000 calls to <code>promiseAdd</code> have been made. This suggests that the actual cost of a JSPI’d function is approximately 1μs — significantly higher than just adding two integers but much smaller than the milliseconds typically required for accessing an external I/O function.</p>\n<h2 id=\"using-jspi-to-load-code-lazily\" tabindex=\"-1\">Using JSPI to load code lazily <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#using-jspi-to-load-code-lazily\">#</a></h2>\n<p>In this next example we are going to look at what may be a somewhat surprising use of JSPI: dynamically loading code. The idea is to <code>fetch</code> a module that contains needed code, but to delay that until the needed function is first called.</p>\n<p>We need to use JSPI because APIs like <code>fetch</code> are inherently asynchronous in nature, but we want to be able to invoke them from arbitrary places in our application—in particular, from the middle of a call to a function that does not yet exist.</p>\n<p>The core idea is to replace a dynamically loaded function with a stub; this stub first of all loads the missing function code, replaces itself by the loaded code, and then calls the newly loaded code with the original arguments. Any subsequent call to the function goes directly to the loaded function. This strategy allows for an essentially transparent approach to dynamically loading code.</p>\n<p>The module we are going to load is fairly simple, it contains a function that returns <code>42</code>:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token comment\">// This is a simple provider of forty-two</span><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;emscripten.h></span></span><br /><br />EMSCRIPTEN_KEEPALIVE <span class=\"token keyword\">long</span> <span class=\"token function\">provide42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> <span class=\"token number\">42l</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>which is in a file called <code>p42.c</code>, and is compiled using Emscripten without building any ‘extras’:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">emcc p42.c <span class=\"token parameter variable\">-o</span> p42.wasm --no-entry -Wl,--import-memory</code></pre>\n<p>The <code>EMSCRIPTEN_KEEPALIVE</code> prefix is an Emscripten macro that makes sure that the function <code>provide42</code> is not eliminated even though it is not used within the code. This results in a WebAssembly module that contains the function that we want to load dynamically.</p>\n<p>The <code>-Wl,--import-memory</code> flag that we added to the build of <code>p42.c</code> is to ensure that it has access to the same memory that the main module has.<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn5\" id=\"fnref5\">[5]</a></sup></p>\n<p>In order to dynamically load code, we use the standard <code>WebAssembly.instantiateStreaming</code> API:</p>\n<pre class=\"language-js\"><code class=\"language-js\">WebAssembly<span class=\"token punctuation\">.</span><span class=\"token function\">instantiateStreaming</span><span class=\"token punctuation\">(</span><span class=\"token function\">fetch</span><span class=\"token punctuation\">(</span><span class=\"token string\">'p42.wasm'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>This expression uses <code>fetch</code> to locate the compiled Wasm module, <code>WebAssembly.instantiateStreaming</code> to compile the result of the fetch and to create an instantiated module from it. Both <code>fetch</code> and <code>WebAssembly.instantiateStreaming</code> return Promises; so we cannot simply access the result and extract our needed function. Instead we wrap this into an JSPI-style import using the <code>EM_ASYNC_JS</code> macro:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">EM_ASYNC_JS</span><span class=\"token punctuation\">(</span>fooFun<span class=\"token punctuation\">,</span> resolveFun<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><br />  console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span><span class=\"token char\">'loading promise42'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  LoadedModule <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>await WebAssembly<span class=\"token punctuation\">.</span><span class=\"token function\">instantiateStreaming</span><span class=\"token punctuation\">(</span><span class=\"token function\">fetch</span><span class=\"token punctuation\">(</span><span class=\"token char\">'p42.wasm'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>instance<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">addFunction</span><span class=\"token punctuation\">(</span>LoadedModule<span class=\"token punctuation\">.</span>exports<span class=\"token punctuation\">[</span><span class=\"token char\">'provide42'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Notice the <code>console.log</code> call, we will use it to make sure that our logic is correct.</p>\n<p>The <code>addFunction</code> is part of the Emscripten API, but to make sure that it is available for us at run-time, we have to inform <code>emcc</code> that it is a required dependency. We do that in the following line:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">EM_JS_DEPS</span><span class=\"token punctuation\">(</span>funDeps<span class=\"token punctuation\">,</span> <span class=\"token string\">\"$addFunction\"</span><span class=\"token punctuation\">)</span></code></pre>\n<p>In a situation where we want to dynamically load code, we would like to make sure that we don’t load code unnecessarily; in this case, we would like to make sure that subsequent calls to <code>provide42</code> will not trigger reloads. C has a simple feature that we can use for this: we don’t call <code>provide42</code> directly, but do so via a trampoline that will cause the function to be loaded, and then, just before actually invoking the function, change the trampoline to bypass itself. We can do this using an appropriate function pointer:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">extern</span> fooFun get42<span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">stub</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />  get42 <span class=\"token operator\">=</span> <span class=\"token function\">resolveFun</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br />fooFun get42 <span class=\"token operator\">=</span> stub<span class=\"token punctuation\">;</span></code></pre>\n<p>From the perspective of the rest of the program, the function that we want to call is called <code>get42</code>. Its initial implementation is via <code>stub</code>, which calls <code>resolveFun</code> to actually load the function. After the successful load, we change get42 to point to the newly loaded function – and call it.</p>\n<p>Our main function calls <code>get42</code> twice:<sup class=\"footnote-ref\"><a href=\"https://v8.dev/blog/jspi#fn6\" id=\"fnref6\">[6]</a></sup></p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"first call p42() = %ld\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"second call = %ld\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The result of running this in the browser is a log that looks like:</p>\n<pre><code>loading promise42\nfirst call p42() = 42\nsecond call = 42\n</code></pre>\n<p>Notice that the line <code>loading promise42</code> only appears once, whereas <code>get42</code> is actually called twice.</p>\n<p>This example demonstrates that JSPI can be used in some unexpected ways: loading code dynamically seems a long way from creating promises. Moreover, there are other ways of dynamically linking WebAssembly modules together; this is not intended to represent the definitive solution to that problem.</p>\n<p>We are definitely looking forward to seeing what you can do with this new capability! Join the discussion at the W3C WebAssembly Community Group <a href=\"https://github.com/WebAssembly/js-promise-integration\">repo</a>.</p>\n<h2 id=\"appendix-a%3A-complete-listing-of-badfib\" tabindex=\"-1\">Appendix A: Complete Listing of <code>badfib</code> <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#appendix-a%3A-complete-listing-of-badfib\">#</a></h2>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdlib.h></span></span><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;time.h></span></span><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;emscripten.h></span></span><br /><br /><span class=\"token keyword\">typedef</span> <span class=\"token keyword\">long</span> <span class=\"token punctuation\">(</span>testFun<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">define</span> <span class=\"token macro-name\">microSeconds</span> <span class=\"token expression\"><span class=\"token punctuation\">(</span><span class=\"token number\">1000000</span><span class=\"token punctuation\">)</span></span></span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">add</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">long</span> y<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> x <span class=\"token operator\">+</span> y<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token comment\">// Ask JS to do the addition</span><br /><span class=\"token function\">EM_JS</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span><span class=\"token punctuation\">,</span> jsAdd<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">long</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> x <span class=\"token operator\">+</span> y<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// promise an addition</span><br /><span class=\"token function\">EM_ASYNC_JS</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span><span class=\"token punctuation\">,</span> promiseAdd<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">long</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> Promise<span class=\"token punctuation\">.</span><span class=\"token function\">resolve</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">__attribute__</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>noinline<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><br /><span class=\"token keyword\">long</span> <span class=\"token function\">localFib</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br /> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br />   <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br /> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><br />   <span class=\"token keyword\">return</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br /> <span class=\"token keyword\">return</span> <span class=\"token function\">add</span><span class=\"token punctuation\">(</span><span class=\"token function\">localFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token function\">localFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">__attribute__</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>noinline<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><br /><span class=\"token keyword\">long</span> <span class=\"token function\">jsFib</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br />    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><br />    <span class=\"token keyword\">return</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">jsAdd</span><span class=\"token punctuation\">(</span><span class=\"token function\">jsFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token function\">jsFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">__attribute__</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>noinline<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><br /><span class=\"token keyword\">long</span> <span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br />    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>x<span class=\"token operator\">==</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><br />    <span class=\"token keyword\">return</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">promiseAdd</span><span class=\"token punctuation\">(</span><span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">-</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">runLocal</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> count<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">long</span> temp <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> ix <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> ix <span class=\"token operator\">&lt;</span> count<span class=\"token punctuation\">;</span> ix<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><br />    temp <span class=\"token operator\">+=</span> <span class=\"token function\">localFib</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> temp <span class=\"token operator\">/</span> count<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">runJs</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span><span class=\"token keyword\">int</span> count<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">long</span> temp <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> ix <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> ix <span class=\"token operator\">&lt;</span> count<span class=\"token punctuation\">;</span> ix<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><br />    temp <span class=\"token operator\">+=</span> <span class=\"token function\">jsFib</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> temp <span class=\"token operator\">/</span> count<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">runPromise</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">long</span> x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> count<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">long</span> temp <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> ix <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> ix <span class=\"token operator\">&lt;</span> count<span class=\"token punctuation\">;</span> ix<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><br />    temp <span class=\"token operator\">+=</span> <span class=\"token function\">promiseFib</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> temp <span class=\"token operator\">/</span> count<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">double</span> <span class=\"token function\">runTest</span><span class=\"token punctuation\">(</span>testFun test<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> limit<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> count<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />  <span class=\"token class-name\">clock_t</span> start <span class=\"token operator\">=</span> <span class=\"token function\">clock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token function\">test</span><span class=\"token punctuation\">(</span>limit<span class=\"token punctuation\">,</span> count<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token class-name\">clock_t</span> stop <span class=\"token operator\">=</span> <span class=\"token function\">clock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">double</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>stop <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> CLOCKS_PER_SEC<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">void</span> <span class=\"token function\">runTestSequence</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> step<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> limit<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> count<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> ix <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> ix <span class=\"token operator\">&lt;=</span> limit<span class=\"token punctuation\">;</span> ix <span class=\"token operator\">+=</span> step<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />    <span class=\"token keyword\">double</span> light <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token function\">runTest</span><span class=\"token punctuation\">(</span>runLocal<span class=\"token punctuation\">,</span> ix<span class=\"token punctuation\">,</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> microSeconds<span class=\"token punctuation\">;</span><br />    <span class=\"token keyword\">double</span> jsTime <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token function\">runTest</span><span class=\"token punctuation\">(</span>runJs<span class=\"token punctuation\">,</span> ix<span class=\"token punctuation\">,</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> microSeconds<span class=\"token punctuation\">;</span><br />    <span class=\"token keyword\">double</span> promiseTime <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token function\">runTest</span><span class=\"token punctuation\">(</span>runPromise<span class=\"token punctuation\">,</span> ix<span class=\"token punctuation\">,</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> count<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> microSeconds<span class=\"token punctuation\">;</span><br />    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"fib(%d) %gμs %gμs %gμs %gμs\\n\"</span><span class=\"token punctuation\">,</span>ix<span class=\"token punctuation\">,</span> light<span class=\"token punctuation\">,</span> jsTime<span class=\"token punctuation\">,</span> promiseTime<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>promiseTime <span class=\"token operator\">-</span> jsTime<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">}</span><br /><br />EMSCRIPTEN_KEEPALIVE <span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">int</span> step <span class=\"token operator\">=</span>  <span class=\"token number\">1</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> limit <span class=\"token operator\">=</span> <span class=\"token number\">15</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">int</span> count <span class=\"token operator\">=</span> <span class=\"token number\">1000</span><span class=\"token punctuation\">;</span><br />  <span class=\"token function\">runTestSequence</span><span class=\"token punctuation\">(</span>step<span class=\"token punctuation\">,</span> limit<span class=\"token punctuation\">,</span> count<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"appendix-b%3A-listing-of-u42.c-and-p42.c\" tabindex=\"-1\">Appendix B: Listing of <code>u42.c</code> and <code>p42.c</code> <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#appendix-b%3A-listing-of-u42.c-and-p42.c\">#</a></h2>\n<p>The <code>u42.c</code> C code represents the main part of our dynamic loading example:</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span><br /><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;emscripten.h></span></span><br /><br /><span class=\"token keyword\">typedef</span> <span class=\"token keyword\">long</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>fooFun<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token comment\">// promise a function</span><br /><span class=\"token function\">EM_ASYNC_JS</span><span class=\"token punctuation\">(</span>fooFun<span class=\"token punctuation\">,</span> resolveFun<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span><br />  console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span><span class=\"token char\">'loading promise42'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  LoadedModule <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>await WebAssembly<span class=\"token punctuation\">.</span><span class=\"token function\">instantiateStreaming</span><span class=\"token punctuation\">(</span><span class=\"token function\">fetch</span><span class=\"token punctuation\">(</span><span class=\"token char\">'p42.wasm'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>instance<span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">addFunction</span><span class=\"token punctuation\">(</span>LoadedModule<span class=\"token punctuation\">.</span>exports<span class=\"token punctuation\">[</span><span class=\"token char\">'provide42'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br /><span class=\"token function\">EM_JS_DEPS</span><span class=\"token punctuation\">(</span>funDeps<span class=\"token punctuation\">,</span> <span class=\"token string\">\"$addFunction\"</span><span class=\"token punctuation\">)</span><br /><br /><span class=\"token keyword\">extern</span> fooFun get42<span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">long</span> <span class=\"token function\">stub</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  get42 <span class=\"token operator\">=</span> <span class=\"token function\">resolveFun</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token keyword\">return</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><br /><br />fooFun get42 <span class=\"token operator\">=</span> stub<span class=\"token punctuation\">;</span><br /><br /><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"first call p42() = %ld\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />  <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"second call = %ld\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token function\">get42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The <code>p42.c</code> code is the dynamically loaded module.</p>\n<pre class=\"language-c\"><code class=\"language-c\"><span class=\"token macro property\"><span class=\"token directive-hash\">#</span><span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;emscripten.h></span></span><br /><br />EMSCRIPTEN_KEEPALIVE <span class=\"token keyword\">long</span> <span class=\"token function\">provide42</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> <span class=\"token number\">42l</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<!-- Footnotes themselves at the bottom. -->\n<h2 id=\"notes\" tabindex=\"-1\">Notes <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi#notes\">#</a></h2>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>If a WebAssembly application is suspended more than once, subsequent suspensions will return to the browser's event loop and will not be directly visible to the web application. <a href=\"https://v8.dev/blog/jspi#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>For the technically curious, see <a href=\"https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md\">the WebAssembly proposal for JSPI</a>. <a href=\"https://v8.dev/blog/jspi#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>Note: we include the complete program below, in Appendix A. <a href=\"https://v8.dev/blog/jspi#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>Note: you need a version of Emscripten that is ≥ 3.1.61. <a href=\"https://v8.dev/blog/jspi#fnref4\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>We do not need this flag for our specific example, but you would likely need it for anything bigger. <a href=\"https://v8.dev/blog/jspi#fnref5\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn6\" class=\"footnote-item\"><p>The complete program is shown in Appendix B. <a href=\"https://v8.dev/blog/jspi#fnref6\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/jspi"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "WebAssembly JSPI has a new API",
    "partialText": "<p>WebAssembly’s JavaScript Promise Integration (JSPI) API has a new API, available in Chrome release M126. We talk about what has changed, how to use it with Emscripten, and what is the roadmap for JSPI.</p>\n<p>JSPI is an API that allows WebAssembly applications that use <em>sequential</em> APIs to access Web APIs that are <em>asynchronous</em>. Many Web APIs are crafted in terms of JavaScript <code>Promise</code> objects: instead of immediately performing the requested operation, they return a <code>Promise</code> to do so. On the other hand, many applications compiled to WebAssembly come from the C/C++ universe, which is dominated by APIs that block the caller until they are completed.</p>\n<p>JSPI hooks into the Web architecture to allow a WebAssembly application to be suspended when the <code>Promise</code> is returned and resumed when the <code>Promise</code> is resolved.</p>\n<p>You can find out more about JSPI and how to use it <a href=\"https://v8.dev/blog/jspi\">in this blog post</a> and in the <a href=\"https://github.com/WebAssembly/js-promise-integration\">specification</a>.</p>\n<h2 id=\"what-is-new%3F\" tabindex=\"-1\">What is new? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#what-is-new%3F\">#</a></h2>\n<h3 id=\"the-end-of-suspender-objects\" tabindex=\"-1\">The end of <code>Suspender</code> objects <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#the-end-of-suspender-objects\">#</a></h3>\n<p>In January 2024, the Stacks sub-group of the Wasm CG <a href=\"https://github.com/WebAssembly/meetings/blob/297ac8b5ac00e6be1fe33b1f4a146cc7481b631d/stack/2024/stack-2024-01-29.md\">voted</a> to amend the API for JSPI. Specifically, instead of an explicit <code>Suspender</code> object, we will use the JavaScript/WebAssembly boundary as the delimiter for determining what computations are suspended.</p>\n<p>The difference is fairly small but potentially significant: when a computation is to be suspended, it is the most recent call into a wrapped WebAssembly export that determines the 'cut point' for what is suspended.</p>\n<p>The implication of this is that a developer using JSPI has a little less control over that cut point. On the other hand, not having to explicitly manage <code>Suspender</code> objects makes the API significantly easier to use.</p>\n<h3 id=\"no-more-webassembly.function\" tabindex=\"-1\">No more <code>WebAssembly.Function</code> <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#no-more-webassembly.function\">#</a></h3>\n<p>Another change is to the style of the API. Instead of characterizing JSPI wrappers in terms of the <code>WebAssembly.Function</code> constructor, we provide specific functions and constructors.</p>\n<p>This has a number of benefits:</p>\n<ul>\n<li>It removes dependency on the <a href=\"https://github.com/WebAssembly/js-types\"><em>Type Reflection</em> Proposal</a>.</li>\n<li>It makes tooling for JSPI simpler: the new API functions no longer need to refer explicitly to the WebAssembly types of functions.</li>\n</ul>\n<p>This change is enabled by the decision to no longer have explicitly referenced <code>Suspender</code> objects.</p>\n<h3 id=\"returning-without-suspending\" tabindex=\"-1\">Returning without suspending <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#returning-without-suspending\">#</a></h3>\n<p>A third change refers to the behavior of suspending calls. Instead of always suspending when calling a JavaScript function from a suspending import, we only suspend when the JavaScript function actually returns a <code>Promise</code>.</p>\n<p>This change, while apparently going against the <a href=\"https://www.w3.org/2001/tag/doc/promises-guide#accepting-promises\">recommendations</a> of the W3C TAG, represents a safe optimization for JSPI users. It is safe because JSPI is actually taking on the role of a <em>caller</em> to a function that returns a <code>Promise</code>.</p>\n<p>This change will likely have minimal impact on most applications; however, some applications will see a notable benefit by avoiding unnecessary trips to the browser's event loop.</p>\n<h3 id=\"the-new-api\" tabindex=\"-1\">The new API <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#the-new-api\">#</a></h3>\n<p>The API is straightforward: there is a function that takes a function exported from a WebAssembly module and converts it into a function that returns a <code>Promise</code>:</p>\n<pre class=\"language-js\"><code class=\"language-js\">Function Webassembly<span class=\"token punctuation\">.</span><span class=\"token function\">promising</span><span class=\"token punctuation\">(</span>Function wsFun<span class=\"token punctuation\">)</span></code></pre>\n<p>Note that even if the argument is typed as a JavaScript <code>Function</code>, it is actually restricted to WebAssembly functions.</p>\n<p>On the suspending side, there's a new class <code>WebAssembly.Suspending</code>, together with a constructor that takes a JavaScript function as an argument. In WebIDL, this is written as follows:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">interface</span> <span class=\"token class-name\">Suspending</span><span class=\"token punctuation\">{</span><br />  <span class=\"token function\">constructor</span> <span class=\"token punctuation\">(</span>Function fun<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Note that this API has an asymmetric feel to it: there's have a function that takes a WebAssembly function and returns a new promising (<em>sic</em>) function; whereas to mark a suspending function, you enclose it in a <code>Suspending</code> object. This reflects a deeper reality about what is happening under the hood.</p>\n<p>The suspending behavior of an import is intrinsically part of the <em>call</em> to the import: i.e., some function inside the instantiated module calls the import and suspends as a result.</p>\n<p>On the other hand, the <code>promising</code> function takes a regular WebAssembly function and returns a new one that can respond to being suspended and which returns a <code>Promise</code>.</p>\n<h3 id=\"using-the-new-api\" tabindex=\"-1\">Using the new API <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#using-the-new-api\">#</a></h3>\n<p>If you are an Emscripten user, then using the new API will typically involve no changes to your code. You must be using a version of Emscripten that is at least 3.1.61, and you must be using a version of Chrome that is at least 126.0.6478.17 (Chrome M126).</p>\n<p>If you are rolling your own integration, then your code should be significantly simpler. In particular, it is no longer necessary to have code that stores the passed-in <code>Suspender</code> object (and retrieve it when calling the import). You can simply use regular sequential code within the WebAssembly module.</p>\n<h3 id=\"the-old-api\" tabindex=\"-1\">The old API <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#the-old-api\">#</a></h3>\n<p>The old API will continue to operate at least until October 29, 2024 (Chrome M128). After that, we plan on removing the old API.</p>\n<p>Note that Emscripten itself will no longer support the old API as of version 3.1.61.</p>\n<h3 id=\"detecting-which-api-is-in-your-browser\" tabindex=\"-1\">Detecting which API is in your browser <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#detecting-which-api-is-in-your-browser\">#</a></h3>\n<p>Changing APIs should never be taken lightly. We are able to do so in this case because JSPI itself is still provisional. There is a simple way that you can test to see which API is enabled in your browser:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">function</span> <span class=\"token function\">oldAPI</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> WebAssembly<span class=\"token punctuation\">.</span>Suspender<span class=\"token operator\">!=</span><span class=\"token keyword\">undefined</span><br /><span class=\"token punctuation\">}</span><br /><br /><span class=\"token keyword\">function</span> <span class=\"token function\">newAPI</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">return</span> WebAssembly<span class=\"token punctuation\">.</span>Suspending<span class=\"token operator\">!=</span><span class=\"token keyword\">undefined</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>The <code>oldAPI</code> function returns true if the old JSPI API is enabled in your browser, and the <code>newAPI</code> function returns true if the new JSPI API is enabled.</p>\n<h2 id=\"what-is-happening-with-jspi%3F\" tabindex=\"-1\">What is happening with JSPI? <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#what-is-happening-with-jspi%3F\">#</a></h2>\n<h3 id=\"implementation-aspects\" tabindex=\"-1\">Implementation aspects <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#implementation-aspects\">#</a></h3>\n<p>The biggest change to JSPI that we are working on is actually invisible to most programmers: so-called growable stacks.</p>\n<p>The current implementation of JSPI is based on allocating stacks of a fixed size. In fact, the allocated stacks are rather large. This is because we have to be able to accommodate arbitrary WebAssembly computations which may require deep stacks to handle recursion properly.</p>\n<p>However, this is not a sustainable strategy: we would like to support applications with millions of suspended coroutines; this is not possible if each stack is 1MB in size.</p>\n<p>Growable stacks refers to a stack allocation strategy that allows a WebAssembly stack to grow as needed. That way, we can start with very small stacks for those applications that only need small stack space, and grow the stack when the application runs out of space (otherwise known as stack overflow).</p>\n<p>There are several potential techniques for implementing growable stacks. One that we are investigating is segmented stacks. A segmented stack consists of a chain of stack regions — each of which has a fixed size, but different segments may have different sizes.</p>\n<p>Note that while we may be solving the stack overflow issue for coroutines, we are not planning to make the main or central stack growable. Thus, if your application runs out of stack space, growable stacks will not fix your problem unless you use JSPI.</p>\n<h3 id=\"the-standards-process\" tabindex=\"-1\">The standards process <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-newapi#the-standards-process\">#</a></h3>\n<p>As of publication, there is an active <a href=\"https://v8.dev/blog/jspi-ot\">origin trial for JSPI</a>. The new API will be live during the remainder of the origin trial — available with Chrome M126.</p>\n<p>The previous API will also be available during the origin trial; however, it is planned to be retired shortly after Chrome M128.</p>\n<p>After that, the main thrust for JSPI revolves around the standardization process. JSPI is currently (at publication time) in phase 3 of the W3C Wasm CG process. The next step, i.e., moving to phase 4, marks the crucial adoption of JSPI as a standard API for the JavaScript and WebAssembly ecosystems.</p>\n<p>We would like to know what you think about these changes to JSPI! Join the discussion at the <a href=\"https://github.com/WebAssembly/js-promise-integration\">W3C WebAssembly Community Group repo</a>.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/jspi-newapi"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "The V8 Sandbox",
    "partialText": "<p>After almost three years since the <a href=\"https://docs.google.com/document/d/1FM4fQmIhEqPG8uGp5o9A-mnPB5BOeScZYpkHjo0KKA8/edit?usp=sharing\">initial design document</a> and <a href=\"https://github.com/search?q=repo%3Av8%2Fv8+%5Bsandbox%5D&amp;type=commits&amp;s=committer-date&amp;o=desc\">hundreds of CLs</a> in the meantime, the V8 Sandbox — a lightweight, in-process sandbox for V8 — has now progressed to the point where it is no longer considered an experimental security feature. Starting today, the <a href=\"https://g.co/chrome/vrp/#v8-sandbox-bypass-rewards\">V8 Sandbox is included in Chrome's Vulnerability Reward Program</a> (VRP). While there are still a number of issues to resolve before it becomes a strong security boundary, the VRP inclusion is an important step in that direction. Chrome 123 could therefore be considered to be a sort of &quot;beta&quot; release for the sandbox. This blog post uses this opportunity to discuss the motivation behind the sandbox, show how it prevents memory corruption in V8 from spreading within the host process, and ultimately explain why it is a necessary step towards memory safety.</p>\n<h1 id=\"motivation\" tabindex=\"-1\">Motivation <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#motivation\">#</a></h1>\n<p>Memory safety remains a relevant problem: all Chrome exploits <a href=\"https://docs.google.com/spreadsheets/d/1lkNJ0uQwbeC1ZTRrxdtuPLCIl7mlUreoKfSIgajnSyY/edit?usp=sharing\">caught in the wild in the last three years</a> (2021 – 2023) started out with a memory corruption vulnerability in a Chrome renderer process that was exploited for remote code execution (RCE). Of these, 60% were vulnerabilities in V8. However, there is a catch: V8 vulnerabilities are rarely &quot;classic&quot; memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither <a href=\"https://www.cisa.gov/resources-tools/resources/case-memory-safe-roadmaps\">switching to a memory safe language</a>, such as Rust, nor using current or future hardware memory safety features, such as <a href=\"https://newsroom.arm.com/memory-safety-arm-memory-tagging-extension\">memory tagging</a>, can help with the security challenges faced by V8 today.</p>\n<p>To understand why, consider a highly simplified, hypothetical JavaScript engine vulnerability: the implementation of <code>JSArray::fizzbuzz()</code>, which replaces values in the array that are divisible by 3 with &quot;fizz&quot;, divisible by 5 with &quot;buzz&quot;, and divisible by both 3 and 5 with &quot;fizzbuzz&quot;. Below is an implementation of that function in C++. <code>JSArray::buffer_</code> can be thought of as a <code>JSValue*</code>, that is, a pointer to an array of JavaScript values, and <code>JSArray::length_</code> contains the current size of that buffer.</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"> <span class=\"token number\">1.</span> <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> index <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> index <span class=\"token operator\">&lt;</span> length_<span class=\"token punctuation\">;</span> index<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br /> <span class=\"token number\">2.</span>     JSValue js_value <span class=\"token operator\">=</span> buffer_<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span><br /> <span class=\"token number\">3.</span>     <span class=\"token keyword\">int</span> value <span class=\"token operator\">=</span> <span class=\"token function\">ToNumber</span><span class=\"token punctuation\">(</span>js_value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token function\">int_value</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /> <span class=\"token number\">4.</span>     <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>value <span class=\"token operator\">%</span> <span class=\"token number\">15</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br /> <span class=\"token number\">5.</span>         buffer_<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token function\">JSString</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"fizzbuzz\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /> <span class=\"token number\">6.</span>     <span class=\"token keyword\">else</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>value <span class=\"token operator\">%</span> <span class=\"token number\">5</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br /> <span class=\"token number\">7.</span>         buffer_<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token function\">JSString</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"buzz\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /> <span class=\"token number\">8.</span>     <span class=\"token keyword\">else</span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>value <span class=\"token operator\">%</span> <span class=\"token number\">3</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><br /> <span class=\"token number\">9.</span>         buffer_<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token function\">JSString</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"fizz\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token number\">10.</span> <span class=\"token punctuation\">}</span></code></pre>\n<p>Seems simple enough? However, there's a somewhat subtle bug here: the <code>ToNumber</code> conversion in line 3 can have side effects as it may invoke user-defined JavaScript callbacks. Such a callback could then shrink the array, thereby causing an out-of-bounds write afterwards. The following JavaScript code would likely cause memory corruption:</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">let</span> array <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">Array</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token keyword\">let</span> evil <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span> <span class=\"token punctuation\">[</span>Symbol<span class=\"token punctuation\">.</span>toPrimitive<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> array<span class=\"token punctuation\">.</span>length <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span> <span class=\"token keyword\">return</span> <span class=\"token number\">15</span><span class=\"token punctuation\">;</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span><br />array<span class=\"token punctuation\">.</span><span class=\"token function\">push</span><span class=\"token punctuation\">(</span>evil<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><span class=\"token comment\">// At index 100, the @@toPrimitive callback of |evil| is invoked in</span><br /><span class=\"token comment\">// line 3 above, shrinking the array to length 1 and reallocating its</span><br /><span class=\"token comment\">// backing buffer. The subsequent write (line 5) goes out-of-bounds.</span><br />array<span class=\"token punctuation\">.</span><span class=\"token function\">fizzbuzz</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Note that this vulnerability could occur both in hand-written runtime code (as in the example above) or in machine code generated at runtime by an optimizing just-in-time (JIT) compiler (if the function was implemented in JavaScript instead). In the former case, the programmer would conclude that an explicit bounds-check for the store operations is not necessary as that index has just been accessed. In the latter case, it would be the compiler drawing the same incorrect conclusion during one of its optimization passes (for example <a href=\"https://en.wikipedia.org/wiki/Partial-redundancy_elimination\">redundancy elimination</a> or <a href=\"https://en.wikipedia.org/wiki/Bounds-checking_elimination\">bounds-check elimination)</a> because it doesn't model the side effects of <code>ToNumber()</code> correctly.</p>\n<p>While this is an artificially simple bug (this specific bug pattern has become mostly extinct by now due to improvements in fuzzers, developer awareness, and researcher attention), it is still useful to understand why vulnerabilities in modern JavaScript engines are difficult to mitigate in a generic way. Consider the approach of using a memory safe language such as Rust, where it is the compiler's responsibility to guarantee memory safety. In the above example, a memory safe language would likely prevent this bug in the hand-written runtime code used by the interpreter. However, it would <em>not</em> prevent the bug in any just-in-time compiler as the bug there would be a logic issue, not a &quot;classic&quot; memory corruption vulnerability. Only the code generated by the compiler would actually cause any memory corruption. Fundamentally, the issue is that <em>memory safety cannot be guaranteed by the compiler if a compiler is directly part of the attack surface</em>.</p>\n<p>Similarly, disabling the JIT compilers would also only be a partial solution: historically, roughly half of the bugs discovered and exploited in V8 affected one of its compilers while the rest were in other components such as runtime functions, the interpreter, the garbage collector, or the parser. Using a memory-safe language for these components and removing JIT compilers could work, but would significantly reduce the engine's performance (ranging, depending on the type of workload, from 1.5–10× or more for computationally intensive tasks).</p>\n<p>Now consider instead popular hardware security mechanisms, in particular <a href=\"https://googleprojectzero.blogspot.com/2023/08/mte-as-implemented-part-1.html\">memory tagging</a>. There are a number of reasons why memory tagging would similarly not be an effective solution. For example, CPU side channels, which can <a href=\"https://security.googleblog.com/2021/03/a-spectre-proof-of-concept-for-spectre.html\">easily be exploited from JavaScript</a>, could be abused to leak tag values, thereby allowing an attacker to bypass the mitigation. Furthermore, due to <a href=\"https://v8.dev/blog/pointer-compression\">pointer compression</a>, there is currently no space for the tag bits in V8's pointers. As such, the entire heap region would have to be tagged with the same tag, making it impossible to detect inter-object corruption. As such, while memory tagging <a href=\"https://googleprojectzero.blogspot.com/2023/08/mte-as-implemented-part-2-mitigation.html\">can be very effective on certain attack surfaces</a>, it is unlikely to represent much of a hurdle for attackers in the case of JavaScript engines.</p>\n<p>In summary, modern JavaScript engines tend to contain complex, 2nd-order logic bugs which provide powerful exploitation primitives. These cannot be effectively protected by the same techniques used for typical memory-corruption vulnerabilities. However, nearly all vulnerabilities found and exploited in V8 today have one thing in common: the eventual memory corruption necessarily happens inside the V8 heap because the compiler and runtime (almost) exclusively operate on V8 <code>HeapObject</code> instances. This is where the sandbox comes into play.</p>\n<h1 id=\"the-v8-(heap)-sandbox\" tabindex=\"-1\">The V8 (Heap) Sandbox <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#the-v8-(heap)-sandbox\">#</a></h1>\n<p>The basic idea behind the sandbox is to isolate V8's (heap) memory such that any memory corruption there cannot &quot;spread&quot; to other parts of the process' memory.</p>\n<p>As a motivating example for the sandbox design, consider the <a href=\"https://en.wikipedia.org/wiki/User_space_and_kernel_space\">separation of user- and kernel space</a> in modern operating systems. Historically, all applications and the operating system's kernel would share the same (physical) memory address space. As such, any memory error in a user application could bring down the whole system by, for example, corrupting kernel memory. On the other hand, in a modern operating system, each userland application has its own dedicated (virtual) address space. As such, any memory error is limited to the application itself, and the rest of the system is protected. In other words, a faulty application can crash itself but not affect the rest of the system. Similarly, the V8 Sandbox attempts to isolate the untrusted JavaScript/WebAssembly code executed by V8 such that a bug in V8 does not affect the rest of the hosting process.</p>\n<p>In principle, <a href=\"https://docs.google.com/document/d/12MsaG6BYRB-jQWNkZiuM3bY8X2B2cAsCMLLdgErvK4c/edit?usp=sharing\">the sandbox could be implemented with hardware support</a>: similar to the userland-kernel split, V8 would execute some mode-switching instruction when entering or leaving sandboxed code, which would cause the CPU to be unable to access out-of-sandbox memory. In practice, no suitable hardware feature is available today, and the current sandbox is therefore implemented purely in software.</p>\n<p>The basic idea behind the <a href=\"https://docs.google.com/document/d/1FM4fQmIhEqPG8uGp5o9A-mnPB5BOeScZYpkHjo0KKA8/edit?usp=sharing\">software-based sandbox</a> is to replace all data types that can access out-of-sandbox memory with &quot;sandbox-compatible&quot; alternatives. In particular, all pointers (both to objects on the V8 heap or elsewhere in memory) and 64-bit sizes must be removed as an attacker could corrupt them to subsequently access other memory in the process. This implies that memory regions such as the stack cannot be inside the sandbox as they must contain pointers (for example return addresses) due to hardware and OS constraints. As such, with the software-based sandbox, only the V8 heap is inside the sandbox, and the overall construction is therefore not unlike the <a href=\"https://webassembly.org/docs/security/\">sandboxing model used by WebAssembly</a>.</p>\n<p>To understand how this works in practice, it is useful to look at the steps an exploit has to perform after corrupting memory. The goal of an RCE exploit would typically be to perform a privilege escalation attack, for example by executing shellcode or performing a return-oriented programming (ROP)-style attack. For either of these, the exploit will first want the ability to read and write arbitrary memory in the process, for example to then corrupt a function pointer or place a ROP-payload somewhere in memory and pivot to it. Given a bug that corrupts memory on the V8 heap, an attacker would therefore look for an object such as the following:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">JSArrayBuffer</span><span class=\"token operator\">:</span> <span class=\"token base-clause\"><span class=\"token keyword\">public</span> <span class=\"token class-name\">JSObject</span></span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />    byte<span class=\"token operator\">*</span> buffer_<span class=\"token punctuation\">;</span><br />    size_t size_<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Given this, the attacker would then either corrupt the buffer pointer or the size value to construct an arbitrary read/write primitive. This is the step that the sandbox aims to prevent. In particular, with the sandbox enabled, and assuming that the referenced buffer is located inside the sandbox, the above object would now become:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">JSArrayBuffer</span><span class=\"token operator\">:</span> <span class=\"token base-clause\"><span class=\"token keyword\">public</span> <span class=\"token class-name\">JSObject</span></span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />    sandbox_ptr_t buffer_<span class=\"token punctuation\">;</span><br />    sandbox_size_t size_<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Where <code>sandbox_ptr_t</code> is a 40-bit offset (in the case of a 1TB sandbox) from the base of the sandbox. Similarly, <code>sandbox_size_t</code> is a &quot;sandbox-compatible&quot; size, <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-internal.h;l=231;drc=5bdda7d5edcac16b698026b78c0eec6d179d3573\">currently limited to 32GB</a>.<br />\nAlternatively, if the referenced buffer was located outside of the sandbox, the object would instead become:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">JSArrayBuffer</span><span class=\"token operator\">:</span> <span class=\"token base-clause\"><span class=\"token keyword\">public</span> <span class=\"token class-name\">JSObject</span></span> <span class=\"token punctuation\">{</span><br />  <span class=\"token keyword\">private</span><span class=\"token operator\">:</span><br />    external_ptr_t buffer_<span class=\"token punctuation\">;</span><br /><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre>\n<p>Here, an <code>external_ptr_t</code> references the buffer (and its size) through a pointer table indirection (not unlike the <a href=\"https://en.wikipedia.org/wiki/File_descriptor\">file descriptor table of a unix kernel</a> or a <a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Table\">WebAssembly.Table</a>) which provides memory safety guarantees.</p>\n<p>In both cases, an attacker would find themselves unable to &quot;reach out&quot; of the sandbox into other parts of the address space. Instead, they would first need an additional vulnerability: a V8 Sandbox bypass. The following image summarizes the high-level design, and the interested reader can find more technical details about the sandbox in the design documents linked from <a href=\"https://chromium.googlesource.com/v8/v8.git/+/refs/heads/main/src/sandbox/README.md\"><code>src/sandbox/README.md</code></a>.</p>\n<figure><img src=\"https://v8.dev/_img/sandbox/sandbox.svg\" alt=\"\" width=\"567\" height=\"458\" loading=\"lazy\" /><figcaption>A high-level diagram of the sandbox design</figcaption></figure>\n<p>Solely converting pointers and sizes to a different representation is not quite sufficient in an application as complex as V8 and there are <a href=\"https://issues.chromium.org/hotlists/4802478\">a number of other issues</a> that need to be fixed. For example, with the introduction of the sandbox, code such as the following suddenly becomes problematic:</p>\n<pre class=\"language-cpp\"><code class=\"language-cpp\">std<span class=\"token double-colon punctuation\">::</span>vector<span class=\"token operator\">&lt;</span>std<span class=\"token double-colon punctuation\">::</span>string<span class=\"token operator\">></span> <span class=\"token class-name\">JSObject</span><span class=\"token double-colon punctuation\">::</span><span class=\"token function\">GetPropertyNames</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />    <span class=\"token keyword\">int</span> num_properties <span class=\"token operator\">=</span> <span class=\"token function\">TotalNumberOfProperties</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    std<span class=\"token double-colon punctuation\">::</span>vector<span class=\"token operator\">&lt;</span>std<span class=\"token double-colon punctuation\">::</span>string<span class=\"token operator\">></span> <span class=\"token function\">properties</span><span class=\"token punctuation\">(</span>num_properties<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br /><br />    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> <span class=\"token function\">NumberOfInObjectProperties</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><br />        properties<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token function\">GetNameOfInObjectProperty</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><br />    <span class=\"token punctuation\">}</span><br /><br />    <span class=\"token comment\">// Deal with the other types of properties</span><br />    <span class=\"token comment\">// ...</span></code></pre>\n<p>This code makes the (reasonable) assumption that the number of properties stored directly in a JSObject must be less than the total number of properties of that object. However, assuming these numbers are simply stored as integers somewhere in the JSObject, an attacker could corrupt one of them to break this invariant. Subsequently, the access into the (out-of-sandbox) <code>std::vector</code> would go out of bounds. Adding an explicit bounds check, for example with an <a href=\"https://chromium.googlesource.com/v8/v8.git/+/0deeaf5f593b98d6a6a2bb64e3f71d39314c727c\"><code>SBXCHECK</code></a>, would fix this.</p>\n<p>Encouragingly, nearly all &quot;sandbox violations&quot; discovered so far are like this: trivial (1st order) memory corruption bugs such as use-after-frees or out-of-bounds accesses due to lack of a bounds check. Contrary to the 2nd order vulnerabilities typically found in V8, these sandbox bugs could actually be prevented or mitigated by the approaches discussed earlier. In fact, the particular bug above would already be mitigated today due to <a href=\"http://issues.chromium.org/issues/40228527\">Chrome's libc++ hardening</a>. As such, the hope is that in the long run, the sandbox becomes a <strong>more defensible security boundary</strong> than V8 itself. While the currently available data set of sandbox bugs is very limited, the VRP integration launching today will hopefully help produce a clearer picture of the type of vulnerabilities encountered on the sandbox attack surface.</p>\n<h2 id=\"performance\" tabindex=\"-1\">Performance <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#performance\">#</a></h2>\n<p>One major advantage of this approach is that it is fundamentally cheap: the overhead caused by the sandbox comes mostly from the pointer table indirection for external objects (costing roughly one additional memory load) and to a lesser extent from the use of offsets instead of raw pointers (costing mostly just a shift+add operation, which is very cheap). The current overhead of the sandbox is therefore only around 1% or less on typical workloads (measured using the <a href=\"https://browserbench.org/Speedometer3.0/\">Speedometer</a> and <a href=\"https://browserbench.org/JetStream/\">JetStream</a> benchmark suites). This allows the V8 Sandbox to be enabled by default on compatible platforms.</p>\n<h2 id=\"testing\" tabindex=\"-1\">Testing <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#testing\">#</a></h2>\n<p>A desirable feature for any security boundary is testability: the ability to manually and automatically test that the promised security guarantees actually hold in practice. This requires a clear attacker model, a way to &quot;emulate&quot; an attacker, and ideally a way of automatically determining when the security boundary has failed. The V8 Sandbox fulfills all of these requirements:</p>\n<ol>\n<li><strong>A clear attacker model:</strong> it is assumed that an attacker can read and write arbitrarily inside the V8 Sandbox. The goal is to prevent memory corruption outside of the sandbox.</li>\n<li><strong>A way to emulate an attacker:</strong> V8 provides a &quot;memory corruption API&quot; when built with the <code>v8_enable_memory_corruption_api = true</code> flag. This emulates the primitives obtained from typical V8 vulnerabilities and in particular provides full read- and write access inside the sandbox.</li>\n<li><strong>A way to detect &quot;sandbox violations&quot;:</strong> V8 provides a &quot;sandbox testing&quot; mode (enabled via either <code>--sandbox-testing</code> or <code>--sandbox-fuzzing</code>) which installs a <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/sandbox/testing.cc;l=425;drc=97b7d0066254778f766214d247b65d01f8a81ebb\">signal handler</a> that determines if a signal such as <code>SIGSEGV</code> represents a violation of the sandbox's security guarantees.</li>\n</ol>\n<p>Ultimately, this allows the sandbox to be integrated into Chrome's VRP program and be fuzzed by specialized fuzzers.</p>\n<h2 id=\"usage\" tabindex=\"-1\">Usage <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#usage\">#</a></h2>\n<p>The V8 Sandbox must be enabled/disabled at build time using the <code>v8_enable_sandbox</code> build flag. It is (for technical reasons) not possible to enable/disable the sandbox at runtime. The V8 Sandbox requires a 64-bit system as it needs to reserve a large amount of virtual address space, currently one terabyte.</p>\n<p>The V8 Sandbox has already been enabled by default on 64-bit (specifically x64 and arm64) versions of Chrome on Android, ChromeOS, Linux, macOS, and Windows for roughly the last two years. Even though the sandbox was (and still is) not feature complete, this was mainly done to ensure that it does not cause stability issues and to collect real-world performance statistics. Consequently, recent V8 exploits already had to work their way past the sandbox, providing helpful early feedback on its security properties.</p>\n<h1 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"bookmark\" href=\"https://v8.dev/blog/sandbox#conclusion\">#</a></h1>\n<p>The V8 Sandbox is a new security mechanism designed to prevent memory corruption in V8 from impacting other memory in the process. The sandbox is motivated by the fact that current memory safety technologies are largely inapplicable to optimizing JavaScript engines. While these technologies fail to prevent memory corruption in V8 itself, they can in fact protect the V8 Sandbox attack surface. The sandbox is therefore a necessary step towards memory safety.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/sandbox"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "WebAssembly JSPI is going to origin trial",
    "partialText": "<p>WebAssembly’s JavaScript Promise Integration (JSPI) API is entering an origin trial, with Chrome release M123. What that means is that you can test whether you and your users can benefit from this new API.</p>\n<p>JSPI is an API that allows so-called sequential code – that has been compiled to WebAssembly – to access Web APIs that are <em>asynchronous</em>. Many Web APIs are crafted in terms of JavaScript <code>Promise</code>s: instead of immediately performing the requested operation they return a <code>Promise</code> to do so. When the action is finally performed, the browser’s task runner invokes any callbacks with the Promise. JSPI hooks into this architecture to allow a WebAssembly application to be suspended when the <code>Promise</code> is returned and resumed when the <code>Promise</code> is resolved.</p>\n<p>You can find out more about JSPI and how to use it <a href=\"https://v8.dev/blog/jspi\">here</a> and the specification itself is <a href=\"https://github.com/WebAssembly/js-promise-integration\">here</a>.</p>\n<h2 id=\"requirements\" tabindex=\"-1\">Requirements <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-ot#requirements\">#</a></h2>\n<p>Apart from registering for an origin trial, you will also need to generate the appropriate WebAssembly and JavaScript. If you are using Emscripten, then this is straightforward. You should ensure that you are using at least version 3.1.47.</p>\n<h2 id=\"registering-for-the-origin-trial\" tabindex=\"-1\">Registering for the origin trial <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-ot#registering-for-the-origin-trial\">#</a></h2>\n<p>JSPI is still pre-release; it is going through a standardization process and will not be fully released until we get to phase 4 of that process. To use it today, you can set a flag in the Chrome browser; or, you can apply for an origin trial token that will allow your users to access it without having to set the flag themselves.</p>\n<p>To register you can go <a href=\"https://developer.chrome.com/origintrials/#/register_trial/1603844417297317889\">here</a>, make sure to follow the registration signup process. To find out more about origin trials in general, <a href=\"https://developer.chrome.com/docs/web-platform/origin-trials\">this</a> is a good starting place.</p>\n<h2 id=\"some-potential-caveats\" tabindex=\"-1\">Some potential caveats <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-ot#some-potential-caveats\">#</a></h2>\n<p>There have been some <a href=\"https://github.com/WebAssembly/js-promise-integration/issues\">discussions</a> in the WebAssembly community about some aspects of the JSPI API. As a result, there are some changes indicated, which will take time to fully work their way through the system. We anticipate that these changes will be <em>soft launched</em>: we will share the changes as they become available, however, the existing API will be maintained until at least the end of the origin trial.</p>\n<p>In addition, there are some known issues that are unlikely to be fully addressed during the origin trial period:</p>\n<p>For applications that intensively create spawned-off computations, the performance of a wrapped sequence (i.e., using JSPI to access an asynchronous API) may suffer. This is because the resources used when creating the wrapped call are not cached between calls; we rely on garbage collection to clear up the stacks that are created.<br />\nWe currently allocate a fixed size stack for each wrapped call. This stack is necessarily large in order to accommodate complex applications. However, it also means that an application that has a large number of simple wrapped calls <em>in flight</em> may experience memory pressure.</p>\n<p>Neither of these issues are likely to impede experimentation with JSPI; we expect them to be addressed before JSPI is officially released.</p>\n<h2 id=\"feedback\" tabindex=\"-1\">Feedback <a class=\"bookmark\" href=\"https://v8.dev/blog/jspi-ot#feedback\">#</a></h2>\n<p>Since JSPI is a standards-track effort, we prefer that any issues and feedback be shared <a href=\"https://github.com/WebAssembly/js-promise-integration/issues\">here</a>. However, bug reports can be raised at the standard Chrome bug reporting <a href=\"https://issues.chromium.org/new\">site</a>. If you suspect a problem with code generation, use <a href=\"https://github.com/emscripten-core/emscripten/issues\">this</a> to report an issue.</p>\n<p>Finally, we would like to hear about any benefits that you uncovered. Use the <a href=\"https://github.com/WebAssembly/js-promise-integration/issues\">issue tracker</a> to share your experience.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/jspi-ot"
  },
  {
    "publisherId": "v8",
    "publisherName": "V8 Blog",
    "specTitle": "V8 엔진",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://v8.dev/blog.atom",
    "title": "Static Roots: Objects with Compile-Time Constant Addresses",
    "partialText": "<p>Did you ever wonder where <code>undefined</code>, <code>true</code>, and other core JavaScript objects come from? These objects are the atoms of any user defined object and need to be there first. V8 calls them immovable immutable roots and they live in their own heap – the read-only heap. Since they are used constantly, quick access is crucial. And what could be quicker than correctly guessing their memory address at compile time?</p>\n<p>As an example, consider the extremely common <code>IsUndefined</code> <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-value.h?q=symbol:%5Cbv8::Value::IsUndefined%5Cb%20case:yes\">API function</a>. Instead of having to look up the address of the <code>undefined</code> object for reference, what if we could simply check if an object's pointer ends in, say, <code>0x61</code> to know if it is undefined. This is exactly what the V8’s <em>static roots</em> feature achieves. This post explores the hurdles we had to take to get there. The feature landed in Chrome 111 and brought performance benefits across the whole VM, particularly speeding up C++ code and builtin functions.</p>\n<h2 id=\"bootstrapping-the-read-only-heap\" tabindex=\"-1\">Bootstrapping the Read-Only Heap <a class=\"bookmark\" href=\"https://v8.dev/blog/static-roots#bootstrapping-the-read-only-heap\">#</a></h2>\n<p>Creating the read-only objects takes some time, so V8 creates them at compile time. To compile V8, first a minimal proto-V8 binary called <code>mksnapshot</code> is compiled. This one creates all the shared read-only objects as well as the native code of builtin functions and writes them into a snapshot. Then, the actual V8 binary is compiled and bundled with the snapshot. To start V8 the snapshot is loaded into memory and we can immediately start using its content. The following diagram shows the simplified build process for the standalone <code>d8</code> binary.</p>\n<figure><img src=\"https://v8.dev/_img/static-roots/static-roots1.svg\" alt=\"\" width=\"927\" height=\"241\" loading=\"lazy\" /></figure>\n<p>Once <code>d8</code> is up and running all the read-only objects have their fixed place in memory and never move. When we JIT code, we can e.g., directly refer to <code>undefined</code> by its address. However, when building the snapshot and when compiling the C++ for libv8 the address is not known yet. It depends on two things unknown at build time. First, the binary layout of the read-only heap and second, where in the memory space that read-only heap is located.</p>\n<h2 id=\"how-to-predict-addresses%3F\" tabindex=\"-1\">How to Predict Addresses? <a class=\"bookmark\" href=\"https://v8.dev/blog/static-roots#how-to-predict-addresses%3F\">#</a></h2>\n<p>V8 uses <a href=\"https://v8.dev/blog/pointer-compression\">pointer compression</a>. Instead of full 64 bit addresses we refer to objects by a 32 bit offset into a 4GB region of memory. For many operations such as property loads or comparisons, the 32 bit offset into that cage is all that is needed to uniquely identify an object. Therefore our second problem — not knowing where in the memory space the read-only heap is placed — is not actually a problem. We simply place the read-only heap at the start of every pointer compression cage thus giving it a known location. For instance of all objects in V8’s heap, <code>undefined</code> always has the smallest compressed address, starting at 0x61 bytes. That’s how we know that if the lower 32 bits of any JS object’s full address are 0x61, then it must be <code>undefined</code>.</p>\n<p>This is already useful, but we want to be able to use this address in the snapshot and in libv8 – a seemingly circular problem. However, if we ensure that <code>mksnapshot</code> deterministically creates a bit identical read-only heap, then we can re-use these addresses across builds. To use them in libv8 itself, we basically build V8 twice:</p>\n<figure><img src=\"https://v8.dev/_img/static-roots/static-roots2.svg\" alt=\"\" width=\"1060\" height=\"396\" loading=\"lazy\" /></figure>\n<p>The first time round calling <code>mksnapshot</code> the only artifact produced is a file that contains the <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/roots/static-roots.h\">addresses</a> relative to the cage base of every object in the read-only heap. In the second stage of the build we compile libv8 again and a flag ensures that whenever we refer to <code>undefined</code> we literally use <code>cage_base + StaticRoot::kUndefined</code> instead; the static offset of <code>undefined</code> of course being defined in the static-roots.h file. In many cases this will allow the C++ compiler creating libv8 and the builtins compiler in <code>mksnapshot</code> to create much more efficient code as the alternative is to always load the address from a global array of root objects. We end up with a <code>d8</code> binary where the compressed address of <code>undefined</code> is hardcoded to be <code>0x61</code>.</p>\n<p>Well, morally this is how everything works, but practically we only build V8 once – ain’t nobody got time for this. The generated static-roots.h file is cached in the source repository and only needs to be recreated if we change the layout of the read-only heap.</p>\n<h2 id=\"further-applications\" tabindex=\"-1\">Further Applications <a class=\"bookmark\" href=\"https://v8.dev/blog/static-roots#further-applications\">#</a></h2>\n<p>Speaking of practicalities, static roots enable even more optimizations. For instance we have since grouped common objects together allowing us to implement some operations as range checks over their addresses. For instance all string maps (i.e., the <a href=\"https://v8.dev/docs/hidden-classes\">hidden-class</a> meta objects describing the layout of different string types) are next to each other, hence an object is a string if its map has a compressed address between <code>0xdd</code> and <code>0x49d</code>. Or, truthy objects must have an address that is at least <code>0xc1</code>.</p>\n<p>Not everything is about the performance of JITed code in V8. As this project has shown, a relatively small change to the C++ code can have significant impact too. For instance Speedometer 2, a benchmark which exercises the V8 API and the interaction between V8 and its embedder, gained about 1% in score on an M1 CPU thanks to static roots.</p>",
    "date": "2026-02-14T18:15:34.849Z",
    "url": "https://v8.dev/blog/static-roots"
  }
]