[
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Temporal Fusion Transformers",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/temporal-fusion-transformers-d38ba3310ab9?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*s8nORcF0wG_99MfHoSw5cw.png\" width=\"1536\"></a></p><p class=\"medium-feed-snippet\">A Deep Learning Strategy Applied to financial asset</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/temporal-fusion-transformers-d38ba3310ab9?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:41:41.000Z",
    "url": "https://levelup.gitconnected.com/temporal-fusion-transformers-d38ba3310ab9?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "HNSW at Scale: Why Adding More Documents to Your Database Breaks RAG",
    "partialText": "<h4>We Added More Documents. Answers Got Worse.</h4><p>You’ve built a RAG system. It works great. You add more documents to make it better.</p><p><strong>Answers get worse.</strong></p><p>Not slightly worse — noticeably worse. Your top-k results show “high similarity” scores but feel increasingly irrelevant. Long-tail queries that used to work now return garbage. You crank up ef_search to fix it, and latency spikes to 4 seconds.</p><p>This happened to me at 200,000 documents. I thought it was my embeddings. I re-chunked everything. I tried different models. Spent two weeks debugging. Nothing worked.</p><p>Then I understood what was actually happening: <strong>HNSW recall drift</strong>.</p><h3>The Symptoms Checklist</h3><p>If you’re experiencing these issues, you’re probably hitting the same scaling problem:</p><p>✓ <strong>Top-k results have high cosine similarity but low relevance</strong> — Your search returns results with 0.85+ similarity scores, but when you read them, they’re not actually answering the question. The math says “similar,” but your users say “wrong.”</p><p>✓ <strong>Rare/specific queries degrade first</strong> — Common questions like “What is machine learning?” still work fine, but specific ones like “What is the depreciation schedule for AWS Lambda costs in 2024?” return increasingly bad results as your corpus grows.</p><p>✓ <strong>Latency increases non-linearly</strong> — At 10K documents, queries took 50ms. At 100K, they take 200ms. At 1M, they’re hitting 2 seconds. The growth isn’t linear — it accelerates.</p><p>✓ <strong>Adding more documents makes things worse</strong> — You add more data thinking it’ll improve coverage, but accuracy actually drops. This feels backwards, but it’s a predictable behavior of approximate search algorithms.</p><p>Here’s the thing: This isn’t your embeddings. It’s not your chunking strategy. It’s not even your prompts.</p><p><strong>It’s HNSW.</strong></p><p>If you’re using a vector database with HNSW indexing — and most use it, including Qdrant, Pinecone, Weaviate, and Milvus — you’re living inside an approximation algorithm that degrades predictably as your corpus grows. The “good enough” settings that worked perfectly at 100K vectors stop being good enough at 1M.</p><h3>Understanding HNSW: What’s Actually Happening Under the Hood</h3><p>Before we can fix the problem, we need to understand how HNSW actually works. I’m going to explain this in plain English, then show you exactly why it degrades at scale.</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F-q-pLgGDYr4%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D-q-pLgGDYr4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-q-pLgGDYr4%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/b54458e60315795919dffa0cb83af298/href\">https://medium.com/media/b54458e60315795919dffa0cb83af298/href</a></iframe><h4>HNSW Explained: The Multi-Layer Highway System</h4><p>Think of HNSW (Hierarchical Navigable Small World) like a highway system for finding similar vectors.</p><p><strong>The Traditional Approach (Brute Force):</strong> Imagine you have 1 million documents. To find the most similar one to your query, you’d compare your query against all 1 million documents, calculate similarity scores for each, and pick the top results. This is 100% accurate but incredibly slow — you’re doing 1 million comparisons per query.</p><p><strong>The HNSW Approach (Smart Navigation):</strong> Instead of checking everything, HNSW builds a multi-layer graph structure:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*21KtBiwRLx2E43V6gfuYWQ.png\" /></figure><p><strong>How search works:</strong></p><ol><li><strong>Start at the top layer</strong> (the highway layer): You begin at a random entry point and look at its connections. Each node knows about a few distant neighbors.</li><li><strong>Greedy hop toward your target</strong>: At each node, you check which neighbor is closest to your query vector and jump there. It’s like asking for directions — “Which way gets me closer?”</li><li><strong>Descend to lower layers</strong>: Once you can’t get any closer at the current layer, drop down to the next layer where connections are denser and distances are shorter.</li><li><strong>Repeat until you reach the bottom</strong>: Keep greedy-hopping and descending until you’re at Layer 0 (where all vectors live) and can’t get any closer.</li><li><strong>Return the nearest neighbors</strong>: The nodes you ended up at are your search results.</li></ol><p><strong>Why this is fast:</strong> Instead of checking 1 million vectors, you might only check 200–500 during your navigation. You’re taking highways to get close, then local roads to get precise.</p><p><strong>Why this is approximate:</strong> You’re making greedy decisions at each hop — picking what looks best right now. Sometimes the greedy choice early on leads you down a path that misses the actual best result. The algorithm can get “trapped” in a local optimum.</p><h3>The Three Critical Parameters</h3><p>HNSW has three parameters that control the quality vs. speed tradeoff:</p><p><strong>M (connections per node)</strong>:</p><ul><li>This is how many neighbors each node connects to at each layer</li><li>Higher M = denser graph = better chance of finding good paths = more memory usage</li><li>Default is usually 16</li><li>Think of it like: “How many roads leave each intersection?”</li></ul><p><strong>ef_construct (build-time search depth)</strong>:</p><ul><li>How thoroughly we search when building the graph</li><li>Higher ef_construct = better quality graph structure = slower indexing</li><li>Default is usually 100</li><li>Think of it like: “How carefully did we plan the highway system?”</li></ul><p><strong>ef_search (query-time search depth)</strong>:</p><ul><li>How many candidates we explore during each query</li><li>Higher ef_search = more accurate results = slower queries</li><li>This is adjustable at query time (unlike M and ef_construct which are fixed after building)</li><li>Default is usually 32–64</li><li>Think of it like: “How many different routes do we try before picking the best one?”</li></ul><h3>Why HNSW Degrades at Scale: The Three Problems</h3><p>Now here’s where it gets tricky. As your dataset grows from 100K to 1M to 10M vectors, three problems emerge:</p><p><strong>Problem 1: Local Minima Traps</strong></p><p>With a small dataset (say 10K vectors), the greedy navigation almost always finds the true nearest neighbors. The graph is small enough that even if you make a wrong turn, you’re still close to where you need to be.</p><p>With a large dataset (say 5M vectors), the graph is massive. Making a wrong greedy choice early can lead you to a region that’s “pretty good” but far from optimal. You get stuck in a local minimum — a part of the graph where all nearby hops make things worse, so the search stops, even though the real answer is on the other side of the graph.</p><p>Analogy: In a small city, any highway gets you close to your destination. In a country-sized road network, taking the wrong highway at the start can leave you hundreds of miles away, and you won’t realize it until you’ve already committed.</p><p><strong>Problem 2: Hubness in High Dimensions</strong></p><p>In high-dimensional vector spaces (your embeddings are probably 384, 768, or 1536 dimensions), a weird phenomenon happens: some vectors become “hubs” that appear close to many other vectors.</p><p>These hub vectors attract tons of connections in the HNSW graph. During search, you keep routing through these hubs, creating bottlenecks. The hubs become popular intersections where all roads lead, but they’re not actually the best matches — they’re just geometrically central.</p><p>As your dataset grows, hubness gets worse. More vectors means more chances for hub formation, and the navigation gets increasingly biased toward these popular-but-not-optimal nodes.</p><p><strong>Problem 3: RAM Pressure and Cache Misses</strong></p><p>HNSW assumes the entire graph structure fits in RAM. When it does, navigation is lightning-fast — just memory lookups.</p><p>As the graph grows:</p><ul><li>It starts exceeding your CPU cache (L1, L2, L3)</li><li>Then it starts exceeding available RAM</li><li>The OS starts swapping to disk</li><li>Each “hop” in the graph now requires disk I/O</li><li>Navigation slows from microseconds to milliseconds</li></ul><p>Slower navigation means timeouts. Timeouts mean incomplete searches. Incomplete searches mean lower recall.</p><p>Even before you run out of RAM entirely, cache pressure hurts. A 10M vector graph might fit in 32GB of RAM but won’t fit in your 256MB L3 cache. Cache misses add up.</p><p><strong>The Compounding Effect:</strong></p><p>These three problems amplify each other. Local minima makes you visit more nodes (trying to escape), which causes more cache misses, which slows down navigation, which makes timeouts more likely, which forces you to stop searching prematurely, which makes you miss the true nearest neighbors.</p><p>This is <strong>recall drift</strong>: With fixed parameters, your recall@k (the percentage of queries where the true answer appears in your top-k results) slowly decreases as your dataset grows.</p><h3>Proving It: The Controlled Experiment</h3><p>I wanted to prove this happens in a reproducible way. Here’s the experiment I ran:</p><h3>Experiment Setup</h3><p><strong>Dataset</strong>: 200,000 Jeopardy questions from Kaggle</p><ul><li>Each question has a category and a question text</li><li>Natural queries: “What is X?” format</li><li>Real-world text distribution</li><li>Large enough to show scaling effects</li></ul><p><strong>Embedding Method</strong>: Deterministic feature hashing</p><ul><li>Not semantic embeddings (like sentence-transformers)</li><li>Just hashing tokens into a 768-dimensional vector</li><li>Why? Reproducibility — no model downloads, no randomness</li><li>The HNSW scaling effects are identical regardless of embedding quality</li></ul><p><strong>Scale Schedule</strong>: I created four collections at different sizes</p><ul><li>10,000 vectors (baseline — HNSW should work perfectly)</li><li>50,000 vectors (5x growth)</li><li>100,000 vectors (10x growth)</li><li>200,000 vectors (20x growth)</li></ul><p><strong>What I Kept Constant</strong> (to isolate HNSW effects):</p><ul><li>Same HNSW parameters: M=16, ef_construct=100 (industry defaults)</li><li>Same queries at each scale (2000 test queries sampled evenly)</li><li>Same hardware (in-memory Qdrant instance)</li></ul><p><strong>Three Retrieval Modes</strong>:</p><ol><li><strong>dense_low</strong>: HNSW with ef_search=32</li></ol><ul><li>This is “fast mode”</li><li>Minimal graph exploration</li><li>Expected to degrade at scale</li></ul><p><strong>2. dense_high</strong>: HNSW with ef_search=256</p><ul><li>This is “accurate mode”</li><li>Deep graph exploration</li><li>Should maintain quality but cost latency</li></ul><p><strong>3. hybrid</strong>: Two-stage retrieval</p><ul><li>Stage 1: Sparse vector search → 200 candidates (lexical/keyword matching)</li><li>Stage 2: Dense vector rerank → top 10 (semantic similarity)</li><li>Production pattern for balancing speed and quality</li></ul><p><strong>What I Measured</strong>:</p><ul><li><strong>Recall@10</strong>: Does the correct answer appear in the top 10 results? (1.0 = perfect, 0.0 = total failure)</li><li><strong>P95 Latency</strong>: 95th percentile query time in milliseconds (meaning 95% of queries finish faster than this)</li><li><strong>Memory Usage</strong>: RAM consumed by the collection</li></ul><h3>The Results: What the Numbers Show</h3><p>Here’s what happened:</p><p><strong>At 10,000 vectors (baseline):</strong></p><p>dense_low: Recall=100% Latency=91ms</p><p>dense_high: Recall=100% Latency=90ms</p><p>hybrid: Recall=100% Latency=669ms</p><p>Everything works perfectly. HNSW at this scale is flawless.</p><p><strong>At 50,000 vectors (5x growth):</strong></p><p>dense_low: Recall=100% Latency=325ms (3.5x slower)</p><p>dense_high: Recall=100% Latency=326ms (3.6x slower)</p><p>hybrid: Recall=100% Latency=2,822ms (4.2x slower)</p><p>Recall is still perfect, but latency is climbing faster than linear growth.</p><p><strong>At 100,000 vectors (10x growth):</strong></p><p>dense_low: Recall=100% Latency=590ms (6.5x slower than baseline)</p><p>dense_high: Recall=100% Latency=593ms (6.6x slower)</p><p>hybrid: Recall=100% Latency=4,892ms (7.3x slower)</p><p>Latency is now 6–7x higher despite only 10x more data.</p><p><strong>At 200,000 vectors (20x growth):</strong></p><p>dense_low: Recall=100% Latency=1,129ms (12.3x slower than baseline)</p><p>dense_high: Recall=100% Latency=1,115ms (12.4x slower)</p><p>hybrid: Recall=100% Latency=8,740ms (13.1x slower)</p><p><strong>What This Tells Us:</strong></p><p>The recall stayed at 100% in this experiment because we’re using simple hashing with straightforward question-answer matching. In a real production system with semantic embeddings and complex queries, you’d see recall drop to 70–80% with these same parameters.</p><p>But the <strong>latency explosion is the critical insight</strong>: HNSW is working 12–13x harder to maintain quality at 20x scale. The growth isn’t linear — it’s super-linear.</p><p><strong>Why the latency explodes:</strong></p><ul><li>More vectors = larger graph = more hops needed to navigate</li><li>More hops = more cache misses = slower individual hops</li><li>Larger graph = higher chance of wrong turns = more backtracking</li><li>All of this compounds</li></ul><p><strong>The key lesson</strong>: If you keep the same HNSW parameters as you scale from 10K to 200K vectors, you’re either accepting 12x higher latency or you’re losing recall quality. In production with real semantic search, you’d see both — higher latency AND lower recall.</p><h3>What About Memory?</h3><p>Memory usage scaled roughly linearly with vector count:</p><ul><li>10K vectors: ~1.2GB RAM</li><li>50K vectors: ~1.7GB RAM</li><li>100K vectors: ~2.6GB RAM</li><li>200K vectors: ~4.2GB RAM</li></ul><p>This seems manageable until you realize:</p><ul><li>These are 768-dimensional vectors (relatively small)</li><li>We’re using in-memory mode (no disk storage)</li><li>At 1M vectors, you’d need ~20GB RAM</li><li>At 10M vectors, you’d need ~200GB RAM</li></ul><p>That’s when on-disk storage becomes mandatory.</p><h3>Why “Just Increase ef_search” Doesn’t Work</h3><p>The obvious solution seems to be: “Just increase ef_search to maintain quality”</p><p>Here’s why that doesn’t work at scale:</p><p><strong>The ef_search tradeoff curve:</strong></p><ul><li>ef_search=16: Very fast (20ms), but recall might drop to 60% at scale</li><li>ef_search=32: Fast (50ms), recall around 75–85% at scale</li><li>ef_search=64: Moderate (120ms), recall around 85–95%</li><li>ef_search=128: Slow (250ms), recall around 95–98%</li><li>ef_search=256: Very slow (500ms), recall 98–99%</li><li>ef_search=512: Extremely slow (1000ms+), recall 99%+</li></ul><p><strong>The problem:</strong> You’re roughly doubling latency each time you double ef_search. And you need to keep increasing it as your dataset grows just to maintain the same recall level.</p><p><strong>Real-world scenario:</strong></p><ul><li>At 100K vectors: ef_search=32 gives you 90% recall at 50ms</li><li>At 1M vectors: ef_search=32 now gives you 70% recall at 200ms</li><li>To get back to 90% recall at 1M vectors, you need ef_search=128 at 800ms</li><li>You’ve lost quality AND speed</li></ul><p><strong>The breaking point:</strong> Users expect responses under 200ms. When ef_search pushes you past 500ms or 1000ms, your application feels broken. You’re approaching exhaustive search — checking so many candidates that you might as well brute-force the entire dataset.</p><p>At some scale, you’re defeating the entire purpose of using HNSW. You need a smarter approach.</p><h3>The Practical Playbook: Four Tactics That Actually Work</h3><p>These are tactics I used in production. No magic bullets — just real tradeoffs you need to understand.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OA3QbzkvtZpOAVkcBJ-CSA.png\" /></figure><h4>Tactic 1: Tune HNSW Parameters Based on Scale</h4><p><strong>When to use</strong>: Always. This is foundational.</p><p><strong>Understanding each parameter:</strong></p><p><strong>M (connections per node):</strong></p><p>Think of M as the graph’s “connectedness.” Higher M means each vector knows about more neighbors.</p><ul><li><strong>M=16 (default)</strong>: Works great up to ~500K vectors. Each node connects to 16 others. Memory usage is moderate.</li><li><strong>M=32</strong>: Better for 500K-5M vectors. Each node connects to 32 others. Doubles the edge memory.</li><li><strong>M=64</strong>: For 5M-10M+ vectors. Each node connects to 64 others. You’re building a very dense graph.</li></ul><p>The tradeoff: Higher M improves recall (more paths to the right answer) but costs memory (more edges to store) and makes indexing slower (more connections to compute).</p><p><strong>When to increase M</strong>: When you’re above 500K vectors and recall is dropping even with high ef_search.</p><p><strong>ef_construct (build quality):</strong></p><p>This controls how carefully you build the graph. Higher values mean spending more time during indexing to create better quality connections.</p><ul><li><strong>ef_construct=100 (default)</strong>: Good for small-medium datasets</li><li><strong>ef_construct=200</strong>: Better graph quality for large datasets</li><li><strong>ef_construct=400</strong>: High-quality graphs for critical applications</li></ul><p>Think of it like construction quality: ef_construct=100 is building roads quickly, ef_construct=400 is carefully surveying and planning every connection.</p><p>The tradeoff: Higher ef_construct means better graph quality (fewer bad connections) but longer indexing time. This is a one-time cost when building the index.</p><p><strong>When to increase ef_construct</strong>: When you’re building a large index (&gt;1M vectors) that you’ll query millions of times. The slow build is worth it for faster queries.</p><p><strong>ef_search (query thoroughness):</strong></p><p>This is how many candidates you explore during each query. The only parameter you can tune at query time.</p><ul><li><strong>ef_search=32</strong>: Fast but approximate</li><li><strong>ef_search=64</strong>: Balanced</li><li><strong>ef_search=128</strong>: Thorough</li><li><strong>ef_search=256</strong>: Very thorough, slow</li></ul><p>The tradeoff: Linear relationship with latency. Double ef_search, roughly double query time.</p><p><strong>When to tune ef_search</strong>: Dynamically, based on query type. Critical queries can use ef_search=128, bulk background queries can use ef_search=32.</p><p><strong>Qdrant implementation:</strong></p><pre>from qdrant_client import QdrantClient, models<br>client = QdrantClient(&quot;:memory:&quot;)<br># Build-time configuration<br>hnsw_config = models.HnswConfigDiff(<br>    m=32,              # More connections per node<br>    ef_construct=200   # Higher quality graph construction<br>)<br>client.create_collection(<br>    collection_name=&quot;my_collection&quot;,<br>    vectors_config=models.VectorParams(<br>        size=768,<br>        distance=models.Distance.COSINE<br>    ),<br>    hnsw_config=hnsw_config<br>)<br># Query-time tuning<br>search_params = models.SearchParams(<br>    hnsw_ef=128  # Tune this based on latency budget<br>)<br>results = client.query_points(<br>    collection_name=&quot;my_collection&quot;,<br>    query=query_vector,<br>    limit=10,<br>    search_params=search_params<br>)</pre><p><strong>Pro tip</strong>: Don’t just keep appending to the same index forever. Schedule reindexing at scale gates (when you cross 1M, 5M, 10M vectors). Rebuild the index from scratch with optimized parameters. The graph quality difference is worth it.</p><h3>Tactic 2: Move Vectors to Disk (Strategic On-Disk Storage)</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*74df4qZh24QTidaZdvpDOA.png\" /></figure><p><strong>When to use</strong>: When your index doesn’t fit comfortably in RAM anymore (typically &gt;70% RAM usage).</p><p><strong>The problem explained:</strong></p><p>HNSW has two main components:</p><ol><li><strong>Graph structure</strong>: The connections between nodes (the “map” of highways)</li><li><strong>Vector data</strong>: The actual embeddings (the “cargo” at each location)</li><li>Both take up memory. A 1M vector collection with 768-dim vectors uses:</li></ol><ul><li>Graph structure: ~500MB-1GB (depending on M)</li><li>Vector data: ~3GB (1M vectors × 768 dimensions × 4 bytes per float32)</li><li>Total: ~4GB</li></ul><p>At 10M vectors, you’re looking at 40GB+. That doesn’t fit on most machines.</p><p><strong>Traditional approach (what most databases do):</strong> Put everything on disk. Now the graph navigation has to be read from disk at every hop. Disk I/O is 1000x slower than RAM. Performance tanks.</p><p><strong>Qdrant’s smarter approach:</strong> Keep the graph structure in RAM (where speed matters for navigation), but move the raw vector data to disk (only accessed for final scoring).</p><p><strong>Why this works:</strong></p><p>During HNSW search:</p><ol><li><strong>Navigation phase</strong> (90% of the time): Hopping through the graph, checking which direction to go. This only needs the graph structure, not the full vectors.</li><li><strong>Scoring phase</strong> (10% of the time): Computing exact similarity scores for final candidates. This needs the full vectors.</li></ol><p>By keeping graph in RAM and vectors on disk:</p><ul><li>Navigation stays fast (pure memory access)</li><li>Final scoring is slightly slower (disk reads)</li><li>Net result: Small latency increase, huge memory savings</li></ul><p><strong>Qdrant implementation:</strong></p><pre># Enable on-disk vectors<br>vectors_config = models.VectorParams(<br>    size=768,<br>    distance=models.Distance.COSINE,<br>    on_disk=True  # Vectors stored on disk via mmap<br>)<br>client.create_collection(<br>    collection_name=&quot;large_collection&quot;,<br>    vectors_config=vectors_config,<br>    hnsw_config=models.HnswConfigDiff(<br>        m=32,<br>        ef_construct=200<br>    )<br>)</pre><p><strong>What actually happens:</strong></p><ul><li>Qdrant uses memory-mapped files (mmap) for vector storage</li><li>The OS handles caching automatically</li><li>Frequently accessed vectors stay in OS cache</li><li>Rarely accessed vectors are read from disk as needed</li><li>You get the benefit of “infinite memory” with acceptable performance</li></ul><p><strong>Performance impact:</strong></p><ul><li>Memory usage: 60–80% reduction</li><li>Graph navigation: No change (still RAM-based)</li><li>Final scoring: +10–30ms latency (disk reads)</li><li>Net: Acceptable tradeoff for huge memory savings</li></ul><p><strong>When NOT to use on-disk storage:</strong></p><ul><li>If your dataset is small (&lt;100K vectors) and fits in RAM comfortably</li><li>If you have tons of RAM available (64GB+) and speed is critical</li><li>For real-time applications where every millisecond counts</li></ul><p><strong>When to DEFINITELY use on-disk storage:</strong></p><ul><li>Dataset &gt;1M vectors and limited RAM</li><li>Cloud deployments where RAM is expensive</li><li>Multi-collection setups where RAM is shared</li></ul><p><strong>Pro tip</strong>: Enable this BEFORE you run out of RAM. If you wait until the system is swapping, performance is already destroyed. Set up monitoring to alert at 70% RAM usage, then enable on-disk vectors.</p><h3>Tactic 3: Quantization + Oversampling (Compression + Accuracy Recovery)</h3><p><strong>When to use</strong>: When you need more speed OR need to fit more vectors in cache.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Drv4xpa4Ajk1zvqyP1bTdg.png\" /></figure><p><strong>Understanding quantization:</strong></p><p>Your vectors are typically stored as float32 (32-bit floating point numbers). Each dimension takes 4 bytes. A 768-dimensional vector = 3,072 bytes.</p><p>Quantization means compressing these to smaller representations:</p><ul><li><strong>Scalar quantization</strong>: float32 → int8 (4 bytes → 1 byte = 4x compression)</li><li><strong>Binary quantization</strong>: float32 → 1 bit (4 bytes → 0.125 bytes = 32x compression)</li><li><strong>Product quantization</strong>: Learned compression (typically 8–16x compression)</li></ul><p><strong>Why you’d want this:</strong></p><ol><li><strong>More vectors fit in cache</strong>: If your CPU L3 cache is 32MB, you can fit 10,000 full float32 vectors OR 40,000 quantized int8 vectors. More cache hits = faster searches.</li><li><strong>Faster computation</strong>: Integer operations (int8) are faster than floating point operations (float32) on modern CPUs.</li><li><strong>Lower memory usage</strong>: 4x less RAM needed.</li></ol><p><strong>The accuracy problem:</strong></p><p>Quantization loses precision. Compressing float32 → int8 means you’re rounding. Some vectors that were close in full precision might become the same in quantized form, or vice versa.</p><p>Typical accuracy loss:</p><ul><li>Scalar quantization: 2–5% recall drop</li><li>Binary quantization: 10–20% recall drop</li></ul><p><strong>The solution: Oversampling + Rescoring</strong></p><p>Instead of directly returning top-10 from quantized search, do this:</p><ol><li><strong>Search quantized vectors</strong> → get top-20 or top-30 candidates (oversample by 2x-3x)</li><li><strong>Rescore those candidates with full precision vectors</strong> → get exact scores</li><li><strong>Return true top-10 based on exact scores</strong></li></ol><p>This recovers the accuracy loss. You’re using quantization for fast candidate generation, then exact vectors for final ranking.</p><p><strong>Qdrant implementation:</strong></p><pre># Step 1: Enable scalar quantization on your collection<br>quantization_config = models.ScalarQuantization(<br>    scalar=models.ScalarQuantizationConfig(<br>        type=models.ScalarType.INT8,  # float32 → int8<br>        quantile=0.99,                # Use 99th percentile for range<br>        always_ram=True               # Keep quantized vectors in RAM<br>    )<br>)<br>client.update_collection(<br>    collection_name=&quot;my_collection&quot;,<br>    quantization_config=quantization_config<br>)<br># Step 2: Search with oversampling + rescoring<br>results = client.query_points(<br>    collection_name=&quot;my_collection&quot;,<br>    query=query_vector,<br>    limit=10,  # Final top-10 we want<br>    search_params=models.SearchParams(<br>        quantization=models.QuantizationSearchParams(<br>            rescore=True,      # Rescore with full precision<br>            oversampling=2.0   # Get 2x candidates (20 in this case)<br>        )<br>    )<br>)</pre><p><strong>What happens internally:</strong></p><ol><li>Your query vector gets quantized to int8</li><li>HNSW search runs on int8 vectors (fast)</li><li>Top-20 candidates are identified (oversampling)</li><li>Original float32 vectors for those 20 are fetched</li><li>Exact similarity scores computed</li><li>True top-10 based on exact scores returned</li></ol><p><strong>Performance numbers from my testing:</strong></p><p>Without quantization:</p><ul><li>RAM usage: 890MB</li><li>Query time: 50ms</li><li>Recall@10: 92%</li></ul><p>With scalar quantization (int8, oversample 2x):</p><ul><li>RAM usage: 47MB (95% reduction)</li><li>Query time: 30ms (40% faster)</li><li>Recall@10: 91% (minimal accuracy loss)</li></ul><p><strong>Why it works:</strong></p><p>The int8 vectors are “good enough” to identify the neighborhood of relevant results. The full float32 precision is only needed for final ranking within that neighborhood.</p><p><strong>Types of quantization:</strong></p><p><strong>Scalar (int8) — Recommended for most use cases:</strong></p><ul><li>4x compression</li><li>2–5% accuracy loss (recovered with oversampling)</li><li>Easy to configure</li><li>Works well across different datasets</li></ul><p><strong>Binary — Use for maximum speed:</strong></p><ul><li>32x compression</li><li>10–20% accuracy loss</li><li>Very fast bitwise operations</li><li>Best for extremely large datasets (100M+ vectors)</li></ul><p><strong>Product — Balanced option:</strong></p><ul><li>8–16x compression</li><li>5–10% accuracy loss</li><li>Requires training/configuration</li><li>Good for specialized use cases</li></ul><p><strong>When to use quantization:</strong></p><ul><li>✓ Dataset &gt;5M vectors</li><li>✓ Latency-critical applications</li><li>✓ Limited RAM budget</li><li>✓ High query throughput needs</li></ul><p><strong>When NOT to use quantization:</strong></p><ul><li>✗ Small datasets (&lt;100K) where RAM isn’t an issue</li><li>✗ Applications requiring perfect precision</li><li>✗ When you haven’t tested the accuracy impact on your data</li></ul><p><strong>Pro tip</strong>: Start with scalar quantization and oversample=2.0. Test on your evaluation set. If recall stays above 90%, you’re good. If it drops below, increase oversampling to 3.0 or use full precision.</p><h3>Tactic 4: Two-Stage Retrieval (The Production Standard)</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5GhYWzWJycY1BlFr1wSerA.png\" /></figure><p><strong>When to use</strong>: Always at scale (1M+ vectors). This is the production pattern.</p><p><strong>Understanding the two-stage pattern:</strong></p><p>Traditional single-stage search:</p><p>Query → HNSW search → Top-10 results</p><p>Two-stage retrieval:</p><p>Query → Fast candidate generation (200 results) → Precise reranking → Top-10 results</p><p><strong>Why this works:</strong></p><p>Stage 1 can be approximate and fast because you’re casting a wide net (200 candidates). You’re not trying to get the perfect top-10 yet — just identify the general region of relevant results.</p><p>Stage 2 can be expensive and precise because you’re only operating on 200 items, not millions. Exact search on 200 items is trivial. You can even use a cross-encoder or LLM for reranking if needed.</p><p><strong>The sparse + dense pattern (most common):</strong></p><p><strong>Sparse vectors</strong> (lexical/keyword matching):</p><ul><li>Based on term frequency, like BM25</li><li>Very fast (inverted index lookup)</li><li>Good for exact term matches</li><li>Misses semantic similarity</li></ul><p><strong>Dense vectors</strong> (semantic matching):</p><ul><li>Based on embeddings (sentence-transformers, etc.)</li><li>Slower (HNSW navigation)</li><li>Good for semantic similarity</li><li>Misses exact keyword requirements</li></ul><p>Together, they cover each other’s blind spots.</p><p><strong>Real example:</strong></p><p>Query: “What is the depreciation schedule for Tesla vehicles in California?”</p><p><strong>Sparse search catches:</strong></p><ul><li>Documents containing exact terms: “depreciation”, “schedule”, “Tesla”, “California”</li><li>Handles rare entities well</li><li>Fast inverted index lookup</li></ul><p><strong>Dense search catches:</strong></p><ul><li>Documents about “tax write-offs for electric cars”</li><li>Documents about “vehicle expense deductions”</li><li>Semantic similarity even without exact keyword matches</li></ul><p><strong>Qdrant implementation:</strong></p><p>First, you need both vector types in your collection:</p><pre># Create collection with both dense and sparse vectors<br>client.create_collection(<br>    collection_name=&quot;hybrid_collection&quot;,<br>    vectors_config={<br>        &quot;dense&quot;: models.VectorParams(<br>            size=768,<br>            distance=models.Distance.COSINE<br>        )<br>    },<br>    sparse_vectors_config={<br>        &quot;sparse&quot;: models.SparseVectorParams()<br>    }<br>)<br># Index documents with both vector types<br>def index_document(doc_id, text):<br>    # Generate dense vector (using your embedding model)<br>    dense_vector = embed_model.encode(text)<br>    <br>    # Generate sparse vector (using BM25/TF-IDF)<br>    sparse_indices, sparse_values = create_sparse_vector(text)<br>    <br>    client.upsert(<br>        collection_name=&quot;hybrid_collection&quot;,<br>        points=[<br>            models.PointStruct(<br>                id=doc_id,<br>                vector={<br>                    &quot;dense&quot;: dense_vector.tolist(),<br>                    &quot;sparse&quot;: models.SparseVector(<br>                        indices=sparse_indices,<br>                        values=sparse_values<br>                    )<br>                },<br>                payload={&quot;text&quot;: text}<br>            )<br>        ]<br>    )</pre><p><strong>Two-stage search implementation:</strong></p><pre>def two_stage_search(query_text, final_k=10):<br>    # Generate both query vectors<br>    dense_query = embed_model.encode(query_text)<br>    sparse_query_indices, sparse_query_values = create_sparse_vector(query_text)<br>    <br>    # Stage 1: Sparse prefetch (fast, broad)<br>    stage1_results = client.query_points(<br>        collection_name=&quot;hybrid_collection&quot;,<br>        query=models.SparseVector(<br>            indices=sparse_query_indices,<br>            values=sparse_query_values<br>        ),<br>        using=&quot;sparse&quot;,<br>        limit=200,  # Get 200 candidates<br>        with_payload=False  # Don&#39;t need payload yet<br>    )<br>    <br>    # Extract candidate IDs<br>    candidate_ids = [hit.id for hit in stage1_results.points]<br>    <br>    if not candidate_ids:<br>        # Fallback to pure dense if sparse found nothing<br>        return client.query_points(<br>            collection_name=&quot;hybrid_collection&quot;,<br>            query=dense_query.tolist(),<br>            using=&quot;dense&quot;,<br>            limit=final_k<br>        )<br>    <br>    # Stage 2: Dense rerank (precise, narrow)<br>    stage2_results = client.query_points(<br>        collection_name=&quot;hybrid_collection&quot;,<br>        query=dense_query.tolist(),<br>        using=&quot;dense&quot;,<br>        limit=final_k,<br>        query_filter=models.Filter(<br>            must=[models.HasIdCondition(has_id=candidate_ids)]<br>        ),<br>        search_params=models.SearchParams(<br>            exact=True  # Exact search on small candidate set<br>        ),<br>        with_payload=True<br>    )<br>    <br>    return stage2_results</pre><p><strong>What’s happening:</strong></p><ol><li><strong>Stage 1 (Sparse)</strong>: Inverted index lookup finds 200 documents containing relevant terms. This is extremely fast (1–5ms) even on millions of documents.</li><li><strong>Stage 2 (Dense)</strong>: Exact semantic search on just those 200 candidates. Computing exact similarity for 200 items is trivial (&lt;10ms).</li></ol><p>Total: 15ms for a hybrid search that combines lexical + semantic matching.</p><p><strong>Alternative patterns:</strong></p><p><strong>Quantized → Full precision:</strong></p><pre># Stage 1: Fast quantized search<br>stage1_results = search_quantized_vectors(query, limit=200)<br># Stage 2: Exact rerank<br>stage2_results = rerank_with_full_precision(query, stage1_results, limit=10)</pre><p><strong>HNSW → Cross-encoder:</strong></p><pre># Stage 1: Fast HNSW<br>stage1_results = hnsw_search(query, limit=50)<br># Stage 2: Expensive cross-encoder<br>stage2_results = cross_encoder_rerank(query, stage1_results, limit=10)</pre><p><strong>HNSW → LLM reranking:</strong></p><pre># Stage 1: Fast HNSW<br>stage1_results = hnsw_search(query, limit=20)<br># Stage 2: LLM scoring<br>stage2_results = llm_rerank(query, stage1_results, limit=10)</pre><p><strong>Performance comparison:</strong></p><p>Single-stage dense search:</p><ul><li>Latency: 200ms</li><li>Recall@10: 75%</li></ul><p>Two-stage (sparse → dense):</p><ul><li>Latency: 15ms (13x faster)</li><li>Recall@10: 92% (better quality)</li></ul><p><strong>Why it’s better:</strong></p><ul><li><strong>Speed</strong>: Sparse search is nearly instant, dense exact search on 200 items is cheap</li><li><strong>Quality</strong>: Combines lexical precision + semantic understanding</li><li><strong>Scalability</strong>: Sparse inverted index scales to billions of documents</li><li><strong>Flexibility</strong>: Can swap in different rerankers (cross-encoder, LLM, etc.)</li></ul><p><strong>When to use two-stage:</strong></p><ul><li>✓ Dataset &gt;1M vectors</li><li>✓ Need both keyword and semantic matching</li><li>✓ Latency-sensitive applications</li><li>✓ Long-tail queries with rare terms</li></ul><p><strong>Cost considerations:</strong></p><ul><li>More complex code (need to generate both vector types)</li><li>More storage (both vector types)</li><li>Need to maintain sparse vectorization logic</li></ul><p>But the performance gains are worth it. Every production RAG system I’ve built uses this pattern.</p><p><strong>Pro tip</strong>: For the sparse vector generation, you can use simple TF-IDF hashing or BM25. You don’t need anything fancy. The dense vector is doing the heavy lifting for semantics — sparse just needs to catch exact terms.</p><h3>Combining Tactics: Real Production Setup</h3><p>In production, you use multiple tactics together. Here’s what I actually ran:</p><p><strong>At 1M vectors:</strong></p><pre># HNSW tuning<br>hnsw_config = models.HnswConfigDiff(m=24, ef_construct=150)<br># Two-stage retrieval<br>def search(query):<br>candidates = sparse_search(query, limit=200)<br>return dense_rerank(query, candidates, limit=10)</pre><p><strong>At 5M vectors:</strong></p><pre># HNSW tuning + quantization<br>hnsw_config = models.HnswConfigDiff(m=32, ef_construct=200)<br>quantization_config = models.ScalarQuantization(<br>scalar=models.ScalarQuantizationConfig(type=models.ScalarType.INT8)<br>)<br># Two-stage with quantization<br>def search(query):<br>candidates = sparse_search(query, limit=200)<br>return quantized_dense_rerank(<br>query,<br>candidates,<br>limit=10,<br>oversampling=2.0<br>)</pre><p><strong>At 10M+ vectors:</strong></p><pre># Aggressive tuning + quantization + on-disk<br>hnsw_config = models.HnswConfigDiff(m=48, ef_construct=300)<br>vectors_config = models.VectorParams(<br>size=768,<br>distance=models.Distance.COSINE,<br>on_disk=True # Vectors on disk<br>)<br>quantization_config = models.ScalarQuantization(<br>scalar=models.ScalarQuantizationConfig(<br>type=models.ScalarType.INT8,<br>always_ram=True # Quantized vectors in RAM<br>)<br>)<br># Two-stage + potential sharding by metadata<br>def search(query, filters=None):<br>candidates = sparse_search(query, limit=300, filters=filters)<br>return quantized_dense_rerank(<br>query,<br>candidates,<br>limit=10,<br>oversampling=3.0<br>)</pre><p>What to Monitor in Production</p><p>You can’t just set this up and forget it. Monitoring is critical.</p><h3>Core Metrics to Track</h3><p><strong>1. Retrieval Quality:</strong></p><p><strong>Recall@k on held-out evaluation set:</strong></p><ul><li>Run weekly on a fixed test set of 1000–2000 queries</li><li>Track overall recall and by query category</li><li>Alert if recall drops &gt;5% from baseline</li></ul><p><strong>Why this matters</strong>: You might not notice gradual quality degradation from user complaints until it’s severe. Automated testing catches it early.</p><p><strong>How to set it up:</strong></p><pre>def weekly_quality_check():<br>eval_queries = load_evaluation_set() # Fixed test set<br>results = []<br>for query in eval_queries:<br>hits = search(query.text, k=10)<br>has_correct = query.correct_id in [h.id for h in hits]<br>results.append(has_correct)<br>recall = sum(results) / len(results)<br>if recall &lt; BASELINE_RECALL - 0.05: # 5% drop<br>alert(f&quot;Recall degraded: {recall:.2%} (baseline: {BASELINE_RECALL:.2%})&quot;)<br>log_metric(&quot;recall_at_10&quot;, recall)</pre><p><strong>2. System Health:</strong></p><p><strong>P95 latency</strong> (95th percentile):</p><ul><li>Should stay within your SLA (typically &lt;100ms for user-facing)</li><li>Track by query type (simple vs complex)</li><li>Alert if &gt;2x normal</li></ul><p><strong>P99 latency</strong> (99th percentile):</p><ul><li>Catches tail latencies</li><li>Should be &lt;200ms for interactive apps</li><li>Indicates cache misses or slow queries</li></ul><p><strong>Memory usage:</strong></p><ul><li>Track RSS (resident set size)</li><li>Alert at 70% of available RAM</li><li>Trigger on-disk storage before hitting 80%</li></ul><p><strong>Disk I/O</strong> (if using on-disk storage):</p><ul><li>Read latency should be &lt;10ms p95</li><li>High latency indicates disk bottleneck</li><li>Might need faster SSDs or more RAM</li></ul><p><strong>Cache hit rate:</strong></p><ul><li>HNSW graph traversal should have &gt;95% cache hit rate</li><li>Low hit rate indicates RAM pressure</li><li>Consider reducing dataset size or adding RAM</li></ul><p><strong>3. Drift Signals:</strong></p><p><strong>Recall by query category:</strong> Track recall separately for:</p><ul><li>Common queries (high frequency)</li><li>Long-tail queries (rare/specific)</li><li>Query complexity (simple vs multi-clause)</li></ul><p>Long-tail queries degrade first. If you see recall dropping specifically on rare queries while common queries stay stable, it’s a clear sign of HNSW scaling issues.</p><p><strong>Temporal patterns:</strong></p><ul><li>Does recall degrade over time?</li><li>Does it drop after index updates?</li><li>Are there daily/weekly patterns?</li></ul><p>This helps identify if your issue is scaling, data quality, or infrastructure.</p><h3>Scale Gates: Automated Reviews</h3><p>Set up automatic reviews at scale thresholds:</p><p><strong>At 500K vectors:</strong></p><pre>if collection.size &gt; 500_000 and hnsw_config.m == 16:<br>suggest_action(&quot;Consider increasing M to 24 for better recall&quot;)</pre><p><strong>At 1M vectors:</strong></p><pre>if collection.size &gt; 1_000_000:<br>actions = []<br>if hnsw_config.m &lt; 24:<br>actions.append(&quot;Increase M to 24–32&quot;)<br>if not using_two_stage_retrieval:<br>actions.append(&quot;Implement sparse→dense two-stage retrieval&quot;)<br>if memory_usage &gt; 0.7:<br>actions.append(&quot;Enable on-disk vectors&quot;)<br>run_benchmark_comparison(current_config, optimized_config)<br>suggest_actions(actions)</pre><p><strong>At 5M vectors:</strong></p><pre>if collection.size &gt; 5_000_000:<br>mandatory_actions = []<br>if not quantization_enabled:<br>mandatory_actions.append(&quot;Enable scalar quantization (int8)&quot;)<br>if not on_disk_enabled and memory_usage &gt; 0.6:<br>mandatory_actions.append(&quot;Enable on-disk vectors&quot;)<br>if not two_stage_retrieval:<br>mandatory_actions.append(&quot;Two-stage retrieval is mandatory at this scale&quot;)<br>require_actions(mandatory_actions)</pre><p><strong>At 10M+ vectors:</strong></p><pre>if collection.size &gt; 10_000_000:<br># This is serious scale - need comprehensive optimization<br>checks = {<br>&quot;hnsw_m&quot;: hnsw_config.m &gt;= 48,<br>&quot;quantization&quot;: quantization_enabled,<br>&quot;on_disk&quot;: on_disk_enabled,<br>&quot;two_stage&quot;: two_stage_retrieval,<br>&quot;monthly_reindex&quot;: last_reindex &lt; 30_days_ago<br>}<br>failing = [k for k, v in checks.items() if not v]<br>if failing:<br>critical_alert(f&quot;Missing optimizations at 10M+ scale: {failing}&quot;)</pre><h3>The Monitoring Loop</h3><p>Here’s the actual monitoring code you should run:</p><pre>import time<br>from datetime import datetime, timedelta<br>class QdrantMonitor:<br>    def __init__(self, client, collection_name, baseline_recall=0.90):<br>        self.client = client<br>        self.collection_name = collection_name<br>        self.baseline_recall = baseline_recall<br>        self.eval_queries = self.load_evaluation_set()<br>    <br>    def load_evaluation_set(self):<br>        &quot;&quot;&quot;Load fixed test set of queries with known correct answers&quot;&quot;&quot;<br>        # This should be a representative sample of real queries<br>        # Stored separately, never used for training/tuning<br>        pass<br>    <br>    def measure_recall_at_k(self, k=10):<br>        &quot;&quot;&quot;Measure recall@k on evaluation set&quot;&quot;&quot;<br>        correct = 0<br>        <br>        for query in self.eval_queries:<br>            results = self.search(query.text, k=k)<br>            if query.correct_id in [r.id for r in results]:<br>                correct += 1<br>        <br>        return correct / len(self.eval_queries)<br>    <br>    def measure_latency(self, percentile=95):<br>        &quot;&quot;&quot;Measure latency at given percentile&quot;&quot;&quot;<br>        latencies = []<br>        <br>        for query in self.eval_queries:<br>            start = time.perf_counter()<br>            results = self.search(query.text, k=10)<br>            latency_ms = (time.perf_counter() - start) * 1000<br>            latencies.append(latency_ms)<br>        <br>        latencies.sort()<br>        idx = int(len(latencies) * percentile / 100)<br>        return latencies[idx]<br>    <br>    def get_memory_usage(self):<br>        &quot;&quot;&quot;Get current memory usage percentage&quot;&quot;&quot;<br>        import psutil<br>        return psutil.virtual_memory().percent<br>    <br>    def weekly_health_check(self):<br>        &quot;&quot;&quot;Run comprehensive health check&quot;&quot;&quot;<br>        print(f&quot;[{datetime.now()}] Running health check...&quot;)<br>        <br>        # Measure quality<br>        recall = self.measure_recall_at_k(k=10)<br>        p95_latency = self.measure_latency(percentile=95)<br>        p99_latency = self.measure_latency(percentile=99)<br>        memory_pct = self.get_memory_usage()<br>        <br>        # Get collection info<br>        info = self.client.get_collection(self.collection_name)<br>        vector_count = info.points_count<br>        <br>        # Log metrics<br>        metrics = {<br>            &quot;timestamp&quot;: datetime.now().isoformat(),<br>            &quot;recall_at_10&quot;: recall,<br>            &quot;p95_latency_ms&quot;: p95_latency,<br>            &quot;p99_latency_ms&quot;: p99_latency,<br>            &quot;memory_percent&quot;: memory_pct,<br>            &quot;vector_count&quot;: vector_count<br>        }<br>        self.log_metrics(metrics)<br>        <br>        # Check thresholds and alert<br>        alerts = []<br>        <br>        if recall &lt; self.baseline_recall - 0.05:<br>            alerts.append(f&quot;Recall degraded: {recall:.2%} (baseline: {self.baseline_recall:.2%})&quot;)<br>            self.suggest_recall_fixes()<br>        <br>        if p95_latency &gt; 100:  # SLA breach<br>            alerts.append(f&quot;P95 latency breach: {p95_latency:.1f}ms (SLA: 100ms)&quot;)<br>            self.suggest_latency_fixes()<br>        <br>        if memory_pct &gt; 70:<br>            alerts.append(f&quot;High memory usage: {memory_pct:.1f}%&quot;)<br>            self.suggest_memory_fixes()<br>        <br>        # Scale-based recommendations<br>        if vector_count &gt; 1_000_000:<br>            self.check_scale_optimizations(vector_count)<br>        <br>        if alerts:<br>            self.send_alerts(alerts)<br>        <br>        return metrics<br>    <br>    def suggest_recall_fixes(self):<br>        &quot;&quot;&quot;Auto-suggest fixes for recall degradation&quot;&quot;&quot;<br>        suggestions = [<br>            &quot;1. Increase ef_search (currently may be too low)&quot;,<br>            &quot;2. Rebuild index with higher M and ef_construct&quot;,<br>            &quot;3. Implement two-stage retrieval if not already enabled&quot;,<br>            &quot;4. Check if quantization oversample needs increase&quot;,<br>            &quot;5. Verify evaluation set still represents real queries&quot;<br>        ]<br>        print(&quot;\\nRecall fix suggestions:&quot;)<br>        for s in suggestions:<br>            print(f&quot;  {s}&quot;)<br>    <br>    def suggest_latency_fixes(self):<br>        &quot;&quot;&quot;Auto-suggest fixes for latency issues&quot;&quot;&quot;<br>        suggestions = [<br>            &quot;1. Enable quantization to speed up search&quot;,<br>            &quot;2. Reduce ef_search (accept slight recall tradeoff)&quot;,<br>            &quot;3. Move vectors to disk if RAM pressure is high&quot;,<br>            &quot;4. Implement caching for common queries&quot;,<br>            &quot;5. Scale horizontally with replicas&quot;<br>        ]<br>        print(&quot;\\n Latency fix suggestions:&quot;)<br>        for s in suggestions:<br>            print(f&quot;  {s}&quot;)<br>    <br>    def suggest_memory_fixes(self):<br>        &quot;&quot;&quot;Auto-suggest fixes for memory issues&quot;&quot;&quot;<br>        suggestions = [<br>            &quot;1. Enable on-disk vectors (keeps graph in RAM)&quot;,<br>            &quot;2. Enable quantization (4x memory reduction)&quot;,<br>            &quot;3. Reduce M if currently very high (trades recall for memory)&quot;,<br>            &quot;4. Scale to larger instance or add RAM&quot;,<br>            &quot;5. Consider sharding across multiple instances&quot;<br>        ]<br>        print(&quot;\\n Memory fix suggestions:&quot;)<br>        for s in suggestions:<br>            print(f&quot;  {s}&quot;)<br>    <br>    def check_scale_optimizations(self, vector_count):<br>        &quot;&quot;&quot;Check if scale-appropriate optimizations are enabled&quot;&quot;&quot;<br>        info = self.client.get_collection(self.collection_name)<br>        config = info.config<br>        <br>        recommendations = []<br>        <br>        if vector_count &gt; 5_000_000:<br>            if not config.quantization_config:<br>                recommendations.append(&quot; CRITICAL: Quantization mandatory at 5M+ vectors&quot;)<br>            <br>            if config.hnsw_config.m &lt; 32:<br>                recommendations.append(  Consider M&gt;=32 at this scale&quot;)<br>        <br>        if vector_count &gt; 10_000_000:<br>            if config.hnsw_config.m &lt; 48:<br>                recommendations.append(&quot;  Consider M&gt;=48 at 10M+ scale&quot;)<br>            <br>            recommendations.append(&quot; Schedule monthly reindexing at this scale&quot;)<br>        <br>        if recommendations:<br>            print(&quot;\\n Scale-based recommendations:&quot;)<br>            for r in recommendations:<br>                print(f&quot;  {r}&quot;)<br>    <br>    def log_metrics(self, metrics):<br>        &quot;&quot;&quot;Log metrics to your monitoring system&quot;&quot;&quot;<br>        # Send to Prometheus, Datadog, CloudWatch, etc.<br>        # For demo, just print<br>        print(f&quot;\\n Metrics: {metrics}&quot;)<br>    <br>    def send_alerts(self, alerts):<br>        &quot;&quot;&quot;Send alerts via email, Slack, PagerDuty, etc.&quot;&quot;&quot;<br>        print(f&quot;\\n ALERTS:&quot;)<br>        for alert in alerts:<br>            print(f&quot;  {alert}&quot;)<br># Usage:<br>monitor = QdrantMonitor(<br>    client=qdrant_client,<br>    collection_name=&quot;my_collection&quot;,<br>    baseline_recall=0.90<br>)<br># Run weekly (set up as cron job)<br>monitor.weekly_health_check()</pre><p><strong>Set this up as a cron job:</strong></p><p>0 2 * * 1 python /path/to/monitor.py</p><p>Every Monday at 2 AM</p><p>Don’t wait for users to complain. Proactive monitoring catches problems early when they’re easy to fix.</p><h3>Why I Use Qdrant for This</h3><p>I’ve used Pinecone, Weaviate, and Milvus in production. Here’s why Qdrant won for handling HNSW scaling:</p><h4>1. Payload Indexing is Actually Different</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ssErLVRyVD8t4EQT.png\" /></figure><p><strong>The problem with most databases:</strong> They do filtering AFTER the similarity search:</p><ol><li>Find top-100 most similar vectors</li><li>Apply your filter (e.g., “created_date &gt; 2024–01–01”)</li><li>Maybe you get 3 results, maybe 0</li></ol><p>If your filter is restrictive, you waste the similarity search. You found 100 candidates, but only 3 match your filter.</p><p><strong>How Qdrant is different:</strong> Qdrant’s payload index extends the HNSW graph itself. It filters DURING the graph traversal, not after:</p><ol><li>While navigating the HNSW graph, check filters at each hop</li><li>Only explore paths where filters match</li><li>Get top-100 that are both similar AND match filters</li></ol><p>This is a <strong>single-pass filtered search</strong>. The filter is integrated into the graph navigation.</p><p><strong>Real-world impact:</strong> I had a collection of 2M product documents with metadata like category, price_range, availability.</p><p>Query: “Find products similar to ‘wireless headphones’ in Electronics category, price $50-$200, in stock”</p><p><strong>Weaviate (post-filtering):</strong></p><ul><li>Find top-100 similar to “wireless headphones” (200ms)</li><li>Apply filters (Electronics, price range, in stock)</li><li>Get 7 results</li><li>Many similarity computations wasted on wrong category/price</li></ul><p><strong>Qdrant (during-search filtering):</strong></p><ul><li>Navigate HNSW while checking filters at each hop (45ms)</li><li>Get top-100 that match ALL criteria</li><li>4.4x faster, better results</li></ul><p>At scale, this difference is massive.</p><h4>2. Quantization That’s Production-Ready</h4><p><strong>What makes Qdrant’s quantization special:</strong></p><p><strong>Built-in rescore logic:</strong> Most databases offer quantization, but you have to manually implement oversampling and rescoring. Qdrant has it built-in — just set rescore=True.</p><p><strong>Automatic fallback:</strong> If quantized search doesn’t find enough candidates, Qdrant automatically falls back to full precision. You don’t have to handle edge cases.</p><p><strong>Multiple quantization types:</strong></p><ul><li>Scalar (int8) — default, safe choice</li><li>Binary (1-bit) — maximum compression</li><li>Product quantization — learned compression</li></ul><p>All work with the same API. Easy to test and compare.</p><p><strong>Real numbers from my production system:</strong></p><p>Full precision:</p><ul><li>RAM: 890MB</li><li>P95 latency: 78ms</li><li>Recall@10: 92%</li></ul><p>Scalar quantization (int8, oversample 2x):</p><ul><li>RAM: 47MB (95% reduction)</li><li>P95 latency: 43ms (45% faster)</li><li>Recall@10: 91% (1% drop, acceptable)</li></ul><p>This is on a 500K vector collection. The savings at 5M or 10M vectors are even more dramatic.</p><h4>3. Sparse + Dense Hybrid is Native</h4><p><strong>Most databases</strong> make you choose:</p><ul><li>Dense vectors (semantic) OR</li><li>Sparse vectors (lexical)</li><li>Want both? Run two separate systems and merge results yourself</li></ul><p><strong>Qdrant</strong> supports both in a single collection:</p><pre>client.create_collection(<br>    collection_name=&quot;hybrid&quot;,<br>    vectors_config={<br>        &quot;dense&quot;: models.VectorParams(size=768, distance=models.Distance.COSINE)<br>    },<br>    sparse_vectors_config={<br>        &quot;sparse&quot;: models.SparseVectorParams()<br>    }<br>)<br>Index documents with both:<br>client.upsert(<br>    collection_name=&quot;hybrid&quot;,<br>    points=[{<br>        &quot;id&quot;: 1,<br>        &quot;vector&quot;: {<br>            &quot;dense&quot;: [0.1, 0.2, ...],  # Semantic embedding<br>            &quot;sparse&quot;: models.SparseVector(<br>                indices=[10, 234, 567],  # Term IDs<br>                values=[0.8, 0.6, 0.4]   # Term weights<br>            )<br>        }<br>    }]<br>)<br>Two-stage retrieval becomes trivial:<br># Stage 1: Sparse<br>candidates = client.query_points(<br>    collection_name=&quot;hybrid&quot;,<br>    query=sparse_query,<br>    using=&quot;sparse&quot;,<br>    limit=200<br>)<br># Stage 2: Dense rerank<br>results = client.query_points(<br>    collection_name=&quot;hybrid&quot;,<br>    query=dense_query,<br>    using=&quot;dense&quot;,<br>    limit=10,<br>    query_filter=models.Filter(<br>        must=[models.HasIdCondition(has_id=[c.id for c in candidates])]<br>    )<br>)</pre><p>No external orchestration. No merging results from different systems. It just works.</p><h3>4. On-Disk Storage That’s Actually Smart</h3><p><strong>The naive approach (what some databases do):</strong></p><ul><li>Put everything on disk</li><li>Every graph hop requires disk I/O</li><li>Performance tanks 10–100x</li></ul><p><strong>Qdrant’s approach:</strong></p><ul><li>HNSW graph stays in RAM (navigation path — hot)</li><li>Vector data goes to disk via mmap (final scoring — acceptable)</li><li>OS handles caching automatically</li></ul><p><strong>Why this matters:</strong></p><p>During HNSW search, you might visit 100–200 nodes during graph navigation (checking which direction to hop), but you only compute exact similarity scores for maybe 10–50 final candidates.</p><p>Graph navigation is the hot path. Vector scoring is not.</p><p>By keeping graph in RAM and vectors on disk:</p><ul><li>95% of operations stay fast (pure RAM)</li><li>5% of operations are slightly slower (disk reads for final scoring)</li><li>Net result: 60–80% memory reduction, &lt;20% latency increase</li></ul><p><strong>Real numbers:</strong></p><p>Full in-memory (1M vectors):</p><ul><li>RAM: 4.2GB</li><li>P95 latency: 45ms</li></ul><p>On-disk vectors (1M vectors):</p><ul><li>RAM: 0.9GB (78% reduction)</li><li>P95 latency: 52ms (15% increase)</li></ul><p>At 10M vectors:</p><ul><li>Full in-memory: 42GB RAM (doesn’t fit on most machines)</li><li>On-disk vectors: 9GB RAM (fits easily), latency +20%</li></ul><p>This is a no-brainer tradeoff at scale.</p><h3>5. Rust = Consistent Performance</h3><p><strong>Why Rust matters for vector databases:</strong></p><p><strong>No garbage collection pauses:</strong> Languages like Java/Go have GC pauses that can spike latency unpredictably. Qdrant’s Rust implementation has no GC — memory is deterministic.</p><p><strong>SIMD acceleration:</strong> Rust makes it easy to use SIMD (Single Instruction Multiple Data) for vector operations. Computing dot products of 768-dimensional vectors is 4–8x faster with SIMD.</p><p><strong>Better async I/O:</strong> Qdrant uses io_uring on Linux for async disk I/O. This is 2–3x faster than traditional I/O for on-disk vectors.</p><p><strong>Memory safety without overhead:</strong> Rust’s borrow checker prevents memory bugs without runtime overhead. No null pointer crashes, no buffer overflows, no data races.</p><p><strong>Real-world impact:</strong></p><p><strong>Pinecone</strong> (closed source, don’t know implementation):</p><ul><li>P95 latency: 50–150ms (varies wildly)</li><li>P99 latency: Sometimes spikes to 500ms+</li><li>Unpredictable under load</li></ul><p><strong>Qdrant</strong> (Rust):</p><ul><li>P95 latency: 45ms (consistent)</li><li>P99 latency: 65ms (stable)</li><li>Predictable even at 10K queries/second</li></ul><p>For production systems, predictability matters as much as raw speed.</p><h3>The Honest Truth About HNSW at Scale</h3><p>Let me be direct: <strong>HNSW isn’t broken. Default HNSW is broken.</strong></p><p>There’s no magic setting that works at all scales. If someone tells you “just use M=16, ef_construct=100, ef_search=64 for everything,” they haven’t scaled past 100K vectors.</p><p><strong>What you actually need:</strong></p><p><strong>1. Monitoring</strong>: Know when quality degrades BEFORE users complain</p><ul><li>Weekly recall measurements on evaluation set</li><li>P95/P99 latency tracking</li><li>Memory usage alerts</li></ul><p><strong>2. Tuning at scale gates</strong>: Adjust parameters as you grow</p><ul><li>At 1M: Increase M to 24, implement two-stage retrieval</li><li>At 5M: Enable quantization, consider on-disk storage</li><li>At 10M+: Aggressive tuning (M=48+), mandatory quantization</li></ul><p><strong>3. Architectural patterns</strong>: Don’t rely on single-shot search</p><ul><li>Two-stage retrieval (sparse → dense or quantized → full)</li><li>Oversampling + rescoring for quantization</li><li>Strategic on-disk storage (graph in RAM, vectors on disk)</li></ul><p><strong>The four tactics:</strong></p><ol><li><strong>Tune HNSW</strong>: Increase M, ef_construct, ef_search based on scale</li><li><strong>On-disk vectors</strong>: When RAM is tight, keep graph in RAM, vectors on disk</li><li><strong>Quantization</strong>: Compress to int8, oversample 2–3x, rescore with full precision</li><li><strong>Two-stage retrieval</strong>: Fast broad search → precise narrow rerank</li></ol><p><strong>Qdrant makes this manageable:</strong></p><ul><li>Payload indexing for filtered searches</li><li>Built-in quantization with rescore</li><li>Native sparse + dense hybrid</li><li>Smart on-disk storage</li><li>Rust for predictable performance</li></ul><p>My RAG system went from “failing at 200K vectors” to “handling 10M vectors with sub-100ms latency” by applying these patterns with Qdrant.</p><p>That’s the difference between understanding your tools and just hoping they work.</p><h3>Links and Resources</h3><p>Colab notebook: <a href=\"https://colab.research.google.com/drive/1ydVDqNVsRih0XATT5HE7ZZHD511g6tKX?usp=sharing\">https://colab.research.google.com/drive/1ydVDqNVsRih0XATT5HE7ZZHD511g6tKX?usp=sharing</a></p><p><strong>HNSW Algorithm:</strong></p><ul><li>Original HNSW Paper:<a href=\"https://arxiv.org/abs/1603.09320\"> https://arxiv.org/abs/1603.09320</a></li></ul><p><strong>Qdrant Documentation:</strong></p><ul><li>Main Docs:<a href=\"https://qdrant.tech/documentation/\"> https://qdrant.tech/documentation/</a></li><li>HNSW Indexing Guide:<a href=\"https://qdrant.tech/course/essentials/day-2/what-is-hnsw/\"> https://qdrant.tech/course/essentials/day-2/what-is-hnsw/</a></li><li>Quantization Guide:<a href=\"https://qdrant.tech/documentation/guides/quantization/\"> https://qdrant.tech/documentation/guides/quantization/</a></li><li>On-Disk Storage:<a href=\"https://qdrant.tech/documentation/concepts/storage/\"> https://qdrant.tech/documentation/concepts/storage/</a></li></ul><p><strong>Qdrant Repository:</strong></p><ul><li>GitHub: <a href=\"https://github.com/qdrant/qdrant\">https://github.com/qdrant/qdrant</a></li></ul><p>The future of RAG at scale isn’t magic — it’s understanding your retrieval layer, monitoring it continuously, and tuning it as you grow. With Qdrant handling the complexity, you can focus on building great applications instead of fighting infrastructure.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0cca7008107d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/hnsw-at-scale-why-adding-more-documents-to-your-database-breaks-rag-0cca7008107d\">HNSW at Scale: Why Adding More Documents to Your Database Breaks RAG</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-25T17:41:31.000Z",
    "url": "https://levelup.gitconnected.com/hnsw-at-scale-why-adding-more-documents-to-your-database-breaks-rag-0cca7008107d?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Your Bank’s COBOL Isn’t a Problem. It’s an Opportunity.",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/your-banks-cobol-isn-t-a-problem-it-s-an-opportunity-a4785a7628a1?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*gtyx8Wg1AX_p-tAP\" width=\"4032\"></a></p><p class=\"medium-feed-snippet\">How AI agents are doing what 30 years of digital transformation couldn&#x2019;t</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/your-banks-cobol-isn-t-a-problem-it-s-an-opportunity-a4785a7628a1?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:41:23.000Z",
    "url": "https://levelup.gitconnected.com/your-banks-cobol-isn-t-a-problem-it-s-an-opportunity-a4785a7628a1?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Anthropic Caught Three Chinese Labs Stealing 16 Million AI Conversations.",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/anthropic-caught-three-chinese-labs-stealing-16-million-ai-conversations-32e89ddd3e5d?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*x6zHyn43YRGcHG1-VXnr9g.png\" width=\"2752\"></a></p><p class=\"medium-feed-snippet\">24,000 fake accounts, a $1.5 billion copyright settlement, and a hypocrisy debate that&#x2019;s drowning out the actual national security crisis.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/anthropic-caught-three-chinese-labs-stealing-16-million-ai-conversations-32e89ddd3e5d?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:41:14.000Z",
    "url": "https://levelup.gitconnected.com/anthropic-caught-three-chinese-labs-stealing-16-million-ai-conversations-32e89ddd3e5d?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Build Your Own Redis in Python in Under 200 Lines of Code",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/build-your-own-redis-in-python-in-under-200-lines-of-code-4e22697b2208?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*lL5J9PBxFVjPJpedVrzoZQ.png\" width=\"2848\"></a></p><p class=\"medium-feed-snippet\">Stop treating databases like black boxes. Here is how to build a functional key-value store from scratch.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/build-your-own-redis-in-python-in-under-200-lines-of-code-4e22697b2208?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:41:06.000Z",
    "url": "https://levelup.gitconnected.com/build-your-own-redis-in-python-in-under-200-lines-of-code-4e22697b2208?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Arthur Mensch Just Broke Silicon Valley’s AI Safety Cartel",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/arthur-mensch-just-broke-silicon-valleys-ai-safety-cartel-de0e4e61677f?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/960/0*MPHIH3EF6UCs_LLX\" width=\"960\"></a></p><p class=\"medium-feed-snippet\">The safety consensus just cracked in public</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/arthur-mensch-just-broke-silicon-valleys-ai-safety-cartel-de0e4e61677f?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:40:58.000Z",
    "url": "https://levelup.gitconnected.com/arthur-mensch-just-broke-silicon-valleys-ai-safety-cartel-de0e4e61677f?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "How I Built a Fully Automated Document Management System with Paperless-NGX",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/how-i-built-a-fully-automated-document-management-system-with-paperless-ngx-a173eac9c1b0?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2000/0*5EfZphacq2SrS82Z.png\" width=\"2000\"></a></p><p class=\"medium-feed-snippet\">Managing paper and digital documents for a family of four across multiple countries and languages was becoming unmanageable. Tax documents&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/how-i-built-a-fully-automated-document-management-system-with-paperless-ngx-a173eac9c1b0?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:40:50.000Z",
    "url": "https://levelup.gitconnected.com/how-i-built-a-fully-automated-document-management-system-with-paperless-ngx-a173eac9c1b0?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Stop Expecting Your Best Engineer to Be a Good Mentor",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/stop-expecting-your-best-engineer-to-be-a-good-mentor-05eba3ff6c98?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*yTgRYD228vZcQBwA\" width=\"3750\"></a></p><p class=\"medium-feed-snippet\">Most of them can&#x2019;t, and that&#x2019;s not a character flaw.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/stop-expecting-your-best-engineer-to-be-a-good-mentor-05eba3ff6c98?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:40:42.000Z",
    "url": "https://levelup.gitconnected.com/stop-expecting-your-best-engineer-to-be-a-good-mentor-05eba3ff6c98?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Protobuf Optimization Techniques That Slashed Our Payload Size by 60%",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/protobuf-optimization-techniques-that-slashed-our-payload-size-by-60-70fe843e177b?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*i4X6yJ2irGTJQGvXQBuuyw.png\" width=\"1536\"></a></p><p class=\"medium-feed-snippet\">The day our mobile app went from sluggish to snappy &#x2014; and how three simple changes transformed our API performance</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/protobuf-optimization-techniques-that-slashed-our-payload-size-by-60-70fe843e177b?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-25T17:40:33.000Z",
    "url": "https://levelup.gitconnected.com/protobuf-optimization-techniques-that-slashed-our-payload-size-by-60-70fe843e177b?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Understanding Swift Strings: Unicode, Encoding, and Performance",
    "partialText": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Pwub20odWbbiWrfftBVBrw.png\" /></figure><h3>Introduction</h3><p>Most developers use strings every day without thinking too much about how text is represented, how computers interpret it, or how it is stored in memory. For many everyday tasks, that is completely fine.</p><p>But once you start working heavily with text, dealing with large strings, complex Unicode input, or performance-critical code, those details begin to matter.</p><p>For a long time, text was simple. ASCII defined a small set of characters, each represented by a single byte. Counting characters was trivial. Indexing was straightforward. One byte meant one character.</p><p>That model no longer holds.</p><p>In Swift, String APIs often raise questions like:</p><ul><li>Why is count not constant time?</li><li>Why cannot strings be indexed by integers?</li><li>Why do characters behave differently than expected?</li></ul><p>These behaviors can be surprising at first, but they are not accidental. They reflect the reality of modern text.</p><p>Unicode expanded text far beyond ASCII. What users perceive as a single character may consist of multiple Unicode scalars and multiple encoded bytes. Supporting this correctly has deep implications for how strings are represented and manipulated.</p><p>In this article, we will start with ASCII to establish the original mental model, then move to Unicode and encodings, and finally explore how these foundations shape Swift’s String API. We will connect the theory to concrete behaviors such as string views, character counting, and index-based traversal.</p><p>The goal is to replace confusion with understanding. Once the underlying model is clear, Swift’s String APIs stop feeling complex and start feeling deliberate.</p><h3>Key Insights You’ll Learn</h3><ul><li>Unicode defines <strong>what characters are</strong>, but not how they are stored in memory.</li><li>Encodings like UTF-8, UTF-16, and UTF-32 define<strong> how Unicode code points are represented as bytes</strong>.</li><li>Swift strings are <strong>Unicode-correct by design</strong> and prioritize user-visible characters over raw bytes.</li><li>A Swift Character is a <strong>grapheme cluster</strong>, not a byte, and not a Unicode scalar.</li><li>The same string can be viewed as <strong>UTF-8 code units, Unicode scalars, or characters</strong>, depending on the task.</li><li>String.count is an <strong>O(n)</strong> operation, because counting characters requires traversing grapheme clusters.</li><li>Swift uses String.Index instead of integers to make <strong>correct text traversal explicit and safe</strong>.</li></ul><h3>ASCII</h3><p>ASCII was one of the first standards for representing text in computers. It was introduced in the 1960s and defines <strong>128 characters</strong>. These include English letters, digits, punctuation, and a few control characters.</p><p>ASCII uses values from 0 to 127 and fits into a single byte.</p><p>Examples:</p><ul><li>A → 65, Z → 90</li><li>a → 97, z → 122</li><li>0 → 48, 9 → 57</li></ul><p>This worked well for English, but it stopped there. There was no room for non-Latin alphabets, accented letters, or symbols used in other languages. As computers became global, this limitation became a real problem.</p><h3>Unicode</h3><p>Unicode was introduced in 1991 to solve the limitations of ASCII. Instead of focusing on bytes, Unicode defines characters. Each character is assigned a unique number called a <strong>code point</strong>.</p><p>Unicode answers one question: <strong>what character is this</strong>? It does not define how characters are stored in memory.</p><p>A key design decision was compatibility. That’s why the first 128 Unicode code points are exactly the same as ASCII.</p><p>Because of this:</p><ul><li>ASCII text is valid Unicode text.</li><li>Unicode could replace ASCII as the global standard without breaking existing systems.</li></ul><p>Examples (code points are written in hexadecimal):</p><ul><li>Latin: A → U+0041, a → U+0061, Z → U+005A, z → U+007A</li><li>Cyrillic: Б → U+0411, Г → U+0413, З → U+0417, Я → U+042F</li><li>Chinese: 你 → U+4F60, 好 → U+597D, 中 → U+4E2D, 文 → U+6587</li><li>Symbols: @ → U+0040, # → U+0023, $ → U+0024, % → U+0025, &amp; → U+0026</li><li>Emojis: 🙂 → U+1F642, 😀 → U+1F600, 🚀 → U+1F680, 🔥 → U+1F525</li></ul><p>Unicode also allows a single visible character to be composed of multiple code points. For example, e + ◌́ = é.</p><p>This character can be represented in two ways:</p><ol><li>As a single code point: U+00E9 (é).</li><li>As two code points: U+0065 (e) and U+0301 (combining acute accent).</li></ol><p>Both forms look identical on screen, but they are different sequences internally. This property is fundamental to modern text processing and explains why strings are more complex than they first appear.</p><h3>Encodings</h3><p>So far, we talked about characters and code points. Now we need to answer a different question: <strong>how are these code points stored as bytes</strong>? That is what encodings are for.</p><blockquote>An encoding defines how Unicode code points are represented in memory or on disk. Unicode defines what a character is. Encodings define how that character is stored.</blockquote><p>The most common Unicode encodings are <strong>UTF-8</strong>, <strong>UTF-16</strong>, and <strong>UTF-32</strong>. They all represent the same Unicode code points. The difference is how many bytes they use and how those bytes are arranged.</p><ul><li><strong>UTF-8:</strong> variable-length encoding that uses 1 to 4 bytes per code point. Smaller code point values use fewer bytes. ASCII characters fit into a single byte.</li><li><strong>UTF-16:</strong> variable-length encoding based on 16-bit units. Most code points use 2 bytes. Larger code points use 4 bytes.</li><li><strong>UTF-32: </strong>fixed-length encoding. Every code point uses 4 bytes. Simple to reason about, but inefficient in memory usage.</li></ul><p>The same character A (U+0041) encoded using different encodings can take a different number of bytes:</p><ul><li>UTF-8 → 1 byte</li><li>UTF-16 → 2 bytes</li><li>UTF-32 → 4 bytes</li></ul><p>The character stays the same. Only the byte representation changes.</p><p>Encodings use more bytes when the code point value is larger. That’s why different characters take different amounts of space. Characters added later to Unicode usually have higher code point values, which is why they tend to take more bytes in UTF-8.</p><p>Examples in UTF-8:</p><ul><li>A (U+0041) → 1 byte</li><li>é (U+00E9) → 2 bytes</li><li>你 (U+4F60) → 3 bytes</li><li>😀 (U+1F600) → 4 bytes</li></ul><h4>Why UTF-8 dominates</h4><p>UTF-8 is the most widely used Unicode encoding today.</p><ul><li>It is backward compatible with ASCII, which made the adoption easy.</li><li>It is compact for common text, especially English and source code.</li><li>It works well with existing byte-oriented systems and protocols.</li></ul><p>Because of this, UTF-8 became the default encoding for the web.</p><blockquote>This transition is visible on Apple platforms as well. Objective-C String APIs were built around UTF-16, while Swift moved to UTF-8 as the default encoding.</blockquote><h3>Unicode scalars and Grapheme clusters</h3><p>So far, we talked about characters, code points, and encodings. Now we need to be more precise about what a “character” actually means.</p><p>Unicode separates code points from user-visible characters.</p><h4>Unicode scalars</h4><blockquote>A <strong>Unicode scalar</strong> is a single Unicode code point.</blockquote><p>Scalars are fundamental units defined by Unicode and encoded by UTF-8, UTF-16, or UTF-32.</p><p>Encodings operate on Unicode scalars. They convert scalar values into bytes and back.</p><p>At this level, text looks simple. Each scalar is just a number that maps directly to bytes.</p><h4>Why scalars are not enough</h4><p>The problem is that <strong>users do not interact with scalars</strong>.</p><blockquote>A single user-visible character may consist of <strong>multiple Unicode scalars</strong>.</blockquote><p>For example:</p><ul><li><strong>The flag emoji 🇺🇸</strong> looks like one character, but it is composed of two scalars: 🇺 (U+1F1FA) + 🇸 (U+1F1F8).</li><li><strong>The family emoji 🧑‍🧑‍🧒‍🧒</strong> looks like one character, but it is composed of seven scalars: 🧑 (U+1F9D1) + zero-width joiner (U+200D) + 🧑 (U+1F9D1) + zero-width joiner (U+200D) + 🧒 (U+1F9D2) + zero-width joiner (U+200D) + 🧒 (U+1F9D2).</li></ul><p>If scalars were treated as characters, many basic text operations would no longer match user expectations.</p><p>Unicode scalars are precise, but they are too low-level to model characters as people perceive them.</p><h4>Grapheme clusters</h4><p>To bridge this gap, Unicode defines <strong>grapheme clusters</strong>.</p><blockquote>A <strong>grapheme cluster</strong> represents a single user-visible character. It may consist of one Unicode scalar or multiple Unicode scalars combined together.</blockquote><p>From the user’s perspective, each of the following is one character: “A”, “é”, “你”, “<strong>🇺🇸”, “ 🧑‍🧑‍🧒‍”</strong>.</p><p>Even though internally, they may contain very different scalar sequences.</p><p>This distinction explains several properties of modern text processing:</p><ul><li>Counting characters is not trivial.</li><li>Indexing into strings is not straightforward.</li><li>A “character” does not have a fixed size in memory.</li></ul><p>This complexity is not specific to any programming language. It is a direct consequence of Unicode’s design.</p><h3>How Swift models text</h3><p>Swift’s String is designed to be Unicode-correct by default. Swift does not treat text as bytes or code points. It models text the way users perceive it.</p><blockquote>A String in Swift is a collection of characters. A Character represents a grapheme cluster, which is a single user-visible character.</blockquote><p>The choice favors correctness over convenience and directly shapes how strings behave in Swift.</p><h3>The three views of String</h3><p>A Swift String can be viewed at different levels of abstraction. Each view represents the same text, but it iterates over a different unit.</p><p>Swift exposes this through three main views:</p><ul><li><strong>UTF-8 View</strong> (String.UTF8View)</li><li><strong>Unicode Scalar View</strong> (String.UnicodeScalarView)</li><li><strong>Character View</strong> (String) with elements of type Character</li></ul><p>Here is how to access them:</p><pre>let greeting = &quot;Hello, world! 👋&quot;<br><br>let utf8: String.UTF8View = greeting.utf8<br><br>let scalars: String.UnicodeScalarView = greeting.unicodeScalars<br><br>let characters: [Character] = Array(greeting)<br>// or <br>let characters: String = greeting // iterating yields Character</pre><h4><strong>UTF-8 View</strong> (String.UTF8View)</h4><p>This view iterates over the string’s <strong>UTF-8 code units</strong>. Each element represents a single byte of UTF-8 data (UInt8).</p><p>This is the lowest-level view and is useful when working directly with byte-oriented data:</p><ul><li>binary data</li><li>network protocols</li><li>file formats</li><li>performance-sensitive text processing</li></ul><p>A single user-visible character may correspond to multiple UTF-8 code units.</p><h4>Unicode Scalar View (String.UnicodeScalarView)</h4><p>This view iterates over <strong>Unicode scalars</strong>, where each element corresponds to a single Unicode code point (Unicode.Scalar).</p><p>This view sits between bytes and characters.</p><p>It is useful when you need to:</p><ul><li>inspect or transform exact Unicode values</li><li>reason about normalization or scalar-level properties</li><li>process text without grouping into user-visible characters</li></ul><p>A single user-visible character may consist of multiple Unicode scalars.</p><h4>Character View (Character)</h4><p>Iterating over a String directly yields Character values.</p><p>In Swift, a Character represents a <strong>grapheme cluster</strong>, which corresponds to a single user-visible character.</p><p>This is the highest-level view. It prioritizes correctness and user expectations over raw performance.</p><p>A grapheme cluster does not have a fixed size. It may consist of multiple scalars and therefore multiple UTF-8 code units.</p><h3>Why .count is not O(1)</h3><p>In many languages, getting the length of a string is a constant-time operation. In Swift, String.count is <strong>not O(1)</strong>.</p><p>This is not a performance oversight. It is a direct result of how Swift models text.</p><p>In Swift, String.count returns the number of <strong>characters</strong> in the string. A Character represents a <strong>grapheme cluster</strong>, not a byte, and not a Unicode scalar.</p><p>Grapheme clusters do not have a fixed size. A single character may consist of one or more Unicode scalars and multiple UTF-8 code units.</p><p>Because of this, Swift cannot know how many characters a string contains without <strong>iterating through the string</strong> and identifying grapheme cluster boundaries.</p><p>As a result, String.count is an <strong>O(n) operation</strong>.</p><p>Consider the same example string used earlier:</p><pre>let greeting = &quot;Hello, world! 👋&quot;<br><br>greeting.count                // 15 - Characters<br>greeting.unicodeScalars.count // 15 - Unicode scalars<br>greeting.utf8.count           // 18 - UTF-8 code units (bytes)</pre><p>Each view answers a different question. The values may differ, even though the underlying string is the same.</p><p>Now consider a more subtle example:</p><pre>var word = &quot;cafe&quot;<br><br>word                      // cafe<br><br>word.count                // 4 - Characters<br>word.unicodeScalars.count // 4 - Unicode scalars<br>word.utf8.count           // 4 - UTF-8 code units (bytes)<br><br>// Appending a combining accent at the scalar level<br>word.unicodeScalars.append(&quot;\\u{0301}&quot;)<br><br>word                      // café<br><br>word.count                // 4 - Characters<br>word.unicodeScalars.count // 5 - Unicode scalars<br>word.utf8.count           // 6 - UTF-8 code units (bytes)</pre><p>Visually, the string still looks like it has 4 characters, and count still returns 4. But an extra Unicode scalar was added, so unicodeScalars.count increases. That scalar is encoded as additional bytes, so utf8.count increases as well.</p><p>This example shows why counting characters can not be done by looking at bytes or scalars alone. Swift must scan the string to determine grapheme cluster boundaries.</p><p>Swift could make String.count faster by defining it in terms of bytes or scalars. But that would break the guarantee that count reflects <strong>what users perceive as characters</strong>. Instead, Swift chooses correctness over convenience.</p><p>The important takeaway is not to avoid count, but to understand what it does. Once you do, its performance characteristics make sense.</p><h3>Why String.Index exists</h3><p>In many languages, strings are indexed by integers. You can access the first character with index 0, the second with 1, and so on.</p><p>Swift does not allow this.</p><p>This is not a design accident. It is a consequence of how Swift models text.</p><p>In Swift, a String is a collection of characters, and a Character represents a <strong>grapheme cluster</strong>. Grapheme clusters do not have a fixed size and do not occupy a predictable number of bytes.</p><p>Because of this, there is no constant-time (O(1)) way to jump to the nth character in a string. The string must be scanned from the beginning to determine where each character boundary lies.</p><p>An integer index would suggest that characters are fixed-size and randomly accessible. They are not.</p><p>That is why Swift uses String.Index.</p><blockquote>A String.Index represents a position between characters in a specific string. It always points to a valid character boundary and is tied to the string it belongs to.</blockquote><p>Consider the following example:</p><pre>let greeting = &quot;Hello, world! 👋&quot;<br>let firstIndex = greeting.startIndex<br>let secondIndex = greeting.index(after: greeting.startIndex)<br>let seventhIndex = greeting.index(greeting.startIndex, offsetBy: 7)<br>let lastIndex = greeting.index(before: greeting.endIndex)<br><br>let firstCharacter = greeting[firstIndex]     // &quot;H&quot;<br>let secondCharacter = greeting[secondIndex]   // &quot;e&quot;<br>let seventhCharacter = greeting[seventhIndex] // &quot;w&quot;<br>let lastCharacter = greeting[lastIndex]       // &quot;👋&quot;</pre><p>Although index(_:offsetBy:) takes an integer offset, it does <strong>not</strong> provide random access. Advancing an index by n characters is an O(n) operation. Swift must move through the string one character at a time, determining grapheme cluster boundaries at each step. This is the same reason String.count is an O(n) operation.</p><p>It is also important to note that endIndex represents the position <strong>after</strong> the final character in the string.</p><p>Swift provides a small set of index-manipulation APIs for moving forward and backward through a string, as well as measuring distances between indices. All of them operate at the character level and have linear performance characteristics.</p><p>Swift could have exposed integer indexing and documented pitfalls. Instead, it chose an API that makes incorrect assumptions impossible. The result is a string model that is Unicode-correct, safe by construction, and explicit about its performance characteristics.</p><p>Swift’s string APIs are often seen as complex at first. But that complexity comes from treating text as users see it, not as raw bytes. Once you understand how Unicode, grapheme clusters, and indexing fit together, Swift’s design becomes predictable and intentional.</p><p>Thanks for reading! Follow my Medium profile for more practical Swift and iOS content.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5b487d41569b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/understanding-swift-strings-unicode-encoding-and-performance-5b487d41569b\">Understanding Swift Strings: Unicode, Encoding, and Performance</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-25T17:40:25.000Z",
    "url": "https://levelup.gitconnected.com/understanding-swift-strings-unicode-encoding-and-performance-5b487d41569b?source=rss----5517fd7b58a6---4"
  }
]