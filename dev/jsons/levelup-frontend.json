[
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Tcl vs. Bash: When Should You Choose Tcl?",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/tcl-vs-bash-when-should-you-choose-tcl-e07c47eb05ff?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1600/1*0-fBBqYpBXw0UV6StWxSoQ.png\" width=\"1600\"></a></p><p class=\"medium-feed-snippet\">Using Tcl to write more readable and portable automation scripts</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/tcl-vs-bash-when-should-you-choose-tcl-e07c47eb05ff?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:53.000Z",
    "url": "https://levelup.gitconnected.com/tcl-vs-bash-when-should-you-choose-tcl-e07c47eb05ff?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Turning Images into Pixel Art the Right Way",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/turning-images-into-pixel-art-the-right-way-ed9ce1a013bc?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1721/1*Q4cclA_zPq8K0qws4g80sw.png\" width=\"1721\"></a></p><p class=\"medium-feed-snippet\">Pixel art is a digital art style that has gained significant popularity in recent years, especially within the video game industry, where&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/turning-images-into-pixel-art-the-right-way-ed9ce1a013bc?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:47.000Z",
    "url": "https://levelup.gitconnected.com/turning-images-into-pixel-art-the-right-way-ed9ce1a013bc?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "What the F-35 can Teach us about Writing Safer Embedded C++",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/what-the-f-35-can-teach-us-about-writing-safer-embedded-c-e67ebb4955e7?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*JNBa0LduD3RlBiRyHSvinw.png\" width=\"1536\"></a></p><p class=\"medium-feed-snippet\">Performance can be optimized later. Reliability must be designed in from the start.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/what-the-f-35-can-teach-us-about-writing-safer-embedded-c-e67ebb4955e7?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:42.000Z",
    "url": "https://levelup.gitconnected.com/what-the-f-35-can-teach-us-about-writing-safer-embedded-c-e67ebb4955e7?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "The Difference Between a Python Script and a Python System",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/the-difference-between-a-python-script-and-a-python-system-39c696424019?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*8VpyFLIvccPozH-Yl0Ib4A.png\" width=\"1536\"></a></p><p class=\"medium-feed-snippet\">Code is not where the finest automation ideas begin.\nPain is where they begin.\nPerhaps it involves manually downloading reports each&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/the-difference-between-a-python-script-and-a-python-system-39c696424019?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:38.000Z",
    "url": "https://levelup.gitconnected.com/the-difference-between-a-python-script-and-a-python-system-39c696424019?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "10 Real-World Automation Projects with Prefect",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/10-real-world-automation-projects-with-prefect-a6ea9c838e7c?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*M7yrsJDHfTWKq-XV\" width=\"4416\"></a></p><p class=\"medium-feed-snippet\">Prefect: The Python Package I Wish I Knew Sooner</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/10-real-world-automation-projects-with-prefect-a6ea9c838e7c?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:34.000Z",
    "url": "https://levelup.gitconnected.com/10-real-world-automation-projects-with-prefect-a6ea9c838e7c?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Data Streaming with CDC",
    "partialText": "<h4>A Hands-On Guide with SQL Server, Debezium, Kafka, and Spring Boot</h4><p>If you’ve ever needed to react to database changes in real time — syncing data across microservices, building audit logs, or feeding analytics pipelines — you’ve probably come across Change Data Capture (CDC). It’s one of those patterns that sounds complex on paper but becomes surprisingly elegant once you see the moving parts working together.</p><p>In this article, I’ll walk you through a complete, working example that captures every INSERT, UPDATE, and DELETE on a SQL Server table and streams those changes through Kafka to a Spring Boot consumer — all without writing a single database trigger.</p><p>The full source code is available on <a href=\"https://github.com/algorythmist/cdc-debezium-kafka\">GitHub</a>.</p><h3>What is Change Data Capture?</h3><p>Change Data Capture is a design pattern that identifies and captures changes made to data in a database, then delivers those changes in real time to downstream consumers.</p><p>Traditional approaches, such as polling the database on a timer or writing triggers that push to a message queue, have serious drawbacks. Polling is wasteful and introduces latency. Triggers couple your application logic to the database and can degrade write performance.</p><p>CDC takes a fundamentally different approach: it reads the database’s transaction log. Every relational database already records every change in its log for crash recovery and replication. CDC simply taps into that existing stream. The result is:</p><ul><li>Zero impact on write performance — no triggers, no additional queries</li><li>Complete fidelity — every change is captured, including the before and after state</li><li>Low latency — changes are available within seconds</li></ul><p>SQL Server has built-in CDC support at the engine level, and Debezium is the open-source platform that turns those CDC logs into Kafka events.</p><h3>Architecture Overview</h3><p>Here’s how the pieces fit together:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/792/1*5rKu3_NQazp6NABCEJ-w2A.png\" /></figure><p>The data flow is:</p><ol><li>A client sends an HTTP request to the Account Manager API (create, update, or delete an account).</li><li>The Account Manager writes the change to SQL Server.</li><li>SQL Server’s CDC agent captures the change from the transaction log.</li><li>Debezium reads the CDC log and publishes a structured event to a Kafka topic.</li><li>The Account Receiver consumes the event from Kafka and processes it.</li></ol><p>In this architecture, services 2 and 5 are completely decoupled. The Account Manager doesn’t know (or care) that anyone is listening to its changes. The Account Receiver doesn’t know (or care) where the data originally came from.</p><h3>Tech Stack Overview</h3><ul><li><strong>MS SQL Server</strong>: Stores the account data and emits change events via CDC.</li><li><a href=\"https://debezium.io\"><strong>Debezium</strong></a>: Captures database changes and publishes them to Kafka.</li><li><a href=\"https://kafka.apache.org\"><strong>Kafka</strong></a><strong>: </strong>The messaging platform for CDC events and its web UI for inspection.</li><li><strong>Spring Boot</strong>: Contains two microservices:</li><li>account-manager: Performs account operations.</li><li>account-receiver: Listens for CDC events and logs them.</li></ul><p>For this implementation, we use <strong>SQL Server’s built-in Change Data Capture (CDC)</strong> to detect data changes at the database level.</p><p>Although SQL Server can propagate CDC changes directly to another SQL Server instance, we avoid that approach to prevent coupling consumers to a specific database technology. Instead, we stream changes to Kafka, allowing any downstream service to consume them independently.</p><p>To bridge SQL Server CDC and Kafka, we use <strong>Debezium, </strong>a connector that reads the database transaction log and publishes structured change events to Kafka topics. On the consumer side, a minimal Spring Boot application listens to these Kafka messages and processes them according to some business logic.</p><h3>Implementation</h3><h4>Setting Up the Infrastructure</h4><p>Everything runs in Docker. The docker-compose.yaml defines five services:</p><pre>version: &#39;3&#39;<br><br>volumes:<br>  mssqldata:<br><br>services:<br><br>  mssql:<br>    image: mcr.microsoft.com/mssql/server:2019-latest<br>    ports:<br>      - &#39;1433:1433&#39;<br>    volumes:<br>      - mssqldata:/var/opt/mssql<br>      # copy the scripts to the container<br>      - ./scripts:/usr/src/app<br>    working_dir: /usr/src/app<br>    # run the Db setup script<br>    command: sh -c &#39;./entrypoint.sh &amp; /opt/mssql/bin/sqlservr&#39;<br>    environment:<br>      SA_PASSWORD: &#39;myStrongPassword123&#39;<br>      ACCEPT_EULA: &#39;Y&#39;<br>      MSSQL_AGENT_ENABLED: &#39;true&#39;<br><br>  zookeeper:<br>    image: confluentinc/cp-zookeeper:latest<br>    environment:<br>      ZOOKEEPER_CLIENT_PORT: 2181<br>      ZOOKEEPER_TICK_TIME: 2000<br>    ports:<br>      - 22181:2181<br><br>  kafka:<br>    image: confluentinc/cp-kafka:latest<br>    depends_on:<br>      - zookeeper<br>    ports:<br>      - 29092:29092<br>    environment:<br>      KAFKA_BROKER_ID: 1<br>      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br>      KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1<br>      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092<br>      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT<br>      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT<br>      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1<br><br>  debezium:<br>    image: quay.io/debezium/connect:latest<br>    ports:<br>      - 8083:8083<br>    links:<br>      - kafka<br>      - mssql<br>    environment:<br>      - BOOTSTRAP_SERVERS=kafka:9092<br>      - GROUP_ID=1<br>      - CONFIG_STORAGE_TOPIC=connect_configs<br>      - OFFSET_STORAGE_TOPIC=connect_offsets<br>      - STATUS_STORAGE_TOPIC=connect_statuses<br><br>  kafka-ui:<br>    image: provectuslabs/kafka-ui:latest<br>    ports:<br>      - 8081:8080<br>    links:<br>      - kafka<br>    environment:<br>      DYNAMIC_CONFIG_ENABLED: true<br>      KAFKA_CLUSTERS_0_NAME: local<br>      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092</pre><p>A few things worth noting:</p><ul><li>MSSQL_AGENT_ENABLED: &#39;true&#39; is critical. SQL Server CDC depends on the SQL Server Agent service to read the transaction log. Without it, CDC won&#39;t capture any changes.</li><li>Kafka exposes two listeners: PLAINTEXT on port 9092 for communication between Docker containers (Debezium, Kafka UI), and PLAINTEXT_HOST on port 29092 for applications running on the host machine (our Spring Boot services).</li><li>Debezium runs as a Kafka Connect worker. It uses three internal Kafka topics (connect_configs, connect_offsets, connect_statuses) to manage its own state, which means it&#39;s stateless and can be restarted without losing its position in the transaction log.</li></ul><h4>Enabling CDC on SQL Server</h4><p>When the SQL Server container starts, an initialization script runs automatically to create the database and enable CDC:</p><pre>IF NOT EXISTS(SELECT * from sys.databases WHERE name = &#39;accounts&#39;)<br>    CREATE DATABASE accounts;<br>GO<br><br>USE accounts;<br>GO<br><br>CREATE TABLE account (<br>  id UNIQUEIDENTIFIER PRIMARY KEY,<br>  name VARCHAR(255) NOT NULL,<br>  bank_name VARCHAR(255) NOT NULL,<br>  account_number VARCHAR(255) NOT NULL,<br>  balance DECIMAL(19,2) NOT NULL,<br>  type VARCHAR(10) NOT NULL<br>);<br>GO<br><br>-- Enable CDC at the database level<br>EXEC sys.sp_cdc_enable_db;<br>GO<br><br>-- Enable CDC on the specific table<br>EXEC sys.sp_cdc_enable_table<br>  @source_schema = N&#39;dbo&#39;,<br>  @source_name = N&#39;account&#39;,<br>  @capture_instance = N&#39;account_CDC&#39;,<br>  @supports_net_changes = 1,<br>  @role_name = NULL;<br>GO</pre><p>Two things happen here that are easy to miss:</p><ol><li>sys.sp_cdc_enable_db turns on CDC for the entire database. This creates a cdc schema and several system tables that track which tables are being monitored.</li><li>sys.sp_cdc_enable_table sets up CDC for a specific table. The @supports_net_changes = 1 parameter enables &quot;net changes&quot; queries, meaning if a row is updated multiple times between polls, you can retrieve just the final state. The @capture_instance name gives you control over naming if you ever need to reconfigure CDC without downtime.</li></ol><h4>The Account Manager: Writing to the Database</h4><p>The Account Manager is a straightforward Spring Boot CRUD application. It exposes a REST API and writes to SQL Server using Spring Data JPA.</p><p><strong>The Entity</strong></p><p>This is the simple object that is being monitored.</p><pre>@Entity<br>@Table(name = &quot;account&quot;)<br>@Getter @Setter<br>@NoArgsConstructor @AllArgsConstructor<br>public class AccountEntity {<br><br>    @Id<br>    @GeneratedValue<br>    private UUID id;<br><br>    @Column(nullable = false)<br>    private String name;<br><br>    @Column(name = &quot;bankName&quot;, nullable = false)<br>    private String bankName;<br><br>    @Column(name = &quot;accountNumber&quot;, nullable = false)<br>    private String accountNumber;<br><br>    @Column(nullable = false)<br>    private BigDecimal balance;<br><br>    @Enumerated(EnumType.STRING)<br>    @Column(nullable = false)<br>    private Type type;<br><br>    public enum Type {<br>        CHECKING, SAVINGS<br>    }<br>}</pre><h4>The REST Controller</h4><p>Just a CRUD controller for accounts:</p><pre>@RestController<br>@RequestMapping(&quot;api&quot;)<br>@RequiredArgsConstructor<br>public class AccountController {<br><br>    private final AccountService accountService;<br><br>    @Operation(summary = &quot;Create account&quot;)<br>    @PostMapping(&quot;/account&quot;)<br>    public ResponseEntity&lt;Account&gt; createAccount(@RequestBody Account account) {<br>        return ResponseEntity.ok(accountService.createAccount(account));<br>    }<br><br>    @Operation(summary = &quot;Update account&quot;)<br>    @PutMapping<br>    public ResponseEntity&lt;Account&gt; update(@RequestBody Account account) {<br>        return ResponseEntity.ok(accountService.updateAccount(account));<br>    }<br><br>    @Operation(summary = &quot;Delete account&quot;)<br>    @DeleteMapping<br>    @ResponseStatus(HttpStatus.NO_CONTENT)<br>    public void delete(@RequestParam String bankName,<br>                       @RequestParam String accountNumber) {<br>        accountService.deleteAccount(bankName, accountNumber);<br>    }<br><br>    @Operation(summary = &quot;Find all accounts&quot;)<br>    @GetMapping<br>    public ResponseEntity&lt;List&lt;Account&gt;&gt; findAll() {<br>        return ResponseEntity.ok(accountService.findAll());<br>    }<br>}</pre><h4>Registering the Debezium Connector</h4><p>Once all the Docker services are running, you need to tell Debezium <em>what</em> to monitor. This is done by POSTing a JSON configuration to the Kafka Connect REST API:</p><pre>{<br>    &quot;name&quot;: &quot;account-connector&quot;,<br>    &quot;config&quot;: {<br>        &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;,<br>        &quot;tasks.max&quot;: &quot;1&quot;,<br>        &quot;topic.prefix&quot;: &quot;mssql&quot;,<br>        &quot;database.hostname&quot;: &quot;mssql&quot;,<br>        &quot;database.port&quot;: &quot;1433&quot;,<br>        &quot;database.user&quot;: &quot;sa&quot;,<br>        &quot;database.password&quot;: &quot;myStrongPassword123&quot;,<br>        &quot;database.names&quot;: &quot;accounts&quot;,<br>        &quot;schema.history.internal.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,<br>        &quot;schema.history.internal.kafka.topic&quot;: &quot;schema-changes.accounts&quot;,<br>        &quot;database.encrypt&quot;: &quot;false&quot;<br>    }<br>}</pre><p>Key configuration points:</p><ul><li>topic.prefix determines the Kafka topic naming. Debezium creates topics using the pattern {prefix}.{database}.{schema}.{table}. With prefix mssql, database accounts, schema dbo, and table account, the topic becomes mssql.accounts.dbo.account.</li><li>schema.history.internal.kafka.topic is where Debezium stores DDL history. If the table schema changes (columns added/removed), Debezium needs to know the schema at each point in time to correctly deserialize the log entries.</li><li>database.hostname uses mssql (the Docker service name), not localhost, because Debezium runs inside the Docker network.</li></ul><h4>The Account Receiver: Consuming CDC Events</h4><p>The Account Receiver is a Spring Boot application that listens to the Kafka topic and processes Debezium change events.</p><p><strong>Kafka Configuration</strong></p><pre>@Configuration<br>@EnableKafka<br>public class KafkaConfig {<br><br>    @Value(value = &quot;${kafka.bootstrapAddress:localhost:29092}&quot;)<br>    private String bootstrapAddress;<br><br>    private String groupId = &quot;consumer-test-group&quot;;<br><br>    @Bean<br>    public ConsumerFactory&lt;String, String&gt; consumerFactory() {<br>        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();<br>        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);<br>        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);<br>        return new DefaultKafkaConsumerFactory&lt;&gt;(props,<br>            new StringDeserializer(), new StringDeserializer());<br>    }<br><br>    @Bean<br>    public ConcurrentKafkaListenerContainerFactory&lt;String, String&gt;<br>            kafkaListenerContainerFactory() {<br>        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory =<br>            new ConcurrentKafkaListenerContainerFactory&lt;&gt;();<br>        factory.setConsumerFactory(consumerFactory());<br>        return factory;<br>    }<br>}</pre><p>Messages are consumed as raw strings (using StringDeserializer) rather than using Avro or a schema registry. This keeps things simple for a demo, though in production, you&#39;d likely want a schema registry for type safety.</p><h4>Understanding the Debezium Message Format</h4><p>A Debezium CDC message is a rich JSON document. The key structure is the payload, which contains:</p><pre>@Data<br>@JsonIgnoreProperties(ignoreUnknown = true)<br>public class Payload&lt;T&gt; {<br><br>    public enum OperationType {<br>        CREATE, UPDATE, DELETE<br>    }<br><br>    private T before;       // State before the change (null for inserts)<br>    private T after;        // State after the change (null for deletes)<br>    private DebeziumMessage.Source source;  // Metadata (LSN, timestamp, etc.)<br>    private String op;      // &quot;c&quot; = create, &quot;u&quot; = update, &quot;d&quot; = delete<br>    private Long ts_ms;     // Timestamp of the event<br><br>    public OperationType getOperationType() {<br>        switch (op) {<br>            case &quot;c&quot;: return OperationType.CREATE;<br>            case &quot;u&quot;: return OperationType.UPDATE;<br>            case &quot;d&quot;: return OperationType.DELETE;<br>            default: throw new IllegalArgumentException(&quot;Unknown operation type &quot; + op);<br>        }<br>    }<br>}</pre><p>For an INSERT, before is null and after contains the new row. For a DELETE, before contains the deleted row and after is null. For an UPDATE, both before and after are populated, giving you a complete diff.</p><p><strong>The Account Model</strong></p><p>Debezium uses the database column names, so we need Jackson annotations to map them:</p><pre>@Data<br>public class DebeziumAccount {<br><br>    private String id;<br>    private String name;<br>    @JsonProperty(&quot;bank_name&quot;)<br>    private String bankName;<br>    @JsonProperty(&quot;account_number&quot;)<br>    private String accountNumber;<br>    private byte[] balance;<br>    private Account.Type type;<br>}</pre><p>Notice that balance is a byte[], not a BigDecimal. This is a Debezium quirk: DECIMAL columns are serialized as raw bytes representing the unscaled value. You need to reconstruct the BigDecimal manually.</p><p><strong>The Listener</strong></p><pre>@Slf4j<br>@Component<br>public class CdcChangeListener {<br><br>    private final ObjectMapper objectMapper;<br><br>    public CdcChangeListener() {<br>        objectMapper = new ObjectMapper();<br>        objectMapper.configure(<br>            DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);<br>    }<br><br>    @KafkaListener(topics = &quot;${kafka.topic}&quot;,<br>                   containerFactory = &quot;kafkaListenerContainerFactory&quot;)<br>    public void receive(String message) throws JsonProcessingException {<br>        DebeziumMessage&lt;DebeziumAccount&gt; debeziumMessage =<br>            objectMapper.readValue(message, DebeziumAccountMessage.class);<br><br>        var payload = debeziumMessage.getPayload();<br>        DebeziumAccount after = payload.getAfter();<br><br>        // Reconstruct BigDecimal from Debezium&#39;s byte[] encoding<br>        var balance = (after == null) ? null :<br>            new BigDecimal(new BigInteger(after.getBalance()), 2);<br><br>        switch (payload.getOperationType()) {<br>            case CREATE:<br>                log.info(&quot;Received CDC create for account {} with balance {}&quot;,<br>                    after.getAccountNumber(), balance);<br>                break;<br>            case UPDATE:<br>                log.info(&quot;Received CDC update for account {} with balance {}&quot;,<br>                    after.getAccountNumber(), balance);<br>                break;<br>            case DELETE:<br>                log.info(&quot;Received CDC delete for account {}&quot;,<br>                    payload.getBefore().getAccountNumber());<br>                break;<br>        }<br>    }<br>}</pre><p>The decimal reconstruction on line new BigDecimal(new BigInteger(after.getBalance()), 2) deserves attention. Debezium encodes DECIMAL(19,2) as the unscaled integer value in bytes. So a balance of 1000000.00 becomes the integer 100000000 encoded as bytes, and we reconstruct it by specifying scale 2.</p><h3>Running the Complete Demo</h3><h4>Prerequisites</h4><ul><li>Docker and Docker Compose</li><li>Java 21</li><li>Maven</li></ul><h4>Step 1: Start the Infrastructure</h4><pre>cd setup<br>docker-compose up -d</pre><p>Wait about 30 seconds for SQL Server to fully initialize (the entrypoint script has a 15-second sleep to wait for the database engine).</p><h4>Step 2: Register the Debezium Connector</h4><pre>curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @register-sqlserver.json</pre><p>If Debezium isn’t ready yet, you’ll get a connection refused error. Wait a few more seconds and retry.</p><p>You can verify the connector is running:</p><pre>curl http://localhost:8083/connectors/account-connector/status</pre><h4>Step 3: Start the Spring Boot Applications</h4><p>In separate terminals:</p><pre># Terminal 1 — Start the CDC consumer first<br>cd account-receiver<br>mvn spring-boot:run<br><br># Terminal 2 — Start the API<br>cd account-manager<br>mvn spring-boot:run</pre><h4>Step 4: Create an Account</h4><p>Open the Swagger UI at <a href=\"http://localhost:8080/swagger-ui/index.html#/account-controller/createAccount\">http://localhost:8080/swagger-ui/index.html#/account-controller/createAccount</a> and use the “Create account” endpoint with this request body:</p><pre>{<br>  &quot;name&quot;: &quot;Gru&quot;,<br>  &quot;bankName&quot;: &quot;Bank of Evil&quot;,<br>  &quot;accountNumber&quot;: &quot;666&quot;,<br>  &quot;balance&quot;: 1000000,<br>  &quot;type&quot;: &quot;CHECKING&quot;<br>}</pre><h4>Step 5: Watch the CDC Event Arrive</h4><p>In the Account Receiver terminal, you should immediately see:</p><pre>Received CDC create for account 666 with balance 1000000.00</pre><h4>Step 6: Explore in Kafka UI</h4><p>Open <a href=\"http://localhost:8081/\">http://localhost:8081</a>, click on Topics, and find mssql.accounts.dbo.account. Click on the Messages tab to see the raw Debezium event — a detailed JSON document containing the full schema, the before/after state, and source metadata including the LSN and commit timestamp.</p><h4>Step 7: Try Updates and Deletes</h4><p>Go back to Swagger and update the account’s balance, then delete it. You’ll see corresponding CDC events in the receiver logs:</p><pre>Received CDC update for account 666 with balance 2000000.00<br>Received CDC delete for account 666</pre><h3>Wrapping Up</h3><p>Using Change Data Capture (CDC) with Debezium requires some initial setup, but once configured, it provides a reliable way to turn database changes into events. Each committed change is captured and published without relying on polling, triggers, or dual writes.</p><p>The approach works because it builds on infrastructure that already exists. SQL Server maintains a transaction log for its own recovery and consistency guarantees. Debezium reads that log and converts the changes into structured events that can be sent to Kafka.</p><p>This pattern is useful in scenarios where multiple services need to stay synchronized, where audit trails are required, or where database changes must feed downstream systems such as search indexes or other event-driven components.</p><p>The complete source code is available on <a href=\"https://github.com/algorythmist/cdc-debezium-kafka\">GitHub</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1546addb0128\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/data-streaming-with-cdc-1546addb0128\">Data Streaming with CDC</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-18T18:21:28.000Z",
    "url": "https://levelup.gitconnected.com/data-streaming-with-cdc-1546addb0128?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Prompt Injection: The SQL Injection of LLMs?",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/prompt-injection-the-sql-injection-of-llms-59f27c0dbfac?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*Pho3OTu0v2xVizRhLmCBMA.png\" width=\"1536\"></a></p><p class=\"medium-feed-snippet\">For decades, software security assumed a simple rule: programs execute code, users provide data. Large language models quietly broke that&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/prompt-injection-the-sql-injection-of-llms-59f27c0dbfac?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:22.000Z",
    "url": "https://levelup.gitconnected.com/prompt-injection-the-sql-injection-of-llms-59f27c0dbfac?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Silently boost your backend performance with Go 1.26",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/silently-boost-your-backend-performance-with-go-1-26-293ef5ae35af?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1400/1*Q9CS6q3psp-futHPP2WDIQ.jpeg\" width=\"1400\"></a></p><p class=\"medium-feed-snippet\">Explaining whats HOT in Go 1.26 release</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/silently-boost-your-backend-performance-with-go-1-26-293ef5ae35af?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:15.000Z",
    "url": "https://levelup.gitconnected.com/silently-boost-your-backend-performance-with-go-1-26-293ef5ae35af?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "The “Maggi” Developer is Dead. Long Live the “Biryani” Architect.",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/the-maggi-developer-is-dead-long-live-the-biryani-architect-b61528b204dd?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*I9pRACVKS9ELI_SJRjunCQ.png\" width=\"2816\"></a></p><p class=\"medium-feed-snippet\">AI has turned coding into &#x201C;2-Minute Noodles.&#x201D; But the companies paying &#x20B9;50 LPA are starving for a slow-cooked Biryani. Here is why.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/the-maggi-developer-is-dead-long-live-the-biryani-architect-b61528b204dd?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:11.000Z",
    "url": "https://levelup.gitconnected.com/the-maggi-developer-is-dead-long-live-the-biryani-architect-b61528b204dd?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "코딩 튜토리얼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "The Hidden Skill of 2026: Knowing When to Ignore AI",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/the-hidden-skill-of-2026-knowing-when-to-ignore-ai-a3bcd5be6300?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*dzJrAzEpB3Tyup7F\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">Why the professionals who question AI outperform those who trust it blindly</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/the-hidden-skill-of-2026-knowing-when-to-ignore-ai-a3bcd5be6300?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding »</a></p></div>",
    "date": "2026-02-18T18:21:06.000Z",
    "url": "https://levelup.gitconnected.com/the-hidden-skill-of-2026-knowing-when-to-ignore-ai-a3bcd5be6300?source=rss----5517fd7b58a6---4"
  }
]