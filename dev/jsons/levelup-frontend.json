[
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Learning from the Crowd: Can One Agent Think Like Many?",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/learning-from-the-crowd-can-one-agent-think-like-many-566e76139546?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1903/1*WGda9HWtqQ_xjVrYIHnw8g.png\" width=\"1903\"></a></p><p class=\"medium-feed-snippet\">How Collective Intelligence Can Be Compressed into a Single Model</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/learning-from-the-crowd-can-one-agent-think-like-many-566e76139546?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:40:56.000Z",
    "url": "https://levelup.gitconnected.com/learning-from-the-crowd-can-one-agent-think-like-many-566e76139546?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Attention, Straight Up: The Definitive Guide",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/attention-straight-up-the-definitive-guide-2106bda7c07f?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*iQBznOzln_lGaTy_\" width=\"6016\"></a></p><p class=\"medium-feed-snippet\">A First-Principles Map and Examples for Data Scientists</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/attention-straight-up-the-definitive-guide-2106bda7c07f?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:40:51.000Z",
    "url": "https://levelup.gitconnected.com/attention-straight-up-the-definitive-guide-2106bda7c07f?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Why Do My AI Agents Perform Better Than Yours?",
    "partialText": "<h4>Is It a SkillÂ Issue?</h4><figure><img alt=\"AI Agent Skills\" src=\"https://cdn-images-1.medium.com/max/1024/0*73vsCrKdE0a1_S6u\" /><figcaption>Photo by <a href=\"https://unsplash.com/@cookiethepom?utm_source=medium&amp;utm_medium=referral\">Cookie the Pom</a> onÂ <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>As developers who use agentic models for coding, we all know the common pitfalls they bring in. Even the cutting-edge latest models fail from time to time to follow proper code practices, recognize the project architecture, and write reliable, consistent code.</p><p>However, most of these pitfalls are now mitigatable to a certain extent by using an SKILL.md file. Anthropic, with its Claude agent, introduced this Agent Skills framework last year, and now almost all popular agentic AI dev tools, such as Cursor, Kiro, Copilot, and Antigravity, are supporting it. This article describes how I used agent skills to make the AI agents I use for development perform significantly better than their typical behavior.</p><h3>Understanding How SkillsÂ Work</h3><p>Progressive disclosure is the architectural principle that underlies agent skills, making them scalable. Instead of dumping everything into context, skills load information inÂ layers:</p><h4>Level 1: Skill Discovery (Metadata Only)</h4><p>At startup, the agent loads only the name and description of each installed skill, which can typically be 30â€“100 tokens per skill. This lightweight metadata helps the agent know which skills exist without consuming context.</p><pre>---<br>name: nextjs-testing<br>description: Guide for testing Next.js applications using Playwright. Use when creating or debugging browser-based tests.<br>---</pre><h4>Level 2: Instructions Loading (On-Demand)</h4><p>When your request matches a skillâ€™s description, the agent loads the full SKILL.md file body into context. Only then do detailed instructions become available.</p><h4>Level 3: Resource Access (AsÂ Needed)</h4><p>Skills can bundle additional files (scripts, templates, examples) that the agent accesses only when explicitly referenced. These resources donâ€™t load untilÂ needed.</p><pre>nextjs-testing/<br>â”œâ”€â”€ SKILL.md              # Core instructions (loaded when skill triggers)<br>â”œâ”€â”€ test-template.ts      # Template file (loaded only if referenced)<br>â”œâ”€â”€ examples/<br>â”‚   â”œâ”€â”€ auth-test.ts      # Example tests (loaded on demand)<br>â”‚   â””â”€â”€ api-test.ts<br>â””â”€â”€ scripts/<br>    â””â”€â”€ setup-playwright.sh</pre><p>This architecture means you can install dozens of skills without bloating your context window. According to <a href=\"https://www.kevnu.com/en/posts/claude-agent-skills-deep-dive-a-new-paradigm-for-expanding-ai-agent-capabilities\">community analysis</a>, <strong>using skills can reduce token consumption by up to 75% </strong>compared to loading all documentation upfront.</p><h3>Skills Usage: VS Code vsÂ Cursor</h3><p>Agent Skills started as an open standard from Anthropic, but different platforms have implemented them with varying approaches. Letâ€™s compare two majorÂ players:</p><h4>VS Code with GitHubÂ Copilot</h4><p><strong>Implementation</strong>: GitHub Copilot in VS Code supports the full Agent Skills standard with both automatic activation and manual invocation via slash commands (/skill-name).</p><p><strong>File Locations</strong>:</p><ul><li>Project skills:Â .github/skills/,Â .claude/skills/,Â .agents/skills/</li><li>Personal skills (stored in profile): ~/.copilot/skills/, ~/.claude/skills/, ~/.agents/skills/</li></ul><p><strong>SKILL.md Format</strong>:</p><pre>---<br>name: webapp-testing<br>description: Guide for testing web applications using Playwright<br>argument-hint: [test file] [options]<br>user-invokable: true<br>disable-model-invocation: false<br>---<br># Skill Instructions<br>Your detailed instructions go here...<br></pre><p><strong>Frontmatter Options</strong>:</p><ul><li>user-invokable: Controls whether skill appears in slash command menu (default: true)</li><li>disable-model-invocation: Prevents automatic loading, requires manual invocation (default: false)</li><li>argument-hint: Hint text shown when skill is invoked as a slashÂ command</li></ul><p>For the full VS Code agent skills doc, <a href=\"https://code.visualstudio.com/docs/copilot/customization/agent-skills\">referÂ here!</a></p><h4>Cursor IDE</h4><p><strong>Implementation</strong>: Cursor supports skills alongside rules, commands, and hooks, which are ideally four distinct ways to shape AIÂ context.</p><p><strong>File Locations</strong>:</p><ul><li>Project skills:Â .cursor/skills/skill-name/SKILL.md</li><li>Personal skills (all projects): ~/.cursor/skills/skill-name/SKILL.md</li></ul><p><strong>Creating a Skill inÂ Cursor</strong>:</p><ol><li>Create the skill folder structure: Cursor suggests each skill be included in a separateÂ folder.</li></ol><pre># Project-specific skill<br>.cursor/skills/code-review/SKILL.md<br><br># User-level skill (all projects)<br>~/.cursor/skills/code-review/SKILL.md</pre><p>2. Create SKILL.md with required frontmatter:</p><pre>---<br>name: code-review<br>description: Perform thorough code reviews focusing on best practices, security, and performance. Use when reviewing pull requests or code changes.<br>---<br><br># Code Review<br>Review code changes systematically for quality, security, and maintainability.<br><br>## When to Use<br>- Use when the user asks for a code review<br>- Use when reviewing pull requests<br>- Use when analyzing code quality<br><br>## Instructions<br>- Review code structure and organization<br>- Check for security vulnerabilities<br>- Assess performance implications<br>- Verify adherence to project conventions<br>- Suggest improvements with explanations</pre><p><strong>Invocation</strong>: Type /skill-name in agent chat to invoke a skill manually.</p><p>For the full Cursor agent skills doc, <a href=\"https://cursor.com/docs/context/skills\">referÂ here!</a></p><blockquote><em>Note: As of right now, it is best to explicitly instruct the agent to use the specific skill, as otherwise it is likely to be ignored. This was noticed not only by me, but also by the Vercel team, as detailed in the following section.</em></blockquote><h3>The Vercel Evaluation: Skills vs AGENTS.md</h3><p>In January 2026, Vercel published surprising evaluation results that challenged assumptions about how agents should consume documentation. They tested two approaches for teaching agents Next.js 16 APIs: Skills and AGENTS.md. To summarize, in that test, the AGENTS.md-based static markdown approach outperformed sophisticated skill-based retrieval, even with fine-tuned triggers.</p><p>Funny enough, the lesser complexity of the AGENTS.md approach (no decisions to make) resulted in it winning over the skills-based approach.</p><h4>The Nuance: When to UseÂ Each</h4><p>This doesnâ€™t mean skills are useless. The keyÂ insight:</p><ul><li><strong>AGENTS.md</strong>: Best for broad, horizontal improvements (framework knowledge that applies to allÂ tasks)</li><li><strong>Skills</strong>: Best for vertical, action-specific workflows (explicit procedures like â€œupgrade Next.js versionâ€ or â€œmigrate to AppÂ Routerâ€)</li></ul><p>The two approaches complement each other. But keep in mind that the skills approach would likely use much less context. The full article is linkedÂ below.</p><p><a href=\"https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals\">AGENTS.md outperforms skills in our agent evals - Vercel</a></p><h3>Listing the Best Publicly Available Skills</h3><p>Over the last few months, Iâ€™ve discovered some great skills written by the community, and some that Iâ€™ve used and found astonishing improvements in how the agentsÂ work.</p><h4><a href=\"https://github.com/forrestchang/andrej-karpathy-skills/blob/main/skills/karpathy-guidelines/SKILL.md\">Karpathy Skill</a></h4><p>This simple yet vital skill allowed me to make sure that the agent writes more focused and clear code. It stops the agent from making assumptions without asking, over-engineering, and touching unnecessary code.</p><h4><a href=\"https://github.com/vercel-labs/agent-skills/blob/main/skills/react-best-practices/SKILL.md\">Vercel React Best Practices</a></h4><p>This best practices guide for React and Next.js was written by the Vercel team and includes priority-based rules to reduce the typical mistakes made byÂ agents.</p><h4><a href=\"https://github.com/nextlevelbuilder/ui-ux-pro-max-skill/blob/main/.claude/skills/ui-ux-pro-max/SKILL.md\">UI UX ProÂ Max</a></h4><p>The ultimate resource with advanced UI/UX design patterns and workflows for designing, developing, and reviewing components for 9 different frontendÂ stacks.</p><h4><a href=\"https://github.com/squirrelscan/skills/blob/main/audit-website/SKILL.md\">Audit Website</a></h4><p>The go-to skill for auditing your website for SEO, performance, security, accessibility, and technical issues.</p><blockquote><em>You can search and view many more skills from the </em><a href=\"https://skillsmp.com/\"><strong><em>Agent skills marketplace</em></strong></a><em> or the </em><a href=\"https://skills.sh/\"><strong><em>Vercel skills ecosystem</em></strong></a><em>.</em></blockquote><h3>Writing Your OwnÂ Skills</h3><p>Just using the publicly available skills is not enough for most of us developers working on different projects. This is why you can create skills to optimize the agent in any way youÂ want.</p><p>Based on research from Anthropic, Vercel, and the broader community, here are proven strategies for creating skills thatÂ work:</p><h4>Write Clear, Concise Descriptions</h4><p>The description field is critical as it&#39;s how agents decide whether to load your skill. Make it specific about both capabilities and useÂ cases.</p><p><strong>Bad</strong>:</p><pre>description: Helps with testing</pre><p><strong>Good</strong>:</p><pre>description: Guide for testing Next.js applications using Playwright. Use when creating browser-based tests, debugging test failures, or setting up test infrastructure.</pre><h4>Structure forÂ Scale</h4><p>When SKILL.md becomes unwieldy:</p><ul><li>Split content into separate files and reference them</li><li>Keep mutually exclusive contexts in separate files to reduce tokenÂ usage</li><li>Use code as both runnable scripts (like bash script.sh) and documentation, since runnable scripts are not loaded into theÂ context</li></ul><h4>Include ConcreteÂ Examples</h4><p>Abstract instructions confuse agents. Provide tiny, concrete examples:</p><pre>## Creating Tests<br>Example:<br>```typescript<br>// tests/auth.spec.ts<br>import { test, expect } from &#39;@playwright/test&#39;;<br>test(&#39;user can log in&#39;, async ({ page }) =&gt; {<br>  await page.goto(&#39;/login&#39;);<br>  await page.fill(&#39;[data-testid=&quot;email&quot;]&#39;, &#39;user@example.com&#39;);<br>  await page.fill(&#39;[data-testid=&quot;password&quot;]&#39;, &#39;password123&#39;);<br>  await page.click(&#39;[data-testid=&quot;submit&quot;]&#39;);<br>  await expect(page).toHaveURL(&#39;/dashboard&#39;);<br>});```</pre><h4>Add Required Checklists</h4><p>Give agents explicit steps toÂ follow:</p><pre>## Before Committing Code<br>The agent MUST:<br>1. Run `npm run lint` and fix all errors<br>2. Run `npm test` and ensure all tests pass<br>3. Check that no console.log statements remain<br>4. Verify TypeScript compilation succeeds<br>5. Update relevant documentation</pre><p>Following the above principles would allow you to write optimal skills for yourÂ agents.</p><p>And of course, you can use the agent to write the skill you want as well; you can specify a template or use a skill such as the <a href=\"https://github.com/anthropics/skills/blob/main/skills/skill-creator/SKILL.md\">skill-creator from Anthropic</a>.</p><h3>How to Get the Best Results When UsingÂ Skills</h3><p>I found the best results with skills when writing separate skills for project architecture, coding practices, specific library-related instructions to follow, and using the skill name to tell the agent to use that skill whenever you are prompting it. Since the skill usage is sequential, if you do not tell the agent which skill should be used, it might just use the first listed skill and ignore theÂ rest.</p><h3>Wrapping Up</h3><p>Skills will probably be the future of making the Agentic AI more efficient, as a model alone being smart is not enough; itâ€™s about smarter context management.</p><p>Start building your skill library today, and transform your AI agent from a helpful yet sometimes annoying assistant into a domain expert that truly understands yourÂ work.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb6a93369366\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/why-do-my-ai-agents-perform-better-than-yours-eb6a93369366\">Why Do My AI Agents Perform Better Than Yours?</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-16T19:40:45.000Z",
    "url": "https://levelup.gitconnected.com/why-do-my-ai-agents-perform-better-than-yours-eb6a93369366?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "What Vibe Coding Taught Me About Maintaining Someone Elseâ€™s AI-Generated Code",
    "partialText": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/699/1*mKF9WCJsLjBED7hEDiRerg.png\" /></figure><p><em>Real examples from an open-source Electron app that went viralâ€Šâ€”â€Šand what I learned refactoring it.</em></p><p><strong>TL;DRâ€Šâ€”â€Š</strong>I forked an open-source Electron project called Interview Coder to build my own app, <a href=\"https://getezzi.com/\">Ezzi</a>. The codebase was clearly vibe codedâ€Šâ€”â€Šfunctional but fragile. I found security holes, any types everywhere, 700-line monolithic files, and client-side credit management that should have been on the server. This article walks through real code examples of what I found and the patterns you should watch for when inheriting AI-generated code.</p><h3>The Rise of VibeÂ Coding</h3><p>â€œVibe codingâ€ is the new hot thing. You open Cursor or Claude, describe what you want, and let the AI write most of the code. Ship fast, iterate faster. The results can be impressiveâ€Šâ€”â€Šentire apps built in aÂ weekend.</p><p>But hereâ€™s what nobody talks about: what happens when someone else has to maintain thatÂ code?</p><p>I found out the hard way. In early 2025, I forked a project called Interview Coderâ€Šâ€”â€Šan Electron app that had blown up online. The concept was brilliant (an invisible overlay for coding interviews), and it worked. The creator had open-sourced it under MIT license. Perfect starting point for my own project,Â <a href=\"https://github.com/GetEzzi/ezzi-app\">Ezzi</a>.</p><p>Except the codebase was aÂ mess.</p><p>Not broken. Not non-functional. It ran. It did what it promised. But the internals told a different storyâ€Šâ€”â€Šone that I think is becoming increasingly common as more projects get built with AI assistance and less human oversight.</p><h3>What I ActuallyÂ Found</h3><p>Let me walk you through real examples. These are not hypothetical â€œbad code smellsâ€ from a textbook. This is actual code from a project with thousands of GitHubÂ stars.</p><h4>1. The â€œanyâ€Â Epidemic</h4><p>The first thing I noticed was TypeScript being used as if it were JavaScript with extra steps. The type system was barely utilized.</p><p>Hereâ€™s the global type definitions file in its entirety:</p><pre>// src/types/global.d.ts<br>interface Window {<br> __IS_INITIALIZED__: boolean<br> __CREDITS__: number<br> __LANGUAGE__: string<br> __AUTH_TOKEN__: string | null<br> supabase: any // Replace with proper Supabase client type if needed<br> electron: any // Replace with proper Electron type if needed<br> electronAPI: any // Replace with proper Electron API type if needed<br>}</pre><p>Three properties typed as any with comments that literally say â€œReplace with proper type if needed.â€ Thatâ€™s AI placeholder text that was never cleaned up. The AI generated a TODO and the developer shippedÂ it.</p><p>The pattern continues in the Electron API definitions:</p><pre>// src/types/electron.d.ts<br>onDebugSuccess: (callback: (data: any) =&gt; void) =&gt; () =&gt; void<br>onProblemExtracted: (callback: (data: any) =&gt; void) =&gt; () =&gt; void<br>onSolutionSuccess: (callback: (data: any) =&gt; void) =&gt; () =&gt; void<br>onUpdateAvailable: (callback: (info: any) =&gt; void) =&gt; () =&gt; void<br>onUpdateDownloaded: (callback: (info: any) =&gt; void) =&gt; () =&gt; void</pre><p>Every single callback parameter is any. The data flowing through the entire IPC layerâ€Šâ€”â€Šthe communication bridge between the Electron main process and the rendererâ€Šâ€”â€Šhas zero type safety. You could send anything and TypeScript wouldÂ shrug.</p><p>And the core state management in the mainÂ process:</p><pre>// electron/main.ts<br>function getProblemInfo(): any {<br> return state.problemInfo<br>}<br>function setProblemInfo(problemInfo: any): void {<br> state.problemInfo = problemInfo<br>}</pre><p>The state object itself had problemInfo: null as any as its definition. This is the central piece of data the app revolves aroundâ€Šâ€”â€Šthe extracted problem from screenshotsâ€Šâ€”â€Šand it has no type definition. Any code that reads this data is flyingÂ blind.</p><p><strong>The lesson:</strong> When AI writes TypeScript, it often defaults to any as a shortcut. It works, it compiles, and the AI moves on to the next function. But it defeats the entire purpose of using TypeScript. If youâ€™re inheriting a codebase like this, search for any first. The count will tell you a lot about how much the original developer reviewed the AIâ€™sÂ output.</p><h4>2. Security That Made Me Uncomfortable</h4><p>This was the part that genuinely alarmedÂ me.</p><p>The app stored the auth token in a global windowÂ object:</p><pre>// src/App.tsx<br>if (data?.session?.access_token) {<br> window.__AUTH_TOKEN__ = data.session.access_token<br>}</pre><p>And then the Electron main process would retrieve it by executing JavaScript in the renderer:</p><pre>// electron/ProcessingHelper.ts<br>private async getAuthToken(): Promise&lt;string | null&gt; {<br> const mainWindow = this.deps.getMainWindow()<br> try {<br>   await this.waitForInitialization(mainWindow)<br>   const token = await mainWindow.webContents.executeJavaScript(<br>     &quot;window.__AUTH_TOKEN__&quot;<br>   )<br> return token<br> } catch (error) {<br>   console.error(&quot;Error getting auth token:&quot;, error)<br>   return null<br> }<br>}</pre><p>executeJavaScript(â€œwindow.__AUTH_TOKEN__â€) from the main process. This is the Electron equivalent of reaching into someoneâ€™s pocket. It works, but it bypasses the entire security model that Electronâ€™s contextBridge was designed to enforce. The proper approach is IPC messaging through the preloadÂ script.</p><p>And then there was the credit management. Subscription credits were subtracted on theÂ client:</p><pre>// src/App.tsx<br>const { data: updatedSubscription, error } = await supabase<br> .from(&quot;subscriptions&quot;)<br> .update({ credits: currentSubscription.credits - 1 })<br> .eq(&quot;user_id&quot;, user.id)<br> .select(&quot;credits&quot;)<br> .single()</pre><p>The client fetches the current credit count, subtracts one, and writes it back. Without Row Level Security properly configured, anyone could modify the query to set their credits to whatever they wanted. Or skip the subtraction entirely. This is business logic that belongs on a server, behind authentication, with proper validation.</p><p><strong>The lesson: </strong>AI tools donâ€™t think about security architecture. They produce code that works for the happy path. When you see Supabase queries modifying sensitive data directly from the client, or auth tokens passed between processes through globals instead of proper IPCâ€Šâ€”â€Šthese are not style issues. These are design flaws that undermine your entire trustÂ model.</p><h4>3. Monolithic Files</h4><p>AI tends to keep adding code to whatever file itâ€™s currently working in. The result is a handful of massive files that do everything.</p><p>ProcessingHelper.tsâ€Šâ€”â€Š700+ lines. One class handling: credential retrieval, language detection, auth token management, screenshot processing, solution generation, debug processing, request cancellation, and error handling for all of theÂ above.</p><p>App.tsxâ€Šâ€”â€Š700+ lines. The root React component that manages: authentication forms, subscription state, credit management, Supabase real-time subscriptions, toast notifications, language preferences, and initialization logic.</p><p>main.tsâ€Šâ€”â€Š660+ lines. The Electron main process containing: application state, window management, helper initialization, auth callback handling, window movement calculations, screenshot delegation, IPC setup, and environment variableÂ loading.</p><p>Three files, over 2,000 lines, doing virtually everything the appÂ does.</p><p>The ProcessingHelper class is a good example of the problem. Hereâ€™s its credit checkingÂ method:</p><pre>// electron/ProcessingHelper.ts<br>private async getCredits(): Promise&lt;number&gt; {<br>  const mainWindow = this.deps.getMainWindow()<br>  if (!mainWindow) return 0<br><br>  try {<br>    await this.waitForInitialization(mainWindow)<br>    const credits = await mainWindow.webContents.executeJavaScript(<br>      &quot;window.__CREDITS__&quot;<br>    )<br><br>    if (<br>      typeof credits !== &quot;number&quot; ||<br>      credits === undefined ||<br>      credits === null<br>    ) {<br>      console.warn(&quot;Credits not properly initialized&quot;)<br>      return 0<br>    }<br><br>    return credits<br>  } catch (error) {<br>    console.error(&quot;Error getting credits:&quot;, error)<br>    return 0<br>  }<br>}</pre><p>Look at that type check: typeof creditsÂ !== â€œnumberâ€ || credits === undefined || credits === null. If typeof creditsÂ !== â€œnumberâ€ is true, then checking for undefined or null is redundant. This reads like an AI pattern-matching from multiple examples without understanding the logic. Itâ€™s defensive coding that defends againstÂ nothing.</p><p>The same file has identical patterns for getLanguage() and getAuthToken()â€Šâ€”â€Šeach method polling the rendererâ€™s global state through executeJavaScript. Three methods, same structure, same problems, no abstraction.</p><p><strong>The lesson:</strong> If your inherited codebase has a few files that are 500+ lines each, thatâ€™s a strong signal of AI-generated code that was never decomposed. AI doesnâ€™t refactor proactively. It just keeps adding functions to the current file until you tell it toÂ stop.</p><h4>4. The â€œwindow.__GLOBAL__â€ Pattern</h4><p>Instead of using React Context, a state management library, or even Electronâ€™s IPC properly, the app synchronized state through global window variables:</p><pre>// src/App.tsx - setting global state<br>const updateCredits = useCallback((newCredits: number) =&gt; {<br>  setCredits(newCredits)          // React state<br>  window.__CREDITS__ = newCredits // also global state<br>}, [])<br><br>const updateLanguage = useCallback((newLanguage: string) =&gt; {<br>  setCurrentLanguage(newLanguage)   // React state<br>  window.__LANGUAGE__ = newLanguage // also global state<br>}, [])<br><br>const markInitialized = useCallback(() =&gt; {<br>  setIsInitialized(true)            // React state<br>  window.__IS_INITIALIZED__ = true  // also global state<br>}, [])</pre><p>Every state update writes to two places: Reactâ€™s state and a global window property. Two sources of truth. If they ever get out of syncâ€Šâ€”â€Šand they willâ€Šâ€”â€Šgood luck debugging which one isÂ correct.</p><p>The reason for the duplication is that the Electron main process needed access to these values. But instead of using IPC (which Electron is designed for), the main process reaches into the renderer via executeJavaScript:</p><pre>// electron/ProcessingHelper.ts<br>const isInitialized = await mainWindow.webContents.executeJavaScript(<br>  &quot;window.__IS_INITIALIZED__&quot;<br>)<br>const credits = await mainWindow.webContents.executeJavaScript(<br>  &quot;window.__CREDITS__&quot;<br>)</pre><p>This creates a tight coupling between the main process and the rendererâ€™s implementation details. Change how React manages state, and the main process breaks. Rename a window variable, and nothing will tell you at compileÂ time.</p><p><strong>The lesson:</strong> AI tools often reach for the simplest working solution. Global state is simpler than proper IPC. It works in the demo, it passes a basic test, and the AI moves on. Watch for window.__anything__ patternsâ€Šâ€”â€Štheyâ€™re a sign that cross-process communication was hacked together rather than designed.</p><h4>5. Duplicated Code Everywhere</h4><p>AI tools donâ€™t maintain awareness of what already exists in the codebase. If you ask for similar functionality in a different component, you get a freshÂ copy.</p><p>The sign-out logic was duplicated across two components, character for character:</p><pre>// src/components/Solutions/SolutionCommands.tsx<br>const handleSignOut = async () =&gt; {<br>  try {<br>    localStorage.clear()<br>    sessionStorage.clear()<br>    const { error } = await supabase.auth.signOut()<br>    if (error) throw error<br>  } catch (err) {<br>    console.error(&quot;Error signing out:&quot;, err)<br>  }<br>}<br><br>// src/components/Queue/QueueCommands.tsx - identical copy<br>const handleSignOut = async () =&gt; {<br>  try {<br>    localStorage.clear()<br>    sessionStorage.clear()<br>    const { error } = await supabase.auth.signOut()<br>    if (error) throw error<br>  } catch (err) {<br>    console.error(&quot;Error signing out:&quot;, err)<br>  }<br>}</pre><p>Same function, same logic, two files. If you need to change the sign-out behavior (and I didâ€Šâ€”â€ŠI stripped out Supabase from the client entirely), you have to find and update everyÂ copy.</p><p>The same pattern repeated with screenshot mapping logic, tooltip visibility handling, and error display components. Each duplicated 2â€“3 times across the codebase.</p><p><strong>The lesson:</strong> After inheriting AI-generated code, search for duplicated blocks. The AI doesnâ€™t know (or care) that it already wrote the same function elsewhere. Deduplication is one of the first refactoring passes you shouldÂ do.</p><h4>6. Error Handling as an Afterthought</h4><p>The error handling throughout the codebase followed a consistent pattern: catch the error, log it, moveÂ on.</p><pre>// src/App.tsx<br>checkExistingSession()  // async function called without await, errors lost</pre><p>This is a fire-and-forget call to an async function. If it throws, nobody catches it. The user wonâ€™t see an error, and the app might be in a half-initialized state.</p><p>The ProcessingHelper had nested try-catch blocks where errors were typed asÂ any:</p><pre>// electron/ProcessingHelper.ts<br>} catch (error: any) {<br>  mainWindow.webContents.send(<br>    this.deps.PROCESSING_EVENTS.INITIAL_SOLUTION_ERROR,<br>    error  // sending the raw error object through IPC<br>  )<br>  console.error(&quot;Processing error:&quot;, error)<br>  if (axios.isCancel(error)) {<br>    mainWindow.webContents.send(<br>      this.deps.PROCESSING_EVENTS.INITIAL_SOLUTION_ERROR,<br>      &quot;Processing was canceled by the user.&quot;<br>    )<br>  } else {<br>    mainWindow.webContents.send(<br>      this.deps.PROCESSING_EVENTS.INITIAL_SOLUTION_ERROR,<br>      error.message || &quot;Server error. Please try again.&quot;<br>    )<br>  }<br>}</pre><p>Three things wrong here. First, the raw error object is sent through IPC before the type checkâ€Šâ€”â€Šso the renderer gets an unserialized error object. Then, depending on whether itâ€™s a cancellation or not, a second message is sent with a proper string. The renderer now gets two error events for one failure. Second, error: any means no type narrowing. Third, the error message fallback assumes error.message exists, which isnâ€™t guaranteed forÂ any.</p><p>And then there was a typo in the event constants that could cause silent failures:</p><pre>// electron/preload.ts<br>UNAUTHORIZED: &quot;procesing-unauthorized&quot;,  // &quot;procesing&quot; - missing a &#39;s&#39;</pre><p>While main.tsÂ had:</p><pre>// electron/main.ts<br>UNAUTHORIZED: &quot;processing-unauthorized&quot;,  // correct spelling</pre><p>Different strings for the same event. The preload listens for â€œprocesing-unauthorizedâ€ but main sends â€œprocessing-unauthorizedâ€. The unauthorized handler would neverÂ fire.</p><p><strong>The lesson:</strong> AI-generated error handling often looks correct at a glance but falls apart under scrutiny. Check for: unhandled promise rejections, catch (error: any) blocks, inconsistent error event naming, and duplicate errorÂ sends.</p><h3>What I Did AboutÂ It</h3><p>When I started building <a href=\"https://github.com/GetEzzi/ezzi-app\">Ezzi</a> from this fork, my first month was almost entirely refactoring. Hereâ€™s the rough order of operations that worked forÂ me:</p><p>1. <strong>Security audit first.</strong> I stripped out the client-side credit management, moved sensitive operations to the server, and replaced the window.__AUTH_TOKEN__ pattern with proper IPC. Security architecture issues are the ones that can actually hurtÂ users.</p><p>2. <strong>Type safety pass.</strong> I searched for every any and replaced them with proper interfaces. This immediately surfaced several bugs where functions were receiving data in unexpected shapes.</p><p>3. <strong>Decompose monolithic files.</strong> I broke ProcessingHelper.ts into smaller, focused modules. Same with App.tsxâ€Šâ€”â€Šextracted the auth form, subscription management, and initialization logic into their own components.</p><p>4. <strong>Deduplicate.</strong> Extracted shared logic into utility functions and custom hooks. The sign-out logic became a single useSignOut hook.</p><p>5. <strong>Clean up error handling.</strong> Typed errors properly, removed duplicate error sends, fixed the event name typos, and made sure async functions were properlyÂ awaited.</p><p>It wasnâ€™t glamorous work. But it was necessary. The app worked before I started, and it worked after. The difference is that now I can actually maintain it, extend it, and trustÂ it.</p><h3>A Checklist for Inheriting AI-Generated Code</h3><p>If youâ€™re about to fork or take over a vibe coded project, hereâ€™s what Iâ€™d checkÂ first:</p><p><strong>Security</strong><br>- Search for hardcoded API keys, URLs, and secrets<br>- Look for client-side operations that should be server-side (credit management, billing)<br>- Check how auth tokens are passed between processes<br>- Verify that sensitive business logic lives on the server, not in theÂ client</p><p><strong>Type Safety</strong><br>- Count the any typesâ€Šâ€”â€Šif there are more than a handful, expect problems<br>- Check if interfaces exist for the core data structures<br>- Look for AI placeholder comments like â€œReplace with properÂ typeâ€</p><p><strong>Architecture</strong><br>- Check file sizesâ€Šâ€”â€Šfiles over 500 lines likely need decomposition<br>- Look for the window.__SOMETHING__ pattern<br>- Search for duplicated code blocks across components<br>- Verify that cross-process communication uses properÂ channels</p><p><strong>Error Handling</strong><br>- Search for catch (error: any) blocks<br>- Look for fire-and-forget async calls (async functions called without await)<br>- Check for duplicate error event sends<br>- Verify error event naming is consistent acrossÂ files</p><h3>The BiggerÂ Picture</h3><p>Iâ€™m not against vibe coding. I used AI assistants extensively while building <a href=\"https://github.com/GetEzzi/ezzi-app\">Ezzi</a>â€Šâ€”â€ŠCursor, Junie, Claude Code. Theyâ€™re genuinely powerful tools that let you moveÂ fast.</p><p>But thereâ€™s a difference between using AI as a collaborator and using it as a replacement for understanding your own code. The original Interview Coder project shipped fast and went viral. Thatâ€™s a success by many metrics. But the code underneath was a liability waiting toÂ happen.</p><p>The AI doesnâ€™t care about maintainability. It doesnâ€™t think about who will read this code next month. It optimizes for â€œdoes it work right nowâ€ and moves on. Thatâ€™s fine for prototypes and hackathons. Itâ€™s not fine for software that handles auth tokens and financial transactions.</p><p>If youâ€™re vibe coding your own project, at least do a review pass before shipping. And if youâ€™re inheriting someone elseâ€™s vibe coded project, budget serious time for refactoring before you start building on top ofÂ it.</p><p>Git is your friend. Commit often. And read the code the AIÂ writes.</p><p>I hope this was helpful. Good luck, and happy engineering!</p><p>If youâ€™re curious about the result of all this refactoring, Ezzi is open source on GitHub: <a href=\"https://github.com/GetEzzi/ezzi-app\">github.com/GetEzzi/ezzi-app</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=73ab88cf8208\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/what-vibe-coding-taught-me-about-maintaining-someone-elses-ai-generated-code-73ab88cf8208\">What Vibe Coding Taught Me About Maintaining Someone Elseâ€™s AI-Generated Code</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-16T19:40:35.000Z",
    "url": "https://levelup.gitconnected.com/what-vibe-coding-taught-me-about-maintaining-someone-elses-ai-generated-code-73ab88cf8208?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Legacy Lobotomyâ€Šâ€”â€ŠSetting up MailPit for Email Testing",
    "partialText": "<h3>Legacy Lobotomyâ€Šâ€”â€ŠSetting up MailPit for EmailÂ Testing</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*cdS6HiSiFUj-oreU\" /><figcaption>Photo by <a href=\"https://unsplash.com/@justin_morgan?utm_source=medium&amp;utm_medium=referral\">Justin Morgan</a> onÂ <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>This is the 19th tutorial in the series on refactoring a legacy Django project. In the previous tutorials, we implemented Django commands and a bash script to seed testÂ data.</p><p>Before we proceed with testing the API, we need to set up a way to handle emails locally. Some API endpoints send emails to users. For example, the password reset endpoints send an email with a link to reset the password. To test these endpoints during development, we need a way to capture and view the emails without sending them to real email addresses or connecting to a real emailÂ server.</p><p>In this article, we will set up MailPitâ€Šâ€”â€Ša local email testing tool that captures all emails sent by the application and displays them in a web interface.</p><p>To get more ideas regarding the project used for this tutorial or check other tutorials in this series, check the introductory article published earlier.</p><p><a href=\"https://levelup.gitconnected.com/legacy-lobotomy-confident-refactoring-of-a-django-project-adbeb064c455\">Legacy Lobotomyâ€Šâ€”â€ŠConfident Refactoring of a Django Project</a></p><h3>Origin branch and destination branch</h3><ol><li>If you want to follow the steps described in this tutorial, you should start from the <a href=\"https://github.com/JeyKip/legacy-lobotomy/tree/building-more-realistic-data\">building-more-realistic-data</a> branch.</li><li>If you want to only see final code state after all changes from this tutorial are applied, you can check the <a href=\"https://github.com/JeyKip/legacy-lobotomy/tree/setting-up-mailpit-for-email-testing\">setting-up-mailpit-for-email-testing</a> branch.</li></ol><h3>ğŸ“§ What isÂ MailPit?</h3><p><a href=\"https://mailpit.axllent.org/\">MailPit</a> is a local email testing tool that acts as a fake email server. It captures all emails sent by the application and displays them in a web interface. This lets you see the emails your application sends without actually delivering them toÂ anyone.</p><h3>ğŸ“ Step-by-step setup</h3><p><strong>Step 1: Configure Docker Compose</strong>. Add the MailPit service to docker-compose.yml under the servicesÂ section:</p><pre>services:<br>  db:<br>    # ... existing database configuration ...<br>  mailpit:<br>      image: axllent/mailpit<br>      restart: no<br>      volumes:<br>        - mailpit:/data<br>      ports:<br>        - &quot;8025:8025&quot;  # Web UI<br>        - &quot;1025:1025&quot;  # SMTP server<br>      environment:<br>        MP_MAX_MESSAGES: 5000<br>        MP_DATABASE: /data/mailpit.db<br>        MP_SMTP_AUTH_ACCEPT_ANY: 1<br>        MP_SMTP_AUTH_ALLOW_INSECURE: 1</pre><p>Then add the mailpit volume to the volumes section at the bottom of theÂ file:</p><pre>volumes:<br>  db: # db volume has been added earlier, add mailpit volume just below it<br>  mailpit:</pre><p>This configuration:</p><ul><li>Runs MailPit in a Docker container</li><li>Makes the web interface available at <a href=\"http://localhost:8025`\">http://localhost:8025</a></li><li>Makes the SMTP server available at port 1025 for the Django application to sendÂ emails</li><li>Stores up to 5000 messages before deleting oldÂ ones</li><li>Accepts any login credentials (useful forÂ testing)</li><li>Persists email data in a Docker volume so emails survive container restarts</li></ul><p>More information about runtime parameters supported by MailPit can be found in the official <a href=\"https://mailpit.axllent.org/docs/configuration/runtime-options/\">documentation</a>.</p><p><strong>Step 2: Configure Django email settings</strong>. Update theÂ .env file with the following SMTP settings:</p><pre>EMAIL_HOST=localhost<br>EMAIL_USE_TLS=False<br>EMAIL_PORT=1025<br>EMAIL_HOST_USER=<br>EMAIL_HOST_PASSWORD=</pre><p>These settings tell DjangoÂ to:</p><ul><li>Send emails to localhost on port 1025 (where MailPit is listening)</li><li>Not use TLS encryption (not needed for localÂ testing)</li><li>Not require a username or password (MailPit accepts emails fromÂ anyone)</li></ul><p><strong>Step 3: Start MailPit</strong>. Run the following command from the root of the project to start the MailPit container using DockerÂ Compose:</p><pre>docker compose up -d mailpit</pre><p>The -d flag runs the container in the background.</p><p><strong>Step 4: Verify MailPit is running</strong>. Open your browser and go to <a href=\"http://localhost:8025`\">http://localhost:8025</a>. You should see the MailPit web interface. It will be empty at first because no emails have been sentÂ yet.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iiylP5-UvkNJQ93MaZJf9g.png\" /><figcaption>MailPit web interface with no incomingÂ emails</figcaption></figure><p>When the Django application sends an email (for example, when a user requests a password reset), the email will appear in the MailPit interface.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qBepzv9-opm8GOoCBXH-Jw.png\" /><figcaption>MailPit web interface with a single incomingÂ email</figcaption></figure><p>You can click on the email to view its content, including any links or tokens needed for passwordÂ reset.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rr-vjjJZ_c08XwbucBzjjQ.png\" /><figcaption>Incoming email detailÂ page</figcaption></figure><h3>Conclusion</h3><p>In this article, we set up MailPit for email testing. Now we can test password reset endpoints and see the emails our application sends without connecting to a real emailÂ server.</p><p>In the next article, we will install Silk for performance profiling. Silk will help us find slow API endpoints and database queries that need optimization.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5bd1d105c5de\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/legacy-lobotomy-setting-up-mailpit-for-email-testing-5bd1d105c5de\">Legacy Lobotomyâ€Šâ€”â€ŠSetting up MailPit for Email Testing</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-16T19:40:29.000Z",
    "url": "https://levelup.gitconnected.com/legacy-lobotomy-setting-up-mailpit-for-email-testing-5bd1d105c5de?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "How to Use AI (Claude) to Build UI â€” Powered by the Design System",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/how-to-use-ai-claude-to-build-ui-powered-by-the-design-system-4a3bb1112f3d?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*eHkGtsC7NweudNfa\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">A practical workflow to stop generating generic code and start building production-ready interfaces</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/how-to-use-ai-claude-to-build-ui-powered-by-the-design-system-4a3bb1112f3d?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:39:57.000Z",
    "url": "https://levelup.gitconnected.com/how-to-use-ai-claude-to-build-ui-powered-by-the-design-system-4a3bb1112f3d?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "I Built a Dad Joke App in Rust Because I Was Bored",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/i-built-a-dad-joke-app-in-rust-because-i-was-bored-0406eb0c14c2?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*a6E_YuEULSaSYuXK\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">A non-programmer&#x2019;s guide to building your first Dioxus web app&#x200A;&#x2014;&#x200A;featuring the world&#x2019;s most important API.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/i-built-a-dad-joke-app-in-rust-because-i-was-bored-0406eb0c14c2?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:39:52.000Z",
    "url": "https://levelup.gitconnected.com/i-built-a-dad-joke-app-in-rust-because-i-was-bored-0406eb0c14c2?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Building a Bayesian Network from Actuarial Data",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/building-a-bayesian-network-from-actuarial-data-21026ae0acf3?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1200/0*6WXl5s11x1QS0aKK.jpeg\" width=\"1200\"></a></p><p class=\"medium-feed-snippet\">Extracting causal structure from claims and underwriting data</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/building-a-bayesian-network-from-actuarial-data-21026ae0acf3?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:39:42.000Z",
    "url": "https://levelup.gitconnected.com/building-a-bayesian-network-from-actuarial-data-21026ae0acf3?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Breaking and Defending HTTPS on Android: A Hands-On Certificate Pinning Lab",
    "partialText": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*U1Ba8FgdtoEGGqQMpIlkTw.png\" /></figure><p>HTTPS is no longer a nice to have in Android apps. It is the baseline expectation. It protects users from passive eavesdropping, active tampering, and a wide class of network level attacks that become trivial the moment traffic drops back to cleartext. Yet HTTPS alone does not solve every problem. If an attacker can convince a device to trust a malicious certificate authority or if the app is written to accept anything that looks like TLS, the lock icon becomes decorative rather than meaningful.</p><p>This is where certificate pinning usually enters the conversation. Pinning narrows trust from the entire public CA ecosystem down to a specific set of expected certificates or public keys. On paper, this sounds like an obvious upgrade. In practice, it comes with tradeoffs that are easy to underestimate.</p><p>Personally, I am not a big fan of certificate pinning as a default strategy. Certificate Transparency, when available, is often a better first line of defense. It provides visibility into misissued certificates without hard coding cryptographic assumptions into the app. More importantly, it avoids one of the most painful failure modes of pinning: certificate rotation and revocation. I have been on the receiving end of a revoked or expired certificate that instantly broke production apps because a pin was no longer valid. That is not a great experience for users or for the engineers onÂ call.</p><p>That said, there are cases where pinning is justified or even required. High risk applications, regulated environments, or threat models that explicitly include compromised CAs sometimes leave no alternative. The key is understanding exactly what pinning does, how it fails, and how to implement it in a way that is intentional rather than cargoÂ culted.</p><p>This article is based on material from the Android Security Training project. The project ships paired secure and deliberately vulnerable implementations for each topic so readers can compare behaviors side by side. The certificate pinning and HTTPS lab follows this approach closely. Secure flavors enforce trust using OkHttpâ€™s CertificatePinner or a manual TrustManager, while vulnerable flavors are designed to trust everything and be intercepted with a proxy duringÂ demos.</p><p><a href=\"https://github.com/LethalMaus/AndroidSecurityTraining\">GitHub - LethalMaus/AndroidSecurityTraining</a></p><p>The lab is driven by build time switches. A MANUAL_PIN flag controls whether pinning is enforced by OkHttp or by a custom TrustManager. A PIN_MODE flag toggles between good pins, bad pins, certificate transparency only, or a demo friendly MITM mode. Secure builds default to HTTPS only traffic, distrust user installed CAs except for debug overrides, and can enable Certificate Transparency via Network SecurityÂ Config.</p><p>In the sections that follow, we will break down each of these pieces in detail, starting with how the build switches route requests through different networkÂ helpers.</p><h3>Build switches and routing networkÂ behavior</h3><p>One of the most useful aspects of the Android Security Training project is that all of the behavior is explicit and switchable. Nothing is hidden behind magic defaults. Whether the app enforces certificate pinning, how it enforces it, or whether it deliberately disables verification is controlled by build time flags. This makes it easy to demonstrate attacks, compare implementations, and reason about the security posture of eachÂ variant.</p><p>At the center of this setup are two BuildConfig flags: MANUAL_PIN and PIN_MODE.</p><p>The MANUAL_PIN flag determines <em>how</em> pinning is enforced. When it is disabled, the app relies on OkHttpâ€™s built in CertificatePinner. This is the path most developers are familiar with and generally the safer option when pinning is required. When MANUAL_PIN is enabled, the app takes a lower level approach and enforces pins inside a custom TrustManager. This exists primarily as a learning tool. It shows what OkHttp is doing under the hood and why rolling your own TLS stack is easy to getÂ wrong.</p><p>The PIN_MODE flag controls <em>what</em> the app is willing to trust. Different modes are compiled into the app to support both secure and insecure scenarios. A â€œgoodâ€ mode uses valid pins that match the serverâ€™s public key and allows the connection to succeed. A â€œbadâ€ mode uses incorrect pins and reliably fails, demonstrating what happens when a pinned certificate changes unexpectedly. A â€œctâ€ mode relies on Certificate Transparency without pinning, aligning with a more operationally friendly security posture. Finally, a â€œmitmâ€ mode exists purely for demos, allowing traffic to be intercepted by a proxy without triggering pin failures.</p><p>These flags are not just configuration values. They actively route execution through different networking helpers. In secure configurations, requests flow through helpers that enforce HTTPS only connections, apply either OkHttp based or manual pinning, and respect Network Security Config constraints. In vulnerable configurations or demo modes, the app intentionally bypasses these checks so that man in the middle attacks are visible and repeatable.</p><p>This routing logic is what makes the lab effective. By flipping a single flag, you can observe how the exact same API call behaves when pins are correct, when they are wrong, when Certificate Transparency is doing the heavy lifting, or when all trust checks are disabled. It also makes one point very clear: security behavior should be explicit, intentional, and easy to reason about. If you cannot explain which path your app is taking and why, you probably should not be implementing it.</p><h3>OkHttp based certificate pinning</h3><p>When pinning is enabled and MANUAL_PIN is turned off, the secure flavor relies on OkHttpâ€™s CertificatePinner. This is generally the preferred way to do pinning on Android if you must do it at all. OkHttp handles the hard parts, integrates cleanly with the TLS stack, and fails closed when pins do notÂ match.</p><p>The core of this setup is the CertificatePinner configuration itself:</p><pre>val goodPinner = CertificatePinner.Builder()<br>    .add(<br>        &quot;api.github.com&quot;,<br>        &quot;sha256/1EkvzibgiE3k+xdsv+7UU5vhV8kdFCQiUiFdMX5Guuk=&quot;,<br>        &quot;sha256/fXkqYy8jL6cDXcYJvLgk0i8V0CVg28t3Tw4eBeaHeoA=&quot;,<br>        //&quot;sha256/KAXeO3wi3a3vmfBT/Q1P6pFoMVe1CI2IC5/f8arkEXE=&quot; //mitmproxy pin<br>    )<br>    .build()</pre><p>This snippet pins the Subject Public Key Info (SPKI) hashes for api.github.com. OkHttp will verify that at least one certificate in the validated server chain matches one of these hashes. If no match is found, the connection fails immediately.</p><p>First, these are <em>public key</em> pins, not full certificate pins. This is intentional. Pinning the SPKI allows the server to rotate certificates as long as the underlying key remains the same. It is still brittle, but far less so than pinning the entire certificate.</p><p>Second, multiple pins are defined. This is not optional boilerplate. Operationally, you should always ship backup pins. Without them, any key rotation or emergency replacement becomes a potential app breaking event. The commented out pin in the snippet is deliberately included for demo purposes. When enabled, it allows a MITM proxyâ€™s certificate to pass pin validation, making it obvious how dangerous even a single extra pin canÂ be.</p><p>Third, failure is explicit and loud. When pins do not match, OkHttp throws and the request fails. There is no silent fallback, no partial trust, and no opportunity for user installed CAs to sneak in if Network Security Config is set up correctly. This is both the strength and the risk of pinning. It works exactly as configured, even when that configuration is no longerÂ correct.</p><p>This is where your earlier point about Certificate Transparency becomes especially relevant. Pinning shifts risk from the CA ecosystem to your release process. If pins are wrong, expired, or revoked, the app is broken until users update. That tradeoff may be acceptable in some environments, but it should never be accidental.</p><p>In the training project, OkHttp based pinning represents the â€œsafeâ€ end of the pinning spectrum. It is still strict, but it avoids many of the footguns that come with manual TLS handling.</p><h3>Manual TrustManager path and SPKI enforcement</h3><p>When MANUAL_PIN is enabled, the lab shifts from using OkHttpâ€™s built in pinning to enforcing pins manually inside a custom TrustManager. This is the part of the project that is most educational and, in production, the part that should make you nervous. The goal here is not to encourage DIY TLS, but to show exactly what is involved when you step outside the guardrails.</p><p>Conceptually, the flow looks likeÂ this:</p><ul><li>The TLS handshake produces a server certificate chain.</li><li>The TrustManager validates the chain asÂ usual.</li><li>After validation, the code extracts the leaf certificateâ€™s public key material.</li><li>The code computes a SHA-256 SPKI pin from that publicÂ key.</li><li>The computed pin is compared against a known allowÂ list.</li><li>If there is no match, the connection fails with a CertificateException.</li></ul><p>The actual enforcement step is compact, but it carries a lot of security weight. In this lab, pin enforcement happens in a dedicated method that takes the server chain and decides whether to fail the connection:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/583a96100dce9232abefc2a31da473ec/href\">https://medium.com/media/583a96100dce9232abefc2a31da473ec/href</a></iframe><h4>The mitm bypass is intentional and dangerous</h4><p>The very first branch checks whether PIN_MODE is set to mitm. If it is, pin enforcement is skipped entirely and the connection is allowed toÂ proceed.</p><p>That line is doing exactly what the log says: it is a bypass for debug and demo only. In the lab, this mode exists so you can demonstrate interception tooling without constantly fighting pin failures. In real apps, anything like this needs to be handled with extreme care. A bypass that ships to production, even accidentally, is effectively a trust-all implementation with better marketing.</p><h4>The method fails closed byÂ default</h4><p>If there is no certificate chain, the code throws. If the computed pin does not match, the code throws. There is no attempt to â€œrecoverâ€, no fallback pin source, and no user prompt. That is what you want for security enforcement, but it also reinforces why pinning becomes an operational burden. You are turning a networking mismatch into a hardÂ outage.</p><h4>It pins the leaf certificateâ€™s SPKI</h4><p>The code uses the leaf certificate (chain[0]) and hashes the SPKI via spkiSha256Pin(leaf). This lines up with the OkHttp approach conceptually, but now you are responsible for every detail around it: correct certificate selection, correct hashing, correct comparison, correct host association, and correct integration with the rest of the trust evaluation.</p><p>This is where manual TrustManagers often go wrong. Many real-world implementations accidentally skip hostname validation, accept incomplete chains, or incorrectly mix â€œpinningâ€ with â€œtrustingâ€ in ways that widen trust rather than narrowing it. Even when the logic is correct, the maintenance cost is high, and the blast radius of mistakes isÂ large.</p><p>In this project, the manual path exists to show how pinning works without relying on OkHttp abstractions, and to demonstrate why the secure flavor defaults to safer mechanisms and stricter configuration.</p><h3>The vulnerable path and the cost of trusting everything</h3><p>The deliberately vulnerable flavor exists for one reason: to make failure visible. It is easy to talk about â€œman in the middle attacksâ€ in the abstract. It is much more convincing when you can flip a build flavor, run a proxy, and watch traffic decrypt in realÂ time.</p><p>The insecure behavior is implemented by creating a TLS stack that accepts any certificate chain, from any issuer, without validation. The lab does that by constructing a custom X509TrustManager whose checks areÂ empty:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/b657f5ec807379a9d5af11f8d284a5da/href\">https://medium.com/media/b657f5ec807379a9d5af11f8d284a5da/href</a></iframe><p>This is the â€œtrust-allâ€ pattern in its purest form. No matter what certificate a server presents, checkServerTrusted does nothing, meaning nothing is rejected. A proxy can generate a certificate on the fly for any host, present it to the client, and the client will accept it as if it were legitimate.</p><p>That breaks the core promise of HTTPS. You still get encryption, but you no longer get authenticity. The app is encrypting traffic to whoever answers the handshake, not necessarily to the server you intended.</p><p>In the context of the lab, this is useful because it creates a reliable baseline for proxyÂ demos:</p><ul><li>You can intercept and inspect requests without pinning failures.</li><li>You can modify responses and observe how the appÂ behaves.</li><li>You can show that â€œit worked on my phoneâ€ is not evidence of security.</li></ul><p>Outside of a lab, this is one of the most dangerous things you can ship. It is effectively an opt-in downgrade from â€œverify the server identityâ€ to â€œaccept any server that speaks TLS.â€ If hostname verification is also relaxed, it becomes even worse, because the app can be tricked into accepting a valid certificate for the wrong hostname or a completely untrusted certificate for the right hostname.</p><p>The main takeaway is simple: a single helper like this changes the security posture of the entire app. It does not create a small vulnerability. It removes the main protection HTTPS is supposed toÂ provide.</p><h3>Network Security Config and secureÂ defaults</h3><p>Code level pinning is only one layer of the story. In the secure flavors of the lab, a significant amount of trust policy is enforced before any networking code runs, using Androidâ€™s Network Security Config. This is where defaults are locked down and where dangerous behavior is made opt-in rather than accidental.</p><p>The secure profile uses the following configuration:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/50c112cc2e2d2d4a7d29fa7ebea62094/href\">https://medium.com/media/50c112cc2e2d2d4a7d29fa7ebea62094/href</a></iframe><h4>HTTPS-only byÂ default</h4><p>At the base level, cleartextTrafficPermitted is set to false. This ensures that all connections must use TLS. Even if a developer accidentally tries to call an http:// endpoint, the platform blocks it. This removes an entire class of mistakes before they reachÂ runtime.</p><h4>User-added CAs are distrusted in secureÂ builds</h4><p>The base configuration trusts only system certificates. User-installed CAs are explicitly excluded. This matters because many interception tools work by installing a user CA on the device. By default, a properly configured secure build will not trust those certificates, even if the user has installed them at the OSÂ level.</p><p>At the same time, the lab includes debug-overrides that re-enable user CAs for debug builds. This is a deliberate compromise. It allows developers and trainers to demonstrate MITM attacks without weakening release builds. The separation is clear and enforced by the platform.</p><h4>Certificate Transparency is enabled at the domainÂ level</h4><p>For api.github.com, Certificate Transparency is enabled directly in the config. This means the app requires that certificates presented for this domain comply with CT expectations, adding an extra layer of protection against misissued certificates.</p><p>Notably, there is no &lt;pin-set&gt; here. That omission is intentional. Pinning is controlled in code so that the lab can switch between good pins, bad pins, CT-only, or demo modes without rebuilding XML. This reinforces a broader point: Network Security Config is excellent for setting guardrails and defaults, while more experimental or instructional behavior can live in code where it is easier to reason about andÂ swap.</p><p>Taken together, this configuration demonstrates a strong baseline: HTTPS-only traffic, minimized trust anchors, debug-only exceptions, and CT as a first-class citizen.</p><h3>Lab flow and hands-on MITM walkthrough</h3><p>All of the pieces described so far come together in the lab itself. The value of this setup is not in reading the code, but in watching the behavior change as you switch build flavors and modes. The lab is designed to be run end to end, with a proxy in the middle, so the security impact of each decision is immediately visible.</p><h4>MITM quickÂ setup</h4><p>The lab starts with a standard man in the middle proxy setup. You follow the quick setup instructions, configure the device or emulator to route traffic through the proxy, and install the proxyâ€™s user CA certificate on the device. At this point, the environment is ready to intercept TLS traffic if the app allowsÂ it.</p><p>Nothing app-specific has changed yet. The difference comes entirely from which build flavor you run and how it is configured.</p><h4>Observing a successful MITM with vulnPinning</h4><p>The first build to run is vulnPinning. This flavor intentionally disables meaningful certificate verification by using the trust-all TLS stack. When you launch the app and make a network request, the proxy successfully intercepts theÂ traffic.</p><p>You can inspect requests and responses in clear text, modify payloads, and replay calls. From the appâ€™s point of view, everything looks normal. From a security point of view, the app has lost any guarantee about who it is talkingÂ to.</p><p>This step is important because it establishes a baseline. It shows that the proxy is correctly configured and that interception works when the app does not defendÂ itself.</p><h4>Watching interception fail with securePinning</h4><p>Next, you build and run securePinning. Without changing the proxy setup, you repeat the same requests. This time, thingsÂ break.</p><p>Depending on the configuration, requests fail during the TLS handshake or immediately after certificate validation. If pinning is enabled and the proxy presents its own certificate, the pins do not match and the connection is rejected. If user CAs are distrusted by Network Security Config, the handshake fails evenÂ earlier.</p><p>From the proxyâ€™s perspective, traffic simply stops. This is the moment where the abstract idea of â€œpinning blocks MITMâ€ becomes concrete.</p><h4>Exploring modes and failureÂ cases</h4><p>The lab does not stop at a single success or failure. It is intentionally flexible so you can explore different scenarios:</p><ul><li>Switch PIN_MODE to a bad pin and observe how even legitimate server trafficÂ fails.</li><li>Switch to a CT-only mode and see how behavior changes when pinning is removed but transparency is still enforced.</li><li>Enable mitm mode to temporarily bypass pin checks for demo purposes and confirm that interception worksÂ again.</li><li>Optionally rotate the server certificate or swap SPKI pins to simulate a real-world certificate change and observe how strict pinning turns that change into anÂ outage.</li></ul><p>These experiments reinforce an important lesson. Security mechanisms are not just theoretical controls. They directly affect availability, debuggability, and operational risk. The lab makes those tradeoffs visible instead of hiding them behind abstractions.</p><p>This hands-on flow is what ties the entire certificate pinning and HTTPS lab together. It gives you a safe place to see both sides: how trivial interception is when trust is too broad, and how unforgiving strict pinning can be when something changes unexpectedly.</p><h3>Conclusion</h3><p>In this article we have walked through how the Android Security Training project approaches certificate pinning and HTTPS security. We started with the <em>why, </em>why HTTPS matters, why certificate pinning exists, and how it shapes the trust decisions an app makes on every network call. We then examined the build switches (MANUAL_PIN and PIN_MODE) that let you explore secure and vulnerable behavior side by side. We looked at how OkHttpâ€™s CertificatePinner is configured and why backup pins matter, and we unpacked a manual TrustManager based SPKI enforcement path to show both its mechanics and its pitfalls. We contrasted that with a deliberately <em>trust-all</em> TLS stack that makes interception trivial. We then saw how Network Security Config provides strong defaults, HTTPS only, minimal trust anchors, and Certificate Transparency and how that ties into real world behavior.</p><p>Finally, we connected all of these pieces back to a <em>hands-on lab flow</em>. By building and running both the vulnerable (vulnPinning) and secure (securePinning) variants through a proxy, you can see how interception succeeds or fails, how wrong pins break connectivity, and how toggling modes like good/bad/ct/mitm changes outcomes. This isnâ€™t just code you read, itâ€™s a live experiment you run to internalize the impact of each decision.</p><p>All of the material and behavior described here is drawn from the Android Security Training project, an open source collection of hands-on labs that pair secure and vulnerable implementations so developers can <em>see</em> why best practices matter and <em>experience</em> the effects of common security mistakes. The project covers ten core Android security areas beyond just certificate pinning and HTTPS, with topics including end-to-end encryption, reverse-engineering resistance, runtime permissions, secure storage, andÂ more.</p><p><a href=\"https://github.com/LethalMaus/AndroidSecurityTraining\">GitHub - LethalMaus/AndroidSecurityTraining</a></p><p>Whether you agree with the default of strict pinning or prefer approaches like Certificate Transparency that reduce operational headaches, the key lesson from this lab is the same: security behavior should be explicit, observable, and testable, notÂ assumed.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d6191f01cbda\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/breaking-and-defending-https-on-android-a-hands-on-certificate-pinning-lab-d6191f01cbda\">Breaking and Defending HTTPS on Android: A Hands-On Certificate Pinning Lab</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-16T19:39:26.000Z",
    "url": "https://levelup.gitconnected.com/breaking-and-defending-https-on-android-a-hands-on-certificate-pinning-lab-d6191f01cbda?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "How to Add a Table of Contents to Your Medium Articles (The Semi-Automated Way)",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/how-to-add-a-table-of-contents-to-your-medium-articles-the-semi-automated-way-1c1bd78d9db7?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2348/1*JOxqIwNLEAw5fgJBVkeDTA.png\" width=\"2348\"></a></p><p class=\"medium-feed-snippet\">Who Is This Article For?</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/how-to-add-a-table-of-contents-to-your-medium-articles-the-semi-automated-way-1c1bd78d9db7?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-16T19:39:06.000Z",
    "url": "https://levelup.gitconnected.com/how-to-add-a-table-of-contents-to-your-medium-articles-the-semi-automated-way-1c1bd78d9db7?source=rss----5517fd7b58a6---4"
  }
]