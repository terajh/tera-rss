[
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "5 Principles for Writing Testable TS Code You Canâ€™t Ignore",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/5-principles-for-writing-testable-ts-code-you-cant-ignore-5180d8d7c5e2?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2496/1*kw2ks7v9Edc3PM4K2u6WXg.png\" width=\"2496\"></a></p><p class=\"medium-feed-snippet\">In this article, we&#x2019;ll walk through:</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/5-principles-for-writing-testable-ts-code-you-cant-ignore-5180d8d7c5e2?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:55.000Z",
    "url": "https://levelup.gitconnected.com/5-principles-for-writing-testable-ts-code-you-cant-ignore-5180d8d7c5e2?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Systems Thinking in an AI Era",
    "partialText": "<h4>Why understanding what we build stillÂ matters</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*tnoIXIzaCVZeKqyP\" /><figcaption>Photo by <a href=\"https://unsplash.com/@melotic?utm_source=medium&amp;utm_medium=referral\">Justin Zhu</a> onÂ <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>I just finished reading <a href=\"https://www.goodreads.com/book/show/3828902-thinking-in-systems\"><em>Thinking in Systems</em></a> by Donella Meadows, and itâ€™s one of those books that quietly rewires how you see theÂ world.</p><p>The book was published in 2008 based on notes she wrote in the 1990s, but what struck me most was how current the problems she described remain, even in 2026. As someone building software in the AI era, this really resonated withÂ me.</p><p>AI can write a lot of code for us now. It can even speed up learning. But it doesnâ€™t replace the judgment required to understand the systems weâ€™re building. That responsibility is stillÂ ours.</p><h3>An Inevitable Failure</h3><p>In late January 2026, Moltbook launched an AI-only social network. The founder proudly announced he â€œdidnâ€™t write a single line of code.â€ Within days, security researchers discovered a breach exposing 1.5 million API tokens, private messages, and email addresses. Security experts werenâ€™t shocked. The vulnerability was predictable: the system optimized for speed and virality, not security or verification.</p><p>Complex systems rarely fail because of bad intent. They behave exactly as theyâ€™re designed to. That idea feels uncomfortably relevant in modern technology. Outcomes that feel shocking or tragic are often the result of systems doing exactly what theyâ€™re optimized to do, even if no one inside the system would describe that as theÂ goal.</p><p>Why do these failures feel familiar instead of surprising? They follow patterns. Systems thinking is the framework for seeingÂ them.</p><h3>What Systems Thinking ActuallyÂ Is</h3><p>Systems thinking changes the question. Instead of asking <em>â€œwho messed up?â€</em> it asks, <em>â€œwhat is this system set up toÂ do?â€</em></p><p>One way to answer that question is to watch for repetition. When the same things keep happening, thatâ€™s often the system revealing how it actuallyÂ works.</p><p>Once you start looking at systems this way, what happens stops feeling random. You begin to see how incentives, feedback loops, and goals shape behavior, no matter who is involved. And once you see that, you start seeing the same patterns everywhere. Thatâ€™s especially true in software. And with AI in the mix, those patterns are showing up faster and at a much largerÂ scale.</p><h4><strong>Escalation</strong></h4><blockquote>â€œIf nothing is done to break the loop, the process usually ends with one or both of the competitors breakingÂ down.â€</blockquote><p><em>Escalation systems</em> reward keeping up, not slowing down. Competition pushes systems towards extremes. Models get bigger, faster, and more powerful. Competitors respond or risk irrelevance. Training costs rise, energy usage increases, and complexity explodes. No one can easily opt out without feeling like theyâ€™re fallingÂ behind.</p><p>Long-term risks stay abstract and delayed. Short-term gains are immediate and visible. The system rewards speed and novelty over restraint.</p><h4><strong>Seeking the wrongÂ goal</strong></h4><p>Systems optimize what you measure, so indicators become the goal. If a behavior persists over time, thereâ€™s likely a mechanism creating it, usually a feedback loop. That loop reveals what the system is actually designed toÂ do.</p><p>Feedback loops can only affect future behavior. They canâ€™t deliver a signal fast enough to correct the behavior that created the current problem. By the time you see the problem, the conditions that created it are alreadyÂ set.</p><p>This is why the â€œmove fast and break thingsâ€ mentality actually does break things. The feedback arrives after youâ€™ve alreadyÂ moved.</p><p>Take tech debt. Under constant pressure to ship, cleanup gets pushed to â€œlater.â€ More pressure, less time, more debt. The cycle continues.</p><p>We see this in workplaces that reward urgency but create burnout, in products that chase engagement at the expense of trust, and in teams that add process instead of fixing system problems.</p><h4><strong>Distance from Consequences</strong></h4><p>Scale and abstraction make harm easier to ignore, and AI increases that distance. We delegate decisions, implementations, and even reasoning without seeing how all those choices play out downstream.</p><p>In the Moltbook example, when things broke, the consequences werenâ€™t abstract. Users were exposed. Security teams scrambled. The distance between the builder and the system mattered.</p><p>Thatâ€™s what understanding the system gives us. A way to stay connected to the consequences.</p><h3>Old Problems, NewÂ Velocity</h3><p>Once you recognize these patterns, something else becomes clear: none of this isÂ new.</p><p>Meadows described these same system dynamics decades ago. What persists isnâ€™t a lack of solutions, but a tendency to treat systemic issues as isolated events. We treat symptoms instead of making structural changes, and then act surprised when the same problems resurface.</p><p>Systems thinking reframes failure. It treats recurring issues not as one-off crises, but as predictable outcomes. Problems feel permanently â€œunsolvedâ€ because they arenâ€™t meant to be fixed once. Theyâ€™re systems to understand and manage overÂ time.</p><p>In software, these dynamics are easy to recognize once you start paying attention. Work that always feels urgent. Decisions are made quickly because there isnâ€™t time to slow down. That kind of pace can feel productive, especially under pressure. But over time, it destabilizes the system and leaves no room to examine whatâ€™s actually causingÂ issues.</p><p>AI didnâ€™t invent these patterns, but it does compress them. What once took months now happens in days. What used to affect a single team can quickly spill across an organization and beyond. If AI is going to scale our systems this fast, understanding what weâ€™re building has to come before dealing with the consequences.</p><h3>What Systems ThinkingÂ Changes</h3><p>Systems thinking shifts your attention. It pushes you to ask questions like:</p><h4><strong>What is this system actually optimized for?</strong></h4><p>The answer isnâ€™t what we say. Itâ€™s what the behavior reveals. If a companyâ€™s mission statement says â€œqualityâ€ but promotions reward speed, then the system is optimized for speed. If AI usage is being monitored and quantified, the system rewards usage, not necessarily quality or learning.</p><h4><strong>What are the high-leverage points?</strong></h4><p>Most changes we make are low-leverage, like tweaking parameters, adding people, piling on process. High-leverage changes are much more uncomfortable. They require changing goals, incentives, and paradigms.</p><p>This could look like changing what you measure, not just how much. Changing what behavior gets rewarded. Questioning what â€œsuccessâ€ looksÂ like.</p><p>This is hard because paradigms defend themselves. Teams may resist new metrics because the old metrics defined their value. This is emotional work, not just technical.</p><p>Modern software is a socio-technical system: code, infrastructure, people, processes, and now, AI. This is why technical fixes often fail without human context. Nothing happens in isolation, and responsibility doesnâ€™t disappear just because systems get abstracted or automated.</p><h3>What We Choose Not ToÂ Optimize</h3><blockquote>â€œLiving successfully in a world of systems requires more than our ability to calculate. It requires our full humanityâ€Šâ€”â€Šour rationality, our intuition, our compassion, our vision, and our morality.â€</blockquote><p>We live in a culture obsessed with what we can measure, but some things donâ€™t show up in dashboards, like trust, meaning, quality, and relationships.</p><p>Systems thinking isnâ€™t about optimizing efficiency. Itâ€™s about protecting what matters while building at scale. Speed, output, and metrics are means, notÂ ends.</p><h3>What ComesÂ Next</h3><p>We are undeniably at an inflection point. AI is accelerating how quickly ideas turn into systems, and how quickly those systems shape real lives. That speed changes theÂ stakes.</p><p>The work ahead isnâ€™t just technical. Itâ€™s paying attention to the patterns weâ€™ve learned to normalize. To question what our systems reward. To notice what gets lost when speed becomes theÂ goal.</p><p>When AI writes the code, weâ€™re responsible for the systems it creates. Understanding them isnâ€™t optional.</p><h4><strong>Further Reading</strong></h4><p>Donella H. Meadows, <a href=\"https://www.goodreads.com/book/show/3828902-thinking-in-systems\"><em>Thinking in Systems: A Primer</em></a><em>Â (</em>2008).</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f37ab8144763\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/systems-thinking-in-an-ai-era-f37ab8144763\">Systems Thinking in an AI Era</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-12T23:40:53.000Z",
    "url": "https://levelup.gitconnected.com/systems-thinking-in-an-ai-era-f37ab8144763?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Python Advanced: 11 Performance Patterns for Data Engineers",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/python-advanced-11-performance-patterns-for-data-engineers-6e40e5231c68?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*S57qOqI-VL8KWQt96bnQ3Q.jpeg\" width=\"3932\"></a></p><p class=\"medium-feed-snippet\">Practical speed techniques that let Python punch above its weight in real-world data pipelines.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/python-advanced-11-performance-patterns-for-data-engineers-6e40e5231c68?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:51.000Z",
    "url": "https://levelup.gitconnected.com/python-advanced-11-performance-patterns-for-data-engineers-6e40e5231c68?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Stop Ignoring Outliers: 10 Data Cleaning Tricks I Wish I Knew Earlier (Pandas & NumPy 2026)",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/stop-ignoring-outliers-10-data-cleaning-tricks-i-wish-i-knew-earlier-pandas-numpy-2026-1dafcc786511?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*Q-qMOCN45E2mRS7F5_YyxA.jpeg\" width=\"3000\"></a></p><p class=\"medium-feed-snippet\">A real story: A single broken value once destroyed a full company dashboard. A sensor misfired and logged:</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/stop-ignoring-outliers-10-data-cleaning-tricks-i-wish-i-knew-earlier-pandas-numpy-2026-1dafcc786511?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:49.000Z",
    "url": "https://levelup.gitconnected.com/stop-ignoring-outliers-10-data-cleaning-tricks-i-wish-i-knew-earlier-pandas-numpy-2026-1dafcc786511?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "12 Pandas DataFrame Tricks That Will Matter Most in 2026",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/12-pandas-dataframe-tricks-that-will-matter-most-in-2026-02db211451e7?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*EiMTvlfR2BvK0VqLf186Wg.jpeg\" width=\"4104\"></a></p><p class=\"medium-feed-snippet\">Pandas keeps evolving, and the new era is all about: nullable dtypes, Arrow-powered I/O, cleaner aggregations, modern chainable workflows&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/12-pandas-dataframe-tricks-that-will-matter-most-in-2026-02db211451e7?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:47.000Z",
    "url": "https://levelup.gitconnected.com/12-pandas-dataframe-tricks-that-will-matter-most-in-2026-02db211451e7?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Pandas 2026: The New Data Cleaning Playbook (with Real-World Examples)",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/pandas-2026-the-new-data-cleaning-playbook-with-real-world-examples-3341cfa5af34?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*En9qtgwZWH-O50-TH0WsrQ.jpeg\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">Data cleaning is still the unglamorous 80% of every data job. But in 2026, Pandas offers far better tools, better dtypes, Arrow support&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/pandas-2026-the-new-data-cleaning-playbook-with-real-world-examples-3341cfa5af34?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:45.000Z",
    "url": "https://levelup.gitconnected.com/pandas-2026-the-new-data-cleaning-playbook-with-real-world-examples-3341cfa5af34?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "8 Pandas Performance Hacks for 2026 (That Actually Work)",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/8-pandas-performance-hacks-for-2026-that-actually-work-0c47fd9d8a61?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*0QjRI3Jq-YvpjSqsFC6N0A.jpeg\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">Pandas is fast when used right&#x200A;&#x2014;&#x200A;and painfully slow when used like it&#x2019;s 2015. Here are the 8 performance upgrades that actually move the&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/8-pandas-performance-hacks-for-2026-that-actually-work-0c47fd9d8a61?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:43.000Z",
    "url": "https://levelup.gitconnected.com/8-pandas-performance-hacks-for-2026-that-actually-work-0c47fd9d8a61?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "10 Cool CodePen Demos (January 2026)",
    "partialText": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/10-cool-codepen-demos-january-2026-b1ef86b4d201?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*FMppw8Nnq3WAimCI\" width=\"6000\"></a></p><p class=\"medium-feed-snippet\">A collection of 10 cool and exciting front-end demos shared on CodePen during January 2026</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/10-cool-codepen-demos-january-2026-b1ef86b4d201?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
    "date": "2026-02-12T23:40:41.000Z",
    "url": "https://levelup.gitconnected.com/10-cool-codepen-demos-january-2026-b1ef86b4d201?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "MySQL vs PostgreSQL: How B-Tree Indexes Store Your Data Differently",
    "partialText": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Lt-MpUs26Xscfiapc9Q1Sg.png\" /></figure><p>Indexes are one of the most fundamental tools in a database engineerâ€™s arsenal. Most developers know <em>what</em> an index doesâ€Šâ€”â€Šbut fewer stop to think about what actually lives insideÂ one.</p><p>MySQL and PostgreSQL both default to B-tree indexes, and on the surface they behave the same way. But there is a critical architectural difference hiding at the leaf level that shapes how each database stores data, and how secondary indexes work as aÂ result.</p><p>In this article, weâ€™ll cover what an index is, how B-trees are constructed in both MySQL and PostgreSQL, and what that difference means when it comes to secondary indexes.</p><h3>What is anÂ Index?</h3><p>When you query a database table, the database engine has to find the rows that match your conditions. Without any help, it does this by scanning every single row in the tableâ€Šâ€”â€Ša process known as a full table scan. For small tables, this is fine. For tables with millions of rows, it becomes a serious performance problem.</p><p>An index is a separate data structure that the database maintains alongside your table. It organizes a copy of one or more columns in a way that makes lookups fast, without having to read every row. Think of it like the index at the back of a bookâ€Šâ€”â€Šinstead of reading every page to find a topic, you jump straight to the right pageÂ number.</p><p>When you create an index on a column, the database keeps that structure updated as rows are inserted, updated, or deleted. You trade a small overhead on writes for a significant gain on reads. Most of the time, that is a trade worthÂ making.</p><h3>B-Trees</h3><p>The most common data structure used for database indexes is the B-tree, and it is the default in both MySQL and PostgreSQL.</p><p>A B-tree is a self-balancing tree where data is stored in sorted order across multiple levels. At the top sits the root node, which points to a set of internal nodes, which in turn point down to the leaf nodes at the bottom. When the database looks up a value, it starts at the root and follows the right path down the treeâ€Šâ€”â€Šcomparing values at each level until it reaches theÂ leaf.</p><p>Because the tree stays balanced, the number of steps needed to find any value is predictable and small, even for very large datasets. This makes B-trees especially efficient for exact lookups, range queries, and sortedÂ reads.</p><p>To be precise, both MySQL and PostgreSQL actually use a variant called the <strong>B+ tree</strong>. The distinction is subtle but important: in a B+ tree, all actual data references are stored exclusively at the leaf levelâ€Šâ€”â€Šinternal nodes only hold keys used for navigation. The leaf nodes are also linked together as a doubly-linked list, which makes range scans particularly efficient since the database can walk across leaves sequentially without climbing back up theÂ tree.</p><p>Where MySQL and PostgreSQL diverge is not in the shape of the treeâ€Šâ€”â€Šit is in what the leaf nodes actually contain. That is where things get interesting.</p><h3>MySQLâ€™s Clustered Index: The Table is theÂ Tree</h3><p>MySQLâ€™s default storage engine, InnoDB, uses what is called a <strong>clustered index</strong>. This means that the primary key index and the actual table data are the same structureâ€Šâ€”â€Šthe table <em>is</em> theÂ B-tree.</p><p>When you define a primary key, InnoDB organizes all rows physically on disk in the order of that key. As it builds the B-tree, the leaf nodes at the bottom of the tree do not just store a reference to the rowâ€Šâ€”â€Šthey store the <strong>entire row data</strong> directly. Every column, every value, all sitting inside the leaf nodeÂ itself.</p><p>If you do not define a primary key, InnoDB will silently pick a unique column to use instead, or generate a hidden 6-byte row ID to serve as one. Either way, a clustered index will alwaysÂ exist.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YDFVVTWyXushXDqSvLDmWw.png\" /></figure><h4>Performance implications</h4><p>The most direct benefit of the clustered index is read performance on primary key lookups. When you query by primary key, MySQL traverses the tree and arrives at the leaf node with everything already thereâ€Šâ€”â€Šthe full row is immediately available with no additional I/O.</p><p>Range queries also benefit significantly. Because rows are physically stored in primary key order, scanning a range of keys means reading contiguous pages on disk. This is much more efficient than jumping around to random locations, and it plays well with how modern storage systems prefetchÂ data.</p><p>However, the clustered index design comes with trade-offs on the write side. Because rows must stay in sorted order, inserting a row with a primary key that falls between two existing rows can force InnoDB to rearrange data. If the relevant page is full, InnoDB splits it into two pages to make roomâ€Šâ€”â€Ša process called a <strong>page split</strong>. Page splits are expensive and, over time, can lead to fragmentation, where pages are only partially filled and the physical order of data on disk no longer matches the logical order of theÂ tree.</p><p>This is why using random or unordered primary keysâ€Šâ€”â€Šlike UUIDsâ€Šâ€”â€Šis generally discouraged in MySQL. Every insert lands at a more or less random position in the tree, causing frequent page splits and fragmentation. Sequential keys, like auto-incrementing integers, always append to the end of the tree, avoiding this problem entirely.</p><h3>PostgreSQLâ€™s Approach: Indexes as Pointers to theÂ Heap</h3><p>PostgreSQL takes a different approach. The table data and the indexes are completely separate structures. Rows are stored in what PostgreSQL calls a <strong>heap file</strong>â€Šâ€”â€Šessentially a collection of pages where rows are written largely in insertion order, without any particular sorting.</p><p>When PostgreSQL builds a B-tree index, the leaf nodes do not contain the row data. Instead, they store a <strong>tuple identifier (TID)</strong>â€Šâ€”â€Ša physical pointer that records exactly which page and which slot within that page holds the actual row. Think of it as a precise address pointing into theÂ heap.</p><p>When you query using an index, PostgreSQL traverses the B-tree to find the matching TID, then does a second lookup into the heap file to fetch the actual row. This extra step is often called a <strong>heapÂ fetch</strong>.</p><p>This design means that in PostgreSQL, no index owns the data. The heap is the source of truth, and all indexesâ€Šâ€”â€Šincluding what you might think of as the â€œprimaryâ€ oneâ€Šâ€”â€Šare simply separate structures pointing back toÂ it.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6WAUrZ6u6a3FbkHlmV69AQ.png\" /></figure><h4>What happens when you update aÂ row</h4><p>This is where PostgreSQLâ€™s architecture reveals one of its more significant performance implications. In PostgreSQL, when a row is updated, PostgreSQL <strong>writes a brand new version of the row</strong> into the heap and marks the old version as dead. It does not modify the existing rowâ€Šâ€”â€Šit creates a newÂ one.</p><p>Now, here is where size matters. If the updated value is larger than the original, the new row version might not fit in the same heap page. PostgreSQL will write it into a different page entirely. This is not just a heap concernâ€Šâ€”â€Šit directly affects every index on thatÂ table.</p><p>Since the new row version lives at a different physical location, its TID has changed. And because every index leaf node stores a TID pointing to the physical location of the row, <strong>every single index on that table now needs to be updated</strong> to point to the new location. If your table has five indexes, all five need to reflect the new TID. This makes wide tables with many indexes particularly sensitive to frequentÂ updates.</p><p>There is one optimization worth knowing about: <strong>HOT updates</strong> (Heap Only Tuple). If the updated row fits on the same heap page and the updated column is not part of any index, PostgreSQL can perform a HOT updateâ€Šâ€”â€Šwriting the new row version on the same page and leaving all indexes untouched. The old and new versions are linked together on the page, and the indexes continue pointing to the original slot, which then redirects to the latest version. This is a significant optimization, but it only applies under those specific conditions.</p><p>Over time, dead row versions accumulate in the heap. PostgreSQL relies on a background process called <strong>VACUUM</strong> to clean them up, reclaim space, and keep performance from degrading. Without regular vacuuming, tables with heavy update workloads can bloat considerably, and index scans become slower as they encounter more dead tuples along theÂ way.</p><h3>MySQL vs PostgreSQL Index Constructionâ€Šâ€”â€ŠKey Differences</h3><p>At first glance, MySQL and PostgreSQL indexes look and behave the same way. Both use B+-trees. Both support fast lookups and range scans. Both are created with the same SQL syntax. But once you look at what lives inside the leaf nodes, the two databases reveal fundamentally different philosophies about where data should live and who should ownÂ it.</p><p>In MySQL, the primary key index <em>is</em> the table. Row data lives directly inside the leaf nodes, which means a primary key lookup is a single, self-contained operationâ€Šâ€”â€Štraverse the tree, reach the leaf, and the full row is already there. This tight coupling between the index and the data delivers excellent read performance on primary key queries, but it also means inserts must respect the physical order of the tree, making the choice of primary key a critical performance decision.</p><p>In PostgreSQL, the index and the data are always separate. Leaf nodes hold nothing but a pointerâ€Šâ€”â€Ša TIDâ€Šâ€”â€Što wherever the actual row lives in the heap. Every index lookup therefore requires two steps: find the TID in the tree, then go fetch the row from the heap. This adds overhead per lookup, but it also means PostgreSQLâ€™s storage model is more flexibleâ€Šâ€”â€Šno index has special ownership of the data, and all indexes are structurally equal.</p><p>This difference has a cascading effect on writes. In MySQL, updating a rowâ€™s primary key is an expensive operation because the row must physically move within the tree. In PostgreSQL, updating any row means writing a new version into the heap and potentially updating every index on the table to reflect the new TIDâ€Šâ€”â€Ša cost that grows with the number of indexes on theÂ table.</p><p>Neither approach is strictly better. MySQLâ€™s clustered index excels in read-heavy workloads with predictable primary key access patterns. PostgreSQLâ€™s heap-based model offers more flexibility, especially for write-heavy workloads and tables with complex indexing needsâ€Šâ€”â€Šthough it requires careful attention to vacuuming and HOT update conditions to stay performant.</p><h3>Secondary Indexes: When the Table Has More Than One WayÂ In</h3><p>A primary index gives you one fast path into your data. But real applications rarely query by primary key alone. You search by email, filter by status, sort by date. This is where secondary indexes come inâ€Šâ€”â€Šand where the architectural difference between MySQL and PostgreSQL becomes even more consequential.</p><p><strong>MySQLâ€Šâ€”â€ŠSecondary Indexes Point to the PrimaryÂ Key</strong></p><p>In MySQL, a secondary index does not point directly to the row. Instead, the leaf nodes of a secondary index store the <strong>primary key value</strong> of the matching row. When you query using a secondary index, MySQL first traverses the secondary index tree to find the primary key, then performs a second traversal down the clustered index to retrieve the full row. This is called a <strong>doubleÂ lookup</strong>.</p><p>This design has a very intentional reason behind it. Because InnoDBâ€™s rows live inside the clustered index and can physically move over timeâ€Šâ€”â€Šfor example, during page splitsâ€Šâ€”â€Šstoring a direct physical pointer in every secondary index would be a maintenance nightmare. Every time a row moved, every secondary index pointing to it would need to be updated. By storing the primary key instead, MySQL insulates secondary indexes from those physical changes. The primary key acts as a stable, logicalÂ address.</p><p>The trade-off is performance. Every secondary index query involves two tree traversals instead of one. This is why choosing a small, compact primary key matters in MySQLâ€Šâ€”â€Ševery secondary index carries a copy of it. A large primary key, like a UUID string, inflates the size of every secondary index on theÂ table.</p><p><strong>PostgreSQLâ€Šâ€”â€ŠSecondary Indexes Point to theÂ Heap</strong></p><p>In PostgreSQL, a secondary index works exactly the same way as a primary oneâ€Šâ€”â€Šbecause there is no structural distinction between them. Every index, whether built on the primary key column or any other column, stores TIDs pointing directly into the heap. There is no doubleÂ lookup.</p><p>On the surface this sounds like an advantage, and for single-row lookups it is. But it comes with the same update cost discussed earlier. When a row is updated and its new version lands at a different heap location, every index on the tableâ€Šâ€”â€Šprimary and secondary alikeâ€Šâ€”â€Šmust be updated to reflect the new TID. The more secondary indexes a table has, the more expensive write operations become.</p><p>There is one meaningful optimization: if all the columns needed to satisfy a query are present within the index itself, PostgreSQL can perform an <strong>index-only scan</strong>, skipping the heap fetch entirely. This is a powerful tool for read performance, but it requires deliberate index design and depends on the visibility map being up to dateâ€Šâ€”â€Šyet another reason regular vacuuming matters in PostgreSQL.</p><h3>Conclusion</h3><p>MySQL and PostgreSQL are both mature, battle-tested databasesâ€Šâ€”â€Šand yet, under the surface, they make very different bets about how data should be stored and accessed.</p><p>MySQL bets on the clustered index. By merging the primary index and the table into a single structure, it optimizes for the most common case: fetching rows by primary key. Secondary indexes pay a small price for thisâ€Šâ€”â€Ša double lookupâ€Šâ€”â€Šbut the overall model is predictable and performs well for read-heavy, primary-key-centric workloads.</p><p>PostgreSQL bets on separation. The heap is the source of truth, and indexes are independent structures that point into it. This gives PostgreSQL more flexibility and a simpler, more uniform index model, but it shifts the burden onto writes and requires disciplined database maintenance to stay healthy overÂ time.</p><p>Neither design is universally superior. The right choice depends on your workload, your query patterns, and how much control you want over the trade-offs. But understanding what actually happens inside the treeâ€Šâ€”â€Šand what sits at the leaf levelâ€Šâ€”â€Šputs you in a far better position to make those decisions with confidence.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=809619a6c4b8\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/mysql-vs-postgresql-how-b-tree-indexes-store-your-data-differently-809619a6c4b8\">MySQL vs PostgreSQL: How B-Tree Indexes Store Your Data Differently</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-12T23:40:39.000Z",
    "url": "https://levelup.gitconnected.com/mysql-vs-postgresql-how-b-tree-indexes-store-your-data-differently-809619a6c4b8?source=rss----5517fd7b58a6---4"
  },
  {
    "publisherId": "levelup",
    "publisherName": "Level Up Coding",
    "specTitle": "ì½”ë”© íŠœí† ë¦¬ì–¼",
    "categories": [
      "frontend"
    ],
    "specUrl": "https://levelup.gitconnected.com/feed",
    "title": "Our Company Banned AI Coding Tools. Productivity Went Up 20%. Hereâ€™s Why.",
    "partialText": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*VqdY8tKz6l22CwpAggFbzA.png\" /></figure><h4>Security concerns killed Copilot. Developers complained. Then metrics showed theÂ truth.</h4><p>October 12, 2025. The email that made developers riot.</p><p><strong>Subject:</strong> Security Policy Update: AI Coding Assistants Prohibited</p><p><strong>Body:</strong> Effective immediately, all cloud-based AI coding tools (GitHub Copilot, Cursor, Claude Code, etc.) are banned on company devices. Security and IP concerns.</p><p>The Slack channel exploded. 47 messages in 10Â minutes.</p><p>â€œThis will kill productivity.â€<br> â€œHow are we supposed to compete without AI?â€<br> â€œIâ€™m gonna be so slowÂ now.â€</p><p>One senior dev threatened toÂ quit.</p><p>I was the VP of Engineering who signed off on the ban. I expectedÂ mutiny.</p><p>Six months later, Iâ€™m looking at ourÂ metrics.</p><p><strong>Productivity is upÂ 20%.</strong></p><h3>The Numbers NobodyÂ Expected</h3><p><strong>Before the ban (July-September 2025):</strong></p><ul><li>Pull requests per week:Â 127</li><li>Average PR size: 847Â lines</li><li>Code review time: 6.2 hours perÂ PR</li><li>Bug escape rate:Â 18%</li><li>P0 incidents: 12</li><li>Developer satisfaction score:Â 3.2/5</li></ul><p><strong>After the ban (November 2025-January 2026):</strong></p><ul><li>Pull requests per week: 156Â (+23%)</li><li>Average PR size: 423 linesÂ (-50%)</li><li>Code review time: 3.8 hours per PRÂ (-39%)</li><li>Bug escape rate: 9%Â (-50%)</li><li>P0 incidents: 4Â (-67%)</li><li>Developer satisfaction score: 4.1/5Â (+28%)</li></ul><p>We banned AI tools expecting productivity toÂ crash.</p><p>Instead, every metric improved.</p><h3>What ActuallyÂ Happened</h3><p><strong>Week 1: TheÂ Panic</strong></p><p>Developers were legitimately slower. Theyâ€™d gotten used to AI autocomplete. Typing feltÂ painful.</p><p>Average commit time increased 40%. I started second-guessing the decision.</p><p><strong>Week 3: The Adjustment</strong></p><p>Something weird happened. PRs got smaller. Instead of massive AI-generated refactors, developers shipped focusedÂ changes.</p><p>Code review became faster. Reviewers could actually understand whatÂ changed.</p><p><strong>Week 6: TheÂ Pattern</strong></p><p>Bugs started dropping. Not gradually. Dramatically.</p><p>Before: AI would suggest â€œoptimizationsâ€ that looked good but broke edge cases.<br> After: Developers wrote defensive code because they thought through eachÂ line.</p><p><strong>Week 12: TheÂ Data</strong></p><p>We ran the analysis. Productivity was up across everyÂ metric.</p><p>The team that fought the ban the hardest? They improved theÂ most.</p><h3>The Real Reasons AI Was Slowing UsÂ Down</h3><h3>1. The TrustÂ Problem</h3><p><strong>With AI:</strong><br> Developer writes prompt â†’ AI generates 200 lines â†’ Developer scans it â†’ Looks good â†’ Ship it â†’ Bug in production</p><p><strong>Without AI:</strong><br> Developer writes 50 lines â†’ Thinks through edge cases â†’ Tests manually â†’ Ships it â†’Â Works</p><p>AI made us lazy reviewers of our ownÂ code.</p><p>We stopped asking â€œwhat could go wrong?â€ because AI code looked confident.</p><h3>2. The ContextÂ Problem</h3><p>Our codebase: 847,000 lines. 8 years old. Custom architecture. Tribal knowledge everywhere.</p><p>AI trained on: Public GitHub repos. Generic patterns. Nothing about our specificÂ system.</p><p><strong>Example disaster:</strong></p><p>AI suggested this â€œoptimizationâ€:</p><pre># AI&#39;s confident suggestion<br>@cache_result(ttl=3600)<br>def get_user_balance(user_id):<br>    return database.query(f&quot;SELECT balance FROM accounts WHERE user_id={user_id}&quot;)</pre><p>Looks fine,Â right?</p><p>Except our system updates balances every 30 seconds during trading hours. The 1-hour cache meant users saw stale data for 59Â minutes.</p><p>Cost us $47,000 in incorrect transactions before we caughtÂ it.</p><p>AI didnâ€™t know our business rules. Developers who trusted it didnâ€™t question the suggestion.</p><h3>3. The â€œLooks Goodâ€Â Problem</h3><p>AI code passes one test: Does it look reasonable?</p><p>It fails another: Does it handle the 15 edge cases specific to ourÂ domain?</p><p><strong>Real example:</strong></p><p>AI-generated payment processing code looked perfect. Handled happy path beautifully.</p><p>What itÂ missed:</p><ul><li>Duplicate charge prevention (2 users got chargedÂ twice)</li><li>Currency conversion edge cases (1 user charged in wrong currency)</li><li>Timeout handling (3 users got â€œfailedâ€ message but wereÂ charged)</li><li>Race conditions (4 concurrent requests created 4Â charges)</li></ul><p>Manual code wouldâ€™ve been slower to write. But we wouldâ€™ve thought through theseÂ cases.</p><h3>4. The Knowledge TransferÂ Problem</h3><p>Junior developer joins. Asks: â€œWhy does this function work thisÂ way?â€</p><p><strong>Before ban:</strong><br> Senior: â€œI donâ€™t know, AI wrote it. Let me checkâ€¦â€<br> <em>Checks code</em><br> â€œHonestly, Iâ€™m not sure. It worksÂ though.â€</p><p><strong>After ban:</strong><br> Senior: â€œBecause we need to handle timezone conversions before comparing dates. Let me show you whyâ€¦â€<br> <em>Actually explains the businessÂ logic</em></p><p>AI code created knowledge silos. Only AI knew why itÂ worked.</p><h3>5. The Code Review TheaterÂ Problem</h3><p><strong>Before ban:</strong><br> â€œLGTM ğŸ‘â€ (didnâ€™t actually read the 800 AI-generated lines)</p><p><strong>After ban:</strong><br> â€œWhy did you choose HashMap over TreeMap here?â€<br> â€œThis method is doing too much. Can we split it?â€<br> â€œWhat happens if the API timesÂ out?â€</p><p>Real code review cameÂ back.</p><h3>The One Thing That Surprised MeÂ Most</h3><p><strong>Developer satisfaction wentÂ UP.</strong></p><p>I expected developers to hate the ban. They loved AI autocomplete.</p><p>But after 3 months, satisfaction scores jumped from 3.2 toÂ 4.1.</p><p>Why?</p><p><strong>Before (with AI):</strong> â€œI spent 2 hours fighting with Copilot suggestions. It kept suggesting broken code. Then spent 4 hours debugging the code I accepted.â€</p><p><strong>After (without AI):</strong> â€œCoding feels slower but Iâ€™m not debugging AI hallucinations. I ship features and theyÂ work.â€</p><p>Turns out, fighting with AI was more frustrating than writing code manually.</p><h3>What We Kept (AndÂ Why)</h3><p>We didnâ€™t ban ALL AI. We banned coding assistants.</p><p><strong>What we stillÂ use:</strong></p><p><strong>1. ChatGPT for documentation</strong><br> â€œExplain this legacy Java codeâ€ â†’ Actually useful<br> Not generating code, just explaining it</p><p><strong>2. AI for test generation</strong><br> Generate unit test templates â†’ Developer reviews and modifies<br> Speeds up the boring part, developer stillÂ thinks</p><p><strong>3. Code review AI (self-hosted)</strong><br> Runs on our infrastructure, doesnâ€™t send code to cloud<br> Catches obvious issues (unused variables, SQL injection patterns)</p><p><strong>4. AI for log analysis</strong><br> Perfect use case. AI reads thousands of log lines, highlights patterns<br> Human still debugs, AI just filtersÂ noise</p><p>The pattern: <strong>AI as assistant, notÂ author.</strong></p><h3>The Controversial Part</h3><p>Some developers still disagree with theÂ ban.</p><p>â€œIâ€™m faster with AI. I know how to use it properly.â€</p><p>Theyâ€™re right. Some developers ARE faster withÂ AI.</p><p>But hereâ€™s the thing: <strong>Weâ€™re not optimizing for individual speed. Weâ€™re optimizing for teamÂ quality.</strong></p><p>One developer shipping fast with AI-generated bugs costs the team more than five developers shipping slower with cleanÂ code.</p><p>We measured it. AI-generated code had 3.2x more bugs per line than human-written code.</p><h3>ğŸ“¬ What Iâ€™m WorkingÂ On</h3><p>While banning AI coding tools, I realized thereâ€™s ONE place AI actually helps: <strong>production incidents.</strong></p><p>Not writing code. Explaining what wentÂ wrong.</p><p>Iâ€™m building <strong>ProdRescue AI</strong>â€Šâ€”â€Šturns messy incident logs into clear postmortem reports in 90Â seconds.</p><p>No code generation. Just log analysis and reportÂ writing.</p><p>Early access is open:<br> ğŸ‘‰ <strong>Join the waitlist (2-minÂ form)</strong></p><p><a href=\"https://prodrescue-ai.vercel.app/\">ProdRescue AI</a></p><p>This is AI used right: analyzing data, not generating code.</p><h3>What Other Companies AreÂ Doing</h3><p>Weâ€™re not alone. After sharing our results internally, I talked to 12 otherÂ CTOs.</p><p><strong>3 companies banned AI coding tools completely</strong> (same results as us)<br> <strong>5 companies restricted to senior engineers only</strong> (juniors canâ€™t use AI)<br> <strong>4 companies still allow it</strong> (but reconsidering after seeingÂ metrics)</p><p>The trend: Companies measuring productivity are pulling back onÂ AI.</p><p>Companies not measuring? Still pushing AI everywhere.</p><h3>The RealÂ Lesson</h3><p>AI coding tools solve the wrongÂ problem.</p><p><strong>The problem isnâ€™t:</strong> â€œHow do I write code faster?â€<br> <strong>The problem is:</strong> â€œHow do I write code that works in production?â€</p><p>AI optimizes for speed. Production requires reliability.</p><p>We chose reliability. Productivity followed.</p><h3>If Youâ€™re Dealing With Production Failures</h3><p>The ban taught us: Prevention beats debugging.</p><p>Writing careful code manually prevents more bugs than AI speedÂ creates.</p><p>If youâ€™re tired of production incidents, these resources mightÂ help:</p><p><strong>ğŸ”§ </strong><a href=\"https://devrimozcay.gumroad.com/l/rlmxr\"><strong>Production Engineering Toolkit</strong></a><br> Real production failures and how to preventÂ them</p><p><strong>ğŸ“š </strong><a href=\"https://devrimozcay.gumroad.com/l/rkdgug\"><strong>Backend Performance Rescue Kit</strong></a><br> Find and fix the 20 bottlenecks killing yourÂ app</p><p><strong>ğŸ¯ </strong><a href=\"https://devrimozcay.gumroad.com/l/xbihfx\"><strong>30 Real Incidents That Cost Companies Thousands</strong></a><br> Full postmortems with prevention steps</p><p>More resources: <a href=\"https://devrimozcay.gumroad.com/\"><strong>devrimozcay.gumroad.com</strong></a></p><h3>The Metrics After 6Â Months</h3><p><strong>January 2026 (Month 6 post-ban):</strong></p><ul><li>Pull requests per week: 167 (+31% vs baseline)</li><li>Code review time: 3.2 hoursÂ (-48%)</li><li>Bug escape rate: 7%Â (-61%)</li><li>P0 incidents: 2Â (-83%)</li><li>Developer satisfaction: 4.3/5Â (+34%)</li><li>Time debugging: -12 hours per week per developer</li></ul><p><strong>The most surprising metric:</strong></p><p>Developer velocity (features shipped per sprint) went upÂ 18%.</p><p>We were worried banning AI would slow feature development.</p><p>Turns out, fewer bugs means more time building features.</p><h3>What Iâ€™d Tell My PastÂ Self</h3><p>October 2025, before the ban, I wasÂ scared.</p><p>â€œWhat if productivity crashes? What if developers quit? What if we fall behind competitors?â€</p><p>Now I know: <strong>The metrics donâ€™tÂ lie.</strong></p><p>AI coding tools made us FEEL productive. They made us actuallyÂ slower.</p><p>Would I ban AI again?Â Yes.</p><p>Would I use AI for other things? Yes. (Log analysis, documentation, test generation)</p><p>The lesson: <strong>AI is a tool. Not all tools fit all problems.</strong></p><p>For code generation? We found a better tool: human developers whoÂ think.</p><h3>The Uncomfortable Truth</h3><p>This article will make peopleÂ angry.</p><p>â€œAI makes ME faster!â€ (Maybe. But does it make your team faster?)<br> â€œYou just donâ€™t know how to use AI!â€ (We had 6 months of data.)<br> â€œMy company would never ban AI!â€ (Thatâ€™s fine. Measure your metrics.)</p><p>Iâ€™m not saying AI coding tools are bad for everyone.</p><p>Iâ€™m saying: <strong>Measure before assuming.</strong></p><p>We assumed AI would help. Metrics showed itÂ hurt.</p><p>Maybe your metrics are different. Great! ShareÂ them.</p><p>But if youâ€™ve never measured productivity with vs without AI, youâ€™re guessing.</p><p>We stopped guessing. Started measuring.</p><h3>One Year LaterÂ (Update)</h3><p>Itâ€™s been a year since theÂ ban.</p><p>Developers who threatened to quit? Still here. They admit theyâ€™reÂ happier.</p><p>Metrics? Still better thanÂ before.</p><p>Competitors who went all-in on AI? Some are rolling back. Others are dealing with increased bugÂ rates.</p><p>The industry is learning: <strong>Fast code generation â‰  GoodÂ code.</strong></p><p>We learned early. Paid the price. GotÂ better.</p><p>Your company will decide forÂ itself.</p><p>Just measure the real metrics. Not â€œdeveloper happiness with AIÂ tools.â€</p><p>Measure: Bugs in production. Time debugging. Code quality. Feature velocity.</p><p>Then decide.</p><p><strong>â€” The developers who fought the ban hardest? Theyâ€™re now the biggest advocates. They didnâ€™t realize how much time they spent debugging AI code until theyÂ stopped.</strong></p><p><strong>â€” We still ban AI coding tools. We still measure metrics. Productivity is still up 20%. I donâ€™t see us reversing this decision.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=077eb325eadf\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://levelup.gitconnected.com/our-company-banned-ai-coding-tools-productivity-went-up-20-heres-why-077eb325eadf\">Our Company Banned AI Coding Tools. Productivity Went Up 20%. Hereâ€™s Why.</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-12T23:40:37.000Z",
    "url": "https://levelup.gitconnected.com/our-company-banned-ai-coding-tools-productivity-went-up-20-heres-why-077eb325eadf?source=rss----5517fd7b58a6---4"
  }
]