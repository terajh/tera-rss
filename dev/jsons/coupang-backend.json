[
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "Accelerating Coupang’s AI Journey with LLMs",
    "partialText": "<p><em>By ML Platform Team</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Z3QK3ZJPdvvWZ0mTTDyLTQ.png\" /></figure><h4><strong>Introduction</strong></h4><p>In the last couple of years, Coupang has been using machine learning [ML/AI] heavily to improve customer experiences in areas like search, ads, catalog and recommendations. ML drives important decision making in pricing, transportation and logistics.</p><p>Coupang’s ML engineer’s toolkit has also grown significantly from simple classical ML techniques to deep learning and now, large language models (LLMs) for generative AI in this short time frame. Coupang’s ML platform has been at the forefront of this journey focused on enabling ML engineers to train and serve models in a resource efficient way. In this blog we write about LLM explorations at Coupang and the technical challenges it poses on the ML platform.</p><h4><strong>ML at Coupang</strong></h4><p>There are three main types of ML models trained with Coupang’s ML infrastructure:</p><ol><li><strong>Recommendation system models:</strong> This is primarily used in personalization and recommendation surfaces such as main home feed, search and ads across Coupang apps for shopping, eats and play. These are trained on large datasets of user interactions (clicks, views, purchases, add to cart) and human labeled relevance judgements.</li><li><strong>Content understanding models:</strong> Coupang has a huge dataset of product catalog data (text, image), user generated content (text — reviews, queries), user and merchant data (text, image). Various ML teams across product groups use deep learning techniques to understand product, customer and merchant representation and then use it to improve shopping experience.</li><li><strong>Forecasting models:</strong> Coupang has over 100+ unique fulfillment centers housing millions of products. Predictive modeling is crucial in the pricing, logistics, delivery, and pricing of these products for our customers. While these models are typically statistical in nature, deep learning techniques are now increasingly being incorporated.</li></ol><p>Foundation models (FM) are large deep learning models trained on massive datasets. FM can adapt to multiple tasks unlike traditional ML models that are trained for specific tasks. FM are trained on text datasets (large language models) or multi-modal (combining multiple modalities such as text and image). These models can learn powerful representations and generate contextually relevant content. The LLMs (and multi-modal) models have been used in several ways to improve existing ML models and improve customer experiences.</p><p>Training and serving LLMs come with significant challenges on ML infrastructure — hardware resources (compute, storage and networking), efficiently scaling training and inference. In this article we describe the applications of LLMs and our key learnings from ML infrastructure related challenges.</p><h4>Applications</h4><p>Coupang’s largest presence is in South Korea and Taiwan. Training data for most ML tasks is relatively small in both Korean and Mandarin. Moreover, Coupang has a vibrant marketplace with global sellers from around the world selling to its customers. These pose unique challenges in several problem areas of e-commerce — especially seller and product understanding in different languages and images with embedded text, customer intent while they are searching. We describe three areas of application inside Coupang.</p><h4>Image &amp; Language Understanding</h4><p>Coupang has large datasets of product and ads images along with corresponding metadata, which includes titles, descriptions, and user queries. The strategy of jointly modeling image and text data through vision and language transformer models yields superior embeddings, as opposed to learning embeddings separately. These embeddings are then used in various downstream models for more effective ad retrieval, similarity search, and serve as features in recommendation models.</p><p>Apart from these, there have been other successful applications of large models in content understanding inside Coupang:</p><ul><li>Translating product titles from Korean to Mandarin</li><li>Improving image quality in shopping feed</li><li>User review summarization</li><li>Keyword generation for products and sellers</li></ul><h4>Generating Weak Labels at Scale:</h4><p>Obtaining labels created by humans is often a challenging and costly task. This issue is magnified when dealing with multilingual content, such as English, Korean, and Mandarin in the context of Coupang.</p><p>However, LLMs present a solution to this problem. They have the ability to produce labels for text-based content on a large scale, with a quality that rivals that of human annotators. Once these generated labels pass some quality checks, they can serve as weak supervision labels for training various models.</p><p>The labels generated by LLMs are particularly useful when starting models for new segments where there’s a shortage of high-quality labels. Internal experiments have shown that these weak labels can enhance the quality of relevance models and have the potential of overcoming the challenges of label scarcity in under-resourced languages.</p><h4>Categorization &amp; Attribute Extraction</h4><p>In the domain of product categorization and attribute extraction, the traditional approach involved deploying a single ML model for each category. This was necessitated by the fact that an unified or multi-class model often yielded noisy predictions for tail categories. However, this imposed an increased operational burden as teams were required to manage multiple models. LLMs provided a deeper understanding of product data (title, description, reviews, seller info). This resulted in a single LLM powered categorizer for all categories with gains in precision across most categories.</p><h4>Choice of Model Architectures</h4><p>This strategy of taking OSS model architectures and fine-tuning them with domain data provides an effective approach to apply LLMs to business problems. It allows ML teams to leverage state of the art pre-trained models and efficient architectures, saving both time and computational resources.</p><p>Naturally, the main interest has been around models which show strong multilingual performance specially in CJK (Chinese, Japanese and Korean) languages. Training can differ due to the unique characteristics of these languages. Key differences include the use of spacing, the character-based nature of these languages as opposed to the word-based structure of English, and the larger vocabulary sizes. Each of these factors influences the tokenizer, which in turn, affects the quality of the language model. For language/NLP tasks the most commonly used models have been based on Qwen [1.1], LLAMA [1.2], T5 [1.3], Phi[1.4] and Polyglot [1.5] amongst others. Parameter sizes ranging from 3B to 20B are favored because they strike a good balance between resource and compute efficiency and quality.</p><p>For image-text multi-modal models, CLIP [1.6] (Contrastive Language Image Pretraining) and TrOCR [1.7] (Transformer based OCR) were the model architectures of choice for their efficiency and performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/773/1*JgeMerV5mS4Ja_PlkJyseA.png\" /></figure><h4>Patterns of Using LLMs</h4><p>There are a few commonly used patterns of using LLMs. We have arranged techniques in increasing order of resource requirements and complexity.</p><ul><li><strong>In-context learning (ICL):</strong> In this mode a pre-trained LLM is provided with a prompt or “context” to guide its answers for a specific task. This process does not involve any additional training, and the same model can be reused for different tasks with different prompts. This is very fast to set up and iterate, cheap — as it involves no training, and versatile as it can be used in several tasks. Internally, this remains one of the most popular ways of prototyping and evaluating usage of LLM in a product.</li><li><strong>Retrieval Augmented Generation (RAG):</strong> RAG is a technique where LLM generated responses are grounded with facts fetched from external sources (knowledge bases such as a corpus of documents, catalog of products, etc). Making the generation and retrieval components work seamlessly in real-time is nontrivial, leading to potential bottlenecks and errors.</li><li><strong>Supervised fine-tuning (SFT):</strong> This refers to further training an existing base LLM on small datasets to improve performance on a specific domain or task. A fine-tuned model on a high quality domain dataset often surpasses the base LLM performance.</li><li><strong>Continued pre-training (CPT): </strong>It refers to further pre-training of an existing base LLM on sizable datasets to improve generalized understanding of the model without focusing on any specific task. This is resource intensive but often produces the best results on downstream tasks like attribute extraction.</li></ul><p>In-context learning and later supervised fine-tuning remains the most popular pattern of using LLM due to their flexibility and resource efficiency.</p><h4>Development Lifecycle &amp; Challenges</h4><p>In this section we describe how developers write LLM training &amp; inference pipelines.</p><ol><li><strong>Exploration phase:</strong></li></ol><p>In the exploration phase, developers use small experiments to determine a list of model lines they want to try out further. Their main focus is on:</p><ul><li>Model architecture</li><li>Model size — Return on Investment (ROI) of using larger sizes for example 70B+ variant vs &lt; 10B parameter variant</li><li>Prompt templates — Changing prompt templates can change model outputs and therefore the performance</li></ul><p>ML infra components:</p><ul><li>Most data preparation and processing is done with Apache Zeppelin [2.1] notebooks which delegate the tasks to underlying processing engines such as Spark [2.2] on Kubernetes.</li><li>Model architecture &amp; prompt template explorations are done on GPU (or multi-GPU) containerized Jupyter notebooks [2.3].</li></ul><p><strong>2. Model training:</strong></p><ul><li>Based on the shortlist, developers use fine-tuning or pre-training from scratch depending on the compute budget, dataset size and comparing model performance.</li><li>Based on model performance on the application, developers finalize the model to put into production. There isn’t any process difference from the non-LLM model development lifecycle here. We will call it the source LLM model.</li></ul><p>ML infra components:</p><ul><li>We use Polyaxon [2.4] underneath for managing ML training lifecycle on Kubernetes.</li><li>LLM training at Coupang uses the model parallel training on Kubernetes distributed training operator for Pytorch (PytorchJob) [2.5].</li></ul><p><strong>3. Path to production:</strong></p><p>For our workloads, we see developers use the following methods to go to production:</p><ul><li>Distillation: Distill a smaller model from the trained source LLM. The smaller model is used in real-time inference.</li><li>Embedding: Embeddings can be exported from the LLMs and used in smaller models. We see this pattern being used in ranking problems.</li></ul><p>ML infra components:</p><ul><li>Batch and nearline inference on GPUs is the most popular way to extract predictions from the source LLMs at scale and then use it for distillation or as embeddings.</li><li>Developers use Ray + vLLM [2.6, 2.7] to write inference pipelines requiring both CPU and GPU processing.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/899/1*RT3-1cV7bDqVri74DELeNA.png\" /><figcaption>Figure 1: ML infra supporting LLM development workflows.</figcaption></figure><p>The key challenges in enabling our developers for LLM development workflows were:</p><ol><li>Resource efficiency and management largely due to supply shortage and high cost of GPUs.</li><li>Capabilities of training and serving large models. Our training stack was not equipped for distributed training (especially model parallel). Before LLMs, our serving was entirely on CPUs which are too slow for the multi-billion parameter models.</li></ol><p>We describe the key learnings and takeaways which enabled us to scale our ML stack for the challenges mentioned above.</p><h4>Choosing the Right Workhorse: Matching Appropriate GPU for the Workload</h4><p>Choice of GPUs: Large Language Models (LLMs) are both compute and memory intensive. When dealing with larger workloads for training and serving, we quickly realized that device memory constraints play a crucial role in both training and serving. The demand for large RAM GPUs, such as the Nvidia A100 &amp; H100. GPUs offered by cloud vendors have a significant wait time. We conducted regular benchmarking with model-building teams to evaluate the price-to-performance ratio of different GPUs for each model line. For the training of models with more than 1 billion parameters in mixed precision mode, we utilized the A100–80 GB version. For testing and lightweight training purposes, we could employ a substantial quantity of A10G-24 [3.3] GB devices. Given that each LLM family is available in multiple parameter sizes, it is highly cost-effective to use a device with lower performance for testing smaller versions of the LLM.</p><h4>Hybrid &amp; Multi-Region AI Clusters</h4><p>In response to the GPU supply shortage, we implemented a multi-region deployment strategy for our ML infrastructure. By leveraging cloud service clusters across various regions (Asia-Pacific &amp; US), we ensure faster access to GPUs, mitigating the wait times that can disrupt execution plans. Additionally, we built an on-prem cluster to provision a significant portion of our computer, especially the higher-end Nvidia GPUs (such as A100/H100).</p><p>This hybrid arrangement has been instrumental in alleviating the shortage of GPUs from the cloud provider and reducing overall cost of training. However, it also presents its own set of challenges, such as ensuring consistent infrastructure (storage &amp; networking) and developer experience.</p><h4>Embracing the Open Source - Frameworks &amp; Tools:</h4><p>At Coupang, all ML training and inference is executed on managed containerized services. The clusters have access to distributed file system on both cloud and on-prem. This worked for LLM training and inference as well. For training and inference frameworks, we could leverage high-quality open-source projects to our advantage. We describe the key projects which helped in accelerating our journey below.</p><p><strong>Model Parallel Training:</strong></p><p>When it comes to LLM training, one of the key obstacles is the inability to fit the model into a single GPU RAM. As a result, the typical method of distributed training — data parallelism alone is insufficient. We support several training frameworks which implement the model sharding strategy, the most popular being DeepSpeed Zero [2.8] due to its quick setup time and availability of trainer recipes for the popular model architectures through hugging face hub. Developers internally experiment and share recipes with smart defaults for hyperparameters, such as the choice of optimizer, gradient accumulation, memory pinning, etc.</p><p><strong>GPU Inference:</strong></p><ul><li><strong>Realtime model serving stack:</strong> The compute-intensive nature of LLMs required the use of GPUs for serving. Our existing serving stack was not equipped for GPUs, prompting us to find an appropriate model serving engine. Nvidia Triton offers a containerized inference solution, complete with features such as dynamic batching, concurrent multi-model execution on GPUs, and compatibility with a broad range of backends. These features are vital for the efficient serving of large models. We do all realtime inference using Nvidia Triton [2.9] on AWS EKS.</li><li><strong>Batch inference:</strong> We also realized that batch inference plays a pivotal role in LLM explorations, as it is used to generate LLM responses for datasets post training. Batch inference often involves both GPU and CPU processing. For instance, text and image data preprocessing can be carried out in a distributed manner on CPU cores, while the primary model inference takes place on the GPU. After experimentation, we settled on Ray + vLLM which excels in managing this type of heterogeneous computing at scale.</li><li><strong>Nearline inference: </strong>Nearline inference combines the efficiency of batch inference (using small batches) and the responsiveness of being near real-time inference (within a certain time of event occurrence). ML systems in e-commerce applications have several content data streams (user and seller generated content, orders etc). Using LLMs in nearline inference mode helps teams to support diverse downstream applications with a smaller resource footprint.</li></ul><h4>Rapid Experimentation and Prototyping</h4><p>The LLM landscape is fast changing with frequent model releases, new state of the art techniques are introduced, and new performance benchmarks are broken. Changes are on all fronts — model architecture, training &amp; inference frameworks, hardware, and optimization techniques. The best way to keep up is to experiment rapidly and learn from the failures.</p><ul><li>Rapid experimentation with newer techniques gives you surprising wins and often a deeper understanding of existing toolkits. For example, through experimentation we observed that vLLM provided us nearly ~20x throughput improvement in multiple workloads with their kernel implementation.</li><li>Similarly, experimenting with techniques like offloading model parameters to CPU helped in creating recipes for fine tuning LLMs on more widely available GPU with less RAM. This unblocked developers to iterate on their training pipeline without being blocked on availability of high end GPUs.</li><li>With Nvidia H100s on block, we see significant opportunities with fp8 quantization and Nvidia’s transformer engine.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/691/1*qkwkfl2YVEvbYShoETjrGQ.png\" /><figcaption>Figure 2: Training and serving stack for rapid prototyping and experimentation.</figcaption></figure><h4>Conclusion</h4><p>Using LLMs have improved various production ML systems and shown promise in several areas including search &amp; discovery, catalog, operations, and ads quality, amongst others. We expect more teams to use LLMs and similar model architectures in the coming quarters and ship wins for our customers.</p><p>We are continuously investing in our training to train larger models and improve resource efficiency of our GPU training and inference stacks. This involves optimization at all levels — hardware (compute, storage, networking), observability, frameworks (model and data sharded training, profiling, utilization).</p><p>If you are interested in working in ML infrastructure and product problems, do check out our ML positions at Coupang. (<a href=\"https://www.linkedin.com/company/coupang/jobs/\">https://www.linkedin.com/company/coupang/jobs/</a>).</p><h4><strong>Acknowledgement</strong></h4><p>We thank our partners in product ML teams, especially Search &amp; Discovery for being the early adopters of LLMs in their applications and sharing the progress and pain-points.</p><p>We thank our Tech Infrastructure teams especially for their support in provisioning compute resources and cluster health.</p><p><em>While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on </em><a href=\"http://ir.aboutcoupang.com\"><em>ir.aboutcoupang.com</em></a><em> for information on our formal investment plans and product development strategies.</em></p><h4>References</h4><p><em>Model architectures:</em></p><ul><li>QWEN: <a href=\"https://huggingface.co/Qwen\">https://huggingface.co/Qwen</a></li><li>LLAMA: <a href=\"https://llama.meta.com/\">https://llama.meta.com/</a></li><li>T5: <a href=\"https://huggingface.co/docs/transformers/en/model_doc/t5\">https://huggingface.co/docs/transformers/en/model_doc/t5</a></li><li>Phi: <a href=\"https://huggingface.co/microsoft/phi-2\">https://huggingface.co/microsoft/phi-2</a></li><li>Polyglot: <a href=\"https://github.com/EleutherAI/polyglot\">https://github.com/EleutherAI/polyglot</a></li><li>CLIP: <a href=\"https://huggingface.co/docs/transformers/en/model_doc/clip\">https://huggingface.co/docs/transformers/en/model_doc/clip</a></li><li>TrOCR: <a href=\"https://huggingface.co/docs/transformers/en/model_doc/trocr\">https://huggingface.co/docs/transformers/en/model_doc/trocr</a></li></ul><p><em>Platform components:</em></p><ul><li>Zeppelin: <a href=\"https://zeppelin.apache.org/\">https://zeppelin.apache.org/</a></li><li>Spark: <a href=\"https://spark.apache.org/\">https://spark.apache.org/</a></li><li>Jupyter notebook: <a href=\"https://jupyter.org/\">https://jupyter.org/</a></li><li>Polyaxon: <a href=\"https://polyaxon.com/\">https://polyaxon.com/</a></li><li>PytorchJob: <a href=\"https://www.kubeflow.org/docs/components/training/user-guides/pytorch/\">https://www.kubeflow.org/docs/components/training/user-guides/pytorch/</a></li><li>Ray: <a href=\"https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html\">https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html</a></li><li>vLLM: <a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></li><li>Deepspeed zero: <a href=\"https://www.deepspeed.ai/tutorials/zero/\">https://www.deepspeed.ai/tutorials/zero/</a></li><li>Nvidia Triton: <a href=\"https://developer.nvidia.com/triton-inference-server\">https://developer.nvidia.com/triton-inference-server</a></li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2817d55004d3\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3\">Accelerating Coupang’s AI Journey with LLMs</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2024-10-14T16:25:07.000Z",
    "url": "https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "쿠팡의 머신러닝 플랫폼을 통한 ML 개발 가속화",
    "partialText": "<h4>쿠팡의 머신러닝 개발 속도를 높이는 쿠팡만의 ML 플랫폼에 대하여</h4><p><strong><em>By</em></strong><em> </em><a href=\"https://www.linkedin.com/in/hyunjung-baek/\"><em>Hyun Jung Baek</em></a><em>, </em><a href=\"https://www.linkedin.com/in/hara-ketha-ab88b111/\"><em>Hara Ketha</em></a><em>, </em><a href=\"https://www.linkedin.com/in/jaideepray/\"><em>Jaideep Ray</em></a><em>, </em><a href=\"https://www.linkedin.com/in/justina-min-649681112/\"><em>Justina Min</em></a><em>, </em><a href=\"https://www.linkedin.com/in/mohamed-sabbah-08bb5418/\"><em>Mohamed Sabbah</em></a><em>, </em><a href=\"https://www.linkedin.com/in/ronakpan/\"><em>Ronak Panchal</em></a><em>, </em><a href=\"https://www.linkedin.com/in/adunuthula/\"><em>Seshu Adunuthula</em></a><em>, </em><a href=\"https://www.linkedin.com/in/thimma-reddy-kalva-a27aa859/\"><em>Thimma Reddy Kalva</em></a><em>, and </em><a href=\"https://www.linkedin.com/in/enhua\"><em>Enhua Tan</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r-iwNRTZ7xDhcZJ0B1ERrg.jpeg\" /></figure><blockquote>본 포스트는 <a href=\"https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172\"><strong>영문</strong></a>으로도 제공됩니다.</blockquote><h3><strong>소개</strong></h3><p>쿠팡은 고객이 앱을 켠 순간부터 주문한 상품이 문 앞에 도착하는 순간까지 최상의 쇼핑 및 배송 경험을 제공하기 위해 끊임없는 혁신을 이어가고 있습니다. 일반적인 전자 상거래 외에도, 쿠팡은 음식 배달 서비스인 쿠팡이츠, 비디오 스트리밍을 제공하는 쿠팡플레이, 결제 서비스 쿠팡페이, 신선 상품을 위한 쿠팡 로켓배송 등 다양한 소비자 서비스를 운영하고 있습니다.</p><p>머신러닝(이하 ML)은 쿠팡 고객의 전자 상거래 경험에 있어 상품 카탈로그, 검색, 가격 책정, 로보틱스, 재고 관리, 그리고 물류 처리와 같은 모든 요소에 영향을 미치고 있습니다. 또한 쿠팡이 새로운 시장으로 진출함에 따라, ML의 역할은 점점 더 중요해지고 있습니다.</p><p>ML은 쿠팡 웹사이트와 앱에서의 검색 기능 및 정보 탐색 기능을 향상시키며, 상품 및 서비스의 가격을 정하고, 물류와 배송을 더욱 효율적으로 이루어지게 합니다. 그리고 스트리밍을 위한 콘텐츠를 최적화하고, 광고의 순위를 정하는 등의 다양한 업무에서 중요한 역할을 수행합니다.</p><p>따라서, 저희는 애드혹(ad-hoc) 탐색부터 모델 훈련용 데이터(training data) 준비, 모델(model) 개발, 그리고 모델을 안정적으로 실서비스에 배포하는 단계까지 머신러닝 개발을 꾸준히 확장하고자 노력하고 있습니다.</p><h4>목차</h4><blockquote>· <a href=\"#ae2a\">배경 및 과제</a><br> ∘ <a href=\"#bd3a\">1. 개발 적용 시간의 단축</a><br> ∘ <a href=\"#45b8\">2. ML 개발 프로세스로 CI/CD 프로세스 통합</a><br> ∘ <a href=\"#2a3c\">3. ML 컴퓨팅의 효율적 확장</a><br>· <a href=\"#c80b\">쿠팡 ML 플랫폼의 주요 서비스</a><br> ∘ <a href=\"#c579\">1. Notebook 환경 제공 및 ML 파이프라인 구축</a><br> ∘ <a href=\"#ccfa\">2. 피처 엔지니어링</a><br> ∘ <a href=\"#1f82\">3. 모델 훈련</a><br> ∘ <a href=\"#8d0e\">4. 모델 추론</a><br> ∘ <a href=\"#95a6\">5. 모니터링 및 관찰</a><br> ∘ <a href=\"#32e8\">6. 트레이닝 및 추론 클러스터</a><br>· <a href=\"#aa47\">성공 사례</a><br> ∘ <a href=\"#73e8\">1. 코버트(Ko-BERT) 트레이닝을 통한 검색 쿼리 이해 개선</a><br> ∘ <a href=\"#f04a\">2. 상품의 실시간 가격 예측</a></blockquote><h3><strong>배경 및 과제</strong></h3><p>쿠팡 ML 플랫폼은 개발자의 작업 효율을 향상시킴으로써 ML 개발을 보다 가속화하기 위해 ‘바로 사용 가능한’ 서비스를 제공하고자 했습니다.</p><p>이 플랫폼에서 제공하는 핵심 서비스로는 관리형 주피터 노트북(Jupyter Notebook), 파이프라인 SDK, 피처 스토어, 모델 학습, 그리고 모델 추론이 있습니다. ML 팀들은 이러한 서비스를 각각 조합하여 ML 파이프라인을 구축할 수 있습니다. 저희의 주요 목표는 다음과 같습니다.</p><h4><strong>1. 개발 적용 시간의 단축</strong></h4><p>쿠팡 ML 플랫폼이 도입되기 전에는, ML 모델을 설계하고 학습시키려면 데이터와 피처 준비, 그리고 모델 훈련 코드 작성에 대한 복잡한 설정 및 기본 코드 작성이 필요했습니다. 또한, 분산 훈련을 통해 모델 훈련을 확장하거나 GPU를 활용하는 데에는 고도의 엔지니어링 지식이 필요했고, 중복되는 작업도 종종 발생했습니다.</p><p>실시간 트래픽에 ML 모델을 배포하는 것은 성능 평가, 자동 스케일링, 보안, 롤백 등의 로직을 반복적으로 적용해야 하므로, 몇 주 동안 많은 시간과 자원이 투입되어야 했습니다. 이러한 이유로 많은 프로덕트 팀들이 ML의 대규모 적용을 미뤄왔습니다. 하지만 쿠팡 ML 플랫폼의 라이프 사이클을 활용하게 되면서, 간단한 모델부터 복잡한 모델까지 표준화된 방식으로 단 며칠 안에 학습, 디버그 및 배포가 가능해졌습니다.</p><h4><strong>2. ML 개발 프로세스로 CI/CD 프로세스 통합</strong></h4><p>ML 개발을 진행할 때, 복잡하고 해결하기 어려운 기술적 부채가 자주 발생할 수 있습니다. ML 팀들이 더 효율적으로 모델을 구축, 배포, 유지 관리할 수 있도록, 쿠팡 ML 플랫폼은 널리 쓰이고 있는 ML 라이브러리들과의 통합 테스트가 완료되어 있는 컨테이너들을 준비하여 제공합니다. 또한, 모델 검증을 위한 라이브러리, 카나리(Canary) 기반의 모델 배포, 그리고 서비스 중의 핵심 지표 모니터링 기능도 함께 제공하고 있습니다.</p><h4><strong>3. ML 컴퓨팅의 효율적 확장</strong></h4><p>쿠팡에서는 딥러닝 훈련에 필요한 GPU, 대규모 데이터 세트 저장, 그리고 분산 훈련 시 필요한 네트워크 대역폭에 대한 컴퓨팅 요구가 급증하고 있습니다. 그리고 플랫폼 상에서 많은 모델들이 훈련되고 있어 클라우드 사용 비용도 증가하고 있습니다. 이를 해결하기 위해 쿠팡 ML 플랫폼 팀은 컴퓨팅과 스토리지 클러스터를 온프레미스(on-premise)와 AWS 상에 운영하는 하이브리드 구조를 차용하였습니다. 온프레미스는 더 저렴한 비용으로 강력한 GPU 클러스터와 다양한 사용자 맞춤 설정을 제공하고, 클라우드는 온프레미스 자원이 충분하지 않을 때 즉각적인 컴퓨팅 자원을 제공합니다.</p><figure><img alt=\"쿠팡의 머신러닝 플랫폼 개요\" src=\"https://cdn-images-1.medium.com/max/838/1*qGpaaTpPXgwaHZw9yldh7w.png\" /><figcaption><strong>그림 1. 쿠팡 ML 플랫폼 개요</strong></figcaption></figure><h3><strong>쿠팡 ML 플랫폼의 주요 서비스</strong></h3><h4><strong>1.</strong> <strong>Notebook 환경 제공 및 ML 파이프라인 구축</strong></h4><p>ML 플랫폼은 개발자들이 아이디어를 지속적으로 개선하고 테스트할 수 있도록 호스팅된 컨테이너 기반의 노트북(Jupyter Notebook) 환경 서비스를 제공합니다. 이 노트북 환경은 CPU나 GPU 상에서, 사용자가 지정한 컨테이너 또는 스탠다드 컨테이너를 통해 실행될 수 있습니다.</p><p>ML 플랫폼 팀은 텐서플로(Tensorflow), 파이토치(Pytorch), 스켈런(Sklearn), 허깅페이스(Huggingface), 트랜스포머스(Transformers) 등의 주요 ML 라이브러리가 포함된 표준 도커 컨테이너(Docker Container)를 관리하고 있습니다. 이 도커 컨테이너들로 패키지 간에 복잡하게 얽힌 의존성 문제를 효과적으로 해결하고, 안정적인 파이프라인을 구축할 수 있습니다.</p><p>파이프라인 구축과 관련해, ML 플랫폼은 데이터 추출, 피처 스토어, 학습, 그리고 추론에 사용할 수 있는 Python SDK 세트를 제공합니다.</p><h4><strong>2. 피처 엔지니어링</strong></h4><p>쿠팡 ML 플랫폼이 제공하는 피처 스토어를 통해 오프라인과 온라인 모드 모두에서 준비된 피처를 손쉽게 활용할 수 있습니다. 이 피처 스토어는 널리 알려진 오픈소스 프로젝트인 피스트(Feast)를 기반으로 구축되었습니다.</p><ul><li>오프라인 피처 스토어는 이미 만들어져 활용 중인 피처들을 공유하는 데 사용되며 모델 훈련에도 사용됩니다. 저희는 다양한 팀과 협력하여, 여러 팀에서도 활용할 수 있도록 고객 인사이트와 같은 핵심 피처들을 추가하고 있습니다.</li><li>추론 과정에서 온라인 피처 스토어를 통해 짧은 지연 시간으로 피처를 가져올 수 있습니다. 온라인 피처 스토어는 모델 피처 생성뿐만 아니라, 복잡한 계산을 필요로 하는 모델의 예측 결과를 임시 저장하는 역할도 합니다.</li></ul><h4><strong>3. 모델 훈련</strong></h4><p>쿠팡의 ML 팀은 파이토치(Pytorch), 텐서플로(Tensorflow), 스켈런(Sklearn), 엑스지부스트(XGBoost)와 같은 널리 알려진 프레임워크부터, 예측 작업을 위한 프로펫(Prophet)과 같은 특수 목적의 프레임워크까지 다양하게 사용하고 있습니다.</p><p>훈련 스택은 프레임워크에 구애받지 않습니다. 사용자가 작성한 파이프라인은 컨테이너화되어 쿠버네티스(Kubernetes) 클러스터에서 실행됩니다. 배치 스케줄러는 원하는 하드웨어 설정으로 작업을 스케줄링합니다. 사용자는 클러스터 내에서 사용 가능한 모든 유형의 CPU나 GPU에서 작업이 실행될 수 있도록 설정할 수 있습니다. 이는 다양한 유형의 CPU와 GPU의 이점들을 작업의 특성에 맞게 활용함으로써 투자 대비 효과를 극대화할 수 있기 때문에 매우 유용합니다. 예를 들어, 사용자는 모델 훈련과 배치 추론을 다른 유형의 GPU에서 실행함으로써, GPU의 속도 향상과 비용 사이에서 최적화를 이룰 수 있습니다.</p><p>배치 스케줄러는 모든 리소스를 할당 또는 리소스를 전혀 할당하지 않는 전략을 따르도록 설정되어 있습니다. 훈련 스택은 대규모 모델 학습을 위한 분산 훈련 전략(분산 데이터 병렬 및 완전 분산 데이터 병렬)을 지원합니다. 쿠팡은 다중 GPU를 사용한 학습으로 모델 훈련 작업 속도를 크게 향상시켰습니다.</p><p>딥러닝 모델을 효과적으로 학습시키기 위해서는 트레이너 변수(trainer parameter)를 조정하는데 상당한 노력이 필요합니다. 저희 플랫폼 팀에서는 내부적으로 자주 사용되는 모델 구조에 대한 트레이너를 벤치마킹하고, 이를 통해 얻은 가장 효과적인 기법과 최선의 방법론을 이 플랫폼을 활용하는 모든 그룹에 공유하고 있습니다.</p><h4><strong>4. 모델 추론</strong></h4><p>훈련 후, 모델은 실험용 또는 실제 트래픽을 서빙하기 위한 프로덕션 환경에 배포됩니다. 쿠버네티스 상에서 모델 추론을 하기 위해 셀던(Seldon) 플랫폼이 이용됩니다. 셀던은 TFServing과 Triton 같은 서빙 라이브러리와 통합될 뿐만 아니라, 사용자 정의 Python 래퍼도 지원합니다. 이를 통해 다양한 모델 프레임워크, 런타임 및 하드웨어(CPU 및 GPU 서빙 포함)를 폭넓게 지원합니다.</p><p>각 ML 모델은 오토스케일링을 지원하는 독립 서비스로 배포할 수 있습니다. ML 모델을 서비스로 배포하면 모델별로 관리할 수 있고, 표준 CI/CD 인프라와의 원활한 통합이 가능합니다. 배포 전에는 모델 크기, 훈련과 예측 사이의 비대칭도 테스트 등 다양한 검증 테스트를 실행한 후 카나리 테스트 단계로 진행합니다. 카나리 테스트가 성공하면 모델은 점진적으로 전체 배포됩니다. 개발자는 이러한 간단한 작업을 통해 매우 손쉽고 안전하게 모델을 프로덕션 트래픽에 적용할 수 있습니다.</p><p>임베딩과 같은 높은 계산 부하를 필요로 하는 기능을 실시간으로 거의 지연 없이 제공하기 위해, 저희는 위에서 언급한 온라인 피처 스토어를 사용합니다. 특히 대규모 모델들(LLMs, 멀티모달 모델)을 위해, CPU 서빙보다 효율적인 처리량을 가진 GPU 기반의 실시간 및 배치 서빙 기술에 주력하고 있습니다.</p><figure><img alt=\"쿠팡 ML 플랫폼의 훈련 워크플로\" src=\"https://cdn-images-1.medium.com/max/720/1*9LVdZNjIgG-5H-TDkeGJYg.png\" /><figcaption><strong>그림 2. 훈련 워크플로</strong></figcaption></figure><figure><img alt=\"쿠팡 ML 플랫폼의 서빙 워크플로\" src=\"https://cdn-images-1.medium.com/max/950/1*DqM5JCopDlyJHsrD3zo9bQ.png\" /><figcaption><strong>그림 3. 서빙 워크플로</strong></figcaption></figure><h4><strong>5. 모니터링 및 관찰</strong></h4><p>쿠팡 ML 플랫폼의 모든 서비스에는 모니터링 기능이 탑재되어 있습니다. 훈련 클러스터에는 사용 중인 GPU, CPU, 메모리에 대한 리소스 및 작업 모니터링 대시보드가 구비되어 있습니다. 또한, 작업의 GPU 및 CPU 활용도에 관한 수치들을 확인할 수 있습니다.</p><p>추론 서비스는 메모리 사용량과 예측 점수에 대해 런타임 모니터링을 진행합니다. 향후에는 피처와 모델 서빙에 대한 데이터 품질 검사(이상 탐지, 드리프트 모니터링)를 도입할 예정입니다.</p><p>더불어, 개발자들은 대시보드를 통해 자원 할당과 스케줄링의 지연 상황을 쉽게 파악할 수 있습니다. 오류 발생 시, 해당 클러스터에서는 애플리케이션 및 자원 사용 로그를 수집하여 대시보드에서 확인할 수 있도록 합니다. 그 외에도 훈련 중인 작업이 정체되거나, 서빙 또는 메모리 상승과 같은 문제 상황에 대한 알림 설정도 마련되어 있습니다.</p><figure><img alt=\"쿠팡 ML 플랫폼의 서빙 모니터링\" src=\"https://cdn-images-1.medium.com/max/548/1*iOlpbfMcpgfp1VFhNgm5ZA.png\" /></figure><figure><img alt=\"쿠팡 ML 플랫폼의 서빙 모니터링\" src=\"https://cdn-images-1.medium.com/max/511/1*4C_fUiko3xcKPBrTlyDYEg.png\" /><figcaption><strong>그림 4. 서빙 모니터링</strong></figcaption></figure><h4><strong>6. 트레이닝 및 추론 클러스터</strong></h4><p>대용량 데이터와 딥러닝 모델의 시대에서, 하드웨어(특히 GPU 같은 가속기)는 ML 개발에서 필수적인 역할을 합니다. 쿠팡은 클라우드 인프라 엔지니어와의 적극적인 협업을 통해 온프레미스 데이터 센터와 AWS 클러스터에서 컴퓨팅 및 스토리지 클러스터를 제공합니다.</p><figure><img alt=\"쿠팡 ML 플랫폼 트레이닝 클러스터의 GPU 사용률 모니터링\" src=\"https://cdn-images-1.medium.com/max/1024/1*CQd2-FaoqZxYKC7wrqDKsw.png\" /><figcaption><strong>그림 5. </strong>트레이닝 클러스터의 GPU 사용률 모니터링</figcaption></figure><p>훈련에는 대용량 메모리가 있는 인스턴스, GPU와 같은 가속기, 분산 훈련을 위한 노드 간 고대역폭 연결, 그리고 훈련 데이터 및 모델 체크포인트와 같은 출력 아티팩트를 저장하기 위한 공유 스토리지 클러스터가 필요합니다.</p><p>서빙을 위해서는 성능과 가용성을 보장하는 높은 I/O 처리량을 가진 머신이 필수적입니다. 여러 가용성 영역에 최적화된 전용 머신들을 구비하고 있고, 오토스케일링 기능으로 클러스터가 트래픽의 급증에도 대응할 수 있습니다.</p><h3><strong>성공 사례</strong></h3><p>쿠팡 ML 팀과의 협업 덕분에, 특정 도메인에서 입증된 해결 방안을 체계적으로 확장하고 일반적으로 적용할 수 있게 되었습니다.</p><p>아래는 쿠팡 ML 플랫폼의 지원을 받은 최근 성공 사례들 중 일부입니다.</p><h4><strong>1. 코버트(Ko-BERT) 트레이닝을 통한 검색 쿼리 이해 개선</strong></h4><p>상품 검색 및 추천을 담당하는 ML 개발자들은 기존의 용어 매칭 기반 검색을 보완하기 위해 임베딩 기반 검색 방식을 적용했습니다. A100 GPU의 다중 GPU 분산 훈련을 통해 이전 세대의 GPU들과 훈련 전략에 비해 버트(BERT)의 훈련 속도가 10배 향상되었습니다.</p><p>버트의 성공 이후 개발자들은 다양한 경로로 유입되는 검색 요청들과 관련해 검색 품질을 향상시키기 위해 대규모 언어 모델(LLMs)을 세부적으로 조정하는 실험을 계속하고 있습니다. 이러한 세부 조정 작업은 ML 플랫폼의 여러 부분을 활용하게 되는데, 그 중에서도 효율적인 클러스터 이용, 분산 훈련 전략, 고처리량 GPU 기반 추론 등이 포함됩니다.</p><p>저희는 ML 플랫폼을 통해 쿠팡 개발자들이 최신 ML 기술들에 쉽게 접근하고 적용하는 데 있어 큰 성과를 거두고 있습니다.</p><h4><strong>2. 상품의 실시간 가격 예측</strong></h4><p>고객 및 성장을 위한 데이터 사이언스 팀은 가격, 수요, 페이지 조회 등을 예측하기 위해 다양한 시계열 데이터를 모델링합니다. 데이터 사이언스 팀은 사용자 정의 추론 스택에서 사용 중인 가격 모델을 저희 ML 플랫폼으로 전환하였습니다. 그 결과, 배포 클러스터를 유지 관리할 필요가 없어졌으며, 모델 개발에 완전히 집중할 수 있게 되었습니다.</p><p>저희의 여정은 아직 초기 단계이지만, 많은 사용자들이 저희 서비스를 ML 파이프라인 구성 요소로 사용하고 있으며 사용자들로부터 좋은 반응을 얻고 있습니다. 지난 1년 동안, ML 플랫폼에서 600개 이상의 ML 프로젝트에서 100,000건 이상의 워크플로가 실행되었습니다. 그리고 실험 중인 모델의 크기가 크게 증가함에 따라 고객들에게 제공되는 서비스들의 품질에서도 여러 가지 성과를 거두었습니다. 쿠팡의 모든 주요 ML 그룹에서는 하나 이상의 쿠팡 ML 플랫폼 서비스를 활용하고 있습니다.</p><p>쿠팡 개발자들은 저희 ML 플랫폼에서 언어 모델링 및 자동화된 머신러닝(AutoML) 같은 도메인 특화 도구를 구축하고 있습니다. 온라인 피처 스토어와 모니터링 등의 베스트 프랙티스(best practice)에서 CI/CD의 도입과 활용에 대한 관심이 크게 증가하고 있습니다.</p><p>다음 포스트에서는 쿠팡의 핵심 서비스와 이를 지원하는 애플리케이션들에 대해 더욱 상세히 소개하고자 합니다.</p><p><em>머신러닝과 인프라와 관련된 도전들에 관심을 갖고 수많은 비즈니스 문제를 함께 해결하면서 고객 경험을 향상시키고 싶으시다면, 쿠팡 </em><a href=\"https://www.coupang.jobs/kr/\"><em>채용 공고</em></a><em>를 확인해 보세요!</em></p><p>본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 <a href=\"https://ir.aboutcoupang.com/English/home/default.aspx\">ir.aboutcoupang.com</a> 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=de29804148bb\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%84-%ED%86%B5%ED%95%9C-ml-%EA%B0%9C%EB%B0%9C-%EA%B0%80%EC%86%8D%ED%99%94-de29804148bb\">쿠팡의 머신러닝 플랫폼을 통한 ML 개발 가속화</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-11-23T05:07:33.000Z",
    "url": "https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%84-%ED%86%B5%ED%95%9C-ml-%EA%B0%9C%EB%B0%9C-%EA%B0%80%EC%86%8D%ED%99%94-de29804148bb?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "Meet Coupang’s Machine Learning Platform",
    "partialText": "<h4>How Coupang’s ML Platform accelerates ML development for Coupang products</h4><p><strong><em>By</em></strong><em> </em><a href=\"https://www.linkedin.com/in/hyunjung-baek/\"><em>Hyun Jung Baek</em></a><em>, </em><a href=\"https://www.linkedin.com/in/hara-ketha-ab88b111/\"><em>Hara Ketha</em></a><em>, </em><a href=\"https://www.linkedin.com/in/jaideepray/\"><em>Jaideep Ray</em></a><em>, </em><a href=\"https://www.linkedin.com/in/justina-min-649681112/\"><em>Justina Min</em></a><em>, </em><a href=\"https://www.linkedin.com/in/mohamed-sabbah-08bb5418/\"><em>Mohamed Sabbah</em></a><em>, </em><a href=\"https://www.linkedin.com/in/ronakpan/\"><em>Ronak Panchal</em></a><em>, </em><a href=\"https://www.linkedin.com/in/adunuthula/\"><em>Seshu Adunuthula</em></a><em>, </em><a href=\"https://www.linkedin.com/in/thimma-reddy-kalva-a27aa859/\"><em>Thimma Reddy Kalva</em></a><em>, and </em><a href=\"https://www.linkedin.com/in/enhua\"><em>Enhua Tan</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r-iwNRTZ7xDhcZJ0B1ERrg.jpeg\" /></figure><blockquote>This post is also available in <a href=\"https://medium.com/coupang-engineering/쿠팡의-머신러닝-플랫폼을-통한-ml-개발-가속화-de29804148bb\"><strong>Korean</strong></a>.</blockquote><h3>Introduction</h3><p>Coupang is reimagining the shopping and delivery experience to wow all the customers from the instant they open the Coupang app to the moment an order is delivered to their door. In addition to e-commerce, Coupang has various other consumer services ranging from Coupang Eats for food delivery, Coupang Play for video streaming, Coupang Pay for payments, and to Coupang Grocery for fresh products amongst others.</p><p>Machine Learning (ML) impacts every aspect of e-commerce experiences of Coupang customers: the product catalog, search, pricing, robotics, inventory, and fulfillment. As Coupang ventures into new markets, ML has continued to play an even more important role.</p><p>ML helps to power search and discovery across Coupang websites and apps, to price products and services, to streamline logistics and delivery, to optimize content for streaming, to rank ads, and to do many more jobs.</p><p>Therefore, we strive to scale machine learning development at all ML lifecycle stages, including ad-hoc exploration, training data preparation, model development, and robust production deployment of models.</p><h4>Table of Contents</h4><blockquote>· <a href=\"#5959\">ML @ Coupang</a><br>· <a href=\"#534f\">Motivation</a><br> ∘ <a href=\"#79be\">1. Reduce time to production</a><br> ∘ <a href=\"#14dc\">2. Incorporate CI/CD in ML development</a><br> ∘ <a href=\"#bd5e\">3. Scale ML compute efficiently</a><br>· <a href=\"#46a6\">Core offerings of Coupang ML Platform</a><br> ∘ <a href=\"#5cf8\">1. Notebooks &amp; ML Pipeline Authoring</a><br> ∘ <a href=\"#4adb\">2. Feature Engineering</a><br> ∘ <a href=\"#a499\">3. Model Training</a><br> ∘ <a href=\"#5b6c\">4. Model Inference</a><br> ∘ <a href=\"#6536\">5. Monitoring &amp; Observability</a><br> ∘ <a href=\"#cf93\">6. Training &amp; Inference Clusters</a><br>· <a href=\"#bcca\">Success Stories</a><br> ∘ <a href=\"#671d\">1. Training Ko-BERT to understand search queries better</a><br> ∘ <a href=\"#2284\">2. Real-time price forecasting of products</a></blockquote><h3>ML @ Coupang</h3><p>ML teams at Coupang are actively developing models in Natural Language Processing (NLP), Computer Vision (CV), Recommendations, and Forecasting. NLP is used to understand search queries, product listings, and ads content. Computer vision-enabled image understanding categorizes similar products and ads. Recommendation models rank content for product search, videos in Coupang Play, and product ads. Forecasting techniques help us understand supply, demand, and pricing for millions of products.</p><p>This post introduces Coupang’s internal ML platform and describes how the platform supports the increasing scale and diversity of workloads across ML frameworks, programming languages, different model architectures, and training &amp; serving paradigms.</p><h3><strong>Motivation</strong></h3><p>The motivation behind Coupang ML Platform is to provide ‘batteries-included’ services to accelerate ML development through improved developer productivity.</p><p>Core services include managed notebooks (Jupyter), pipeline SDK, feature-store, model training, and model inference. ML teams can use the services independently to compose their ML pipeline. Our focus areas are as follow:</p><h4><strong>1. Reduce time to production</strong></h4><p>Before Coupang ML Platform, authoring and training a ML model required hours of non-trivial setup work and boilerplate code for preparing data, features and writing trainer code. Tasks like scaling model training through distributed training, using GPUs took deep engineering work resulting in duplicate stack. <br>Deploying the ML model for serving real-time traffic took weeks of effort, replicating logic for model benchmarking, auto-scaling, security and rollback. These were blockers for product groups to adopt ML at a larger scale. By leveraging ML Platform lifecycle services, one can train, debug and deploy simple to complex models in production within days in a standardized way.</p><h4>2. Incorporate CI/CD in ML development</h4><p>ML development can quickly incur heavy technical debt. To make it easier for ML teams to build, deploy and maintain models, we provide integration tested prepackaged containers with popular ML libraries. <br>Moreover, we provide libraries to validate model, add canary in model deployment and monitoring primary metrics during serving.</p><h4>3. Scale ML compute efficiently</h4><p>There is surging demand for compute in Coupang — GPUs for deep learning training, storage for large datasets, and network bandwidth for distributed training. Cloud costs are high, given the large fleet of models training on the platform. Coupang ML Platform team manages a hybrid setup with compute and storage clusters on on-prem and AWS. The on-prem setup provides more customization and a powerful GPU cluster at lower costs while the cloud setup can scale on demand if on-prem resources are insufficient.</p><figure><img alt=\"An overview of Coupang ML Platform\" src=\"https://cdn-images-1.medium.com/max/838/1*ki27OtAMLOA91Tqg_djvFw.png\" /><figcaption><strong>Figure 1. </strong>Coupang ML Platform overview</figcaption></figure><h3>Core offerings of Coupang ML Platform</h3><h4>1. Notebooks &amp; ML Pipeline Authoring</h4><p>ML platform provides a hosted, containerized notebook service for developers to iterate on their ideas. The notebook can be launched using custom or standard containers on CPUs or GPUs.</p><p>A set of standard docker containers are maintained by the platform team containing popular ML libraries such as Tensorflow, Pytorch, Sklearn, Huggingface, Transformers, etc. The docker containers help in avoiding dependency complexity and help in writing repeatable pipelines.</p><p>For pipeline authoring, the platform provides a set of Python SDKs for data fetching, feature-store, training, and inference.</p><h4>2. Feature Engineering</h4><p>Coupang ML Platform offers a feature-store built to access prepared features easily in both offline and online modes. The feature store is built on top of the popular open-source project Feast.</p><ul><li>Offline feature stores are used to share prepared features and are also used for model training. We are working with teams to onboard fundamental features such as customer insights which can be consumed by multiple downstream teams.</li><li>Online feature store is used to fetch features with low latency during inference. This serves as a model feature generator as well as prediction response cache for compute-intensive models.</li></ul><h4>3. Model Training</h4><p>ML teams at Coupang use different modeling frameworks, from the popular ones such as Pytorch, Tensorflow, Sklearn, XGBoost, to the niche ones such as Prophet for forecasting.</p><p>The training stack is agnostic of framework. User written pipelines are containerized and launched on the Kubernetes cluster. A batch scheduler schedules the jobs on the desired hardware setup. Users can configure their jobs to run on any CPU type or GPU type available in the cluster. This is very useful as jobs can benefit from various CPU and GPU types depending on their characteristics and can optimize the return on investment. For example, users can configure their model training and batch inference tp rim on different GPU types, optimizing itself for speedup vs. cost of GPU. <br>The scheduler is configured to follow all-or-nothing resource allocation strategy. The training stack supports distributed training strategies (distributed data parallel and fully sharded data parallel) to train large models. Multi-GPU training has sped up model training workloads significantly across Coupang.</p><p>It requires significant effort to tune trainer parameters to efficiently train deep learning models. As the platform team, we benchmark trainers for popular model architectures used internally and share the most effective techniques and best practices amongst all groups who use the platform.</p><h4>4. Model Inference</h4><p>Post training, a model is deployed for experimentation or production for serving real traffic. The Seldon platform is used on Kubernetes for model inference. Seldon has integrations with serving libraries such as TFServing and Triton while it can also support custom python wrappers. Through this, it can cover a wide range of model frameworks, runtimes and hardware (CPU &amp; GPU Serving).</p><p>Each ML model can be deployed as a standalone service with autoscaling. Deploying each ML model as a service provides isolation and allows integration with standard CI/CD infrastructure. Model deployment jobs run multiple validation tests (model size, training-prediction skew tests, etc) before moving into a canary phase. If canary results are successful, the model can be gradually rolled out. Developers need minimum effort (adding hooks for model validation and canary results verification) to safely get their model serving production traffic. <br> <br>To serve compute-intensive features, such as embedding, in real time with low latency, we use the online feature store mentioned above. For very large models (LLMs, multimodal models), we are investing in batch and real-time GPU based serving which provides a high throughput compared to CPU serving.</p><figure><img alt=\"Coupang ML Platform’s training workflow\" src=\"https://cdn-images-1.medium.com/max/720/1*Fc2rm6L1FqWmAxSzbO-UpQ.png\" /><figcaption><strong>Figure 2.</strong> Training workflow</figcaption></figure><figure><img alt=\"Coupang ML Platform’s serving workflow\" src=\"https://cdn-images-1.medium.com/max/950/1*G8BK042RU2PBvCLm9x4Y1g.png\" /><figcaption><strong>Figure 3.</strong> Serving workflow</figcaption></figure><h4>5. Monitoring <strong>&amp; Observability</strong></h4><p>All Coupang ML Platform services have monitoring enabled. Training cluster has resource and job monitoring dashboards (GPUs, CPUs, Memory in use). There are GPU and CPU utilization metrics for workloads. <br>Inference service has runtime monitoring for memory usage, prediction scores. We have plans to introduce data quality checks (anomaly detection, drift monitoring) across feature and model serving. <br>Cluster usage dashboards are used by developers to understand resource allocations and scheduling delays. For error debugging, the application and resource usage logs are collected from clusters and made available to developers through dashboards. There also are alerts set up for various events such as stuck or idle training jobs, inability to launch instances for training, serving or memory spikes.</p><figure><img alt=\"Coupang ML Platform’s monitoring serving\" src=\"https://cdn-images-1.medium.com/max/548/1*JoMJ0x8dWJf_ORGd82Iheg.png\" /></figure><figure><img alt=\"Coupang ML Platform’s monitoring serving\" src=\"https://cdn-images-1.medium.com/max/511/1*ps5ORx_eAKTUaFKOnrsdog.png\" /><figcaption><strong>Figure 4.</strong> Monitoring serving</figcaption></figure><h4>6. Training &amp; Inference Clusters</h4><p>In the era of large datasets and deep learning models, hardware (especially accelerators such as GPU) plays a crucial role in ML development. Through an active collaboration with the the cloud infrastructure engineers at Coupang, we provide compute and storage clusters in the on-prem data center and AWS cluster.</p><figure><img alt=\"Coupang ML Platform’s monitoring GPU utilization of its training cluster\" src=\"https://cdn-images-1.medium.com/max/1024/1*_RtWkS0_mz_vv00dttkdGQ.png\" /><figcaption><strong>Figure 5.</strong> Monitoring GPU utilization of Training cluster</figcaption></figure><p>Training requires instances with large memory, accelerators such as GPUs, high bandwidth connection between nodes for distributed training, and a shared storage cluster to store training data and output artifacts such as model checkpoints.</p><p>Serving requires high I/O throughput machines for performance and availability. We have a dedicated set of machines optimized for serving in multiple availability zones. Autoscaling ensures that the cluster can handle traffic spikes.</p><h3><strong>Success Stories</strong></h3><p>Through our partnership with ML teams at Coupang, we are able to systematically scale solutions which have been proven in one domain and can be generalized.</p><p>The following is a couple of recent customer success stories supported by Coupang ML Platform:</p><h4><strong>1. Training Ko-BERT to understand search queries better</strong></h4><p>ML developers working in search and recommendations launched embedding-based retrieval to augment classical term matching-based retrieval. Multi-GPU distributed training on A100 GPUs provided 10x speed up for BERT training compared to older generation GPUs and training strategy.</p><p>After success of BERT, the developers are experimenting with finetuned large language models (LLMs) to improve search quality across different surfaces. Large Language model finetuning exercises various parts of the ML platform — efficient cluster usage, distributed training strategies, high throughput GPU-based inference, etc. <br>We have been fairly successful in adapting and democratizing the new ML innovations through our platform.</p><h4><strong>2. Real-time price forecasting of products</strong></h4><p>Data science teams in Customer and Growth model various time series data for forecasting price, demand, page-view amongst others. The team onboarded their entire suite of pricing models from custom inference stack to our ML Platform serving. The team no longer has to maintain their deployment cluster. They can focus entirely on developing better models.</p><p>Even though we are still early in our journey, we see good traction in customers using the services as building blocks in their ML pipeline. Over the past year, there have been 100K+ workflow runs on the platform spanning 600+ ML projects. We saw massive increase in size of models being experimented on resulting in several wins in quality of Coupang services. All major ML groups at Coupang use one or more Coupang ML Platform services. <br>We see developers building domain-specific toolkits on the Coupang ML Platform, such as language modeling and AutoML. There has been strong interest in and adoption of CI/CD in best practice features such as online feature store and monitoring.</p><p>The coming posts will describe Coupang’s core services and applications supported by them in more detail. If you are interested in tackling Machine Learning and infra challenges that enable developers to solve hundreds of business problems and improve customer experience, consider applying for a <a href=\"https://www.coupang.jobs/en/\">role</a> on our team!</p><p><em>While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on </em><a href=\"http://ir.aboutcoupang.com/\"><em>ir.aboutcoupang.com</em></a><em> for information on our formal investment plans and product development strategies.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd00e9ccc172\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172\">Meet Coupang’s Machine Learning Platform</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-09-08T06:21:33.000Z",
    "url": "https://medium.com/coupang-engineering/meet-coupangs-machine-learning-platform-cd00e9ccc172?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "쿠팡 로켓배송: 공간 색인 기반의 새로운 배송 영역 관리 시스템",
    "partialText": "<h4>배송 영역 시각화 프로젝트를 통해 구축한 직관적이고 최적화된 배송 시스템에 대해</h4><p><em>By </em><a href=\"https://www.linkedin.com/in/jinonearth/\"><em>Geo J Son</em></a><em> &amp; </em><a href=\"https://www.linkedin.com/in/%ED%95%9C%EC%83%98-%EC%A0%84-a5a21994/\"><em>Sam HS Jeon</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HXH-fv_TNeDAcCvGWXSGgw.jpeg\" /></figure><blockquote>본 포스트는 <a href=\"https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63\"><strong>영문</strong></a>으로도 제공됩니다.</blockquote><p><a href=\"https://www.youtube.com/watch?v=cZNRGjcfiKw&amp;t=3s\">로켓배송</a>은 쿠팡의 가장 중요하고 매력적인 서비스들 중 하나입니다. 고객들이 “쿠팡 없이 어떻게 살았을까?”라고 생각하게 만들고 고객들의 삶이 더 나아질 수 있도록, 저희는 최적화된 배송 시스템을 마련하고 배송 기사인 쿠팡친구(이하 쿠친)들과 함께 노력하고 있습니다.</p><p>쿠친이 택배 상자와 봉투들을 가장 효율적으로 배송하려면, 배송 시스템은 정확히 배송지 주소를 파악하고 이를 바탕으로 가장 적합한 배송 영역을 찾아낸 다음 쿠친에게 운전 경로를 안내해 주어야 합니다. 또한 시스템에 의해 택배 상자와 봉투들이 배송 영역으로 균등하게 할당되고 분배되려면, 경험 많은 캠프 작업자들이 배송 영역 정보를 수시로 업데이트할 수 있어야 합니다.</p><p>저희의 기존 시스템은 텍스트로 된 주소와 우편번호에 의존했고, 직관적이고 기능적인 부분도 부족하여 저희의 복잡한 운영 니즈를 만족시키지 못했습니다. 이에 저희는 배송 영역을 지도 위에 시각화하고, 직접 지도를 수정할 수 있는, 그리고 최적화에 필요한 수치 및 통계가 추가로 제공되는 시스템의 개발을 위한 프로젝트를 2021년에 시작했습니다.</p><h4>목차</h4><blockquote>· <a href=\"#2177\">과제: 텍스트 기반의 우편번호</a><br>· <a href=\"#9315\">배송 영역 관리 시스템의 새로운 목표</a><br>· <a href=\"#6c02\">시스템에 H3 도입하기</a><br> ∘ <a href=\"#8da1\">지형공간 색인 시스템</a><br> ∘ <a href=\"#fc70\">공간 색인 시스템</a><br>· <a href=\"#dadf\">새로운 기술들로 시스템 재설계하기</a><br> ∘ <a href=\"#b46d\">격자 해상도</a><br> ∘ <a href=\"#d6aa\">데이터 관리</a><br> ∘ <a href=\"#6df3\">육각형 그룹을 육각형으로 채워진 다각형으로</a><br>· <a href=\"#f3ef\">시스템에 새로운 기술들 적용하기</a><br>· <a href=\"#db37\">향후 계획</a></blockquote><h3>과제: 텍스트 기반의 우편번호</h3><p>우편번호는 정부 및 각종 공공기관들이 보장하는 가장 잘 조직된 시스템 중 하나로, 특정 배송 영역에 할당된 코드를 가리킵니다. 지형적 요소와 배송 수요 측면에서 봤을 때 할당된 영역의 물리적 넓이는 제각각이긴 하지만 우편번호는 최적화된 실질 배송 효율을 제공합니다. 그래서 로켓배송은 처음부터 우편번호를 배송 영역의 기본 단위로 채택했습니다. 서비스 초기에는 잘 작동했지만, 쿠팡이 성장하면서 훨씬 더 많은 것들이 필요해졌습니다. 우편번호당 매일 처리해야 하는 배송 건수가 두 자릿수에서 세 자릿수로 증가했습니다. 한명의 쿠친이 하루에 처리하기에는 너무 많은 양이었습니다.</p><p>그러다보니 단일 우편번호를 분할해 처리 가능한 여러 영역으로 세분화해야 할 필요가 생겨났습니다. 처음에는 아파트 단지 몇개가 단일 영역이었지만, 분할되어 단지 하나가 단일 영역이 되었고, 결국엔 단지도 분할되어 아파트 동 하나가 단일 영역으로 세분화되었습니다. 그러나 텍스트로만 되어 있는 주소에서 공간 관련 정보를 식별해내기는 쉽지 않기 때문에 배송 영역의 특성을 잘 알고 있는 숙련된 캠프 리더와 쿠친 그룹 리더만이 세분화 작업을 수행할 수 있었습니다.</p><h3><strong>배송 영역 관리 시스템의 새로운 목표</strong></h3><ol><li>지도 위에 배송 영역들을 좀 더 효율적이고 쉽게 시각화하기</li><li>관계자들 모두 쉽게 배송 영역들을 확인하고, 만들고, 수정하고, 다른 사람들과 공유하기</li><li>신축 및 철거 건물과 같은 변동 상황 발생 시에도 지속적으로 운영 가능하도록 배송 영역들을 우편번호 및 문자로 된 주소가 아닌 공간 데이터 기반으로 관리하기</li></ol><h3><strong>시스템에 H3 도입하기</strong></h3><p>저희는 배송 영역 관리 시스템의 새로운 목표를 달성하기 위한 프로젝트를 시작했습니다. 지도 위에 그려진 다각형(polygon)들로 직관적인 공간 영역 관리가 가능하도록 육각형 격자(hexagon grid) 기반의 공간 색인 시스템(spatial indexing system)을 활용했습니다.</p><p>지도 위 영역들을 다각형으로 그리는 것이 간단해 보일 수도 있지만, 영역들 모두를 중복되고 생략되는 것 없이 깔끔하고 명확한 경계들로 그려내 빈틈없이 관리하는 것은 매우 어렵습니다. 이런 다각형 세트들이 상호배제와 전체포괄(mutually exclusive and collectively exhaustive, MECE) 원칙 하에 관리될 수 있게끔, 공간 단위를 정의하는 일부터 시작했습니다.</p><h4><strong>지형공간 색인 시스템</strong></h4><p>먼저 GIS 전문가들에 의해 개발된 다양한 지형공간 색인 시스템(geospatial indexing system)들을 검토했습니다. 지형공간 색인 시스템은 일종의 격자 세트(a set of grids)로, 고유 식별자를 갖는 격자들로 지리적 표면을 채워 관리합니다. 행정기관에 의해 유연하게 변경될 수 있는 우편번호와 행정구역 같은 경우에는 불규칙한 형태의 격자를 가집니다. 이렇게 공간 영역에 고유 식별자가 할당되면서도 공간의 경계는 유연한 시스템은 결합 시계열 분석(analysis of integrated time series) 시에 이점이 있습니다.</p><h4><strong>공간 색인 시스템</strong></h4><p><a href=\"https://what3words.com/\"><strong>W3W (What3Words)</strong></a>는 가장 널리 사용되는 공간 색인 시스템(spatial indexing system)들 중 하나이며, 전 세계를 3m² 정사각형으로 구분하여 각각의 정사각형을 3개의 단어로 표현합니다. 위도-경도 좌표계는 긴 숫자 배열로 구성되어 있어 말로 전달하기 어려운 반면, W3W는 3개 단어들의 고유한 조합만으로 위치를 나타낼 수 있습니다. 예를 들어 쿠팡의 사무실들 중 하나인 로켓 연구소의 위치는 위도-경도 좌표로는 [<strong>37.503819,</strong> <strong>127.0481493</strong>]이고 W3W로는 [<a href=\"https://what3words.com/scoring.eager.patch\"><strong>scoring.eager.patch</strong></a>]입니다. 하지만 아쉽게도 분석에 필요한 해상도 조정은 불가능합니다.</p><figure><img alt=\"쿠팡 로켓 연구소 사무실의 W3W 주소\" src=\"https://cdn-images-1.medium.com/max/484/1*lrtZxW5BP6ON5y8M2CCUTQ.png\" /><figcaption><strong>그림 1</strong>. W3W 예시</figcaption></figure><p><a href=\"https://s2geometry.io/\"><strong>Google의 S2</strong></a>와 <a href=\"https://h3geo.org/\"><strong>Uber의 H3</strong></a>는 잘 알려져 있는 불변 공간 색인 시스템으로 격자들이 부모-자식(parent-child)의 상하위 위계 구조를 갖습니다. N개의 하위 격자가 1:N 관계에 의해 하나의 상위 격자로 매핑됩니다. 하위 격자들을 합쳐 하나의 상위 격자를 만들 수 있고, 반대로 상위 격자를 하위 격자들로 분할할 수도 있습니다. 설정 가능한 격자의 크기는 다양하며 지리 좌표로 변환하려는 지도의 목적과 규모에 따라 유연하게 선택할 수 있습니다. 저희의 프로젝트 같은 경우 확장성 및 구현의 이점을 위해 H3를 채택했습니다.</p><figure><img alt=\"Google S2와 Uber H3의 사양 비교\" src=\"https://cdn-images-1.medium.com/max/1024/1*S2Mr01Zai2YMhdUovAUrVA.jpeg\" /><figcaption><strong>표 1</strong>. S2와 H3 비교 (출처: <a href=\"https://www.uber.com/en-KR/blog/h3\">Uber blog</a>)</figcaption></figure><ul><li><strong>왜곡도</strong>: 위의 표에서 알 수 있듯이, S2와 H3의 가장 큰 차이점은 기본 격자의 모양입니다. H3는 육각형 격자로 구성되어 있어 공간 대부분에서 왜곡이 최소화됩니다. S2는 구에서 정방형으로 지구를 투영(projection)하는 과정에서 상당한 오류를 발생시킵니다. 따라서 같은 레벨의 격자들이라도 투영 중심(center of projection)으로부터의 거리에 따라 서로 크기가 다를 수도 있습니다. 한편, H3는 육각형마다 투영 중심점이 존재하는 <a href=\"https://en.wikipedia.org/wiki/Fullerene\">풀러린</a>이나 축구공 모양과 유사합니다. 이는 왜곡을 최소화하는 데 도움이 됩니다.</li><li><strong>이웃 격자</strong>: H3는 동일한 영역을 더 적은 수의 격자들로 변환할 수 있습니다. 특정 영역을 나타낼 때, S2는 영역을 더 작은 사각형, 엄밀히 말해 마름모로 분할합니다. H3은 육각형을 이용하기에 서로 인접해 있는 격자들의 표면적이 더 넓어 영역이 상대적으로 더 큼직하게 분할됩니다.</li><li><strong>위계 구조 &amp; 조밀도: </strong>H3 같은 경우, 상위 격자들이 물리적으로 모든 하위 격자들을 포함하지는 않습니다. 바다처럼 배송지가 없는 곳에는 육각형 대신 예외적으로 오각형 격자로 분할합니다.</li></ul><h3><strong>새로운 기술들로 시스템 재설계하기</strong></h3><h4><strong>격자 해상도</strong></h4><p>쿠팡의 배송 서비스가 제공되는 영역들 전부를 육각형 격자로 정확히 표현해내려면 어떻게 해야 할까요? 기본적으로 최적의 크기를 갖는 육각형으로 대한민국 전체를 담아낼 수 있어야 합니다. 크기가 너무 크면 하나의 같은 육각형에 여러 건물 또는 주소들이 할당될 수 있습니다. 크기가 너무 작으면 하나의 건물이나 주소가 여러 다른 육각형들로 할당될 수 있습니다. <br> <br>아래 그림 2의 왼쪽 부분은 약 300m²의 해상도를 가진 12 레벨 육각형의 예를 보여줍니다. 그러나 이 크기의 육각형으로는 기본적으로 도로와 서로 이웃한 ​​건물들로 나누어지는 <a href=\"https://en.wikipedia.org/wiki/City_block\">블록(block)들</a>을 구분해낼 수가 없습니다. 블록을 배달 영역들로 분할하고 주변의 다른 블록들과 상호 배타적으로 만드는 것이 매우 중요하기 때문에 저희는 6.3m²의 해상도를 가진 14 레벨 육각형을 채택했습니다. 이를 통해 해상도로 입구와 우편함 같은 정확한 위치의 위도와 경도 좌표를 대체할 수 있었습니다.</p><figure><img alt=\"격자 해상도를 활용한 쿠팡 로켓배송 지도 상에서의 12 레벨 육각형 격자(왼쪽)와 14 레벨 육각형 격자(오른쪽) 비교\" src=\"https://cdn-images-1.medium.com/max/1024/1*KMo1GFvRIUp3epfrnBJy7A.png\" /><figcaption><strong>그림 2</strong>. 12 레벨 육각형 격자(왼쪽)와 14 레벨 육각형 격자(오른쪽) 비교</figcaption></figure><h4><strong>데이터 관리</strong></h4><p>서비스 목적에 맞는 육각형의 크기를 정의했으니, 그 다음으로는 해당 크기의 데이터를 확장성 있게 관리하는 방법을 정의해야 했습니다. 대한민국은 12 레벨의 해상도로 약 4억 3200만 개의 육각형으로 나눌 수 있습니다. 상위 육각형에는 7개의 하위 육각형이 있으므로 13 레벨 해상도는 31억 육각형으로, 14 레벨 해상도는 217억 육각형으로 변환됩니다. 확장성을 충분히 확보하면서 데이터를 관리할 수 있는 방법이 필요했습니다.</p><p>데이터를 관리하는 방법들 중 하나는 RDBMS를 사용하는 것입니다. 육각형의 ID를 키로 사용해 데이터를 저장할 수 있습니다. 그러나 RDBMS는 읽기/쓰기(read/write)의 성능 유지를 위해 일반적으로 테이블의 행(row)을 1억에서 10억개로 제한합니다. ID를 행 단위로 관리하면 12 레벨 해상도로 대한민국 전역을 하나의 테이블에 담아낼 수 있지만, 14 레벨 해상도를 적용하려면 지역별로 테이블을 나누거나 다른 방법을 채택해야 합니다. 그리고 데이터베이스 샤딩(database sharding)을 통해 어떻게든 데이터를 관리하더라도 국내외 더 많은 지역들에 서비스를 적용하게 될 경우 데이터 저장 공간이 늘어날 수밖에 없습니다. 따라서 저희는 RDBMS에 육각형 ID를 키로 해 특정 데이터를 저장하는 것은 저희의 유즈 케이스에 적합하지 않다는 결론을 내렸습니다.</p><p>이에 각 배송 영역이 영역 내 육각형들의 정보를 갖도록 설계하였습니다. 물론 하나의 매우 큰 영역의 경우 수없이 많은 14 레벨 육각형들의 정보를 갖고 있어야 하기 때문에 메모리 또는 데이터 크기와 관련된 문제가 발생할 수 있습니다. 이를 방지하기 위해 H3에서 제공하는 <a href=\"https://h3geo.org/docs/api/hierarchy/\">압축(compaction)</a> 관련 기능들을 사용하는 것을 고려하였습니다. 하위 육각형들이 모두 존재하는 경우 해당 기능들을 사용하면, 하위 육각형들(finer)과 상위 육각형(coarser)의 해상도를 오고 가면서 원하는 작업을 수행할 수 있습니다. 그리고 이를 통해 데이터 전체를 저장하는데 필요한 메모리 양도 줄일 수 있습니다.</p><figure><img alt=\"쿠팡이 배송 영역을 H3 육각형으로 채우고 압축하는 과정\" src=\"https://cdn-images-1.medium.com/max/1024/1*LztkDevUVe7O3BPKhjeRrQ.png\" /><figcaption><strong>그림 3. </strong>H3로 배송 영역을 육각형으로 채우고 압축하는 과정</figcaption></figure><p>이 디자인으로 배달 영역들을 효율적으로 저장하고, 많은 영역들을 저희가 원하는 만큼 14 레벨 해상도로 표현할 수 있습니다. 지역 제한이나 확장성에도 문제가 없습니다. 따라서 아래의 그림 4과 같이 특정 위도-경도 좌표인 [<strong>34.111, 127.111</strong>]에 대한 영역을 검색하는 경우, 간단히 좌표를 14 단계 해상도를 갖는 육각형의 ID인 <strong>85283473fffffff</strong>로 변환하고, 이 육각형 ID가 포함된 육각형 그룹을 찾습니다. <a href=\"https://postgis.net/\">PostGIS</a>나 다른 지오데이터베이스(Geodatabase) 모듈이 설치되어 있지 않은 환경에서도 육각형 ID와 육각형 그룹 사이의 포함 관계를 쉽게 분석할 수 있습니다.</p><figure><img alt=\"쿠팡 로켓배송의 지도가 좌표 값에서 변환된 특정 육각형의 ID가 포함된 육각형 그룹을 조회하는 방법\" src=\"https://cdn-images-1.medium.com/max/1024/1*SmgqFfuKWgdKrYJc_tE4lA.png\" /><figcaption><strong>그림 4</strong>. 좌표 값에서 변환된 특정 육각형의 ID를 포함하고 있는 육각형 그룹 조회 방법</figcaption></figure><h4><strong>육각형 그룹을 육각형으로 채워진 다각형으로</strong></h4><p>시스템 개발의 첫 단계를 완료한 후 몇 가지 개선이 필요한 부분들을 발견했습니다. 예를 들어 육각형 ID 목록 사용 시 제한되는 것들이 몇 가지 있었습니다. 복잡한 공간을 분석하거나 특정 지점에서 멀리 떨어져 있는 배달 영역들 몇몇을 찾아내는 것이 어려웠습니다. 이를 개선하기 위해 영역의 기본 단위는 육각형 격자로 유지하되 실제 데이터는 다각형 포맷으로 저장했습니다. 어떤 다각형이든 <a href=\"https://h3geo.org/docs/api/regions\">polyfill</a> 기능을 이용해 14 레벨의 육각형들로 변환한 다음 경계를 다시 다각형으로 바꿔 저장할 수 있습니다. 이것은 픽셀 아트(pixel art)를 그리는 것과 비슷합니다. 벡터화된 .ai 파일을 .jpg 비트맵 파일로 변환한 다음, 변환된 파일을 분할해 다시 벡터화된 이미지를 만듭니다.</p><p>또한 위치 정보 검색 및 배송 영역 관리에 있어 육각형으로 채워진 다각형들을 활용하면 겹치고 누락되는 영역 없이 좀 더 쉽게 원하는 결과를 얻을 수 있습니다. 아래 그림 5와 같이 클라이언트 사용자가 지도 UI에 사각의 녹색 다각형을 그리면 서버는 이를 육각형 그룹으로 변환한 다음 외부와 접하는 경계만 다시 다각형으로 변환하여 육각형으로 채워진 적색 다각형을 만들어냅니다. 따라서 사용자가 그린 녹색 다각형과 육각형으로 채워진 적색 다각형으로 단일 영역을 표시할 수 있습니다.</p><p>이 방법의 유일한 단점은 클라이언트 측에서 매우 복잡한 다각형을 만들어내는 경우 육각형으로 변환하는 데 시간이 조금 더 걸릴 수 있다는 것입니다. 이를 해결하기 위해 사용자가 처음 생성한 다각형은 그대로 저장하고, 나중에 육각형으로 채워진 다각형을 비동기적으로 생성해 활용했습니다.</p><figure><img alt=\"쿠팡 로켓배송 지도에서 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 변환하는 과정\" src=\"https://cdn-images-1.medium.com/max/985/1*euZiKJWYi28BDFbJNOxK6g.png\" /></figure><figure><img alt=\"쿠팡 로켓배송 지도에서 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 처리하는 과정\" src=\"https://cdn-images-1.medium.com/max/739/1*Jsp4RdTbi5VcKZFazgRmPA.png\" /><figcaption><strong>그림 5.</strong> 사용자가 그린 사각의 다각형을 육각형으로 채워진 다각형으로 변환하는 과정</figcaption></figure><h3><strong>시스템에 새로운 기술들 적용하기</strong></h3><p>사용자가 어떻게 육각형 그룹들을 생성하고 관리할 수 있는지에 대해 다루었습니다. 이번 섹션에서는 이러한 기술들을 실제 배송 영역 관리 시스템에 적용한 방법을 소개하고자 합니다. 로켓배송 서비스는 앞에서 언급한 바와 같이 우편번호를 배송 영역의 기본 단위로 두고 필요에 따라 영역을 세분화해 적용할 수 있습니다. 따라서 실제 시스템에서는 목록에서 우편번호 선택 시 해당영역의 공간이 지도 위에 표시됩니다.</p><p>아래의 그림 6과 같이 실제 시스템에서 사용자는 우편번호가 가리키는 영역의 안쪽에 자유롭게 다각형을 그리고 영역에 대한 설명을 채워 넣을 수 있습니다. 저장 버튼을 누르면 영역 관련 데이터와 사용자가 그린 다각형이 서버로 전송되어 육각형으로 채워진 다각형으로 변환됩니다.</p><figure><img alt=\"쿠팡의 배송 영역 관리 시스템 예시\" src=\"https://cdn-images-1.medium.com/max/1024/1*pe-GLprJjJbWj98CY0xq4g.png\" /><figcaption><strong>그림 6. </strong>쿠팡의 배송 영역 관리 시스템</figcaption></figure><p>예를 들어, 서울의 우편번호인 <strong>06961</strong>을 선택하면 육각형 그룹들로 구성된 두 개의 영역이 지도 위에 나타납니다. 두 영역 모두 선택한 우편 번호 내에 위치한 개별 아파트 단지입니다. 지도 영역 상단에 제공되는 그리기 도구로 영역들을 그리면 각 영역은 육각형이 채워진 다각형으로 자동 변환됩니다. 각 영역의 테두리를 자세히 살펴보면 테두리가 매끄러운 직선이 아닌 육각형 모양으로 잘려져 있는 것을 볼 수 있습니다.</p><p>그림 6의 오른쪽은 <strong>Edit </strong>버튼을 클릭할 경우 파란색 윤곽선으로 표시된 영역이 수정되는 걸 보여줍니다. 노란색 윤곽선 영역은 여전히 ​​육각형이 채워진 다각형으로 표시되는 반면, 파란색 윤곽선 영역은 사용자가 그린 다각형의 윤곽이 매끄러운 선으로 다시 그려졌습니다. 수정이 완료되면 다각형은 서버에서 처리된 다음 새로운 육각형이 채워진 다각형으로 업데이트됩니다.</p><h3><strong>향후 계획</strong></h3><p>이번 포스트에서는 지오데이터베이스와의 일관성을 유지하면서 배송 영역을 효율적으로 관리하기 위해 시스템을 어떻게 설계했는지에 대해 소개했습니다. 새 시스템은 사용자에게 더 향상된 경험을 제공하기 위해 기존의 시스템을 개선한 것으로, 기존 시스템과 계속 함께 사용할 수 있습니다. 또한 배송 영역의 텍스트 주소 모두를 GIS 기반의 공간 데이터로 변환하면서, 시스템을 앞으로도 더 개선할 수 있는 기회들을 갖게 되었습니다.</p><p>배송 상태 및 관련 수치들을 육각형 격자에 기반해 분석하게 되면서, 더 이상 우편 번호나 주소로부터 영향받지 않고, 공간 기반의 절대(absolute) 통계를 생성할 수 있게 되었습니다. 어떤 이유로 배송 영역에 변화가 일어나더라도 이전과 동일한 기준으로 통계들을 비교할 수 있습니다. 배송 영역에 대해 정확하게 평가할 수 있으며, 비효율적인 배송 영역을 빠르게 식별해 낼 수 있습니다. 이를 통해 저희 팀은 배송 속도의 저하 요인을 심층적으로 분석하고 문제를 해결하기 위한 대책을 찾을 수 있게 되었습니다.</p><p>또한 배송 난이도 평가, 배송 영역간 거리 측정, 이동시간 등 배송 영역의 상황도 분석 가능해졌습니다. 덕분에 1) 배송 영역의 난이도를 보다 정확하게 평가하고, 2) 배송 물량에 따라 배송 영역을 동적으로 관리하며, 3) 쿠친이 보다 안전하고 빠르게 배송할 수 있도록 최적화된 이동 경로를 추천할 수 있게 되었습니다.</p><p>로켓배송 서비스가 처음 시작됐을 때, 주문한 상품이 하루 만에 배송되는 일은 불가능해 보인다는 의구심도 있었습니다. 하지만 지금 익일 배송은 대한민국 쇼핑 문화의 새로운 표준이 되었습니다. 쿠팡은 앞으로도 더 많은 기술 혁신으로 배송 시스템을 발전시켜 고객들을 더욱 더 감동시킬 것입니다.</p><p><em>비전을 공유하고 저희와 함께 기술 혁신을 이끌 새로운 팀원을 적극적으로 찾고 있습니다. 쿠팡의 </em><a href=\"https://bit.ly/3bw6NjT\"><em>채용 웹사이트</em></a><em>를 확인해보세요!</em></p><p>본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 <a href=\"https://ir.aboutcoupang.com/English/home/default.aspx\">ir.aboutcoupang.com</a> 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a59006bc4b6e\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EB%B0%B0%EC%86%A1-%EA%B3%B5%EA%B0%84-%EC%83%89%EC%9D%B8-%EA%B8%B0%EB%B0%98%EC%9D%98-%EB%B0%B0%EC%86%A1-%EC%98%81%EC%97%AD-%EA%B4%80%EB%A6%AC-%EC%8B%9C%EC%8A%A4%ED%85%9C-a59006bc4b6e\">쿠팡 로켓배송: 공간 색인 기반의 새로운 배송 영역 관리 시스템</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-04-18T04:51:37.000Z",
    "url": "https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EB%B0%B0%EC%86%A1-%EA%B3%B5%EA%B0%84-%EC%83%89%EC%9D%B8-%EA%B8%B0%EB%B0%98%EC%9D%98-%EB%B0%B0%EC%86%A1-%EC%98%81%EC%97%AD-%EA%B4%80%EB%A6%AC-%EC%8B%9C%EC%8A%A4%ED%85%9C-a59006bc4b6e?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "Coupang Rocket Delivery’s spatial index-based delivery management system",
    "partialText": "<h4>Our initiative to visualize delivery areas for an optimized and intuitive delivery system</h4><p><em>By </em><a href=\"https://www.linkedin.com/in/jinonearth/\"><em>Geo J Son</em></a><em> &amp; </em><a href=\"https://www.linkedin.com/in/%ED%95%9C%EC%83%98-%EC%A0%84-a5a21994/\"><em>Sam HS Jeon</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HXH-fv_TNeDAcCvGWXSGgw.jpeg\" /></figure><blockquote>This post is also available in <a href=\"https://medium.com/coupang-engineering/쿠팡-로켓배송-공간-색인-기반의-배송-영역-관리-시스템-a59006bc4b6e\"><strong>Korean</strong></a>.</blockquote><p>Coupang’s <a href=\"https://www.youtube.com/watch?v=0nTc6M8asao\"><strong>Rocket Delivery</strong></a> service is the most crucial and attractive feature to WOW the customers. This is achieved by the combined efforts of our delivery drivers, referred to as Coupang Friends (CPFs), and the optimized delivery system.</p><p>To enable CPFs to deliver the parcels in the most efficient way possible, the delivery system must have a correct understanding of the delivery address and then direct the drive to the best delivery area for the delivery address. Also, to evenly assign and distribute parcels to each area for optimization, experienced camp workers must be able to frequently update the area information.</p><p>Our existing system was dependent on text-based addresses and postal codes, meaning that the system was not as intuitive, functional, or utilitarian as we needed for our complex operations.</p><p>Therefore, we started a project in 2021 to provide the foundation for visualizing the delivery areas on the map, enabling direct modification to the map, and providing further metrics and statistics for optimization.</p><h4>Table of Contents</h4><blockquote>· <a href=\"#9f67\">Challenge: Text-based postal code</a><br>· <a href=\"#7fc4\">What did we need?</a><br>· <a href=\"#1a50\">Adopting H3 into delivery area management system</a><br> ∘ <a href=\"#0225\">Geospatial indexing systems</a><br> ∘ <a href=\"#fc24\">Spatial indexing systems</a><br>· <a href=\"#1e61\">Redesigning the system with new techniques</a><br> ∘ <a href=\"#13c9\">Grid resolution</a><br> ∘ <a href=\"#6277\">Data management</a><br> ∘ <a href=\"#d1f4\">From a hexagon group to a hexagonized polygon</a><br>· <a href=\"#6ee1\">Applying new techniques to the system</a><br>· <a href=\"#2bfc\">What’s next?</a></blockquote><h3><strong>Challenge: Text-based postal code</strong></h3><p>Postal codes are one of the most well-organized systems guaranteed by the government and authorities that assign a code to a specific delivery area. The physical areas of postal codes vary in terms of geometric element and delivery demands, but the efficiency of deliveries they provide is highly optimized. That is why Rocket Delivery adopted the use of postal codes in the beginning as the basic unit for assigning delivery areas. It worked for us at first, but the growth of Coupang demanded much more. The number of deliveries that needed to be made each day per postal code increased from double digits to triple digits. That was too much for a single CPF to cover in one day.</p><p>Therefore, we needed to segment a single postal code and break it down into multiple processable areas. For segmentation, for example, we started by perceiving the postal area as multiple apartment complexes. Then, we segmented that into a single complex, and then further into individual buildings. However, because addresses were solely in text with no or little spatial information, only experienced camp leaders and CPF group leaders were able to manage segmentation since they were familiar with the characteristics of the delivery areas.</p><h3><strong>What did we need?</strong></h3><ol><li>A more efficient and easier way to visualize delivery areas on a map.</li><li>A way to allow all related personnel to easily view, create, and modify delivery areas on the map and share them with others.</li><li>Including and utilizing consistent spatial data of delivery areas, not just postal codes and text addresses that require to be updated when buildings were newly built or demolished.</li></ol><h3><strong>Adopting H3 into delivery area management system</strong></h3><p>To address our needs, we started an initiative. We used a hexagon grid-based spatial indexing system that provides an intuitive way to manage spatial areas by drawing polygons on a map.</p><p>Drawing polygons on a map might seem simple, but it is difficult to meticulously cover all areas by clear and definite boundaries without duplicating or omitting areas. To manage mutually exclusive and collectively exhaustive (MECE) polygon sets, we started by defining the spatial units.</p><h4><strong>Geospatial indexing systems</strong></h4><p>We first reviewed various domain geospatial indexing systems created by GIS professionals. A geospatial indexing system is a set of grids which fully cover the specified geographic surface with unique identifiers. Postal codes and administrative divisions are good examples of irregular-shaped grids, and flexibly editable by authorities. For an analysis of integrated time series, mutable spatial boundaries, of which identifiers do not change for areas that they are assigned to, have more advantages.</p><h4><strong>Spatial indexing systems</strong></h4><p><a href=\"https://what3words.com/\"><strong>W3W (What3Words)</strong></a> is one of the most popular spatial indexing systems, which splits the world into 3-meter square grids and gives each grid a unique combination of three words. Latitude-longitude coordination system has difficulties in delivering on verbal messages as it is made up of a long sequence of numbers, while W3W can place the location with the combination of only unique 3 words. For example, <a href=\"https://www.coupang.jobs/en/locations/seoul/\">Rocket Laboratory</a>, one of the Coupang offices, is located at [<strong>37.503819, 127.0481493</strong>] in latitude-longitude coordinates and the W3W is [<a href=\"https://what3words.com/scoring.eager.patch\"><strong>scoring.eager.patch</strong></a>]. Unfortunately, the size of the resolution cannot be adjusted for analysis.</p><figure><img alt=\"Coupang Rocket Laboratory’s W3W address\" src=\"https://cdn-images-1.medium.com/max/484/1*lrtZxW5BP6ON5y8M2CCUTQ.png\" /><figcaption><strong>Figure 1</strong>. Example of W3W</figcaption></figure><p><a href=\"https://s2geometry.io/\"><strong>Google’s S2</strong></a><strong> </strong>and<strong> </strong><a href=\"https://h3geo.org/\"><strong>Uber’s H3</strong></a> are popular immutable spatial indexing systems with parent-child hierarchies. N number of child grids are mapped to a single parent grid by 1:N relation. Child grids can be accumulated into their parent, and in reverse, parent grid can be divided into child grids. The level of grid size varies and can be flexibly selected according to the purpose and scale. For our project, we adopted H3 for benefits in scalability and implementation.</p><figure><img alt=\"Specification comparison of Google’s S2 and Uber’s H3\" src=\"https://cdn-images-1.medium.com/max/1024/1*e-IH2DaNqMIQcLjmYvFTYA.jpeg\" /><figcaption><strong>Table 1</strong>. Comparison of S2 and H3 (Source: <a href=\"https://www.uber.com/blog/h3/\">Uber blog</a>)</figcaption></figure><ul><li><strong>Distortion:</strong> As shown in the table above, the biggest difference between S2 and H3 is the shape of the basic grid. H3 is made up of hexagon grids, which have minimal distortion in most spaces. S2 has significant errors when projecting the Earth from a sphere to a cubic square. Therefore, even grids of the same level can vary in terms of size depending on the distance from the center of projection. On the other hand, H3 is similar to the <a href=\"https://en.wikipedia.org/wiki/Fullerene\">fullerene</a> or soccer ball shape, of which the projection center point exists for each hexagon. This helps to minimize distortion.</li><li><strong>Neighborhood:</strong> H3 can convert the same area into a fewer number of grids. When representing an area, S2 divides it into much smaller squares — strictly, diamond squares. Whereas H3 is hexagonal, the surface extent of the neighboring grids is larger, which divides areas into relatively coarse sections.</li><li><strong>Hierarchy &amp; Compactness:</strong> When using H3, note that parent grids do not physically cover all child grids, and there are exceptional pentagonal grids instead of hexagons, mainly in the sea, which has no delivery address.</li></ul><h3><strong>Redesigning the system with new techniques</strong></h3><h4><strong>Grid resolution</strong></h4><p>How can we accurately express delivery areas in hexagon grids for Coupang, having in mind that we need to cover all areas of the delivery service? The size of the hexagon needs to be optimal as we cover the entire area within South Korea. If the size is too big, multiple buildings or addresses can be assigned to the same single hexagon. If the size is too small, a single building or address can be assigned to the different multiple hexagons.</p><p>In Figure 2 below, the image on the left shows a 12-level hexagon example with a resolution of about 300-square meters. However, the size of the hexagon in the image cannot distinguish <a href=\"https://en.wikipedia.org/wiki/City_block\">blocks</a> which are divided by road and neighboring buildings. It is critical to segment a block into delivery areas and make it mutually exclusive to other blocks surrounding it. From this perspective, we adopted a 14-level hexagon with a resolution of 6.3-square meters. The resolution means that it can replace the latitude and longitude coordinates of precise locations like entrance and mailboxes.</p><figure><img alt=\"Comparison of 12-level hexagon (left) and 14-level hexagon (right) grids, using grid resolution on Coupang Rocket Delivery map\" src=\"https://cdn-images-1.medium.com/max/1024/1*KMo1GFvRIUp3epfrnBJy7A.png\" /><figcaption><strong>Figure 2</strong>. Comparison of 12-level hexagon (left) and 14-level hexagon (right) grids</figcaption></figure><h4><strong>Data management</strong></h4><p>Once we have defined the size of the hexagon for our service, we then needed to define how to manage data with scalability. South Korea can be divided into approximately 432 million hexagons in 12-level resolution. A parent hexagon has seven child hexagons, so a 13-level resolution would be converted to 3.1 billion hexagons and a 14-level resolution to 21.7 billion hexagons. We needed a way to manage data while ensuring sufficient scalability.</p><p>One way to manage data is to use RDBMS. We can use a hexagon ID as a key and put data into it. However, for general usage RDBMS restricts <strong>read/write </strong>function from 0.1 to 1 billion counts of table rows. If we manage the IDs row-by-row, we may be able to control the entire area of Korea in 12-level resolution, but it will be impossible to do so in 14-level without dividing the tables according to regions or adopting other standards. And even if we do somehow manage data by database sharing, we would not be able to avoid increasing the storage for data once we apply the service to more regions outside Korea. We concluded that it is not suitable for our use case to put specific data of which keys are individual hexagon IDs into RDBMS.</p><p>Therefore, we designed each delivery area to have information of hexagons. This means that a single large area can have a large number of 14-level hexagons, encountering potential memory or data size problems. To avoid this, we considered using the <a href=\"https://h3geo.org/docs/api/hierarchy/\">compaction functions</a> provided by H3. It allows to convert between resolutions of child hexagons (finer) and their parent hexagon (coarser) when all child hexagons exist. We can also reduce the amount of memory for whole records.</p><figure><img alt=\"Overview of how Coupang Rocket Delivery map compacts H3 hexagons\" src=\"https://cdn-images-1.medium.com/max/1024/1*j_HtbkISZ1c0HyZcwzUVgQ.png\" /><figcaption><strong>Figure 3</strong>. Overview of how to compact H3 hexagons</figcaption></figure><p>This design can store delivery areas efficiently and represents as many areas as required in the 14-level resolution. There are no regional restrictions or scalability issues. As shown in Figure 4 below, in order to search an area for a specific latitude-longitude coordinate as [<strong>34.111, 127.111]</strong>, we simply convert the coordinate to an ID of a hexagon grid with 14-level resolution as <strong>85283473fffffff</strong> and look up which hexagon group contains this hexagon ID. The inclusion relationship between hexagon ID and hexagon groups can be easily analyzed even in the environment that is not installed with <a href=\"https://postgis.net/\">PostGIS</a> or other geodatabase modules.</p><figure><img alt=\"Overview of how Coupang Rocket Delivery map looks up hexagon groups including a specific hexagon ID converted from a coordinate value\" src=\"https://cdn-images-1.medium.com/max/1024/1*F2tBKbCIVpVGgbwsigljqw.png\" /><figcaption><strong>Figure 4</strong>. Overview of how to look up hexagon groups including a specific hexagon ID converted from a coordinate value</figcaption></figure><h4><strong>From a hexagon group to a hexagonized polygon</strong></h4><p>Once we completed the first phase of the system, we found some areas to be improved. For example, there were limitations with using hexagon ID lists. It was difficult to perform complex spatial analysis or find certain delivery areas that are distant from a specific point. To improve this, we kept the basic unit of the areas as hexagon grids but stored the actual data in a polygon format. Whatever polygon you draw, you can convert it to 14-level hexagon by the <a href=\"https://h3geo.org/docs/api/regions\">polyfill</a> function and then store the boundaries back into polygon. This resembles drawing a pixel art. Convert a vectorized .ai file to a .jpg bitmap file, and segmentize it back into a vectorized image again.</p><p>Hexagonized polygons also allow easy geosearching and managing delivery areas without overlapping or omitted areas. As shown in Figure 5 below, when a client draws a green square polygon on a map UI, the server converts it to a group of hexagons, and then generates only the outer boundary back into the polygon as a red hexagonized polygon. Therefore, a single area can be displayed with both user-drawn green polygon and the red hexagonized polygon.</p><p>The only one flaw of this method is, if the client side creates highly complex polygons, it may take a little longer to convert them into hexagons. We resolved to first store the polygon as drawn by users, and then later asynchronously generate and utilize the hexagonized polygon from it.</p><figure><img alt=\"Process of how Coupang Rocket Delivery map converts a user-drawn square polygon into a hexagonized polygon\" src=\"https://cdn-images-1.medium.com/max/977/1*OCXQ95SlwWvNdWRrxh8IpQ.png\" /></figure><figure><img alt=\"Overview of how Coupang Rocket Delivery map handles a user-drawn square polygon and converts it to a hexagonized polygon\" src=\"https://cdn-images-1.medium.com/max/1024/1*J39jV91lg5FTlROBZeogSg.png\" /><figcaption><strong>Figure 5.</strong> How a user-drawn square polygon is converted to a hexagonized polygon</figcaption></figure><h3><strong>Applying new techniques to the system</strong></h3><p>We have covered the concept of how users can generate and manage hexagon groups. This section introduces how to apply these techniques to the actual delivery area management system. As mentioned in the introduction, Rocket Delivery service considers the postal code as the basic unit of delivery areas, and subdivisions can be applied if necessary. Therefore, in the actual system, when the postal code is selected from the list, the spatial extent is displayed on the map.</p><p>As shown in Figure 6 below, users can freely draw polygons within the postal code area and add area description together in the actual system. When the <strong>Save </strong>button is pressed, the area data and polygon drawn by the user are transmitted to the server and converted into hexagonized polygons.</p><figure><img alt=\"Example of Coupang’s delivery area management system\" src=\"https://cdn-images-1.medium.com/max/1024/1*pe-GLprJjJbWj98CY0xq4g.png\" /><figcaption><strong>Figure 6.</strong> Coupang’s delivery area management system</figcaption></figure><p>For example, the number 06961, a post code in Seoul, is selected and displayed on the map, which is then divided into two areas of hexagon groups. Both areas are individual apartment complexes located within this postal code. Each area is drawn with a drawing tool provided at the top of the map section, and the conversion to hexagonized polygon is automatically carried out. If you focus on the border of each area closely, you will see that the border is cut into hexagonal shapes and not a smooth straight line.</p><p>On the right side of Figure 6, the <strong>Edit </strong>button is pressed to make corrections to the area outlined in blue. The area outlined in yellow is still represented by hexagonized polygon, while the area outlined in blue is redrawn by smooth lines with user-drawn polygons. When the modification is completed, the modified polygon is processed by the server and updated to the new hexagonized polygon.</p><h3><strong>What’s next?</strong></h3><p>This post introduced how we designed our system to manage the delivery area efficiently by maintaining consistency with geodatabase. The new system is an improvement of our old system to provide better experiences for users, and it can still be used with the legacy system. Also, we have new further opportunities to improve the system as we utilize GIS-based spatial data as we have converted all textual addresses of delivery areas into spatial data.</p><p>The status and metrics of each delivery can be analyzed for each hexagon grid, which can produce spatially absolute statistics that are no longer bound to postal codes or addresses. Even if there is a change in the delivery area for some reason, the metrics can be compared on the same basis as before. Delivery areas can be accurately evaluated, and the less efficient delivery area can be identified quickly. This leads our team to be able to deep-dive the slowing factors and find the countermeasures to solve the problem.</p><p>Furthermore, we can analyze the conditions of delivery areas, from rating delivery difficulties, measuring distances to moving time between delivery areas and so on. These achievements lead to 1) evaluating the difficulty of the delivery area more precisely, 2) managing the delivery area dynamically according to the volume of the shipment, and 3) recommending the optimized trip routes for CPFs to deliver safer and faster.</p><p>Some were skeptical about our Rocket Delivery service when it was first released, as it seemed impossible<em> </em>to guarantee overnight delivery of goods. But now, it is a new standard of the Korean online shopping culture. Coupang will continue to develop the delivery system through technological innovation to continue to wow the customer.</p><p><em>We’re actively looking for new teammates who share our vision and will accomplish great things together. Interested? Check out our </em><a href=\"https://bit.ly/3Q7AKWS\"><em>career website</em></a><em>.</em></p><p><em>While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on </em><a href=\"http://ir.aboutcoupang.com/\"><em>ir.aboutcoupang.com</em></a><em> for information on our formal investment plans and product development strategies.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=26940eaaee63\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63\">Coupang Rocket Delivery’s spatial index-based delivery management system</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-04-17T00:10:08.000Z",
    "url": "https://medium.com/coupang-engineering/coupang-rocket-deliverys-spatial-index-based-delivery-management-system-26940eaaee63?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "클라우드 서비스 사용량 관리를 통한 운영 비용 최적화",
    "partialText": "<h4>쿠팡 엔지니어링 조직들이 클라우드 비용을 줄이기 위해 들인 노력과 그 결과에 대하여</h4><p><em>By </em><a href=\"https://www.linkedin.com/in/luketravers\"><em>Luke Travers</em></a><em> &amp; </em><a href=\"https://www.linkedin.com/in/amitarora58\"><em>Amit Arora</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*9Z5qFMsAjOUCQowytGnyCA.jpeg\" /></figure><blockquote>본 포스트는 <a href=\"https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b\"><strong>영문</strong></a>으로도 제공됩니다.</blockquote><p>쿠팡의 파이낸스 및 엔지니어링 팀들은 지난 몇 분기 동안 로드맵을 바탕으로 서로 협력하며 클라우드 서비스에 지출되는 온디맨드(on-demand) 비용을 관리하고 최적화해왔습니다. 여러 엔지니어링 조직들이 하나의 팀이 되어 노력했기에 가능한 일이었습니다.</p><p>하나의 팀으로서 저희는 다음과 같은 세 가지 핵심 원칙 하에 비용 최적화 작업을 수행했고, 이 포스트를 통해 어떤 작업이 이루어졌는지 공유드리고자 합니다.</p><ul><li>예산 할당 및 준수</li><li>목표 절감액</li><li>쿠팡 리더십 원칙인<strong> Hate Waste</strong>에 기반해 서비스 신뢰성, 지속 가능성 및 서비스 사용량 통제 부분에 집중</li></ul><h4>목차</h4><blockquote>· <a href=\"#421f\">배경 및 과제</a><br>· <a href=\"#cf9e\">1 단계: 최적화 프로젝트 팀 구성하기</a><br>· <a href=\"#4a2d\">2 단계: 더 적게 쓰고 더 적게 지불하기</a><br> ∘ <a href=\"#2642\">인스턴스 세대 조정</a><br> ∘ <a href=\"#5843\">EMR</a><br> ∘ <a href=\"#56a5\">스토리지</a><br>· <a href=\"#97ba\">결론</a></blockquote><h3>배경 및 과제</h3><p>비용 최적화 작업을 시작할 당시 쿠팡은 다음과 같은 상황에 처해있었습니다.</p><ol><li>클라우드 서비스를 효율적으로 사용하는 방법에 대한 엔지니어링 팀들은 이해도가 그리 높지 않은 상태였습니다. 그로 인해 필요 이상으로 서비스를 더 많이 사용하면서 불필요한 비용을 지출하고 있었습니다.</li><li>파이낸스 팀들은 어떤 팀이 클라우드 서비스 사용료를 지출하고 있는지, 그리고 성장하는 비즈니스에 영향을 주지 않으면서도 지출을 억제할 수 있는 방법이 무엇인지 이해하고자 고군분투 중이었습니다.</li><li>리더십 팀은 클라우드 서비스 비용 지출에 대해 충분히 분석하고 있지 못했습니다.</li></ol><p>이에 리더십 팀은 클라우드 서비스의 가변 비용 모델에 대한 재정적 책임을 명확히 해야 한다는 판단을 내렸습니다. 리더십 팀의 지원 하에 클라우드와 관련해 전문성을 지닌 동료들이 엔지니어링 팀들에 합류하게 되었고, 모두 다 함께 클라우드 서비스를 효율적으로 사용하면서 동시에 비용도 절감할 수 있는 방법을 찾기 시작했습니다.</p><h3>1 단계: 최적화 프로젝트 팀 구성하기</h3><p>클라우드 인프라 엔지니어와 테크니컬 프로그램 관리자(Technical Program Manager, TPM)를 주축으로 한 최적화 프로젝트 팀이 만들어졌습니다. 팀 구성원들은 서로 협력하며 클라우드 서비스 비용을 효율적으로 지출할 수 있는 방법을 찾아냈습니다.</p><p>프로젝트 팀은 각 도메인 팀과의 협업을 통해, 클라우드 서비스를 소유하고 사용할 때에는 서비스의 가변 비용 모델을 활용해야 한다는 점을 도메인 팀이 이해할 수 있도록 도왔습니다. 예를 들어, 저희는 Amazon S3에 저장된 데이터를 이해하고 유휴 상태(at rest)에서 스토리지 구조를 최적화할 수 있도록 도메인 팀과 협업했습니다. 또한 AWS Spot Instances 및 ARM 기반 AWS Graviton과 같은 서비스를 활용해 데이터 저장 및 처리 비용을 획기적으로 줄일 수 있는 방법을 찾아냈습니다. 그리고 프로젝트 팀은 정확한 분석이 제공되는 환경을 만들었고, 이에 도메인 팀들이 데이터에 기반해 의사 결정을 내릴 수 있게 되면서 클라우드 서비스를 더 잘 활용할 수 있게 되었습니다.</p><p>프로젝트 팀은 도메인 팀들 전체가 클라우드 서비스의 효율적인 사용을 하나의 엔지니어링 문화로 받아들일 수 있게끔, 팀들이 필요로 하는 애널리틱스(analytics), 도구 및 프로세스를 만들어 제공했습니다. 가령 Amazon Athena를 통해 처리된 Amazon CloudWatch 데이터를 사용해 맞춤형 대시보드를 개발했고 AWS CUR(Cost &amp; Usage Reports) 데이터 처리를 위해 저희의 BI(Business Intelligence) 대시보드도 활용했습니다. 다른 한편으로는 파이낸스 팀이 저희와 함께 도메인 팀들을 지원하였고, 그 과정에서 도메인 팀들은 팀에 할당된 월별 및 분기별 예산을 관리하는 것이 얼마나 중요한지를 이해하게 되었습니다.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M12QIbXip5i44vJKSPL1tA.jpeg\" /></figure><h3>2 단계: 더 적게 쓰고 더 적게 지불하기</h3><p>프로젝트 팀은 도메인 팀들이 필요로 하는 애널리틱스와 도구들을 개발할 때 다음과 같이 서로 연관되어 있지만 대조되는 두 가지 방법에 중점을 두고 필요한 작업들을 진행했습니다.</p><ol><li><strong>더 적게 쓰기(사용량을 줄여 비용 절감):</strong> 비-프로덕션(non-prod) 환경에서 필요 시에만 AWS 리소스가 자동 시작되게끔 하였습니다. 이를 통해 쿠팡은 비-프로덕션 환경에서 25%의 비용을 절감할 수 있었습니다.</li><li><strong>더 적게 지불하기(조정을 통해 사용량을 적정 수준으로 줄이기):</strong> 관련 데이터를 분석해 파악한 클라우드 서비스 사용 패턴을 바탕으로 프로젝트 팀은 쿠팡 내 전체 도메인 팀들과 긴밀히 협력하며 사용되지 않는 EC2 리소스를 수동으로 제거했습니다.</li></ol><p>사용량 최적화 및 비용 절감을 주요 목표로 잡고, 다음과 같은 방법들을 통해 AWS 클라우드 서비스의 2021년 사용료를 수백만 달러(온디맨드 비용) 이상 절감했습니다.</p><p>저희가 채택한 최적화 기술은 비용 절감에 도움이 되었을 뿐만 아니라 클라우드 리소스를 더 효율적으로 활용할 수 있게 해주었습니다. AWS의 모범 사례(best practices) 및 권장 사항을 참고해 다음과 같은 작업들을 수행했습니다.</p><h4><strong>인스턴스 세대 조정</strong></h4><p>저희는 성능 향상, 비용 절감 및 가용성 향상을 위해 쿠팡의 모든 단일 인스턴스를 최신 세대로 조정하고 싶었습니다. 이를 위해서는 AMD 및 ARM과 같은 다양한 칩 아키텍처를 탐색할 뿐만 아니라 각각의 모든 인스턴스 유형을 테스트해야 했고 도메인 팀들과의 광범위한 협업이 필요했습니다. 힘든 테스트 과정을 거쳐 저희는 내부에서 사용되는 제품 전체를 AMD CPU 기반 클라우드 서비스로 성공적으로 이전할 수 있었고, 이전 버전에 비해 가격 대비 성능을 20% 더 향상시킬 수 있었습니다.</p><h4><strong>EMR</strong></h4><p>저희는 EMR에 Spot Instances를 사용하는 것을 선호하지만, 잘 알려져 있듯이 피크 시간대에 Spot을 사용하면 원하는 만큼의 용량을 확보하기 어려울 수도 있습니다. 복잡하게 통합되어 있는 EMR 시스템으로 서비스 중단 없이 툴체인(toolchain) 각 부분의 업데이트를 확실히 보장해야만 했습니다. 이를 위해 소프트웨어 버전을 업그레이드하여 AWS EMR의 <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html\">인스턴스 플릿 기능</a>을 활용하였고, 이 업그레이드를 통해 전체 EMR 비용을 25% 절감할 수 있었습니다.</p><h4><strong>스토리지</strong></h4><p>EBS 및 S3의 AWS 스토리지 비용을 다음과 같이 절감할 수 있었습니다.</p><ul><li>Amazon EBS: 스토리지의 경우 차세대 EBS인 GP3가 저희의 요구 사항들을 앞으로도 계속 충족시키면서 동시에 내부 구성원들의 신뢰도 얻을 수 있을 것이라는 판단 하에, GP3에 대한 광범위한 테스트를 수행했습니다. 내부 구성원들의 신뢰를 얻기 위해, 다양한 도구들로 폭넓은 성능 테스트를 수행했습니다. 먼저 개발용 계정에서 모든 테스트를 수행한 결과, 별다른 영향 없이 한 번에 500–1000개의 라이브 볼륨을 수월하게 마이그레이션할 수 있음을 확인했습니다.</li><li>Amazon S3: 50 페타바이트(PB) 이상을 Intelligent-Tiering(IT)으로 옮겼습니다. 그 과정에서 저희는 모든 워크로드가 IT와 원활하게 작동하는 것은 아니며 개체 크기에 매우 주의해야 한다는 비싼 교훈을 얻었습니다. 평균 크기가 매우 작은 오브젝트들(object)이 수십억 개가 있는 경우, 해당 워크로드에 대한 전체 S3 비용은 크게 증가할 수 있습니다. 이 경우 관련 정책을 조정하려면 S3 수명 주기 필터(lifecycle filter)를 사용해야 합니다. S3의 복잡한 과금 패턴과 문제를 일으키지 않으려면, 이를 ‘일회성으로 수행되는(one and done)’ 프로세스(process)가 아니라 ‘계속 이어지는(ongoing)’ 주기(cycle)로 이해하고 긴 시간 동안 많은 주의를 기울여야 합니다.</li></ul><figure><img alt=\"클라우드 사용 비용을 최적화하기 위한 쿠팡의 노력으로, Amazon S3의 사용량이 증가함에도 AWS 사용 금액은 절감됨\" src=\"https://cdn-images-1.medium.com/max/1024/1*eKqrpaDIcrlhPe0mVp7WOQ.png\" /><figcaption>그림 1. 증가하는 Amazon S3 사용량 대비 줄어드는 저장 용량 단위당 AWS 비용 추이</figcaption></figure><h3>결론</h3><p>위와 같은 방법들을 통해 저희는 2021년에 수백만 달러의 온디맨드 비용을 절감함으로써 AWS 클라우드 서비스로의 지출을 최소화할 수 있었습니다. 최적화할 영역을 정확히 파악해내고 최적화 달성에 집중하면서 다양한 프로세스를 직접 수행해냈습니다. 저희는 비용 절감과 효율성 향상을 위해 여전히 최적화가 필요한 영역을 최선을 다해 찾고 있습니다.</p><p>클라우드 서비스 비용을 수백만 달러 절약했지만 아직 저희의 여정은 끝나지 않았습니다. 다음 단계로 보다 복잡한 분석 도구들에 대한 투자를 늘려 클라우드 핀옵스(Cloud FinOps) 사고방식이 쿠팡에 자리 잡을 수 있도록 노력할 것입니다. 또한 클라우드 서비스 비용의 최적화에 필요한 모니터링 및 분석 프로세스들 중 일부를 자동화할 예정입니다.</p><p><em>클라우드 최적화와 비용 효율화에 대해 깊이 이해하고 열정을 가지고 계시다면, 쿠팡 </em><a href=\"https://bit.ly/3bw6NjT\"><em>채용 공고</em></a><em>를 확인해 보세요!</em></p><p>본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 <a href=\"https://ir.aboutcoupang.com/English/home/default.aspx\">ir.aboutcoupang.com</a> 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1521565c64ec\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%82%AC%EC%9A%A9%EB%9F%89-%EA%B4%80%EB%A6%AC%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9A%B4%EC%98%81-%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%A0%81%ED%99%94-1521565c64ec\">클라우드 서비스 사용량 관리를 통한 운영 비용 최적화</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-03-27T01:40:09.000Z",
    "url": "https://medium.com/coupang-engineering/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%82%AC%EC%9A%A9%EB%9F%89-%EA%B4%80%EB%A6%AC%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9A%B4%EC%98%81-%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%A0%81%ED%99%94-1521565c64ec?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "Cloud expenditure optimization for cost efficiency",
    "partialText": "<h4>The roadmap and execution for cutting down cloud spending across Coupang engineering organizations</h4><p><em>By </em><a href=\"https://www.linkedin.com/in/luketravers\"><em>Luke Travers</em></a><em> &amp; </em><a href=\"https://www.linkedin.com/in/amitarora58\"><em>Amit Arora</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*9Z5qFMsAjOUCQowytGnyCA.jpeg\" /></figure><blockquote>This post is also available in <a href=\"https://medium.com/coupang-engineering/클라우드-서비스-사용량-관리를-통한-운영-비용-최적화-1521565c64ec\"><strong>Korean</strong></a>.</blockquote><p>In this post, we share how the finance and engineering teams at Coupang have partnered together over the past few quarters to provide a roadmap to manage and optimize cloud expenditure. We will also detail how multiple engineering teams formed a Central team to further optimize the cloud spending for on-demand cost.</p><p>The Central team’s efforts were narrowed down to the following three principles:</p><ul><li>Budget allocation and its compliance</li><li>Savings as the goal</li><li>Focus areas of credibility, sustainability, and control in line with the company’s leadership principle of “<strong>Hate Waste</strong>”</li></ul><h4>Table of Contents</h4><blockquote>· <a href=\"#2d73\">Background &amp; Challenges</a><br>· <a href=\"#dc48\">Stage 1: Forming a Central team</a><br>· <a href=\"#8d88\">Stage 2: Spending Less &amp; Paying Less</a><br> ∘ <a href=\"#9946\">Instance generation alignment</a><br> ∘ <a href=\"#2e5f\">EMR</a><br> ∘ <a href=\"#3ad5\">Storage</a><br>· <a href=\"#f7a1\">Conclusion</a></blockquote><h3>Background &amp; Challenges</h3><p>As a company we were in a classic situation where:</p><ol><li>engineering teams were spending more than they needed on cloud, with little understanding of cloud efficiency.</li><li>finance teams were struggling to understand what teams were spending on, and how to curb expenditures without impacting business growth.</li><li>the leadership team did not have enough analytics into cloud spending.</li></ol><p>To bring financial accountability to the variable spending models of cloud, the leadership team aided the engineering teams by engaging the right people to find the opportunities for efficiency and cost savings.</p><h3>Stage 1: Forming a Central team</h3><p>Our cloud infrastructure engineers and technical program managers collaborated as a Central team to identify a few initiatives for cloud spending efficiency.</p><p>The Central team collaborated with each domain team and helped them understand that while they are the owners of cloud usage, they must also take advantage of the cloud’s variable cost models. For instance, we helped one of our domain teams to understand their data stored in Amazon S3 and how the storage structure could be optimized at rest. Also, we shed light on how we could use tools such as AWS Spot Instances and ARM-based AWS Graviton, resulting in the dramatic cost reduction on storing and processing data. The Central team made sure that the right analytics was available, helping teams to take data-driven decisions based on the value of cloud.</p><p>The Central team understood the importance of the right analytics, tools, and processes to derive cloud efficiency as a culture across the domain teams. In that sense, we created custom dashboards using Amazon CloudWatch data processed through Amazon Athena, and we also utilized our BI (Business Intelligence) dashboards for processing AWS CUR (Cost &amp; Usage Reports) data. The finance team also partnered with us from the other end and helped us to push forward the importance of managing the domain teams to their assigned monthly and quarterly budgets.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M12QIbXip5i44vJKSPL1tA.jpeg\" /></figure><h3>Stage 2: Spending Less &amp; Paying Less</h3><p>The Central team equipped with the right analytics and tools focused on optimizing in the following two interrelated yet contrasting methods:</p><ol><li><strong>Spending Less (Expenditure Reduction by Using Less):</strong> Automating the launch of AWS resources on non-production environment on a need basis. This helped the company save 25% in costs on non-prod environments.</li><li><strong>Paying Less (Usage Reduction by Rightsizing):</strong> With the right data to analyze the usage patterns, the Central team worked closely with the domain teams across the company to manually eliminate unutilized and underutilized EC2 resources.</li></ol><p>With usage optimization and cost savings as our main goals, the following initiatives helped save millions of dollars (On-Demand cost) in 2021 on AWS Cloud.</p><p>The optimization techniques we adopted not only helped us save in costs, but also unlocked more efficient cloud resources. Based on the best practices and recommendations from AWS, we implemented the following initiatives.</p><h4>Instance generation alignment</h4><p>We wanted to bring every single instance in Coupang up to the current generation for improved performance, lower cost, and higher availability. This required extensive collaboration with the domain teams to test on each and every instance type for us, as well as exploring different chip architecture such as AMD and ARM. After this arduous testing process, we successfully moved entire families of internal products onto AMD CPUs, gaining 20% in better price performance in comparison to the older versions.</p><h4><strong>EMR</strong></h4><p>We love using Spot Instances for EMR, but as we all know, it can be difficult to get ideal capacity at peak times using Spot. With our intricate and integrated EMR systems, we had to carefully ensure that each part of our toolchain was updated without causing an interruption to our service. Therefore, we had to upgrade our software versions to take advantage of the <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html\">instance fleets feature</a> of AWS EMRs. This upgrade helped us to get 25% cost reduction on total EMR costs.</p><h4><strong>Storage</strong></h4><p>In this section, we discuss how we managed to cut our AWS storage costs for EBS and S3.</p><ul><li><strong>Amazon EBS: </strong>For storage we found that there was extensive testing required to gain internal customer trust that the newer generation of EBS, GP3 would continue to meet our needs. To gain this trust, extensive performance testing was conducted using various tools. With all tests done in our development account first, we found that we could comfortably migrate 500–1000 live volumes at a time in parallel without any tangible impact.</li><li><strong>Amazon S3: </strong>We moved 50+PB to Intelligent-Tiering (IT). During the process, we learned the hard way that not all workloads work well with IT, and you need to be very careful with object size. If the average object size is too low and you have multiple billions of objects, you can end up drastically increasing your overall S3 costs for that workload. In that case, the usage of S3 lifecycle filters is required to tune the policy. This is not a ‘one and done’ process but an ongoing cycle that requires extensive time, care and attention to not fall afoul of S3’s complex billing patterns.</li></ul><figure><img alt=\"Result of Coupang’s effort to optimize the cloud expenditure shows the reduced AWS cost against the increasing amount of Amazon S3 usage\" src=\"https://cdn-images-1.medium.com/max/1024/1*szPiCc1YDKeK_E1EkpLzFg.png\" /><figcaption>Figure 1. Crossing trend of increasing Amazon S3 usage versus decreasing AWS cost per size</figcaption></figure><h3>Conclusion</h3><p>By adopting the methods above, we were able to minimize our AWS costs by millions of dollars (On-Demand) in 2021. A lot of the process was manually done, focusing on identifying the right optimization areas and achieving them. We are still working hard to identify additional areas for cost savings and improved efficiency.</p><p>Although we managed to save the company millions in cloud costs, we are not done yet. As next steps, we wish to invest in more complex analytic tools to drive the Cloud FinOps mindset at Coupang. Additionally, we will be automating some of the monitoring and analytics processes required for cost optimization in cloud.</p><p><em>If you are a passionate engineer with a deep understanding of cloud optimization and cost efficiency, </em><a href=\"https://bit.ly/3Q7AKWS\"><em>join our talented team</em></a><em>.</em></p><p><em>While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on </em><a href=\"http://ir.aboutcoupang.com/\"><em>ir.aboutcoupang.com</em></a><em> for information on our formal investment plans and product development strategies.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=44e9bea3d91b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b\">Cloud expenditure optimization for cost efficiency</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-03-21T06:03:32.000Z",
    "url": "https://medium.com/coupang-engineering/cloud-expenditure-optimization-for-cost-efficiency-44e9bea3d91b?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "기계 학습 모델을 활용한 물류 입고 프로세스 최적화",
    "partialText": "<h4>쿠팡 풀필먼트 센터로의 제품 입고 시 필요한 운송 트럭의 적정 수량을 데이터에 기반해 예측하기</h4><p><em>By Austin Yang &amp; JY Cho</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b9zku0Izrt8kvjRPLNbLzw.jpeg\" /></figure><blockquote>본 포스트는 <a href=\"https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304\"><strong>영문</strong></a>으로도 제공됩니다.</blockquote><p>쿠팡은 물류 입고 프로세스의 최적화를 위해 끊임없이 노력하고 있습니다. 풀필먼트 센터로의 제품 입고과정에서 낭비되는 자원을 최소화하면, 적기에 제품을 판매하고 더 많은 고객들에게 더 빠르게 배송할 수 있습니다. 이를 위해 쿠팡은 기계 학습을 통해 직매입 제품들의 풀필먼트 센터 입고 프로세스를 효율적으로 개선해 왔습니다. 본문을 통해 어떤 개선이 있었는지를 살펴보겠습니다.</p><h4>목차</h4><blockquote>· <a href=\"#f0aa\">배경 및 과제</a><br>· <a href=\"#dac3\">트럭 수량 예측 모델 학습하기</a><br> ∘ <a href=\"#601a\">특징 추출</a><br> ∘ <a href=\"#11f1\">모델 학습: LightGBM 알고리즘</a><br> ∘ <a href=\"#eba0\">모델 하이퍼 파라미터 탐색 : 베이지안 최적화</a><br> ∘ <a href=\"#6d8f\">입고 예약 시스템과 모델 연계 구성</a><br> ∘ <a href=\"#45d4\">과소 예측과 과대 예측 사이의 트레이드 오프</a><br>· <a href=\"#66f4\">모델 적용 결과</a><br>· <a href=\"#00f9\">향후 계획</a></blockquote><h3>배경 및 과제</h3><p>매일 각지의 수많은 업체들이 트럭에 다양한 종류의 제품을 적재해 쿠팡 풀필먼트 센터로 제품을 반입합니다. 각 풀필먼트 센터에는 트럭을 세워두고 물건을 하역하는 도크(dock)가 있습니다. 센터마다 시간대별로 사용 가능한 도크의 최대 개수는 정해져 있습니다. 제품 하역 작업 시 한 대의 트럭이 하나의 도크를 정해진 시간 동안 사용하게 되는데, 이 시간을 슬롯(slot)이라고 부릅니다.</p><p>정해진 개수의 슬롯들로 여러 업체들이 납품하는 제품들이 효율적으로 입고되려면 각 입고에 필요한 슬롯 개수가 사전에 정확히 예측되어야 합니다. 만약 사전에 예측한 필요 슬롯 개수가 실제 필요 슬롯보다 적을 경우 입고 과정에서 지연이 발생할 수 있으며, 실제 필요 슬롯보다 많은 경우 한정된 자원인 슬롯을 불필요하게 낭비하게 됩니다.</p><figure><img alt=\"벤더에서 쿠팡 풀필먼트 센터로의 납품 과정에서 발생할 수 있는 자원 낭비\" src=\"https://cdn-images-1.medium.com/max/1024/1*O9LCAZ4eN38TbJvqvpy-2Q.png\" /><figcaption>그림 1. 납품 과정에서 발생할 수 있는 자원 낭비</figcaption></figure><p>이에 저희는 업체 특성과 풀필먼트 센터로 입고되는 제품의 특성을 바탕으로, 입고 예약 신청을 하는 업체들에게 적정한 필요 슬롯 개수를 예측해 제공하는 시스템을 개발하기 시작했습니다. 목표는 낭비되는 슬롯의 개수를 줄이고, 슬롯이 부족해 입고가 지연되는 문제를 최소화하는 것이었습니다. 이 목표를 달성하기 위해 저희가 활용한 기술들을 다음 섹션에서 자세히 소개하겠습니다.</p><h3>트럭 수량 예측 모델 학습하기</h3><p>저희는 데이터로 문제를 풀고자 했고 이를 위해:</p><ol><li>그동안 축적해온 물류 데이터 및 입고 신청 정보로부터 트럭 대수에 영향을 미치는 특징들(feature)을 도출하고, 각 물류 입고에 실제 사용된 트럭 대수 데이터를 결합해 학습 데이터를 구성했습니다.</li><li>적정 트럭 입차 수량을 예측하는 머신 러닝(ML) 모델을 학습시켰습니다.</li><li>학습된 모델을 입고 예약 시스템과 연결해 업체가 입고 신청을 하면 시스템으로 바로 적정 트럭 대수를 확인할 수 있게끔 설계했습니다.</li></ol><p>이렇게 저희는 입고 예약 시스템에 새로운 예측 자동화 기능을 더해 프로젝트의 목표였던 슬롯 운영 효율을 달성할 수 있었습니다.</p><h4>특징 추출</h4><p>쿠팡에 축적된 대량의 물류 데이터들로부터 트럭 수량 예측 모델에 적합한 특징들을 찾아내기 위해 탐색적 데이터 분석(EDA: Exploratory Data Analysis) 과정을 거쳤습니다. 하지만 데이터 사이에 숨겨져 있는 의미를 찾아내기 위해서는 도메인 전문가들의 조언이 필요했습니다. 물류 담당자들과 심층 인터뷰를 진행하면서 물류 입고 과정에서 나타나는 의미있는 패턴들을 다양하게 파악할 수 있었으며, 이러한 과정을 통해 트럭 수량 예측에 유용하게 활용할 수 있는 다수의 특징들을 발견하였습니다. 그리고 이렇게 도출된 특징들을 피처 엔지니어링(Feature Engineering)으로 가공해 최종적인 피처 셋(Feature Set)을 확정했습니다.</p><h4>모델 학습: LightGBM 알고리즘</h4><p>약 2년여의 기간 동안 수집된 입고 신청 데이터로부터 약 80만 건의 학습 데이터를 추출했습니다. 데이터 세트의 크기가 작지 않다 보니 빠른 학습과 튜닝이 가능한 알고리즘을 찾게 되었습니다. 또한 확정된 특징들 중 많은 것들이 범주형(categorical) 특징이었습니다. 이러한 특성을 갖는 데이터 세트에서 높은 예측 성능을 보이는 LightGBM 알고리즘을 사용하기로 결정했습니다.</p><p>LightGBM은 트리 기반 부스팅 모델이며, 많은 기계 학습 문제에서 성능을 입증한 알고리즘입니다. 다른 트리 기반 알고리즘들은 트리를 수평적으로 확장하는 level-wise tree growth 방식을 사용하는 반면, LightGBM은 트리를 수직적으로 확장하는 leaf-wise tree growth 방식을 사용해 학습 속도가 빠르다는 것이 특징입니다. Level-wise tree growth 방식은 각 트리 깊이가 모두 확장될 때까지 기다려야 하지만, leaf-wise 방식은 이를 기다리지 않고 가장 손실이 높은 리프 노드를 분할해 나가면서 수직으로 확장합니다. 이러한 방식 덕분에 LightGBM은 다른 알고리즘들보다 빠른 속도로 학습이 수행할 수 있습니다.</p><p>또한 다른 대부분의 알고리즘들과 달리 LightGBM은 범주형 특징에 대해 별도의 원핫 인코딩(one-hot encoding)을 해주지 않아도 됩니다. LightGBM이 <a href=\"https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479\">Fisher</a> 알고리즘을 적용해 데이터의 클래스를 최적으로 분할하는 학습을 수행해 나가기 때문입니다. 이로 인해 LightGBM은 일반적으로 보다 빠른 학습 속도와 함께 높은 예측 성능을 보여줍니다. 검증(validation) 데이터 세트로 다른 주요 트리 기반 알고리즘들과 비교 평가한 결과, LightGBM은 저희가 원하는 학습 속도와 예측 성능을 보여주었습니다.</p><figure><img alt=\"쿠팡 풀필먼트 센터로 인입되는 입고 요청들의 검증(validation) 데이터 셋에 대한 주요 트리(tree) 기반 알고리즘들의 예측 성능 확인 결과\" src=\"https://cdn-images-1.medium.com/max/819/1*8ZgbAZ-e4Fl8KjdtPOmDaA.png\" /><figcaption>그림 2. 주요 트리(tree) 기반 알고리즘들의 검증(validation) 데이터에 대한 예측 성능 확인 결과</figcaption></figure><h4>모델 하이퍼 파라미터 탐색 : 베이지안 최적화</h4><p>모델의 하이퍼 파라미터 탐색은 <a href=\"https://en.wikipedia.org/wiki/Bayesian_optimization\">베이지안 최적화</a>(Bayesian Optimization)를 활용해 자동으로 진행되도록 구성하였습니다. 베이지안 최적화는 목적 함수 <em>f</em>의 함수 값 <em>f(x)</em>를 최대로 만드는 입력 값 <em>x</em>의 전역 최적해(global optimization) <em>x*</em>를 찾기 위한 방법이며, 목적 함수가 명시적이지 않고 함수 값 <em>f(x)</em>의 계산 비용이 클 때 최적해를 효과적으로 찾아낼 수 있습니다. 머신 러닝에서는 최적의 하이퍼 파라미터 조합을 효율적으로 찾아내는 방법 중 하나로 많이 사용됩니다.</p><p>베이지안 최적화가 이루어지는 과정을 개략적으로 설명하면 다음과 같습니다.</p><p>(1) 미리 지정된 하이퍼 파라미터의 탐색 범위 내에서 n개의 값들을 랜덤하게 선택하여 모델을 학습하고, 학습된 모델의 함수 값을 계산</p><p>(2) 입력 값과 함수 값 쌍으로 집합을 구성하고, 이를 바탕으로 Gaussian Process 등의 방법을 사용하여 미지의 목적 함수 <em>f</em> 를 확률적인 방법으로 추정</p><p>(3) 현재까지 목적 함수에 대해 추정한 결과를 바탕으로, <em>x*</em>를 찾을 수 있을 것으로 예상되는 다음 입력 값 후보를 선택하여 모델을 학습하고, 학습된 모델의 함수 값을 계산하여 입력 값과 함수 값 쌍의 집합에 추가</p><p>(4) 위 (2)~(3)의 과정을 정해진 횟수만큼 반복하면서 추정 함수를 갱신한 후, 추정 함수의 함수 값을 최대로 만드는 최적 하이퍼 파라미터 <em>x*</em>를 선택</p><p>이 과정은 한 달 주기로 새로운 데이터들을 추가 반영하여 반복되면서 모델을 업데이트해 나가게 됩니다.</p><h4>입고 예약 시스템과 모델 연계 구성</h4><p>모델은 <a href=\"https://aws.amazon.com/ko/sagemaker/\">SageMaker</a>에서 학습됩니다. 업체가 입고 예약 시스템으로 슬롯 예약 신청을 하면, 예약 시스템은 SageMaker 엔드포인트를 호출해 모델 예측 결과 값을 반환 받아 업체에게 적정 트럭 대수를 알려줍니다.</p><figure><img alt=\"쿠팡의 입고 예약 시스템과 머신 러닝 모델 간의 연계 구성\" src=\"https://cdn-images-1.medium.com/max/1024/1*OBO6u162sg5ENJo5eYcJOw.png\" /><figcaption>그림 3. 입고 예약 시스템과 모델 연계 구성</figcaption></figure><h4>과소 예측과 과대 예측 사이의 트레이드 오프</h4><p>기계 학습 예측 모델은 필연적으로 오차를 동반합니다. 저희가 해결하고자 하는 문제의 경우 실제 적정 슬롯 개수보다 적은 개수를 예측하면 과소 예측이, 많은 개수를 예측하면 과대 예측이 발생합니다. 과소 예측과 과대 예측 사이에는 트레이드 오프가 있는데, 같은 수준의 오차율 내에서 예측 모델이 적정 트럭 개수를 가급적 적게 예측하는 경향을 띄면 과대 예측은 줄어들지만 과소 예측이 늘어나고, 트럭 개수를 가급적 많게 예측하는 경향을 갖게 되면 과소 예측은 줄어들지만 과대 예측이 늘어납니다.</p><p>저희의 기본적인 목표는 과대 예측을 줄여 불필요하게 예약되는 슬롯들을 최소화하는 것이지만, 과대 예측을 지나치게 최소화하는 방향으로 모델의 예측 슬롯 개수를 결정하면, 업체들 입장에서는 너무 적은 슬롯들이 배정된다고 느끼게 되는 의도치 않은 상황이 발생할 수도 있습니다.</p><p>따라서 저희는 관련 부서들과 논의 끝에 적정한 수준의 과소 예측 비율을 유지하면서 과도 예측을 최대한 줄이는 것이 쿠팡과 업체 모두에게 이로운 방식임을 확인하였으며, 이를 반영해 최종 모델을 도출하였습니다.</p><h3>모델 적용 결과</h3><p>최종 학습된 모델의 적정 트럭 대수를 과소 예측하는 비율은 2.53% 수준이며, 과대 예측하는 비율은 5.04% 수준입니다. 이는 이전에 업체들이 직접 적정 트럭 대수를 예측해 입고 예약 시스템에 등록할 때의 과소 예측 비율 8.71%와 과대 예측 비율 44.45%로부터 크게 개선된 수치입니다.</p><p>이 학습 모델을 활용한 결과, 슬롯이 부족해 업체가 입고일을 변경하는 사례가 67.9% 감소했고, 결과적으로 업체는 원하는 일정으로 제품을 풀필먼트 센터로 입고할 수 있게 되었고 쿠팡은 불필요하게 낭비되는 비용을 줄이고 필요한 수량의 물품들을 원하는 일정에 납품 받을 수 있게 되었습니다.</p><figure><img alt=\"쿠팡 풀필먼트 센터로의 상품 입고에 필요한 벤더들의 트럭 수를 예측하는 머신러닝 모델의 적용 후 결과\" src=\"https://cdn-images-1.medium.com/max/654/1*Bmd09fvTtGJJMUhMpD8baA.png\" /><figcaption>표 1. 트럭 수량 예측 모델 적용 결과</figcaption></figure><h3>향후 계획</h3><p>쿠팡의 비즈니스 영역이 점점 더 확대될수록, 쿠팡이 취급하는 제품군도 다양해지고 있습니다. 최근에는 가전 제품을 전문적으로 취급하는 풀필먼트 센터들도 생겨나는 등, 쿠팡의 작업 방식도 변해가면서 이에 맞는 정확한 적정 트럭 대수 예측이 필요해진 상황입니다. 따라서 이런 새로운 유형의 제품과 작업 방식의 변화들에 대해서도 모델이 적절한 예측 성능을 보일 수 있도록, 저희는 앞으로도 파생 특징들을 발굴하고 데이터를 추가하면서 지속적으로 모델을 개선해 나갈 예정입니다.</p><p><em>물류 프로세스 혁신에 관심이 있으시다면, 쿠팡 </em><a href=\"https://bit.ly/3bw6NjT\"><em>채용 공고</em></a><em>를 확인해 보세요!</em></p><p>본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 <a href=\"https://ir.aboutcoupang.com/English/home/default.aspx\">ir.aboutcoupang.com</a> 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fe4490e44514\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AC%BC%EB%A5%98-%EC%9E%85%EA%B3%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%B5%9C%EC%A0%81%ED%99%94-fe4490e44514\">기계 학습 모델을 활용한 물류 입고 프로세스 최적화</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-03-13T01:33:09.000Z",
    "url": "https://medium.com/coupang-engineering/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AC%BC%EB%A5%98-%EC%9E%85%EA%B3%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%B5%9C%EC%A0%81%ED%99%94-fe4490e44514?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "Optimizing the inbound process with a machine learning model",
    "partialText": "<h4>How we predict the adequate number of delivery trucks needed for vendors to send their products to Coupang’s fulfillment centers</h4><p><em>By Austin Yang &amp; JY Cho</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b9zku0Izrt8kvjRPLNbLzw.jpeg\" /></figure><blockquote>This post is also available in <a href=\"https://medium.com/coupang-engineering/기계-학습-모델을-활용한-물류-입고-프로세스-최적화-fe4490e44514\"><strong>Korean</strong></a>.</blockquote><p>Coupang continuously strives to optimize the inbound logistics process. By minimizing the resources wasted while receiving products at fulfillment centers, we can sell products in a more timely fashion and deliver them to more customers faster. To this end, Coupang has been efficiently improving the process of receiving products at fulfillment center that Coupang directly purchases from vendors through machine learning. Let’s look at what improvements we have made so far in this post.</p><h4>Table of Contents</h4><blockquote>· <a href=\"#13ce\">Background and challenges</a><br>· <a href=\"#2b55\">Training a model to predict the number of trucks</a><br> ∘ <a href=\"#a118\">Feature extraction</a><br> ∘ <a href=\"#bf13\">Model learning: LightGBM algorithm</a><br> ∘ <a href=\"#817c\">Model hyper parameter search: Bayesian optimization</a><br> ∘ <a href=\"#d283\">Inbound reservation system integrated with the model</a><br> ∘ <a href=\"#b050\">Trade-off between underprediction and overprediction</a><br>· <a href=\"#8b3b\">Result of applying the model</a><br>· <a href=\"#49a2\">Future plan</a></blockquote><h3>Background and challenges</h3><p>Every day, thousands of vendors all over the place load different types of products onto truck to send them to Coupang’s fulfillment centers. Each fulfillment center has docks where trucks are parked and goods are unloaded. The number of docks at each center that can be used per hour is fixed. To unload goods, one truck uses one dock for a certain period of time, and this is called a slot.</p><p>The number of required slots for each inbound must be precisely predicted so that the products from various vendors can be efficiently unloaded to the set number of slots. If the predicted number of slots is smaller than the actual number of required slots, it could cause a delay in the inbound process. On the other hand, if the predicted number of slots turns out to be bigger than necessary, it would end up wasting our limited resource.</p><figure><img alt=\"Potential resource waste in the supply process from vendors to Coupang fulfillment centers\" src=\"https://cdn-images-1.medium.com/max/1024/1*jCJozrpL1x9-mwI1nBAW0A.png\" /><figcaption>Figure 1. Potential resource waste in the supply process</figcaption></figure><p>To address this issue, we have worked on developing a system that predicts the appropriate number of slots based on the characteristics of goods to be supplied and the characteristics of vendors show when reserving slots. The system aimed to reduce the number of wasted slots and prevent IB delay resulting from a lack of slots. In the next section, we will explain in more detail what techniques were used to achieve this goal.</p><h3>Training a model to predict the number of trucks</h3><p>We decided to solve the problem by using data and:</p><p>1. Extracted features that had an impact on the number of trucks based on the logistics data and inbound requests accumulated over the years, and then prepared training data by incorporating them with the data on the number of trucks that were actually used for inbound.</p><p>2. Trained a machine learning model to predict an adequate number of trucks that should arrive at a dock.</p><p>3. Integrated the trained model with the reservation system, and made the adequate number of trucks displayed on the system right away when vendors make a request for inbound.</p><p>By adding this new automated prediction to the inbound reservation system, we have achieved our project’s goal which is the efficiency in our slot operations.</p><h4>Feature extraction</h4><p>We went through an exploratory data analysis (EDA) process to find the right features for the model which predicts the number of trucks to be used for inbound, utilizing the massive logistics data accumulated at Coupang. However, we soon learned that we needed knowledge from the domain experts to read between the lines among those data. Through a series of interviews with the Coupang logistics managers, we found certain patterns from the inbound process. Based on the findings, we came to discover multiple useful features for predicting the number of trucks. After processing those discovered features via feature engineering, we were able to define the final feature set.</p><h4>Model learning: LightGBM algorithm</h4><p>We extracted about 800,000 training data sets from the inbound request data collected for over two years. Since the size of the data sets wasn’t small, we looked for an algorithm which could be trained fast and tuned. In addition, many of the identified features were categorical features. We decided to use the LightGBM algorithm, which has a high predictive accuracy for data sets which have such features.</p><p>LightGBM is a tree-based boosting model as well as an algorithm that has demonstrated effective performance in many machine learning problems. Because LightGBM applies leaf-wise tree growth where trees are grown vertically while other tree-based algorithms apply level-wise tree growth where trees are grown horizontally, LightGBM enables fast training. In level-wise tree growth, we have to wait until each tree is fully grown, but the leaf-wise growth approach grows the tree vertically by splitting the data at the leaf nodes with the highest loss change. Because of this, LightGBM is faster in learning than other algorithms.</p><p>In addition, unlike most of the other algorithms, LightGBM does not require separate one-hot encoding for categorical features, because it applies <a href=\"https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479\">Fisher</a> algorithm to find the optimal split of data classes. Thanks to this, LightGBM generally shows a high predictive accuracy and a higher learning rate. When compared with other major tree-based algorithms based on the validation data set, LightGBM showed the learning rate and predictive performance we desired.</p><figure><img alt=\"Major tree-based algorithms’ predictive performance on the validation data set of the inbound requests to Coupang’s fulfillment centers\" src=\"https://cdn-images-1.medium.com/max/819/1*ukrx0gxBz2S5BwxViVIxuA.png\" /><figcaption>Figure 2. Major tree-based algorithms’ predictive performance on the validation set</figcaption></figure><h4>Model hyper parameter search: Bayesian optimization</h4><p>We configured a set of hyperparameters of this model to be automatically chosen using <a href=\"https://en.wikipedia.org/wiki/Bayesian_optimization\">Bayesian Optimization</a>. Bayesian Optimization is a method to find the global optimization <em>x*</em> out of <em>x</em> that maximizes the <em>f(x)</em> value of the objective function, and shows a high efficiency for acquiring the optimal solution when the objective function is not specified and getting the <em>f(x)</em> is computationally expensive. It is also one of the commonly used methods in machine learning when it comes to figuring out the optimal combinations of hyperparameters.</p><p>The Bayesian optimization process is as follows.</p><p>(1) Train a model selecting N values randomly within the set hyperparameter bandwidth, and calculate the function value from the model.</p><p>(2) Configure the group which consists of the sets of input values and function values in order to assume the objective function, utilizing a probabilistic method such as Gaussian Process.</p><p>(3) Based on the assumptions of the objective function so far, train the model to select the input value candidates that are expected for finding the optimal <em>x*</em>. Calculate the function from the trained model and then add the set of input values and function values to the group.</p><p>(4) Repeat the mentioned (2) and (3) for an assigned number of rounds, renewing the assumed function. Select the optimal hyperparameter <em>x* </em>which maximizes the value of the function.</p><p>We run this process on a monthly basis with new data, continuously updating the model.</p><h4>Inbound reservation system integrated with the model</h4><p>The model is deployed on <a href=\"https://aws.amazon.com/ko/sagemaker/\">SageMaker</a>. When a vendor requests a slot reservation on the reservation system, the reservation system calls SageMaker endpoint, receives the result predicted by the model and informs the vendor of the appropriate number of trucks.</p><figure><img alt=\"How the Coupang’s inbound reservation system and the ML model are integrated\" src=\"https://cdn-images-1.medium.com/max/1024/1*V41Hm0JNBqHLonDpRSYlrA.png\" /><figcaption>Figure 3. How the IB reservation system and the model are integrated</figcaption></figure><h4>Trade-off between underprediction and overprediction</h4><p>Errors are inevitably present in the machine learning predictive model. As for the issue we’re trying to resolve, predicting fewer slots than the actual number of needed slots leads to underprediction, and predicting more slots leads to overprediction. There is a trade-off between underprediction and overprediction. Within the same range of error rates, if a predictive model tends to predict fewer trucks as the adequate number of trucks, overprediction decreases but underprediction increases. On the other hand, if a predictive model tends to predict more trucks as the adequate number of trucks, underprediction decreases but overprediction increases.</p><p>Basically, our goal is to minimize the number of slots that are unnecessarily reserved by reducing overprediction. But if we train a model to overly minimize overprediction, vendors might feel that too few slots are allocated to them resulting in unintended inconvenience.</p><p>After discussing with relevant departments, we agreed that it would be beneficial for both Coupang and vendors to reduce overprediction as much as possible while maintaining the ratio of underprediction at an appropriate level at the same time and came up with a final model reflecting this consensus.</p><h3>Result of applying the model</h3><p>The final model underpredicts the number of trucks by 2.53% and overpredicts it by 5.04%. This is a significant improvement from the underprediction rate of 8.71% and the overprediction rate of 44.45% we had when vendors had predicted the appropriate number of trucks by themselves and registered it on the reservation system.</p><p>As a result of using this learning model, the number of cases where a vendor changes the delivery date due to a lack of slots has decreased by 67.9%. Now they can supply their products to fulfillment centers according to a schedule they want. Coupang can reduce unnecessary expenses and receive as many products as needed on a desired date.</p><figure><img alt=\"Result of applying a ML model which predicts the number of trucks that a vendor needs to supply their products to Coupang fulfillment centers\" src=\"https://cdn-images-1.medium.com/max/711/1*Ikm7Y1TLNj5sy4VhXbJn1Q.png\" /><figcaption>Table 1. Result of applying a model which predicts the number of trucks</figcaption></figure><h3>Future plan</h3><p>As Coupang is expanding its business, we are handing a wider variety of products and are also operating recently built fulfillment centers that only handle home appliances. Thus, we need more accurate prediction on the number of trucks that meets Coupang’s new needs. In order to make the model show an adequate performance in predicting the number of trucks for these new types of products in a new environment, we will continuously improve the model by identifying derived features and adding data.</p><p><em>If the innovation in fulfillment process interests you, come and check out our </em><a href=\"https://bit.ly/3Q7AKWS\"><em>open positions</em></a><em>.</em></p><p><em>While the technical content in this article reflects our company’s research activities, it does not necessarily indicate our actual investment plans or product development directions. This content is purely for research purposes and should not be used as a basis for investment decisions. Please refer to our official announcements on </em><a href=\"http://ir.aboutcoupang.com/\"><em>ir.aboutcoupang.com</em></a><em> for information on our formal investment plans and product development strategies.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2db48bbbc304\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304\">Optimizing the inbound process with a machine learning model</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2023-03-09T01:37:49.000Z",
    "url": "https://medium.com/coupang-engineering/optimizing-the-inbound-process-with-a-machine-learning-model-2db48bbbc304?source=rss----fb028911af07---4"
  },
  {
    "publisherId": "coupang",
    "publisherName": "쿠팡 테크블로그",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/coupang-tech",
    "title": "쿠팡 SCM 워크플로우: 효율적이고 확장 가능한 low-code, no-code 플랫폼 개발",
    "partialText": "<h4>개발자, 비개발자 모두 쉽게 사용할 수 있는 데이터 구축 및 서비스 엔지니어링 플랫폼에 대하여</h4><p><em>By </em><a href=\"https://www.linkedin.com/in/atinjin/\"><em>Ryan Donghyun Jin</em></a><em> (</em><a href=\"https://medium.com/@atinjin\"><em>@atinjin</em></a><em>)</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/836/1*sQCIlFHe8A9G0Yuy4lxW9Q.jpeg\" /></figure><blockquote>본 포스트는 <a href=\"https://medium.com/coupang-engineering/coupang-scm-workflow-building-a-low-code-no-code-platform-3d035c1fa37a\"><strong>영문</strong></a>으로도 제공됩니다.</blockquote><p>SCM(Supply Chain Management)은 상품 공급자, 운송 업체, 배송 서비스, 제품 등으로 이루어진 복잡한 네트워크를 데이터 흐름에 기반해 관리합니다. 쿠팡은 로켓 배송을 포함한 다양한 종류의 배송 서비스들을 제공하기 위해 전국 곳곳에 자체 풀필먼트 센터(fulfillment center, FC)를 구축했습니다. SCM 팀은 상품 수요를 예측하고, 구매 주문이행에 필요한 파이프라인을 만들고 관리합니다. 주요 업무는 구매 주문 및 센터 간 물류를 효율적으로 분배하기 위해 다양한 알고리즘 및 모델을 개발하고, 운영팀과 협업하는 것입니다. 팀에서는 물류 이동 정보 등 다양한 데이터가 다뤄지고 있습니다.</p><p>쿠팡 SCM 팀은 업무 특성상 운영팀과 개발자, 비즈니스 분석가(Business Analyst, BA), 데이터 사이언티스트(Data Scientist) 등 다양한 직군의 구성원들이 모여 함께 시스템을 만들고 운영합니다. 그렇기 때문에 모두들 시스템의 데이터를 활용하고, 그 데이터를 다시 시스템에 피드백으로 주면서, 팀의 플랫폼을 개발해나가고 있습니다.</p><p>이렇게 정보 및 아이디어 교환이 많은 팀에서 개발자로 일하면서 다음과 같은 이슈들을 자주 접하게 되었습니다.</p><blockquote>“누구나 웹을 통해 데이터를 생성 또는 수정하고 생산한 데이터를 프로덕션 시스템까지 쉽게 연동할 수 있으면 좋을텐데…”</blockquote><blockquote>“새로운 시스템을 구축하지 않고, 도메인을 확장할 수 있는 방법은 없을까?”</blockquote><blockquote>“사용 중인 파이프인의 코드를 변경하지 않고도 바뀐 요구사항을 반영할 수는 없을까?”</blockquote><blockquote>“Low-code, no-code 시스템 개발?”</blockquote><p>문제 해결 과정에서 점점 더 많은 주목을 받고 있는 기술 트렌드인 low-code, no-code가 큰 인사이트를 주었습니다. 개발자는 알고리즘 개발에 집중할 수 있고, 비개발자도 개발자 못지않은 결과물을 만들어 낼 수 있는 플랫폼이 필요하다는 생각에 no-code 데이터 빌더 및 low-code 서비스 빌더를 제공하는 SCM 워크플로우 플랫폼(SCM Workflow platform)을 개발했습니다.</p><p>저희는 앞으로 SCM 워크플로우를 통해 기존의 <a href=\"https://www.jenkins.io/\">Jenkins</a>, <a href=\"https://airflow.apache.org/\">Airflow</a>, <a href=\"https://jupyter.org/\">Notebook</a> 같은 툴이 가진 복잡성 및 연결성을 개선하여 완전히 대체하는 것은 물론, 한발 더 나아가 누구나 간단한 조작만으로도 데이터를 만들어내고 시각화해 간단한 서비스까지 론칭할 수 있는 플랫폼을 만들어나가려고 합니다.</p><h4>목차</h4><blockquote>· <a href=\"#caae\">No-code 데이터 빌더</a><br>· <a href=\"#71a4\">Low-code 서비스 빌더</a><br>· <a href=\"#443a\">기타 특징 및 기능</a><br>· <a href=\"#a6ee\">쿠팡 워크플로우 웹 인터페이스</a><br>· <a href=\"#07dc\">향후 계획</a></blockquote><h3>No-code 데이터 빌더</h3><p>쿠팡은 데이터에 기반해 모든 의사결정을 내립니다. 대량의 데이터가 마이크로서비스 아키텍처(MSA) 내의 도메인 단위로 나누어지고, 나누어진 데이터는 해당 도메인을 담당하는 팀에 의해 관리됩니다. 각 도메인 팀은 ODS, EDW, API 서비스 등 다양한 인터페이스를 통해 다른 팀들에게 데이터를 제공합니다.</p><p>하지만 비즈니스가 성장하고 다양해지면서 요구사항 또한 빠르게 바뀌고 많아졌습니다. 데이터 소스와 분석 쿼리 또한 수시로 변하게 되었고 이를 시스템에 빠르게 반영하고 프로덕션 결과까지 연동해야 했습니다. 간단한 데이터 검색 및 분석 작업도 여러 툴에 접속해야만 가능했기 때문에 담당 개발자가 아닌 사용자 입장에서는 작업 수행에 어려움이 있었습니다.</p><p>예를 들어 비즈니스 분석가 및 데이터 사이언티스트가 새 모델을 만들고 적용하려면, 담당 개발자에게 비즈니스 요구사항에 맞는 새로운 데이터 파이프라인을 개발해달라고 요청해야 했습니다. 정형화된 프로세스로 이루어진 작업이지만 막상 관련 툴을 개발하기도 애매한 일이라 개발자는 개발자대로, 관련 작업자는 작업자대로 매번 요청하고 기다리는 식의 프로세스 아닌 프로세스가 반복되고 있었습니다. 이와 같은 작업들로 인해 팀원들 간에 많은 커뮤니케이션 비용이 발생했습니다.</p><p>이런 문제를 해결하기 위해 SCM 워크플로우의 no-code 데이터 빌더(data builder) 기능을 개발했습니다. 데이버 빌더를 통해 데이터 소스 접근, 데이터 질의(query) 및 추출(export), 그리고 시스템 연동을 쉽게 수행할 수 있습니다. 개발자는 물론 비즈니스 분석가, 데이터 사이언티스트, 실무 운영진까지 코드 없이 안전하고 쉽게 데이터에 접근하고 데이터를 처리하고 시스템 연동까지 해낼 수 있습니다. 데이터 프로세서(data processor)로서 SCM 워크플로우는 팀 내에서 사용 중인 다음과 같은 공용 데이터 소스로 접근 가능합니다: <a href=\"https://aws.amazon.com/redshift/\">Redshift</a>, <a href=\"https://hive.apache.org/\">Hive</a>, <a href=\"https://prestodb.io/\">Presto</a>, <a href=\"https://aws.amazon.com/rds/aurora/\">Aurora</a>, <a href=\"https://www.mysql.com/\">MySQL</a>, <a href=\"https://www.elastic.co/\">Elasticsearch</a>, <a href=\"https://aws.amazon.com/s3/\">S3</a>.</p><p>SCM 워크플로우의 데이터 빌더를 사용하는 과정은 다음과 같습니다. 예를 들어 풀필먼트 센터 간 상품 이관이 필요한 상황 에서 어떤 상품을 얼마만큼 언제 이관해야 하는지를 결정해야 할 경우, 데이터 빌더 사용자는 일종의 데이터 생산자로서 상품 이관 워크플로우를 다음과 같이 만들 수 있습니다.</p><ol><li>Hive, Redshift, Aurora 등 다양한 데이터 소스에 저장되어 있는 상품 재고 및 주문 데이터를 추출하는 노드를 생성합니다.</li><li>각 노드에 데이터를 추출할 수 있는 쿼리를 작성해 넣습니다.</li><li>데이터 추출이 완료되면 이를 입력 삼아 이관 상품수량을 계산하는 노드를 생성해 앞서 생성된 노드들과 연결합니다.</li></ol><p>이런 간단한 과정을 통해 만든 워크플로우로 사용자는 이관이 필요한 상품의 수량을 계산할 수 있습니다.</p><figure><img alt=\"이관 상품 수량 계산을 위한 쿠팡 SCM 워크플로우\" src=\"https://cdn-images-1.medium.com/max/800/1*tLme86SZaYTNGy7PApM8Pg.png\" /><figcaption>그림 1. 이관 상품 수량 계산을 위한 워크플로우</figcaption></figure><p>또한 이 과정에서 복잡한 쿼리를 분할해 다수의 쿼리들로 만들 경우, 원하는 만큼 병렬 처리가 가능합니다. 순차 실행보다 더 빨리 결과를 얻을 수 있습니다. 질의의 결과는 사용자가 따로 저장하지 않고, 데이터 소비자가 될 개발자 또는 다른 팀에 워크플로우 ID 또는 URL, 파일 경로를 전달해 주면 됩니다. 콜백(callback) 기능을 사용해 다른 시스템과 연동도 쉽게 할 수 있습니다.</p><p>워크플로우의 스케줄링 기능을 통해 매일 정해진 시간에 데이터를 생성할 수 있습니다. 또한 시, 일, 주 단위 등으로 반복 처리되는 작업이 있다면 스케줄링을 통해 해당 작업을 자동화할 수도 있습니다.</p><figure><img alt=\"다수의 분할된 쿼리들을 병렬로 처리하는 쿠팡 SCM 워크플로우\" src=\"https://cdn-images-1.medium.com/max/800/1*zzrSwPZam0XwZr3LHByy8A.png\" /><figcaption>그림 2. 다수의 분할된 쿼리들을 병렬로 처리</figcaption></figure><h3>Low-code 서비스 빌더</h3><p>전국 쿠팡 풀필먼트 센터의 입고 및 출고는 상품 수요 예측 시스템, 풀필먼트 센터 인력 관리 시스템, 구매 주문 분배 시스템, 구매 주문 시스템, 센터 간 물류 이동 시스템 등 다양한 SCM 관련 시스템들에 의해 관리되고 있으며 해당 시스템들은 쿠팡의 공급망 관리(SCM)를 책임지고 있습니다. 각 도메인 팀은 담당하고 있는 시스템의 운영을 위해 서버를 구축하고 수많은 배치 시스템과 설정을 관리하고 있습니다.</p><p>문제는 비즈니스 도메인이 확장하면서 비슷한 시스템들이 계속 생겨나고 유지 및 보수 관련 공수가 계속 늘어나는 것이었습니다. 특히 수요 예측, 주문량 분배, 주문, 풀필먼트 센터 간 물류 이관 같은 서비스들은 시스템에서 생성된 데이터를 서로 주고받으면서 사용 및 운영되기 때문에, 각각의 시스템에는 비슷한 기능을 수행하는 코드들이 존재합니다. 핵심 알고리즘을 제외하면 대부분 데이터를 받고, 처리하고, 전달하는 과정들의 반복입니다. 이런 반복적인 일을 간편하게 처리할 수 있도록 SCM 워크플로우는 low-code 서비스 빌더를 제공합니다. 해당 서비스를 통해 개발자는 주요 알고리즘과 데이터 처리 부분에만 집중해 개발할 수 있게 되며 개발 효율성 및 시스템 안정성을 개선할 수 있습니다.</p><p>SCM 워크플로우의 low-code 서비스 빌더의 주요 기능은 다음과 같습니다.</p><h4>개발 플랫폼 제공</h4><p>서비스 개발을 위해 필수적으로 필요한 버저닝(versioning), 설정(configuration), 에러 로그 뷰어를 제공합니다. 그래서 개발자는 웹을 통해 디버깅이 가능하며 별도의 버전 관리 툴이 필요하지 않습니다. 또한 운영상 필요한 알람 기능, 워크플로우의 상태를 실시간으로 모니터링할 수 있는 대시보드(dashboard)를 제공하여 시스템 운영을 보조합니다.</p><h4>공용 서비스 컴포넌트 제공</h4><p>시스템 구축에 필수적인 서비스 컴포넌트를 제공하여 손쉬운 서비스 구성이 가능합니다. 메신저 알림(notification), REST 콜백(callback), 요청 재시도(retry) 등을 프로그래밍 관점의 기본 컴포넌트로 제공합니다. 또한 파일 업로드, 다운로드, 복사, 큐 메시지 발송 및 수신 등과 AWS 인프라를 쉽게 사용할 수 있는 서비스 블록을 활용해, 데이터와 메시지를 주고받는 시스템 구축이 가능합니다. 또한 사용자들의 피드백을 바탕으로 새로운 공용 서비스 컴포넌트를 계속 추가하고 있습니다.</p><h4>커스텀 서비스 컴포넌트 등록 및 사용</h4><p>커스텀(custom) 서비스 컴포넌트를 등록해 SCM 워크플로우의 확장성 및 유연성을 강화할 수 있습니다. 도메인 개발자는 핵심 알고리즘을 커스텀 서비스 컴포넌트로 만들고 이를 시스템에 등록하면 SCM 워크플로우는 이를 하나의 블록으로 만들 수 있게 됩니다. 등록한 블록을 통해 해당 도메인의 특화 기능을 워크플로우로 쉽게 통합할 수 있습니다. 이를 통해 머신러닝 모델의 동작도 가능한 복잡도가 높은 서비스도 구축 가능합니다. 커스텀 서비스 컴포넌트를 수정하거나 재사용해 보다 쉽게 시스템을 업그레이드하고 도메인을 확장할 수 있습니다.</p><h4>그래픽 워크플로우 모델링</h4><p>기존의 상용 워크플로우와 비교해 SCM 워크플로우가 가진 가장 큰 장점 중 하나는 캔버스 에디터(canvas editor)와 서비스 컴포넌트(블록) 간의 입출력 데이터 연동 방식입니다. 블록과 캔버스를 사용해 모델을 그리는 것은 Low code no-code 개발 플랫폼의 영향이 컸습니다. 블록화된 SCM 워크플로우의 서비스 컴포넌트들을 드래그 앤 드랍(drag &amp; drop)으로 캔버스 에디터로 손쉽게 배치하고 연결 및 조합할 수 있습니다. 또한 워크플로우 내 모든 컴포넌트들은 런타임(runtime) 중에 다른 컴포넌트의 입출력 값을 참조해 각 컴포넌트의 입력 변수(input parameter)로 사용할 수 있습니다. 이를 통해 워크플로우 모델은 하나의 프로그램으로 동작할 수 있게 됩니다. 여기에 조건 분기(conditional statement), 반복(iteration), 병렬 처리(parallelism)와 같은 프로그래밍적 요소들도 추가되었고, 사용자는 이 요소들을 활용해 좀 더 복잡한 프로그램을 작성하고 고도화된 내부 애플리케이션을 만들어낼 수도 있습니다.</p><h3>기타 특징 및 기능</h3><p>No-code, low-code 외에도 SCM 워크플로우는 다음과 같은 특징과 기능을 갖고 있습니다.</p><h4>확장성</h4><p>SCM 워크플로우의 아키텍처는 확장성과 안정성에 주안점을 두고 설계되었습니다. 다양한 데이터 소스들에 대한 데이터 쿼리와 프로세싱이 안정적으로 이뤄져야 함은 물론 개별 사용자 및 각종 시스템에서 하고 있던 작업이 시스템 내로 통합될 수 있어야 하기 때문입니다. 모든 작업을 떠안지만 단일 장애점(Single Point of Failure, SPOF)은 되지 않도록, 각각의 서비스를 3개의 레이어로 구성되게끔 설계했습니다. 각 레이어를 클러스터링(clustering)해 처리량이 늘면 각 레이어의 클러스터가 스케일 아웃(scale out)될 수 있도록 하였습니다.</p><h4>작업 간 독립성 및 안정성</h4><p>SCM 워크플로우 엔진은 모델을 생성하고 관리하는 것은 물론 각각의 모델로부터 생성되는 인스턴스를 실행하고 작동시키는 일도 합니다. 워크플로우 인스턴스는 실행 중 자신만의 고유 상태를 가지며 엔진은 모델링된 상태 정보를 이용해 사용자가 정의한 작업을 순차, 병렬, 반복 처리하며 완료하게 합니다. 클러스터 내에서 작동할 수 있는 스케줄러를 통해 사용자가 원하는 시간에 원하는 개수만큼 인스턴스를 생성해 작업을 수행할 수 있습니다. 일련의 작업 수행 시 특정 작업에 오랜 시간이 걸리거나 특정 작업의 과도한 리소스 사용으로 인해 다른 작업이 받는 영향을 줄이기 위해, 각 작업은 SCM 워크플로우 시스템 내 클러스터에서 비동기 요청을 통해 독립적으로 실행됩니다. 워크플로우 엔진은 각 작업의 성공 및 실패를 모니터링하고 사용자의 설정에 따라 워크플로우를 진행시키는 역할만 합니다. 수많은 워크플로우의 계속되는 요청들을 안정적으로 처리할 수 있도록 워크플로우 엔진과 워커(worker)를 독립적으로 설계했습니다.</p><h4>공용 데이터 처리 서비스</h4><p>SCM 워크플로우 프로세서는 자체 데이터 처리(data processing) 서비스를 가지고 있습니다. 기본적으로 팀 내에서 서비스되는 각종 데이터 소스에 대한 정보를 공용 서비스 컴포넌트 형태로 엔진에 제공 가능하며, 사용자는 별도의 설정 없이도 데이터 소스에 대한 작업을 수행할 수 있습니다. 또한 데이터 프로세싱 유틸리티 기능과 파일 업로드, 다운로드, 병합(Merge) 등 실제 서비스에 쓰이는 기능을 제공해 사용자의 서비스 구현을 돕고 있습니다.</p><figure><img alt=\"쿠팡 SCM 워크플로우 아키텍처\" src=\"https://cdn-images-1.medium.com/max/800/1*q8oLd6R8kfdF4LcYX8ilKw.jpeg\" /><figcaption>그림 3. SCM 워크플로우 아키텍처</figcaption></figure><h3>쿠팡 워크플로우 웹 인터페이스</h3><p>SCM 워크플로우 웹은 사용자 웹 인터페이스를 제공합니다. 워크플로우 모델 및 스케줄과 실행 히스토리를 생성, 편집, 관리할 수 있습니다. 워크플로우 웹 캔버스를 이용해 사용자는 등록된 서비스 컴포넌트들을 직관적으로 캔버스 위에 배치하고 컴포넌트들을 의존성에 따라 연결하고 컴포넌트들의 실행 순서를 지정할 수 있습니다. 코딩에 익숙하지 않은 사용자도 이 툴로 손쉽게 워크플로우를 만들 수 있습니다.</p><h4>워크플로우 모델러</h4><figure><img alt=\"쿠팡 SCM 워크플로우의 모델링 캔버스\" src=\"https://cdn-images-1.medium.com/max/800/1*yucG-AXV69VCRSLB-M-HcQ.png\" /><figcaption>그림 4. 워크플로우 모델링 캔버스</figcaption></figure><p>인스턴스 실행에 대한 성공/실패 히스토리를 확인할 수 있습니다. 어떤 부분에서 얼마큼 시간이 걸렸는지, 실패했을 경우 에러 로그를 확인할 수도 있어 디버깅에 도움을 주고 있습니다.</p><figure><img alt=\"쿠팡 SCM 워크플로우의 인스턴스 히스토리\" src=\"https://cdn-images-1.medium.com/max/800/1*Q90809jZobfHZ2TzqVqRQw.png\" /><figcaption>그림 5. 워크플로우 인스턴스 히스토리</figcaption></figure><h4>워크플로우 매니저</h4><p>워크플로우 매니저를 이용해 각 팀은 프로젝트 또는 팀 별로 그룹을 생성하고 그룹 폴더 내에서 모델들을 관리할 수 있습니다. 그룹 폴더의 권한 설정으로 모델의 생성, 수정, 삭제 등을 제한할 수도 있습니다. 향후에는 개별 사용자 플레이그라운드(playground)와 같은 개인 워크스페이스(workspace)를 제공하여 프로덕션(production) 워크플로우 외에도 실험적인 작업이 가능하도록 지원할 계획입니다.</p><figure><img alt=\"쿠팡 SCM 워크플로우의 모델 매니저\" src=\"https://cdn-images-1.medium.com/max/800/1*8j1gK5JotMln2Tg0gsQEEw.png\" /><figcaption>그림 6. 모델 그룹 관리</figcaption></figure><h4>워크플로우 스케줄러</h4><p>모델 별로 스케줄을 설정해 사용자가 원하는 시간에 원하는 작업을 자동으로 실행하고 완료할 수 있도록 지원합니다. 비개발자를 위해 Cron Expression을 생성할 수 있는 사용자 인터페이스도 별도로 제공하고 있습니다.</p><figure><img alt=\"\b쿠팡 SCM 워크플로우의 작업 스케줄러\" src=\"https://cdn-images-1.medium.com/max/800/1*SDLXxmcTIY9wVq2Ivw-PsQ.png\" /><figcaption>그림 7. 작업 스케줄 관리하기</figcaption></figure><h4>워크플로우 대시보드</h4><p>대시보드를 통해 워크플로우 엔진의 상태를 모니터링할 수 있습니다. 사용자는 개별 인스턴스 설정에 따라 메신저 알림을 수신할 수도 있지만, 대시보드를 통해 현재 팀 내에서 관리 중인 서비스 워크플로우의 상태를 확인하고 실시간으로 대응할 수 있습니다.</p><figure><img alt=\"쿠팡 SCM 워크플로우 대시보드 내 인스턴스 모니터링 페이지\" src=\"https://cdn-images-1.medium.com/max/800/1*axZ8y6geUYJri5wyI5gBRQ.png\" /><figcaption>그림 8. 대시보드 내 인스턴스 모니터링 페이지</figcaption></figure><h3>향후 계획</h3><p>SCM 워크플로우로 새로운 컴포넌트와 시스템들이 계속 추가되고 있습니다. 이를 뒷받침하기 위해 다양한 플랫폼 기능들 또한 함께 개발되고 업그레이드되고 있습니다.</p><p>저희의 최종 목표는 SCM 워크플로우를 통해 팀 내 작은 프로젝트들을 마이그레이션해 관리 가능한 형태로 바꾸는 것입니다. 또한 데이터 처리 부분을 더 강화해 간단한 엑셀 작업부터 대용량 데이터 처리 작업까지 각각의 용도에 맞는 적정 데이터 기술을 제공하려고 합니다. 또 다른 목표는 일반 사용자를 위한 애플리케이션 생성 플랫폼으로의 성장입니다. 사용자가 입출력 인터페이스를 직접 구성하고 다른 사용자가 만든 애플리케이션을 활용해 시스템과 상호작용할 수 있게 만드는 것입니다.</p><p>앞으로도 계속 SCM 워크플로우를 개발자 및 비개발자 모두 함께 사용할 수 있는, 사용하기 쉽고 편리하면서 동시에 확장성을 가진 플랫폼으로 발전시켜 나가겠습니다.</p><p><em>SCM 워크플로우를 더 발전된 플랫폼으로 만들어 나가는데 관심이 있으시다면, 쿠팡 </em><a href=\"https://bit.ly/3bw6NjT\"><em>채용 공고</em></a><em>를 확인해 보세요!</em></p><p>본 기사에 포함된 기술적 내용은 당사의 연구 활동을 반영하지만, 이는 반드시 당사의 실제 투자 계획이나 제품 개발 방향을 나타내는 것은 아닙니다. 본 내용은 순수한 연구 목적으로 작성되었으며, 투자 결정의 근거로 사용되어서는 안 됩니다. 당사의 공식적인 투자 계획 및 제품 개발 방향은 <a href=\"https://ir.aboutcoupang.com/English/home/default.aspx\">ir.aboutcoupang.com</a> 홈페이지 내 공식 발표를 통해 확인하시기 바랍니다.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7d997644d14\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-scm-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B4%EA%B3%A0-%ED%99%95%EC%9E%A5-%EA%B0%80%EB%8A%A5%ED%95%9C-low-code-no-code-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C-7d997644d14\">쿠팡 SCM 워크플로우: 효율적이고 확장 가능한 low-code, no-code 플랫폼 개발</a> was originally published in <a href=\"https://medium.com/coupang-engineering\">Coupang Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2022-12-12T00:45:29.000Z",
    "url": "https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-scm-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B4%EA%B3%A0-%ED%99%95%EC%9E%A5-%EA%B0%80%EB%8A%A5%ED%95%9C-low-code-no-code-%ED%94%8C%EB%9E%AB%ED%8F%BC-%EA%B0%9C%EB%B0%9C-7d997644d14?source=rss----fb028911af07---4"
  }
]