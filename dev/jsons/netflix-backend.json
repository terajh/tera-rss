[
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "Automating RDS Postgres to Aurora Postgres Migration",
    "partialText": "<p><a href=\"https://www.linkedin.com/in/ramsrivatsa/\">Ram Srivasta Kannan,</a> <a href=\"https://www.linkedin.com/in/wale-akintayo-30782a82/\">Wale Akintayo</a>, <a href=\"https://www.linkedin.com/in/jay-bharadwaj-4b310ab8/\">Jay Bharadwaj</a>, <a href=\"https://www.linkedin.com/in/john-crimmins-39730b3a/\">John Crimmins</a>, <a href=\"https://www.linkedin.com/in/shengwei4721/\">Shengwei Wang,</a> <a href=\"https://www.linkedin.com/in/zhitao-cathy-zhu/\">Zhitao Zhu</a></p><h3>Introduction</h3><p>In 2024, the Online Data Stores team at Netflix conducted a comprehensive review of the relational database technologies used across the company. This evaluation examined functionality, performance, and total cost of ownership across our database ecosystem. Based on this analysis, we decided to standardize on <strong>Amazon Aurora PostgreSQL as the primary relational database</strong> offering for Netflix teams.</p><p>Several key factors influenced this decision:</p><ul><li><strong>PostgreSQL already </strong>underpinned<strong> </strong>the majority of our relational workloads, which made it a natural foundation for standardization. Internal evaluations revealed that Aurora PostgreSQL had supported over 95% of the applications and workloads running on other relational databases across our internal services.</li><li><strong>Industry momentum had continued to shift toward PostgreSQL, </strong>driven by its open ecosystem, strong community support, and broad adoption across modern data platforms.</li><li><strong>Aurora’s cloud-native, distributed architecture</strong> provided clear advantages in scalability, high availability, and elasticity compared to traditional single-node PostgreSQL deployments.</li><li>Aurora PostgreSQL offered a <strong>rich feature set</strong>, along with a <strong>strong, forward-looking roadmap</strong> aligned with the needs of large-scale, globally distributed applications.</li></ul><h3>A Clear Migration Path Forward</h3><p>As part of this strategic shift, one of our key initiatives for 2024/2025 was migrating existing users to Aurora PostgreSQL. This effort began with RDS PostgreSQL migrations and will expand to include migrations from other relational systems in subsequent phases.</p><p>As a data platform organization, our goal is to make this evolution predictable, well-supported, and minimally disruptive. This allows teams to adopt Aurora PostgreSQL at a pace that aligns with their product and operational roadmaps, while we move toward a unified and scalable relational data platform across the organization.</p><h3>Database Migration: More Than a Simple Transfer</h3><p>Migrating a database involves far more than copying rows from one system to another. It is a coordinated process of transitioning both data and database functionality while preserving correctness, availability, and performance. At scale, a well-designed migration must minimize disruption to applications and ensure a clean, deterministic handoff from the old system to the new one.</p><p>Most database migrations follow a common set of high-level steps:</p><ol><li><strong>Data Replication</strong>: Data is first copied from the source database to the destination, typically using replication, so that ongoing changes are continuously captured and applied.</li><li><strong>Quiescence</strong>: Write traffic to the source database is halted, allowing the destination to fully catch up and eliminate any remaining divergence.</li><li><strong>Validation</strong>: The system verifies that the source and destination databases are fully synchronized and contain identical data.</li><li><strong>Cutover</strong>: Client applications are reconfigured to point to the destination database, which becomes the new primary source of truth.</li></ol><h3>Challenges</h3><h4>Operational Challenges</h4><p>Migrating to a new relational database at Netflix scale presents substantial operational challenges. With a fleet approaching 400 PostgreSQL clusters, manually migrating each one is simply not scalable for the data platform team. Such an approach would require a significant amount of time, introduce the risk of human error, and necessitate considerable hands-on engineering effort. Compounding the problem, coordinating downtime across the many interconnected services that depend on each database is extremely cumbersome at this scale.</p><p>To address these challenges, we designed a self-service migration workflow that enables service owners to run their own RDS PostgreSQL to Aurora PostgreSQL migrations. The workflow automatically handles orchestration, safety checks, and correctness guarantees end-to-end, resulting in lower operational overhead and a predictable, reliable migration experience.</p><h3>Technical challenges</h3><ul><li><strong>Zero data loss</strong> — We must guarantee that all data from the source cluster is fully and safely migrated to the destination within a very tight window, with no possibility of data loss.</li><li><strong>Minimal downtime — </strong>Some downtime is unavoidable during migration, as applications must briefly pause write traffic while cutting over to Aurora PostgreSQL. For higher-tier services that power critical parts of the Netflix ecosystem, this window must be kept extremely short to prevent user-facing impact and maintain service reliability.</li><li><strong>No control over client applications</strong> — As the platform team, we manage the databases, but application teams handle the read and write operations. We cannot assume that they have the ability to pause writes on demand, nor do we want to expose such controls to them, as mistakes could lead to data inconsistencies post migration. Therefore, building a self-service migration pipeline requires creative control-plane solutions to halt traffic, ensuring that no writes occur during the validation and cutover phases.</li><li><strong>No direct access to RDS credentials</strong> — The migration automation must perform replication, quiescence, and validation without requesting database credentials from users or relying on manual authentication. Source databases are often tightly secured, allowing access only from client applications, but more importantly, requiring credential access — even if it were possible — would significantly increase operational overhead and risk. At the same time, the migration platform may operate in environments without direct access to the source database, making traditional verification or parity checks impossible.</li><li><strong>No Degradation in Performance</strong> — The migration process must not impact the performance or stability of production databases once they are running in the Aurora PostgreSQL ecosystem.</li><li><strong>Full Ecosystem Parity</strong> — Beyond migrating the core database, associated components such as parameter groups, read replicas, and replication slots must also be migrated to ensure functional equivalence.</li></ul><p><strong>Minimal User Effort</strong> — Since we rely on teams who are not database experts to perform migrations, the process must be simple, intuitive, and fully self-guided.</p><h3>AWS recommended migration techniques</h3><h4>Using a snapshot</h4><p>One of the simplest AWS-recommended approaches for migrating from RDS PostgreSQL to Aurora PostgreSQL is based on snapshots. In this model, write traffic to the source PostgreSQL database is first stopped. A manual snapshot of the RDS PostgreSQL instance is then taken and migrated to Aurora, where AWS converts it into an Aurora-compatible format.<br> <br>Once the conversion completes, a new Aurora PostgreSQL cluster is created from the snapshot. After the cluster is brought online and validated, application traffic is redirected to the Aurora endpoint, completing the migration.</p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.RDSPostgreSQL.Import.Console.html\">Reference</a></p><h4>Using an Aurora read replica</h4><p>In the read-replica–based approach, an Aurora PostgreSQL read replica is created from an existing RDS PostgreSQL instance. AWS establishes continuous, asynchronous replication from the RDS source to the Aurora replica, allowing ongoing changes to be streamed in near real time.</p><p>Because replication runs continuously, the Aurora replica remains closely synchronized with the source database. This enables teams to provision and validate the Aurora environment — including configuration, connectivity, and performance characteristics — while production traffic continues to flow to the source.</p><p>When the replication lag is sufficiently low, write traffic is briefly paused to allow the replica to fully catch up. The Aurora read replica is then promoted to a standalone Aurora PostgreSQL cluster, and application traffic is redirected to the new Aurora endpoint. This approach significantly reduces downtime compared to snapshot-based migrations and is well-suited for production systems that require minimal disruption.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*_xydKq_VWpaxyqcwCwQyuA.png\" /><figcaption>Migration Strategy Trade-Offs</figcaption></figure><p>These differences represent the key considerations when choosing a migration strategy from RDS PostgreSQL to Aurora PostgreSQL. For our automation, we opted for the Aurora Read Replica approach, trading increased implementation complexity for a significantly shorter downtime window for client applications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/682/1*SALMQ9wXHsJY9kBTmZ1D5w.png\" /><figcaption>Netflix RDS PostgreSQL Deployment Architecture</figcaption></figure><p>In Netflix’s RDS setup, a <a href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\">Data Access Layer</a> (DAL) sits between applications and backend databases, acting as middleware that centralizes database connectivity, security, and traffic routing on behalf of client applications.</p><p>On the client side, applications connect through a forward proxy that manages mutual TLS (mTLS) authentication and establishes a secure tunnel to the Data Gateway service. The Data Gateway, acting as a reverse proxy for database servers, terminates client connections, enforces centralized authentication and authorization, and forwards traffic to the appropriate RDS PostgreSQL instance.</p><p>This layered design ensures that applications never handle raw database credentials, provides a consistent and secure access pattern across all datastore types, and delivers isolated, transparent connectivity to managed PostgreSQL clusters. While the primary goal of this architecture is to enforce strong security controls and standardize how applications access external AWS data stores, it also allows backend databases to be switched transparently via configuration, enabling controlled, low-downtime migrations.</p><h3>Migration Process</h3><p>The Platform team’s goal is to deliver a fully automated, self-service workflow that helps with the migration of customer RDS PostgreSQL instances to Aurora PostgreSQL clusters. This migration tool orchestrates the entire process — from preparing the source environment, initializing the Aurora read replica, and maintaining continuous synchronization, all the way through to cutover — without requiring any database credentials or manual intervention from the customer.</p><p>Designed for minimal downtime and seamless user experience, the workflow ensures full ecosystem parity between RDS and Aurora, preserving performance characteristics and operational behavior while enabling customers to benefit from Aurora’s improved scalability, resilience, and cost efficiency.</p><h3>Data Replication Phase</h3><h4>Enable Automated Backups</h4><p>Automated backups must be enabled on the source database because the Aurora <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.Configuration.html?utm_source=chatgpt.com\">read replica</a> is initialized from a consistent snapshot of the source and then kept in sync through continuous replication. Automated backups provide the stable snapshot required to bootstrap the replica, along with the continuous streaming of write-ahead log (WAL) records needed to keep the read replica closely synchronized with the source.</p><h4>Port RDS parameters to an Aurora parameter group</h4><p>We create a dedicated Aurora parameter group for each cluster and migrate all RDS-compatible parameters from the source RDS instance. This ensures that the Aurora cluster inherits the same configuration settings — such as memory configuration, connection limits, query planner behavior, and other PostgreSQL engine parameters that have equivalents in Aurora. Parameters that are unsupported or behave differently in Aurora are either omitted or adjusted according to Aurora best practices.</p><h4>Create an Aurora read replica cluster and instance</h4><p>Creating an Aurora read replica cluster is a critical step in migrating from RDS PostgreSQL to Aurora PostgreSQL. At this stage, the Aurora cluster is created and attached to the RDS PostgreSQL primary as a replica, establishing continuous replication from the source RDS PostgreSQL instance. These Aurora read replicas stay nearly in sync with ongoing changes by streaming write-ahead logs (WAL) from the source, enabling minimal downtime during cutover. The cluster is fully operational for validation and performance testing, but it is not yet writable — RDS remains the authoritative primary.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*a3JN0YW1wuYTuez8oWw5SA.png\" /></figure><h4>Quiescence Phase</h4><p>The goal of the quiescence phase is to transition client applications from the source RDS PostgreSQL instance to the Aurora PostgreSQL cluster as the new primary database, while preserving data consistency during cutover.</p><p>The first step in this process is to stop all write traffic to the source RDS PostgreSQL instance to guarantee consistency. To achieve this, we instruct users to halt application-level traffic, which helps prevent issues such as retry storms, queue backlogs, or unnecessary resource consumption when connectivity changes during cutover. This coordination also gives teams time to prepare operationally, for example, by suppressing alerts, notifying downstream consumers, or communicating planned maintenance to their customers.</p><p>However, relying solely on application-side controls is unreliable. Operational gaps, misconfigurations, or lingering connections can still modify the source database state, potentially resulting in changes that are not replicated to the destination and leading to data inconsistency or loss. To enforce a clean and deterministic cutover, we also block traffic at the infrastructure layer. This is done by detaching the RDS instance’s security groups to prevent new inbound connections, followed by a reboot of the instance. With security groups removed, no new SQL sessions can be established, and the reboot forcibly terminates any existing connections.</p><p>This approach intentionally avoids requiring database credentials or logging into the PostgreSQL server to manually terminate connections. While it may be slower than application- or database-level intervention, it provides a reliably automated and repeatable mechanism to fully quiesce the source RDS PostgreSQL instance before Aurora promotion, eliminating the risk of divergent writes or an inconsistent WAL state.</p><h4>Validation Phase</h4><p>To determine whether the Aurora read replica has fully caught up with the source RDS PostgreSQL instance, we track replication progress using Aurora’s OldestReplicationSlotLag metric. This metric represents how far the Aurora replica is behind the source in applying write-ahead log (WAL) records.</p><p>Once client traffic is halted during quiescence, the source RDS PostgreSQL instance stops producing meaningful WAL entries. At that point, the replication lag should converge to zero, indicating that all WAL records corresponding to real writes have been fully replayed on Aurora.</p><p>However, in practice, our experiments show that the metric never settles at a steady zero. Instead, it briefly drops to <strong>0</strong>, then quickly returns to <strong>64 MB</strong>, repeating this pattern every few minutes as shown in the figure below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/674/1*0pgUo0X6RiwoeATejg6NfQ.png\" /><figcaption>OldestReplicationSlotLag</figcaption></figure><p>This behavior stems from how OldestReplicationSlotLag is calculated. Internally, the lag is derived using the following query:</p><pre>SELECT<br>  slot_name,<br>  pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) AS slot_lag_bytes<br>FROM pg_replication_slots;</pre><p>Conceptually, this translates to:</p><pre>OldestReplicationSlotLag = current_WAL_position_on_RDS <br>                           – restart_lsn </pre><p>See AWS references <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.Monitor.html\">here</a> and <a href=\"https://repost.aws/knowledge-center/rds-postgresql-use-logical-replication\">here</a>.</p><p>The <a href=\"https://www.morling.dev/blog/postgres-replication-slots-confirmed-flush-lsn-vs-restart-lsn/#:~:text=And%20this%20is%20exactly%20the,has%20a%20few%20important%20implications:\"><em>restart_lsn</em></a> represents the oldest write-ahead log (WAL) record that PostgreSQL must retain to ensure a replication consumer can safely resume replication.</p><p>When PostgreSQL performs a WAL segment switch, Aurora typically catches up almost immediately. At that moment, the restart_lsn briefly matches the source’s current WAL position, causing the reported lag to drop to 0. During idle periods, PostgreSQL performs an empty WAL segment rotation approximately every five minutes, driven by the archive_timeout = 300s setting in the database parameter group.</p><p>Immediately afterward, PostgreSQL begins writing to the new WAL segment. Since this new segment has not yet been fully flushed or consumed by Aurora, the WAL position in source RDS PostgreSQL advances ahead of the restart_lsn of Aurora PostgreSQL by exactly one segment. As a result, OldestReplicationSlotLag jumps to 64 MB, which corresponds to the configured WAL segment size at database initialization, and remains there until the next segment switch occurs.</p><p>Because idle PostgreSQL performs an empty WAL rotation approximately every five minutes, this zero-then-64 MB oscillation is expected. Importantly, the moment when the lag drops to 0 indicates that all meaningful WAL records have been fully replicated, and the Aurora read replica is fully caught up with the source.</p><h4>Cutover Phase</h4><p>Once the Aurora read replica has fully caught up with the source RDS PostgreSQL instance — as confirmed through replication lag analysis — the final step is to promote the replica and redirect application traffic. Promoting the Aurora read replica converts it into an independent, writable Aurora PostgreSQL cluster with its own writer and reader endpoints. At this point, the source RDS PostgreSQL instance is no longer the authoritative primary and is made inaccessible.</p><p>Because Netflix’s RDS ecosystem is fronted by a Data Access Layer (DAL), consisting of client-side forward proxies and a centralized Data Gateway, switching databases does not require application code changes or database credential access. Instead, traffic redirection is handled entirely through configuration updates in the reverse-proxy layer. Specifically, we update the runtime configuration of the Envoy-based Data Gateway to route traffic to the newly promoted Aurora cluster. Once this configuration change propagates, all client-initiated database connections are transparently routed through the DAL to the Aurora writer endpoint, completing the migration without requiring any application changes.</p><p>This proxy-level cutover, combined with Aurora promotion, enables a seamless transition for service owners, minimizes downtime, and preserves data consistency throughout the migration process.</p><h3>Customer Experience: Migrating a Business-Critical Partner Platform</h3><p>One of the critical teams to adopt the RDS PostgreSQL to Aurora PostgreSQL migration workflow was the Enablement Applications team. This team owns a set of databases that model Netflix’s entire ecosystem of partner integrations, including device manufacturers, discovery platforms, and distribution partners. These databases power a suite of enterprise applications that partners worldwide rely on to build, test, certify, and launch Netflix experiences on their devices and services.</p><p>Because these databases sit at the center of Netflix’s partner enablement and certification workflows, they are consumed by a diverse set of client applications across both internal and external organizations. <strong>Internally</strong>, reliability teams use this data to identify streaming failures for specific devices and configurations, supporting quality improvements across the device ecosystem. At the same time, these databases directly serve <strong>external</strong> partners operating across many regions. Device manufacturers rely on them to configure, test, and certify new hardware, while payment partners use them to set up and launch bundled offerings with Netflix.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*DJHayui83MkdSMYYd4TIQw.png\" /><figcaption>Simplified Enablement Applications Overview</figcaption></figure><p><strong>Device Lifecycle Management</strong></p><p>Netflix works with a wide range of device partners to ensure Netflix streams seamlessly across a diverse ecosystem of consumer devices. A core responsibility of Device Lifecycle Management is to provide tools and workflows that allow partners to develop, test, and certify Netflix integrations on their devices.</p><p>As part of the device lifecycle, partners run Netflix-provided test suites against their NRDP implementation. We store <strong>signals that represent the current stage for each device in the certification process</strong>. This certification data forms the backbone of Netflix’s device enablement program, ensuring that only validated devices can launch Netflix experiences.</p><p><strong>Partner Billed Integrations</strong></p><p>In addition to device enablement, the same partner metadata is also consumed by Netflix’s Partner Billed Integrations organization. This group enables external partners to offer Netflix as part of bundled subscription and billing experiences.</p><p>Any disruption in these databases affects partner integration workflows. If the database is unavailable, partners may be unable to configure or launch service bundles with Netflix. Maintaining high availability and data correctness is essential to preserving smooth integration operations.</p><p>The global nature of these workflows makes it difficult to schedule downtime windows. Any disruption would impact partner productivity and risk eroding trust in Netflix’s integration and certification processes.</p><h3>Preparation</h3><p>Given the criticality of the Enablement Applications databases, thorough preparation was essential before initiating the migration. The team invested significant effort upfront to understand traffic patterns, identify all consumers, and establish clear communication channels.</p><p><strong>Understand Client Fan-Out and Traffic Patterns<br></strong>The first step was to gain a complete view of how the databases were being used in production. Using observability tools like CloudWatch metrics, the team analyzed PostgreSQL connection counts, read and write patterns, and overall load characteristics. This helped establish a baseline for normal behavior and ensured there were no unexpected traffic spikes or hidden dependencies that could complicate the migration.</p><p>Just as importantly, this baseline gave the Enablement Applications team a rough idea of the post-migration behavior on Aurora. For example, they expected to see a similar number of active database connections and comparable traffic patterns after cutover, making it easier to validate that the migration had preserved operational characteristics.</p><p><strong>Identify and Enumerate All Database Consumers<br></strong>Unlike most databases, where the set of consumers is well known to the owning team, these databases were accessed by a wide range of internal services and external-facing systems that were not fully enumerated upfront. To address this, we leveraged a tool called flowlogs, an eBPF-based network attribution tooling was used to capture TCP flow data to identify the services and applications establishing connections to the database(<a href=\"https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc\">link</a>).<br> <br>This approach allowed the team to enumerate active consumers, including those that were not previously documented, ensuring no clients were missed during migration planning.</p><p><strong>Establish Dedicated Communication Channels<br></strong>Once all consumers were identified, a dedicated communication channel was created to provide continuous updates throughout the migration process. This channel was used to share timelines, readiness checks, status updates, and cutover notifications, ensuring that all stakeholders remained aligned and could respond quickly if issues arose.</p><h3>Migration Process</h3><p>After completing application-side preparation, the Enablement Applications team initiated the data replication phase of the migration workflow. The automation successfully provisioned the Aurora read replica cluster and ported the RDS PostgreSQL parameter group to a corresponding Aurora parameter group, bringing the destination environment up with equivalent configuration.</p><h4><strong>Unexpected Replication Slot Behavior</strong></h4><p>However, shortly after replication began, we observed that the OldestReplicationSlotLag metric was unexpectedly high. This was counterintuitive, as Aurora read replicas are designed to remain closely synchronized with the source database by continuously streaming write-ahead logs (WAL).</p><p>Further investigation revealed the presence of an inactive logical replication slot on the source RDS PostgreSQL instance. An inactive replication slot can cause elevated OldestReplicationSlotLag because PostgreSQL must retain all WAL records required by the slot’s last known position (restart_lsn), even if no client is actively consuming data from it. Replication slots are intentionally designed to prevent data loss by ensuring that a consumer can resume replication from where it left off. As a result, PostgreSQL will not recycle or delete WAL segments needed by a replication slot until the slot advances. When a slot becomes inactive — such as when a client migration task is stopped or abandoned — the slot’s position no longer moves forward. Meanwhile, the database continues to generate WAL, forcing PostgreSQL to retain increasingly older WAL files. This growing gap between the current WAL position and the slot’s restart_lsn manifests as a high OldestReplicationSlotLag.</p><p>Identifying and addressing these inactive replication slots was a critical prerequisite to proceeding safely with the migration and ensuring accurate replication state during cutover.</p><p><strong>Successful Migration After Remediation<br> </strong>After identifying the inactive logical replication slot, the team safely cleaned it up on the source RDS PostgreSQL instance and resumed the migration workflow. With the stale slot removed, replication progressed as expected, and the Aurora read replica quickly converged with the source. The migration then proceeded smoothly through the quiescence phase, with no unexpected behavior or replication anomalies observed.</p><p>Following promotion, application traffic transitioned seamlessly to the newly writable Aurora PostgreSQL cluster. Through the Data Access Layer, new client connections were automatically routed to Aurora, and observability metrics confirmed healthy behavior — connection counts, read/write patterns, and overall load closely matched pre-migration baselines. From the application and partner perspective, the cutover was transparent, validating both the correctness of the migration workflow and the effectiveness of the preparation steps.</p><h3>Open questions</h3><h4>How do we select target Aurora PostgreSQL instance types based on the existing production RDS PostgreSQL instance?</h4><p>When selecting the target Aurora PostgreSQL instance type for a production migration, our guidance is intentionally conservative. We prioritize stability and performance first, and optimize for cost only after observing real workload behavior on Aurora.</p><p>In practice, the recommended approach is to adopt Graviton2-based instances (particularly the <em>r6g</em> family) whenever possible, maintain the same instance family and size where feasible, and — at minimum — preserve the memory footprint of the existing RDS instance.</p><p>Unlike RDS PostgreSQL, Aurora does not support the <em>m</em>-series, making a direct family match impossible for those instances. In such cases, simply keeping the same “size” (e.g., 2xlarge → 2xlarge) is not meaningful because the memory profiles differ across families. Instead, we map instances by memory equivalence. For example, an Aurora <em>r6g.xlarge</em> provides a memory footprint comparable to an RDS <em>m5.2xlarge</em>, making it a practical replacement. This memory-aligned strategy offers a safer and more predictable baseline for production migrations.</p><h4><strong>Downtime During RDS → Aurora Cutover?</strong></h4><p>To achieve minimal downtime during an RDS PostgreSQL → Aurora PostgreSQL migration, we front-load as much work as possible into the preparation phase. By the time we reach cutover, the Aurora read replica is already provisioned and continuously replicating WAL from the source RDS instance. Before initiating downtime, we ensure that the replication lag between Aurora and RDS has stabilized within an acceptable threshold. If the lag is large or fluctuating significantly, forcing a cutover will only inflate downtime.</p><p>Downtime begins the moment we remove the security groups from the source RDS instance, blocking all inbound traffic. We then reboot the instance to forcibly terminate existing connections, which typically takes up to a minute. From this point forward, no writes can be performed.</p><p>After traffic is halted, the next objective is to verify that Aurora has fully replayed all meaningful WAL records from RDS. We track this using <strong>OldestReplicationSlotLag</strong>. We first wait for the metric to drop to <strong>0</strong>, indicating that Aurora has consumed all WAL with real writes. Under normal idle behavior, PostgreSQL triggers an empty WAL switch every five minutes. After observing one data point at 0, we wait for an additional idle WAL rotation and confirm that the lag oscillates within the expected <strong>0 → 64 MB</strong> pattern — signifying that the only remaining WAL segments are empty ones produced during idle time. At this point, we know the Aurora replica is fully caught up and can be safely promoted.</p><p>While these validation steps run, we perform the configuration updates on the Envoy reverse proxy in parallel. Once promotion completes and Envoy is restarted with the new runtime configuration, all client-initiated connections begin routing to the Aurora cluster. In practice, the total write-downtime observed across services averages <strong>around 10 minutes</strong>, dominated largely by the RDS reboot and the idle WAL switch interval.</p><p><strong>Optimization: Reducing Idle-Time Wait</strong></p><p>For services requiring stricter downtime budgets, waiting the full five minutes for an idle WAL switch can be prohibitively expensive. In such cases, we can force a WAL rotation immediately after traffic is cut off by issuing:</p><p>SELECT pg_switch_wal();</p><p>Once the switch occurs, OldestReplicationSlotLag will drop to 0 again as Aurora consumes the new (empty) WAL segment. This approach eliminates the need to wait for the default archive_timeout interval, which can significantly reduce overall downtime.</p><h4>How do we migrate CDC consumers?</h4><p>As part of the data platform organization in Netflix, we provide a managed Change Data Capture (CDC) service across a variety of datastores. For PostgreSQL, logical replication slots is the way of implementing change data capture. At Netflix, we build a managed abstraction on top of these replication slots called <strong>datamesh</strong> to manage customers who are leveraging them (<a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">link</a>).</p><p>Each logical replication slot tracks a consumer’s position in the write-ahead log (WAL), ensuring that WAL records are retained until the consumer has successfully processed them. This guarantees ordered and reliable delivery of row-level changes to downstream systems. At the same time, it tightly couples the lifecycle of replication slots to database operations, making their management a critical consideration during database migrations.</p><p>A key challenge in migrating from RDS PostgreSQL to Aurora PostgreSQL is transitioning these CDC consumers safely — without data loss, stalled replication, or extended downtime — while ensuring that replication slots are correctly managed throughout the cutover process.</p><p>Each row-level change in PostgreSQL is emitted as a CDC event with an operation type of INSERT, UPDATE, DELETE, or REFRESH. REFRESH events are generated during backfills by querying the database directly and emitting the current state of rows in chunks. Downstream consumers are designed to be idempotent and eventually consistent, allowing them to safely process retries, replays, and backfills.</p><p><strong>Handling Replication Slots During Migration</strong></p><p>Before initiating database cutover, we temporarily pause CDC consumption by stopping the infrastructure responsible for consuming from PostgreSQL replication slots and writing into datamesh source. This also drops the replication slot from the database and cleans up our internal state around replication slot offsets. This essentially resets the state of the connector to one of a brand new one.</p><p>This step is critical for two reasons. First, it prevents replication slots from blocking WAL recycling during migration. Second, it ensures that no CDC consumers are left pointing at the source database once traffic is quiesced and cutover begins. While CDC consumers are paused, downstream systems temporarily stop receiving new change events, but remain stable. Once CDC consumers are paused, we proceed with stopping other client traffic and executing the RDS-to-Aurora cutover.</p><p><strong>Reinitializing CDC After Cutover</strong></p><p>After the Aurora PostgreSQL cluster has been promoted and traffic has been redirected, CDC consumers are reconfigured to point to the Aurora endpoint and restarted. Because their previous state was intentionally cleared, consumers initialize as if they are starting fresh.</p><p>On startup, new logical replication slots are created on Aurora, and a full backfill is performed by querying the database and emitting REFRESH events for all existing rows. These events let the consumer know that a manual refresh was done from Aurora and to treat this as an upsert operation. This establishes a clean and consistent baseline from which ongoing CDC can resume. Consumers are expected to handle these refresh events correctly as part of normal operation.</p><p>By explicitly managing PostgreSQL replication slots as part of the migration workflow, we are able to migrate CDC consumers safely and predictably, without leaving behind stalled slots, retained WAL, or consumers pointing to the wrong database. This approach allows CDC pipelines to be cleanly re-established on Aurora while preserving correctness and operational simplicity.</p><h4>How do we roll back in the middle of the process?</h4><p><strong>Pre-quiescence<br></strong>Rolling back before the pre-quienscence phase is quite easy. Your primary RDS database is still the source. Rolling back before the quiescence phase is straightforward. At this stage, the primary RDS PostgreSQL instance continues to serve as the sole source of truth, and no client traffic has been redirected.</p><p>If a rollback is required, the migration can be safely aborted by deleting the newly created Aurora PostgreSQL cluster along with its associated parameter groups. No changes are needed on the application side, and normal operations on RDS PostgreSQL can continue without impact.</p><p><strong>During-quiescence<br></strong>Rolling back during the quiescence phase is more involved. At this point, client traffic to the source RDS PostgreSQL instance has already been stopped by detaching its security groups. To roll back safely, access must first be restored by reattaching the original security groups to the RDS instance, allowing client connections to resume. In addition, any logical replication slots removed during the migration must be recreated so that CDC consumers can continue processing changes from the source database.</p><p>Once connectivity and replication slots are restored, the RDS PostgreSQL instance can safely resume its role as the primary source of truth.</p><p><strong>Post-quiescence <br></strong>Rolling back after cutover, once the Aurora PostgreSQL cluster is serving production traffic, is significantly more complex. At this stage, Aurora has become the primary source of truth, and client applications may already have written new data to it.</p><p>In this scenario, rollback requires setting up replication in the opposite direction, with Aurora as the source and RDS PostgreSQL as the destination. This can be achieved using a service such as AWS Database Migration Service (DMS). AWS provides detailed guidance for setting up this reverse replication flow, which can be followed to migrate data back to RDS if necessary.</p><h3>Conclusion</h3><p>Standardizing and reducing the surface area of data technologies is crucial for any large-scale platform. For the Netflix platform team, this strategy allows us to concentrate engineering effort, deliver deeper value on a smaller set of well-understood systems, and significantly cut the operational overhead of running multiple database technologies that serve similar purposes. Within the relational database ecosystem, Aurora PostgreSQL has become the paved-path datastore — offering strong scalability, resilience, and consistent operational patterns across the fleet.</p><p>Migrations of this scale demand solutions that are reliable, low-touch, and minimally disruptive for service owners. Our automated RDS PostgreSQL → Aurora PostgreSQL workflow represents a major step forward, providing predictable cutovers, strong correctness guarantees, and a migration experience that works uniformly across diverse workloads.</p><p>As we continue this journey, the Relational Data Platform team is building higher-level abstractions and capabilities on top of Aurora, enabling service owners to focus less on the complexities of database internals and more on delivering product value. More to come — stay tuned.</p><h3>Acknowledgements</h3><p>Special thanks to our other stunning colleagues/customers who contributed to the success of the RDS PostgreSQL to Aurora PostgreSQL migration. <a href=\"https://www.linkedin.com/in/sumanth-pasupuleti\">Sumanth Pasupuleti</a>, <a href=\"https://www.linkedin.com/in/coleaperez\">Cole Perez</a>, <a href=\"https://www.linkedin.com/in/akhaku\">Ammar Khaku</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=261ca045447f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f\">Automating RDS Postgres to Aurora Postgres Migration</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-02-12T14:07:19.000Z",
    "url": "https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "The AI Evolution of Graph Search at Netflix",
    "partialText": "<h3>The AI Evolution of Graph Search at Netflix: From Structured Queries to Natural Language</h3><p>By <a href=\"https://www.linkedin.com/in/ahutter/\">Alex Hutter</a> and <a href=\"https://www.linkedin.com/in/bartosz-balukiewicz/\">Bartosz Balukiewicz</a></p><p>Our previous blog posts (<a href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf\">part 1</a>, <a href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c\">part 2</a>, <a href=\"https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576\">part 3</a>) detailed how Netflix’s Graph Search platform addresses the challenges of searching across federated data sets within Netflix’s enterprise ecosystem. Although highly scalable and easy to configure, it still relies on a structured query language for input. Natural language based search has been possible for some time, but the level of effort required was high. The emergence of readily-available AI, specifically Large Language Models (LLMs), has created new opportunities to integrate AI search features, with a smaller investment and improved accuracy.</p><p>While Text-to-Query and Text-to-SQL are established problems, the complexity of distributed Graph Search data in the GraphQL ecosystem necessitates innovative solutions. This is the first in a three-part series where we will detail our journey: how we implemented these solutions, evaluated their performance, and ultimately evolved them into a self-managed platform.</p><h3>The Need for Intuitive Search: Addressing Business and Product Demands</h3><p>Natural language search is the ability to use everyday language to retrieve information as opposed to complex, structured query languages like the Graph Search Filter Domain Specific Language (DSL). When users interact with 100’s of various UIs within the suite of Content and Business Products applications, a frequent task is filtering a data table like the one below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WszSpPjY6IOWZU_lf2qTHg.png\" /><figcaption>Example Content and Business Products application view</figcaption></figure><p>Ideally, a user simply wants to satisfy a query like <strong>“I want to see all movies from the 90s about robots from the US.”</strong> Because the underlying platform operates on the Graph Search Filter DSL, the application acts as an intermediary. Users input their requirements through UI elements — toggling facets or using query builders — and the system programmatically converts these interactions into a valid DSL query to filter the data.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*vCXVNhRXGbjLjseNY2vmlQ.png\" /><figcaption>The Complexity of filtering and DSL generation</figcaption></figure><p>This process presents a few issues.</p><p>Today, many applications have bespoke components for collecting user input — the experience varies across them and they have inconsistent support for the DSL. Users need to “learn” how to use each application to achieve their goals.</p><p>Additionally, some domains have hundreds of fields in an index that could be faceted or filtered by. A <em>subject matter expert </em>(SME) may know exactly what they want to accomplish, but be bottlenecked by the inefficient pace of filling out a large scale UI form and translating their questions in order to encode it in a representation Graph Search needs.</p><p>Most importantly, users think and operate using natural language, not technical constructs like query builders, components, or DSLs. By requiring them to switch contexts, we introduce friction that slows them down or even prevents their progress.</p><p>With readily-available AI components, our users can now interact with our systems through natural language. The challenge now is to make sure our offering, searching Netflix’s complex enterprise state with natural language, is an intuitive and trustworthy experience.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*EklLSPYRp9yrPP3wLW5qfg.png\" /><figcaption>Natural language queries translated into Graph Search Filter DSL</figcaption></figure><p>We’ve made a decision to pursue generating Graph Search Filter statements from natural language to meet this need. Our intention is to augment and not replace existing applications with <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\">retrieval augmented generation</a> (RAG), providing tooling and capabilities so that applications in our ecosystem have newly accessible means of processing and presenting their data in their distinct domain flavours. It should be noted that all the work here has direct application to building a RAG system on top of Graph Search in the future.</p><h3>Under the Hood: Our Approach to Text-to-Query</h3><p>The core function of the text-to-query process is converting a user’s (often ambiguous) natural language question into a structured query. We primarily achieve this through the use of an LLM.</p><p>Before we dive deeper, let’s quickly revisit the structure of Graph Search Filter DSL. Each Graph Search index is <a href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf#:~:text=of%20configuration%20required.-,Configuration,-For%20collecting%20the\">defined by a GraphQL query</a>, made up of a collection of fields. Each field has a type e.g. boolean, string, and some have their permitted values governed by controlled vocabularies — a standardized and governed list of values (like an enumeration, or a foreign key). The names of those fields can be used to construct expressions using comparison (e.g. &gt; or ==) or inclusion/exclusion operators (e.g. IN). In turn those expressions can be combined using logical operators (e.g. AND) to construct complex statements.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/615/1*UbP1eIXDDqt5Q8nlCdqIKQ.png\" /><figcaption>Graph Search Filter DSL</figcaption></figure><p>With that understanding, we can now more rigorously define the conversion process. We need the LLM to generate a Graph Search Filter DSL statement that is syntactically, semantically, and pragmatically correct.</p><p><strong>Syntactic correctness</strong> is easy — does it parse? To be syntactically correct, the generated statement must be well formed<strong> </strong>i.e. follow the grammar of the Graph Search Filter DSL.</p><p><strong>Semantic correctness </strong>adds some additional complexity as it requires more knowledge of the index itself. To be semantically correct:</p><ul><li>it must respect the field types i.e. only use comparisons that make sense given the underlying type;</li><li>it must only use fields that are actually present in the index, i.e. does not <em>hallucinate;</em></li><li>when the values of a field are constrained to a controlled vocabulary, any comparison must only use values from that controlled vocabulary.</li></ul><p><strong>Pragmatic correctness</strong> is much more difficult. It asks the question: does the generated filter actually capture the intent of the user’s query?</p><p>The following sections will detail how we pre-process the user’s question to create appropriate context for the instructions that we will provide to the LLM — both of <a href=\"https://developers.google.com/machine-learning/resources/intro-llms\">which are fundamental to LLM interaction</a> — as well as post-processing we perform on the generated statement to validate it, and help users understand and trust the results they receive.</p><p>At a high level that process looks like this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/284/1*s34kKg5FI3TDd6UeXaQWOA.png\" /><figcaption>Graph Search FIlter DSL generation process</figcaption></figure><h3>Context Engineering</h3><p>Preparation for the filter generation task is predominantly engineering the appropriate context. The LLM will need access to the fields of an index and their metadata in order to construct semantically correct filters. As the indices are defined by GraphQL queries, we can use the type information from the GraphQL schema to derive much of the required information. For some fields, there is additional information we can provide beyond what’s available in the schema as well, in particular permissible values that pull from controlled vocabularies.</p><p>Each field in the index is associated with metadata as seen below, and that metadata is provided as part of the context.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/655/1*Z34Ek0ib34dd600ZH9iT0Q.png\" /><figcaption>Graph Search index representation</figcaption></figure><ul><li>The <strong>field</strong> is derived from the document path as characterized by the GraphQL query.</li><li>The <strong>description</strong> is the comment from the GraphQL schema for the field.</li><li>The <strong>type</strong> is derived from the GraphQL schema for the field e.g. Boolean, String, enum. We also support an additional controlled vocabulary type we will discuss more of shortly.</li><li>The <strong>valid values</strong> are derived from enum values for the enum type or from a controlled vocabulary as we will now discuss.</li></ul><p>A <em>controlled vocabulary</em> is a specific field type that consists of a finite set of allowed values, which are defined by a SMEs or domain owners. Index fields can be associated with a particular controlled vocabulary, e.g. countries with members such as Spain and Thailand, and any usage of that field within a generated statement must refer to values from that vocabulary.</p><p>Naively providing all the metadata as context to the LLM worked for simple cases but did not scale. Some indices have hundreds of fields and some controlled vocabularies have thousands of valid values. Providing all of those, especially the controlled vocabulary values and their accompanying metadata, expands the context; this proportionally increases latency and decreases the correctness of generated filter statements. Not providing the values wasn’t an option as we needed to ground the LLMs generated statements- without them, the LLM would frequently hallucinate values that did not exist.</p><p>Curating the context to an appropriate subset was a problem we addressed using the well known RAG pattern.</p><h4>Field RAG</h4><p>As mentioned previously, some indices have hundreds of fields, however, most user’s questions typically refer only to a handful of them. If there was no cost in including them all, we would, but as mentioned prior, there is a cost in terms of the latency of query generation as well as the correctness of the generated query (e.g. needle-in-the-hackstack problem) and non-deterministic results.</p><p>To determine which subset of fields to include in the context, we “match” them against the intent of the user’s question.</p><ul><li>Embeddings are created for index fields and their metadata (name, description, type) and are indexed in a vector store</li><li>At filter generation time, the user’s question is chunked with an overlapping strategy. For each chunk, we perform a vector search to identify the top K most relevant values and the fields to which they belong.</li><li><strong>Deduplication:</strong> The top K fields from each chunk are both consolidated and deduplicated before being provided as context to the system instructions.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/599/1*_fkaUvg0ljRYJnws1xu_Rw.png\" /><figcaption>Field RAG process (chunking, merge, deduplicate)</figcaption></figure><h4>Controlled Vocabularies RAG</h4><p>Index fields of the controlled vocabulary type are associated with a particular controlled vocabulary, again, countries are one example. Given a user’s question, we can infer whether or not it refers to values of a particular controlled vocabulary. In turn, by knowing which controlled vocabulary values are present, we can identify additional, related index fields that should be included in the context that may not have been identified by the field RAG step.</p><p>Each controlled vocabulary value has:</p><ul><li>a unique<strong> identifier</strong> within its type;</li><li>a human readable <strong>display name;</strong></li><li>a <strong>description</strong> of the value;</li><li>also-known-as values or <strong>AKA</strong> display names, e.g. “romcom” for “Romantic Comedy”.</li></ul><p>To determine which subset of values to include in the context for controlled vocabulary fields (and also possibly infer additional fields), we “match” them against the user’s question.</p><ul><li>Embeddings are created for controlled vocabulary values and their metadata, and these are indexed in a vector store. The controlled vocabularies are available via GraphQL and are regularly fetched and reindexed so this system stays up to date with any changes in the domain.</li><li>At filter generation time, the user’s question is chunked. For each chunk, we perform a vector search to identify the top K most relevant values (but only for the controlled vocabularies that are associated with fields in the index)</li><li>The top K values from each chunk are deduplicated by their controlled vocabulary type. The associated field definition is then injected into the context along with the matched values.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/461/1*N70dw5GEqNDVDYPmpRmqUQ.png\" /><figcaption>Controlled Vocabularies RAG</figcaption></figure><p><strong>Combining both approaches, the RAG of fields and controlled vocabularies, we end up with the solution that each input question resolves in available and matched fields and values:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/706/1*hIALZVLkkOjZ5ePgsdVWIA.png\" /><figcaption>Field and CV RAG</figcaption></figure><p>The quality of results generated by the RAG tool can be significantly enhanced by tuning its various parameters, or “levers.” These include strategies for reranking, chunking, and the selection of different embedding generation models. The careful and systematic evaluation of these factors will be the focus of the subsequent parts of this series.</p><h3>The Instructions</h3><p>Once the context is constructed, it is provided to the LLM with a set of instructions and the user’s question. The instructions can be summarised as follows: <strong>“<em>Given a natural language question, generate a syntactically, semantically, and pragmatically correct filter statement given the availability of the following index fields and their metadata</em>.”</strong></p><ul><li>In order to generate a <em>syntactically</em> correct filter statement, the instructions include the syntax rules of the DSL.</li><li>In order to generate a <em>semantically</em> correct filter statement, the instructions tell the LLM to ground the generated statement in the provided context.</li><li>In order to generate a <em>pragmatically</em> correct filter statement, so far we focus on better context engineering to ensure that only the most relevant fields and values are provided. We haven’t identified any instructions that make the LLM just “do better” at this aspect of the task.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/893/1*gCfHD-yVJhpcj6zPYRhzqQ.png\" /><figcaption>Graph Search Filter DSL generation</figcaption></figure><p>After the filter statement is generated by the LLM, we deterministically validate it prior to returning the values to the user.</p><h3>Validation</h3><h4>Syntactic Correctness</h4><p>Syntactic correctness ensures the LLM output is a parsable filter statement. We utilize an Abstract Syntax Tree (AST) parser built for our custom DSL. If the generated string fails to parse into a valid AST, we know immediately that the query is malformed and there is a fundamental issue with the generation.</p><p>The other approach to solve this problem could be using the <a href=\"https://platform.openai.com/docs/guides/structured-outputs\">structured outputs</a> modes provided by some LLMs. However, our initial evaluation yielded mixed results, as the custom DSL is not natively supported and requires further work.</p><h4>Semantic Correctness</h4><p>Despite careful context engineering using the RAG pattern, the LLM sometimes hallucinates both fields and available values in the generated filter statement. The most straightforward way of preventing this phenomenon is validating the generated filters against available index metadata. This approach does not impact the overall latency of the system, as we are already working with an AST of the filter statement, and the metadata is freely available from the context engineering stage.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/553/1*0LUUAK1G7CtwFDBZsAjgDA.png\" /><figcaption>DSL verification &amp; hallucinations</figcaption></figure><p>If a hallucination is detected it can be returned as an error to a user, indicating the need to refine the query, or can be provided back to the LLM in the form of a feedback loop for self correction.</p><p>This increases the filter generation time, so should be used cautiously with a limited number of retries.</p><h3>Building Confidence</h3><p>You probably noticed we are not validating the generated filter for pragmatic correctness. That task is the hardest challenge: The filter parses (<em>syntactic</em>) and uses real fields (<em>semantic</em>), but is it what the user meant? When a user searches for <strong>“Dark”</strong>, do they mean <strong>the specific German sci-fi series <em>Dark</em>, </strong>or are they browsing for the mood category<strong> “dark TV shows”</strong>?</p><p>The gap between what a user intended and the generated filter statement is often caused by ambiguity. Ambiguity stems from the <a href=\"https://en.wikipedia.org/wiki/Semantic_compression\">compression of natural language</a>. A user says <strong>“German time-travel mystery with the missing boy and the cave”</strong> but the index contains <strong>discrete metadata fields</strong> like <strong>releaseYear</strong>, <strong>genreTags</strong>, and <strong>synopsisKeywords</strong>.</p><p>How do we ensure users aren’t inadvertently led to wrong answers or to answers for questions they didn’t ask?</p><h4>Showing Our Work</h4><p>One way we are handling ambiguity is by <em>showing our work</em>. We visualise the generated filters in the UI in a user-friendly way allowing them to very clearly see if the answer we’re returning is what they were looking for so they can trust the results..</p><p>We cannot show a raw DSL string (e.g., <em>origin.country == ‘Germany’ AND genre.tags CONTAINS ‘Time Travel’ AND synopsisKeywords LIKE ‘*cave*’</em>) to a non-technical user. Instead, we reflect its underlying AST into UI components.</p><p>After the LLM generates a filter statement, we parse it into an AST, and then map that AST to the existing “Chips” and “Facets” in our UI (see below). If the LLM generates a filter for <em>origin.country == ‘Germany’</em>, the user sees the “Country” dropdown pre-selected to “Germany.” This gives users immediate visual feedback and the ability to easily fine-tune the query using standard UI controls when the results need improvement or further experimentation.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_Gx0THjlWW9_jwb8KwCrYw.png\" /><figcaption>Generated filters visualisation</figcaption></figure><h4>Explicit Entity Selection</h4><p>Another strategy we’ve developed to remove ambiguity happens at query time. We give users the ability to constrain their input to refer to known entities using “@mentions”. Similar to Slack, typing @ lets them search for entities directly from our specialized UI Graph Search component, giving them easy access to multiple controlled vocabularies (plus other identifying metadata like launch year) to feel confident they’re choosing the entity they intend.</p><p>If a user types, “When was <em>@dark</em> produced”, we explicitly know they are referring to the <em>Series</em> controlled vocabulary, allowing us to bypass the RAG inference step and hard-code that context, significantly increasing pragmatic correctness (and building user trust in the process).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*u3YrDuaYJr4LlVB1286Xzg.png\" /><figcaption>Example @mentions usage in the UI</figcaption></figure><h3>End-to-end architecture</h3><p>As mentioned previously, the solution architecture is divided into <em>pre-processing</em>, filter statement generation, and then <em>post-processing</em> stages. The pre-processing handles context building and involves a RAG pattern for similarity search, while the post-processing validation stage checks the correctness of the LLM-generated filter statements and provides visibility into the results for end users. This design strategically balances LLM involvement with more deterministic strategies.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lw47a7h67i4EUmvhlRKcYQ.png\" /><figcaption>End-to-end architecture</figcaption></figure><p>The end-to-end process is as follows:</p><ol><li>A user’s natural language question (with optional `@mentions` statements) are provided as input, along with the Graph Search index context</li><li>The context is scoped by using the RAG pattern on both fields and possible values</li><li>The pre-processed context and the question are fed into the LLM with an instruction asking for<em> a syntactically and semantically correct filter statement</em></li><li>The generated filer statement DSL is verified and checked for hallucinations</li><li>The final response contains the related AST in order to build “Chips” and “Facets”</li></ol><h3>Summary</h3><p>By combining our existing Graph Search infrastructure with the power and flexibility of LLMs, we’ve bridged the gap between complex filter statements and user intent. We moved from requiring users to speak our language (DSL) to our systems understanding theirs.</p><p>The initial challenge for our users was successfully addressed. However, our next steps involve transforming this system into a comprehensive and expandable platform, rigorously evaluating its performance in a live production environment, and expanding its capabilities to support GraphQL-first user interfaces. These topics, and others, will be the focus of the subsequent installments in this series. Be sure to follow along!</p><p>You may have noticed that we have a lot more to do on this project, including named entity recognition and extraction, intent detection so we can route questions to the appropriate indices, and query rewriting among others. If this kind of work interests you, reach out! We’re hiring in our Warsaw office, check for open roles <a href=\"https://explore.jobs.netflix.net/careers?location=Warsaw%2C%20Masovian%20Voivodeship%2C%20Poland&amp;pid=790302168096&amp;domain=netflix.com&amp;sort_by=relevance&amp;triggerGoButton=false\">here</a>.</p><h3>Credits</h3><p>Special thanks to <a href=\"https://www.linkedin.com/in/quesadaalejandro/\">Alejandro Quesada</a>, <a href=\"https://www.linkedin.com/in/yevgeniya-li-9877ba160/\">Yevgeniya Li</a>, <a href=\"https://www.linkedin.com/in/dkyrii/\">Dmytro Kyrii</a>, <a href=\"https://www.linkedin.com/in/razvan-gabriel-gatea/\">Razvan-Gabriel Gatea</a>, <a href=\"https://www.linkedin.com/in/milodorif/\">Orif Milod</a>, <a href=\"https://www.linkedin.com/in/michal-krol-45973411a/\">Michal Krol</a>, <a href=\"https://www.linkedin.com/in/jeffbalis/\">Jeff Balis</a>, <a href=\"https://www.linkedin.com/in/czhao/\">Charles Zhao</a>, <a href=\"https://www.linkedin.com/in/shilpamotukuri/\">Shilpa Motukuri</a>, <a href=\"https://www.linkedin.com/in/shervineamidi/\">Shervine Amidi</a>, <a href=\"https://www.linkedin.com/in/aborysov/\">Alex Borysov</a>, <a href=\"https://www.linkedin.com/in/mike-azar-7064883b/\">Mike Azar</a>, <a href=\"https://www.linkedin.com/in/bernardo-g-4414b41/\">Bernardo Gomez Palacio</a>, <a href=\"https://www.linkedin.com/in/haoyuan-h-98b587134/\">Haoyuan He</a>, <a href=\"https://www.linkedin.com/in/edyr96/\">Eduardo Ramirez</a>, <a href=\"https://www.linkedin.com/in/yujiaxie2019/\">Cynthia Xie</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d416ec5b1151\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151\">The AI Evolution of Graph Search at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2026-01-26T19:01:27.000Z",
    "url": "https://netflixtechblog.com/the-ai-evolution-of-graph-search-at-netflix-d416ec5b1151?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "How Temporal Powers Reliable Cloud Operations at Netflix",
    "partialText": "<p>By <a href=\"https://www.linkedin.com/in/jacobmeyers35/\">Jacob Meyers</a> and <a href=\"https://www.linkedin.com/in/robzienert/\">Rob Zienert</a></p><p><a href=\"https://temporal.io/\">Temporal</a> is a <a href=\"https://docs.temporal.io/evaluate/understanding-temporal#durable-execution\">Durable Execution</a> platform which allows you to write code “as if failures don’t exist”. It’s become increasingly critical to Netflix since its initial adoption in 2021, with users ranging from the operators of our <a href=\"https://about.netflix.com/en/news/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience\">Open Connect</a> global CDN to our <a href=\"https://medium.com/netflix-techblog/behind-the-streams-live-at-netflix-part-1-d23f917c2f40\">Live</a> reliability teams now depending on Temporal to operate their business-critical services. In this post, I’ll give a high-level overview of what Temporal offers users, the problems we were experiencing operating Spinnaker that motivated its initial adoption at Netflix, and how Temporal helped us reduce the number of transient deployment failures at Netflix from <strong>4% to 0.0001%</strong>.</p><h3>A Crash Course on (some of) Spinnaker</h3><p><a href=\"https://netflixtechblog.com/global-continuous-delivery-with-spinnaker-2a6896c23ba7\">Spinnaker</a> is a multi-cloud continuous delivery platform that powers the vast majority of Netflix’s software deployments. It’s composed of several (mostly nautical themed) microservices. Let’s double-click on two in particular to understand the problems we were facing that led us to adopting Temporal.</p><p>In case you’re completely new to Spinnaker, Spinnaker’s fundamental tool for deployments is the <em>Pipeline</em>. A Pipeline is composed of a sequence of steps called <em>Stages</em>, which themselves can be decomposed into one or more <em>Tasks</em>, or other Stages. An example deployment pipeline for a production service may consist of these stages: Find Image -&gt; Run Smoke Tests -&gt; Run Canary -&gt; Deploy to us-east-2 -&gt; Wait -&gt; Deploy to us-east-1.</p><figure><img alt=\"An example Spinnaker Pipeline\" src=\"https://cdn-images-1.medium.com/max/1024/1*7sGhc8LhyqQlW9Uiq76TWQ.png\" /><figcaption>An example Spinnaker Pipeline for a Netflix service</figcaption></figure><p>Pipeline configuration is extremely flexible. You can have Stages run completely serially, one after another, or you can have a mix of concurrent and serial Stages. Stages can also be executed conditionally based on the result of previous stages. This brings us to our first Spinnaker service: <em>Orca</em>. Orca is the <a href=\"https://raw.githubusercontent.com/spinnaker/orca/refs/heads/master/logo.jpg\">orca-stration</a> engine of Spinnaker. It’s responsible for managing the execution of the Stages and Tasks that a Pipeline unrolls into and coordinating with other Spinnaker services to actually execute them.</p><p>One of those collaborating services is called <em>Clouddriver</em>. In the example Pipeline above, some of the Stages will require interfacing with cloud infrastructure. For example, the canary deployment involves creating ephemeral hosts to run an experiment, and a full deployment of a new version of the service may involve spinning up new servers and then tearing down the old ones. We call these sorts of operations that mutate cloud infrastructure <em>Cloud Operations</em>. Clouddriver’s job is to decompose and execute Cloud Operations sent to it by Orca as part of a deployment. Cloud Operations sent from Orca to Clouddriver are relatively high level (for example: createServerGroup), so Clouddriver understands how to translate these into lower-level cloud provider API calls.</p><p>Pain points in the interaction between Orca and Clouddriver and the implementation details of Cloud Operation execution in Clouddriver are what led us to look for new solutions and ultimately migrate to Temporal, so we’ll next look at the anatomy of a Cloud Operation. Cloud Operations in the OSS version of Spinnaker still work as described below, so motivated readers can follow along in <a href=\"https://github.com/spinnaker/clouddriver\">source code</a>, however our migration to Temporal is entirely closed-source following a fork from OSS in 2020 to allow Netflix to make larger pivots to the product such as this one.</p><h4><strong>The Original Cloud Operation Flow</strong></h4><p>A Cloud Operation’s execution goes something like this:</p><ol><li>Orca, in orchestrating a Pipeline execution, decides a particular Cloud Operation needs to be performed. It sends a POST request to Clouddriver’s /ops endpoint with an untyped bag-of-fields.</li><li>Clouddriver attempts to resolve the operation Orca sent into a set of AtomicOperation s— internal operations that only Clouddriver understands.</li><li>If the payload was valid and Clouddriver successfully resolved the operation, it will immediately return a Task ID to Orca.</li><li>Orca will immediately begin polling Clouddriver’s GET /task/&lt;id&gt; endpoint to keep track of the status of the Cloud Operation.</li><li>Asynchronously, Clouddriver begins executing AtomicOperations using <em>its own</em> internal orchestration engine. Ultimately, the AtomicOperations resolve into cloud provider API calls. As the Cloud Operation progresses, Clouddriver updates an internal state store to surface progress to Orca.</li><li>Eventually, if all went well, Clouddriver will mark the Cloud Operation complete, which eventually surfaces to Orca in its polling. Orca considers the Cloud Operation finished, and the deployment can progress.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y57y00EsM2YGRph9IRNmLQ.png\" /><figcaption>A sequence diagram of a Cloud Operation execution</figcaption></figure><p>This works well enough on the happy path, but veer off the happy path and dragons begin to emerge:</p><ol><li>Clouddriver has its own internal orchestration system independent of Orca to allow Orca to query the progress of Cloud Operation. This is largely undifferentiated lifting relative to Clouddriver’s goal of actuating cloud infrastructure changes, and ultimately adds complexity and surface area for bugs to the application. Additionally, Orca is tightly coupled to Clouddriver’s orchestration system — it must understand how to poll Clouddriver, interpret the status, and handle errors returned by Clouddriver.</li><li>Distributed systems are messy — networks and external services are unreliable. While executing a Cloud Operation, Clouddriver could experience transient network issues, or the cloud provider it’s attempting to call into may be having an outage, or any number of issues in between. Despite all of this, Clouddriver must be as reliable as reasonably possible as a core platform service. To deal with this shape of issue, Clouddriver internally evolved complex retry logic, further adding cognitive complexity to the system.</li><li>Remember how a Cloud Operation gets decomposed by Clouddriver into AtomicOperations? Sometimes, if there’s a failure in the middle of a Cloud Operation, we need to be able to roll back what was done in AtomicOperations prior to the failure. This led to a homegrown Saga framework being implemented inside Clouddriver. While this did result in a big step forward in reliability of Cloud Operations facing transient failures because the Saga framework <em>also</em> allowed replaying partially-failed Cloud Operations, it added yet more undifferentiated lifting inside the service.</li><li>The task state kept by Clouddriver was <em>instance-local</em>. In other words, if the Clouddriver instance carrying out a Cloud Operation crashed, that Cloud Operation state was lost, and Orca would eventually time out polling for the task status. The Saga implementation mentioned above mitigated this for certain operations, but was not widely adopted across all cloud providers supported by Spinnaker.</li></ol><p>We introduced a <em>lot</em> of incidental complexity into Clouddriver in an effort to keep Cloud Operation execution reliable, and despite all this deployments still failed around 4% of the time due to transient Cloud Operation failures.</p><p>Now, I can already hear you saying: “So what? Can’t people re-try their deployments if they fail?” While true, some pipelines take <em>days</em> to complete for complex deployments, and a failed Cloud Operation mid-way through requires re-running the <em>whole</em> thing. This was detrimental to engineering productivity at Netflix in a non-trivial way. Rather than continue trying to build a faster horse, we began to look elsewhere for our reliable orchestration requirements, which is where Temporal comes in.</p><h3>Temporal: Basic Concepts</h3><p>Temporal is an open source product that offers a durable execution platform for your applications. Durable execution means that the platform will ensure your programs run to completion despite adverse conditions. With Temporal, you organize your business logic into <em>Workflows</em>, which are a deterministic series of steps. The steps inside of Workflows are called <em>Activities</em>, which is where you encapsulate all your non-deterministic logic that needs to happen in the course of executing your Workflows. As your Workflows execute in processes called <em>Workers</em>, the Temporal server durably stores their execution state so that in the event of failures your Workflows can be retried or even migrated to a different Worker. This makes Workflows incredibly resilient to the sorts of transient failures Clouddriver was susceptible to. Here’s a simple example Workflow in Java that runs an Activity to send an email once every 30 days:</p><pre>@WorkflowInterface<br>public interface SleepForDaysWorkflow {<br>    @WorkflowMethod<br>    void run();<br>}<br><br>public class SleepForDaysWorkflowImpl implements SleepForDaysWorkflow {<br><br>    private final SendEmailActivities emailActivities =<br>            Workflow.newActivityStub(<br>                    SendEmailActivities.class,<br>                    ActivityOptions.newBuilder()<br>                            .setStartToCloseTimeout(Duration.ofSeconds(10))<br>                            .build());<br><br>    @Override<br>    public void run() {<br>        while (true) {<br>            // Activities already carry retries/timeouts via options.<br>            emailActivities.sendEmail();<br><br>            // Pause the workflow for 30 days before sending the next email.<br>            Workflow.sleep(Duration.ofDays(30));<br>        }<br>    }<br>}<br><br>@ActivityInterface<br>public interface SendEmailActivities {<br>    void sendEmail();<br>}</pre><p>There’s some interesting things to note about this Workflow:</p><ol><li>Workflows and Activities are just code, so you can test them using the same techniques and processes as the rest of your codebase.</li><li>Activities are automatically retried by Temporal with configurable exponential backoff.</li><li>Temporal manages all the execution state of the Workflow, including timers (like the one used by Workflow.sleep). If the Worker executing this workflow were to have its power cable unplugged, Temporal would ensure another Worker continues to execute it (even during the 30 day sleep).</li><li>Workflow sleeps are not compute-intensive, and they don’t tie up the process.</li></ol><p>You might already begin to see how Temporal solves a lot of the problems we had with Clouddriver. Ultimately, we decided to pull the trigger on migrating Cloud Operation execution to Temporal.</p><h3>Cloud Operations with Temporal</h3><p>Today, we execute Cloud Operations as Temporal workflows. Here’s what that looks like.</p><ol><li>Orca, using a Temporal client, sends a request to Temporal to execute an UntypedCloudOperationRunner Workflow. The contract of the Workflow looks something like this:</li></ol><pre>@WorkflowInterface<br>interface UntypedCloudOperationRunner {<br>  /**<br>   * Runs a cloud operation given an untyped payload.<br>   *<br>   * WorkflowResult is a thin wrapper around OutputType providing a standard contract for<br>   * clients to determine if the CloudOperation was successful and fetching any errors.<br>   */<br>  @WorkflowMethod<br>  fun &lt;OutputType : CloudOperationOutput&gt; run(stageContext: Map&lt;String, Any?&gt;, operationType: String): WorkflowResult&lt;OutputType&gt;<br>}</pre><p>2. The Clouddriver Temporal worker is constantly polling Temporal for work. A worker will eventually see a task for an UntypedCloudOperationRunner Workflow and start executing it.</p><p>3. Similar to before with resolution into AtomicOperations, Clouddriver does some pre-processing of the bag-of-fields in stageContext and resolves it to a strongly typed implementation of the CloudOperation Workflow interface based on the operationType input and the stageContext:</p><pre>interface CloudOperation&lt;I : CloudOperationInput, O : CloudOperationOutput&gt; {<br>  @WorkflowMethod<br>  fun operate(input: I, credentials: AccountCredentials&lt;out Any&gt;): O<br>}</pre><p>4. Clouddriver starts a <a href=\"https://docs.temporal.io/child-workflows\">Child Workflow</a> execution of the CloudOperation implementation it resolved. The child workflow will execute Activities which handle the actual cloud provider API calls to mutate infrastructure.</p><p>5. Orca uses its Temporal Client to await completion of the UntypedCloudOperationRunner Workflow. Once it’s complete, Temporal notifies the client and sends the result and Orca can continue progressing the deployment.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*leM3bH8iyb65_cmtl3vm4A.png\" /><figcaption>Sequence diagram of a Cloud Operation execution with Temporal</figcaption></figure><h3>Results and Lessons Learned from the Migration</h3><p>A shiny new architecture is great, but equally important is the non-glamorous work of refactoring legacy systems to fit the new architecture. How did we integrate Temporal into critical dependencies of all Netflix engineers transparently?</p><p>The answer, of course, is a combination of abstraction and dynamic configuration. We built a CloudOperationRunner interface in Orca to encapsulate whether the Cloud Operation was being executed via the legacy path or Temporal. At runtime, <a href=\"https://netflixtechblog.com/announcing-archaius-dynamic-properties-in-the-cloud-bc8c51faf675\">Fast Properties</a> (Netflix’s dynamic configuration system) determined which path a stage that needed to execute a Cloud Operation would take. We could set these properties quite granularly — by Stage type, cloud provider account, Spinnaker application, Cloud Operation type (createServerGroup), and cloud provider (either AWS or <a href=\"https://netflix.github.io/titus/\">Titus</a> in our case). The Spinnaker services themselves were the first to be deployed using Temporal, and within two quarters, all applications at Netflix were onboarded.</p><h4>Impact</h4><p>What did we have to show for it all? With Temporal as the orchestration engine for Cloud Operations, the percentage of deployments that failed due to transient Cloud Operation failures dropped from 4% to 0.0001%. For those keeping track at home, that’s a four and a half order of magnitude reduction. Virtually eliminating this failure mode for deployments was a huge win for developer productivity, especially for teams with long and complex deployment pipelines.</p><p>Beyond the improvement in deployment success metrics, we saw a number of other benefits:</p><ol><li>Orca no longer needs to directly communicate with Clouddriver to start Cloud Operations or poll their status with Temporal as the intermediary. The services are less coupled, which is a win for maintainability.</li><li>Speaking of maintainability, with Temporal doing the heavy lifting of orchestration and retries inside of Clouddriver, we got to remove a lot of the homegrown logic we’d built up over the years for the same purpose.</li><li>Since Temporal manages execution state, Clouddriver instances became stateless and Cloud Operation execution can bounce between instances with impunity. We can treat Clouddriver instances more like cattle and enable things like <a href=\"https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116\">Chaos Monkey</a> for the service which we were previously prevented from doing.</li><li>Migrating Cloud Operation steps into Activities was a forcing function to re-write the logic to be idempotent. Since Temporal retries activities by default, it’s generally recommended they be idempotent. This alone fixed a number of issues that existed previously when operations were retried in Clouddriver.</li><li>We set the retry timeout for Activities in Clouddriver to be two hours by default. This gives us a long leash to fix-forward or rollback Clouddriver if we introduce a regression before customer deployments fail — to them, it might just look like a deployment is taking longer than usual.</li><li>Cloud Operations are much easier to introspect than before. Temporal ships with a great UI to help visualize Workflow and Activity executions, which is a huge boon for debugging live Workflows executing in production. The Temporal SDKs and server also emit a lot of useful metrics.</li></ol><figure><img alt=\"A Cloud Operation Workflow as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup\" src=\"https://cdn-images-1.medium.com/max/1024/1*zmCyjwzTXji921mulJjmTw.png\" /><figcaption>Execution of a resizeServerGroup Cloud Operation as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup</figcaption></figure><h4>Lessons Learned</h4><p>With the benefit of hindsight, there are also some lessons we can share from this migration:</p><p>1. <strong>Avoid unnecessary Child Workflows</strong>: Structuring Cloud Operations as an UntypedCloudOperationRunner Workflow that starts Child Workflows to actually execute the Cloud Operation’s logic was unnecessary and the indirection made troubleshooting more difficult. There are <a href=\"https://community.temporal.io/t/purpose-of-child-workflows/652\">situations</a> where Child Workflows are appropriate, but in this case we were using them as a tool for code organization, which is generally unnecessary. We could’ve achieved the same effect with class composition in the top-level parent Workflow.</p><p>2. <strong>Use single argument objects</strong>: At first, we structured Workflow and Activity functions with variable arguments, much as you’d write normal functions. This can be problematic for Temporal because of Temporal’s <a href=\"https://community.temporal.io/t/workflow-determinism/4027\">determinism constraints</a>. Adding or removing an argument from a function signature is <strong>not</strong> a backward-compatible change, and doing so can break long-running workflows — and it’s not immediately obvious in code review your change is problematic. The preferred pattern is to use a single serializable class to host all your arguments for Workflows and Activities — these can be more freely changed without breaking determinism.</p><p>3. <strong>Separate business failures from workflow failures</strong>: We like the pattern of the WorkflowResult type that UntypedCloudOperationRunner returns in the interface above. It allows us to communicate business process failures without failing the Workflow itself and have more overall nuance in error handling. This is a pattern we’ve carried over to other Workflows we’ve implemented since.</p><h3>Temporal at Netflix Today</h3><p>Temporal adoption has skyrocketed at Netflix since its initial introduction for Spinnaker. Today, we have hundreds of use cases, and we’ve seen adoption double in the last year with no signs of slowing down.</p><p>One major difference between initial adoption and today is that Netflix migrated from an on-prem Temporal deployment to using <a href=\"https://temporal.io/cloud\">Temporal Cloud</a>, which is Temporal’s SaaS offering of the Temporal server. This has let us scale Temporal adoption while running a lean team. We’ve also built up a robust internal platform around Temporal Cloud to integrate with Netflix’s internal ecosystem and make onboarding for our developers as easy as possible. Stay tuned for a future post digging into more specifics of our Netflix Temporal platform.</p><h3>Acknowledgement</h3><p>We all stand on the shoulders of giants in software. I want to call out that I’m retelling the work of my two stunning colleagues <a href=\"https://www.linkedin.com/in/chris-smalley/\">Chris Smalley</a> and <a href=\"https://www.linkedin.com/in/robzienert/\">Rob Zienert</a> in this post, who were the two aforementioned engineers who introduced Temporal and carried out the migration.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=73c69ccb5953\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953\">How Temporal Powers Reliable Cloud Operations at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-12-15T23:51:59.000Z",
    "url": "https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "Netflix Live Origin",
    "partialText": "<p><a href=\"https://www.linkedin.com/in/xiaomei-liu-b475711/\">Xiaomei Liu</a>, <a href=\"https://www.linkedin.com/in/joseph-lynch-9976a431/\">Joseph Lynch</a>, <a href=\"https://www.linkedin.com/in/chrisnewton2/\">Chris Newton</a></p><h3>Introduction</h3><p><a href=\"https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967\">Behind the Streams: Building a Reliable Cloud Live Streaming Pipeline for Netflix</a> introduced the architecture of the streaming pipeline. This blog post looks at the custom Origin Server we built for Live — the Netflix Live Origin. It sits at the demarcation point between the cloud live streaming pipelines on its upstream side and the distribution system, Open Connect, Netflix’s in-house Content Delivery Network (CDN), on its downstream side, and acts as a broker managing what content makes it out to Open Connect and ultimately to the client devices.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*44sJszKXEHZvSHnEQgYiiw.png\" /><figcaption><strong>Live Streaming Distribution and Origin Architecture</strong></figcaption></figure><p>Netflix Live Origin is a multi-tenant microservice operating on EC2 instances within the AWS cloud. We lean on standard HTTP protocol features to communicate with the Live Origin. The Packager pushes segments to it using PUT requests, which place a file into storage at the particular location named in the URL. The storage location corresponds to the URL that is used when the Open Connect side issues the corresponding GET request.</p><p>Live Origin architecture is influenced by key technical decisions of the live streaming architecture. First, resilience is achieved through redundant regional live streaming pipelines, with failover orchestrated at the server-side to reduce client complexity. The implementation of <a href=\"https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967\">epoch locking at the cloud encoder</a> enables the origin to select a segment from either encoding pipeline. Second, Netflix adopted a manifest design with <a href=\"https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40\">segment templates and constant segment duration</a> to avoid frequent manifest refresh. The constant duration templates enable Origin to predict the segment publishing schedule.</p><h3>Multi-pipeline and multi-region aware origin</h3><p>Live streams inevitably contain defects due to the non-deterministic nature of live contribution feeds and strict real-time segment publishing timelines. Common defects include:</p><ul><li><strong>Short segments:</strong> Missing video frames and audio samples.</li><li><strong>Missing segments:</strong> Entire segments are absent.</li><li><strong>Segment timing discontinuity:</strong> Issues with the Track Fragment Decode Time.</li></ul><p>Communicating segment discontinuity from the server to the client via a segment template-based manifest is impractical, and these defective segments can disrupt client streaming.</p><p>The redundant cloud streaming pipelines operate independently, encompassing distinct cloud regions, contribution feeds, encoder, and packager deployments. This independence substantially mitigates the probability of simultaneous defective segments across the dual pipelines. Owing to its strategic placement within the distribution path, the live origin naturally emerges as a component capable of intelligent candidate selection.</p><p>The Netflix Live Origin features multi-pipeline and multi-region awareness. When a segment is requested, the live origin checks candidates from each pipeline in a deterministic order, selecting the first valid one. Segment defects are detected via lightweight media inspection at the packager. This defect information is provided as metadata when the segment is published to the live origin. In the rare case of concurrent defects at the dual pipeline, the segment defects can be communicated downstream for intelligent client-side error concealment.</p><h3>Open Connect streaming optimization</h3><p>When the Live project started, Open Connect had become highly optimised for VOD content delivery — <a href=\"https://freenginx.org/en/\">nginx</a> had been chosen many years ago as the Web Server since it is highly capable in this role, and a number of enhancements had been added to it and to the underlying operating system (BSD). Unlike traditional CDNs, Open Connect is more of a distributed origin server — VOD assets are pre-positioned onto carefully selected server machines (OCAs, or Open Connect Appliances) rather than being filled on demand.</p><p>Alongside the VOD delivery, an on-demand fill system has been used for non-VOD assets — this includes artwork and the downloadable portions of the clients, etc. These are also served out of the same <a href=\"https://freenginx.org/en/\">nginx</a> workers, albeit under a distinct server block, using a distinct set of hostnames.</p><p>Live didn’t fit neatly into this ‘small object delivery’ model, so we extended the proxy-caching functionality of <a href=\"https://freenginx.org/en/\">nginx</a> to address Live-specific needs. We will touch on some of these here related to optimized interactions with the Origin Server. Look for a future blog post that will go into more details on the Open Connect side.</p><p>The segment templates provided to clients are also provided to the OCAs as part of the Live Event Configuration data. Using the Availability Start Time and Initial Segment number, the OCA is able to determine the legitimate range of segments for each event at any point in time — requests for objects outside this range can be rejected, preventing unnecessary requests going up through the fill hierarchy to the origin. If a request makes it through to the origin, and the segment isn’t available yet, the origin server will return a 404 Status Code (indicating File Not Found) with the expiration policy of that error so that it can be cached within Open Connect until just before that segment is expected to be published.</p><p>If the Live Origin knows when segments are being pushed to it, and knows what the live edge is — when a request is received for the immediately next object, rather than handing back another 404 error (which would go all the way back through Open Connect to the client), the Live Origin can ‘hold open’ the request, and service it once the segment has been published to it. By doing this, the degree of chatter within the network handling requests that arrive early has been significantly reduced. As part of this, millisecond grain caching was added to <a href=\"https://freenginx.org/en/\">nginx</a> to enhance the standard HTTP Cache Control, which only works at second granularity, a long time when segments are generated every 2 seconds.</p><h4>Streaming metadata enhancement</h4><p>The HTTP standard allows for the addition of request and response headers that can be used to provide additional information as files move between clients and servers. The HTTP headers provide notifications of events within the stream in a highly scalable way that is independently conveyed to client devices, regardless of their playback position within the stream.</p><p>These notifications are provided to the origin by the live streaming pipeline and are inserted by the origin in the form of headers, appearing on the segments generated at that point in time (and persist to future segments — they are cumulative). Whenever a segment is received at an OCA, this notification information is extracted from the response headers and used to update an in-memory data structure, keyed by event ID; and whenever a segment is served from the OCA, the latest such notification data is attached to the response. This means that, given any flow of segments into an OCA, it will always have the most recent notification data, even if all clients requesting it are behind the live edge. In fact, the notification information can be conveyed on any response, not just those supplying new segments.</p><h4>Cache invalidation and origin mask</h4><p>An invalidation system has been available since the early days of the project. It can be used to “flush” all content associated with an event by altering the key used when looking up objects in cache — this is done by incorporating a version number into the cache key that can then be bumped on demand. This is used during pre-event testing so that the network can be returned to a pristine state for the test with minimal fuss.</p><p>Each segment published by the Live Origin conveys the encoding pipeline it was generated by, as well as the region it was requested from. Any issues that are found after segments make their way into the network can be remedied by an enhanced invalidation system that takes such variants into account. It is possible to invalidate (that is, cause to be considered expired) segments in a range of segment numbers, but only if they were sourced from encoder A, or from Encoder A, but only if retrieved from region X.</p><p>In combination with Open Connect’s enhanced cache invalidation, the Netflix Live Origin allows <em>selective encoding pipeline masking</em> to exclude a range of segments from a particular pipeline when serving segments to Open Connect. The enhanced cache invalidation and origin masking enable live streaming operations to hide known problematic segments (e.g., segments causing client playback errors) from streaming clients once the bad segments are detected, protecting millions of streaming clients during the DVR playback window.</p><h3>Origin storage architecture</h3><p>Our original storage architecture for the Live Origin was simple: just use <a href=\"https://aws.amazon.com/s3/\">AWS S3</a> like we do for SVOD. This served us well initially for our low-traffic events, but as we scaled up we discovered that Live streaming has unique latency and workload requirements that differ significantly from on-demand where we have significant time ahead-of-time to pre-position content. While S3 met its stated uptime guarantees, our strict 2-second retry budget inherent to Live events (where every write is critical) led us to explore optimizations specifically tailored for real-time delivery at scale. AWS S3 is an amazing object store, but our Live streaming requirements were closer to those of a global low-latency highly-available database. So, we went back to the drawing board and started from the requirements. The Origin required:</p><ol><li>[HA Writes] Extremely high <em>write</em> availability, ideally as close to full write availability within a single AWS region, with low second replication delay to other regions. Any failed write operation within 500ms is considered a bug that must be triaged and prevented from re-occurring.</li><li>[Throughput] High write throughput, with hundreds of MiB replicating across regions</li><li>[Large Partitions] Efficiently support O(MiB) writes that accumulate to O(10k) keys per partition with O(GiB) total size per event.</li><li>[Strong Consistency] Within the same region, we needed read-your-write semantics to hit our &lt;1s read delay requirements (must be able to read published segments)</li><li>[Origin Storm] During worst-case load involving Open Connect edge cases, we may need to handle O(<strong>GiB</strong>) of read throughput <em>without affecting writes</em>.</li></ol><p>Fortunately, Netflix had previously invested in building a <a href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\">KeyValue Storage Abstraction</a> that cleverly leveraged <a href=\"https://youtu.be/sQ-_jFgOBng?t=1061\">Apache Cassandra</a> to provide chunked storage of MiB or even GiB values. This abstraction was initially built to support cloud saves of Game state. The Live use case would push the boundaries of this solution, however, in terms of availability for writes (#1), cumulative partition size (#3), and read throughput during Origin Storm (#5).</p><h4>High Availability for Writes of Large Payloads</h4><p>The <a href=\"https://youtu.be/paTtLhZFsGE?t=1077\">KeyValue Payload Chunking and Compression Algorithm</a> breaks O(MiB) work down so each part can be idempotently retried and hedged to maintain strict latency service level objectives, as well as spreading the data across the full cluster. When we combine this algorithm with Apache Cassandra’s local-quorum consistency model, which allows write availability even with an entire Availability Zone outage, plus a write-optimized <a href=\"https://en.wikipedia.org/wiki/Log-structured_merge-tree\">Log-Structured Merge Tree</a> (LSM) storage engine, we could meet the first four requirements. After iterating on the performance and availability of this solution, we were not only able to achieve the write availability required, but did so with a P99 <em>tail</em> latency that was similar to the status quo’s P50 <em>average </em>latency while also handling cross-region replication behind the scenes for the Origin. This new solution was significantly more expensive (as expected, databases backed by SSD cost more), but minimizing cost was <em>not</em> a key objective and low latency with high availability was:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bUPc4gC-mSDcybayhBJQ8g.png\" /><figcaption><strong>Storage System Write Performance</strong></figcaption></figure><h4>High Availability Reads at Gbps Throughputs</h4><p>Now that we solved the write reliability problem, we had to handle the Origin Storm failure case, where potentially dozens of Open Connect top-tier caches could be requesting multiple O(MiB) video segments at once. Our back-of-the-envelope calculations showed worst-case read throughput in the O(100Gbps) range, which would normally be extremely expensive for a strongly-consistent storage engine like Apache Cassandra. With careful tuning of chunk access, we were able to respond to reads at network line rate (100Gbps) from Apache Cassandra, but we observed unacceptable performance and availability degradation on concurrent writes. To resolve this issue, we introduced write-through caching of chunks using our distributed caching system <a href=\"https://github.com/Netflix/EVCache\">EVCache</a>, which is based on Memcached. This allows almost all reads to be served from a highly scalable cache, allowing us to easily hit 200Gbps and beyond without affecting the write path, achieving read-write separation.</p><h4>Final Storage Architecture</h4><p>In the final storage architecture, the Live Origin writes and reads to KeyValue, which manages a write-through cache to EVCache (memcached) and implements a safe chunking protocol that spreads large values and partitions them out across the storage cluster (Apache Cassandra). This allows almost all read load to be handled from cache, with only misses hitting the storage. This combination of cache and highly available storage has met the demanding needs of our Live Origin for over a year now.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yA9H-BFemM_-99FXBlVOMg.png\" /><figcaption><strong>Storage System High Level Architecture</strong></figcaption></figure><p>Delivering this consistent low latency for large writes with cross-region replication and consistent write-through caching to a distributed cache required solving numerous hard problems with novel techniques, which we plan to share in detail during a future post.</p><h3>Scalability and scalable architecture</h3><p>Netflix’s live streaming platform must handle a high volume of diverse stream renditions for each live event. This complexity stems from supporting various video encoding formats (each with multiple encoder ladders), numerous audio options (across languages, formats, and bitrates), and different content versions (e.g., with or without advertisements). The combination of these elements, alongside concurrent event support, leads to a significant number of unique stream renditions per live event. This, in turn, necessitates a high Requests Per Second (RPS) capacity from the multi-tenant live origin service to ensure publishing-side scalability.</p><p>In addition, Netflix’s global reach presents distinct challenges to the live origin on the retrieval side. During the Tyson vs. Paul fight event in 2024, a historic peak of 65 million concurrent streams was observed. Consequently, a scalable architecture for live origin is essential for the success of large-scale live streaming.</p><h4>Scaling architecture</h4><p>We chose to build a highly scalable origin instead of relying on the traditional origin shields approach for better end-to-end cache consistency control and simpler system architecture. The live origin in this architecture directly connects with top-tier Open Connect nodes, which are geographically distributed across several sites. To minimize the load on the origin, only designated nodes per stream rendition at each site are permitted to directly fill from the origin.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jW7eBCQtjlna0VKaWrKz_A.png\" /><figcaption><strong>Netflix Live Origin Scalability Architecture</strong></figcaption></figure><p>While the origin service can autoscale horizontally using EC2 instances, there are other system resources that are not autoscalable, such as storage platform capacity and AWS to Open Connect backbone bandwidth capacity. Since in live streaming, not all requests to the live origin are of the same importance, the origin is designed to prioritize more critical requests over less critical requests when system resources are limited. The table below outlines the request categories, their identification, and protection methods.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dYKFJkq22KI8sDBW_Njmog.png\" /></figure><h4>Publishing isolation</h4><p>Publishing traffic, unlike potentially surging CDN retrieval traffic, is predictable, making path isolation a highly effective solution. As shown in the scalability architecture diagram, the origin utilizes separate EC2 publishing and CDN stacks to protect the latency and failure-sensitive origin writes. In addition, the storage abstraction layer features distinct clusters for key-value (KV) read and KV write operations. Finally, the storage layer itself separates read (EVCache) and write (Cassandra) paths. This comprehensive path isolation facilitates independent cloud scaling of publishing and retrieval, and also prevents CDN-facing traffic surges from impacting the performance and reliability of origin publishing.</p><h4>Priority rate limiting</h4><p>Given Netflix’s scale, managing incoming requests during a traffic storm is challenging, especially considering non-autoscalable system resources. The Netflix Live Origin implemented priority-based rate limiting when the underlying system is under stress. This approach ensures that requests with greater user impact are prioritized to succeed, while requests with lower user impact are allowed to fail during times of stress in order to protect the streaming infrastructure and are permitted to retry later to succeed.</p><p>Leveraging Netflix’s microservice platform priority rate limiting feature, the origin prioritizes live edge traffic over DVR traffic during periods of high load on the storage platform. The live edge vs. DVR traffic detection is based on the predictable segment template. The template is further cached in memory on the origin node to enable priority rate limiting without access to the datastore, which is valuable especially during periods of high datastore stress.</p><p>To mitigate traffic surges, TTL cache control is used alongside priority rate limiting. When the low-priority traffic is impacted, the origin instructs Open Connect to slow down and cache identical requests for 5 seconds by setting a max-age = 5s and returns an HTTP 503 error code. This strategy effectively dampens traffic surges by preventing repeated requests to the origin within that 5-second window.</p><p>The following diagrams illustrate origin priority rate limiting with simulated traffic. The nliveorigin_mp41 traffic is the low-priority traffic and is mixed with other high-priority traffic. In the first row: the 1st diagram shows the request RPS, the 2nd diagram shows the percentage of request failure. In the second row, the 1st diagram shows datastore resource utilization, and the 2nd diagram shows the origin retrieval P99 latency. The results clearly show that only the low-priority traffic (nliveorigin_mp41) is impacted at datastore high utilization, and the origin request latency is under control.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-_YP6H3sEDaw1lS8prH4mQ.png\" /><figcaption><strong>Origin Priority Rate Limiting</strong></figcaption></figure><h4>404 storm and cache optimization</h4><p>Publishing isolation and priority rate limiting successfully protect the live origin from DVR traffic storms. However, the traffic storm generated by requests for non-existent segments presents further challenges and opportunities for optimization.</p><p>The live origin structures metadata hierarchically as event &gt; stream rendition &gt; segment, and the segment publishing template is maintained at the stream rendition level. This hierarchical organization allows the origin to preemptively reject requests with an HTTP 404(not found)/410(Gone) error, leveraging highly cacheable event and stream rendition level metadata, avoiding unnecessary queries to the segment level metadata:</p><ul><li>If the event is unknown, reject the request with 404</li><li>If the event is known, but the segment request timing does not match the expected publishing timing, reject the request with 404 and cache control TTL matching the expected publishing time</li><li>If the event is known, the requested segment is never generated or misses the retry deadline, reject the request with a 410 error, preventing the client from repeatedly requesting</li></ul><p>At the storage layer, metadata is stored separately from media data in the control plane datastore. Unlike the media datastore, the control plane datastore does not use a distributed cache to avoid cache inconsistency. Event and rendition level metadata benefits from a high cache hit ratio when in-memory caching is utilized at the live origin instance. During traffic storms involving non-existent segments, the cache hit ratio for control plane access easily exceeds 90%.</p><p>The use of in-memory caching for metadata effectively handles 404 storms at the live origin without causing datastore stress. This metadata caching complements the storage system’s distributed media cache, providing a complete solution for traffic surge protection.</p><h3>Summary</h3><p>The Netflix Live Origin, built upon an optimized storage platform, is specifically designed for live streaming. It incorporates advanced media and segment publishing scheduling awareness and leverages enhanced intelligence to improve streaming quality, optimize scalability, and improve Open Connect live streaming operations.</p><h3>Acknowledgement</h3><p>Many teams and stunning colleagues contributed to the Netflix live origin. Special thanks to <a href=\"https://www.linkedin.com/in/flavioribeiro/?originalSubdomain=br\">Flavio Ribeiro</a> for advocacy and sponsorship of the live origin project; to <a href=\"https://www.linkedin.com/in/rummadis/\">Raj Ummadisetty</a>, <a href=\"https://www.linkedin.com/in/prudhviraj9/\">Prudhviraj Karumanchi</a> for the storage platform; to <a href=\"https://www.linkedin.com/in/rosanna-lee-197920/\">Rosanna Lee</a>, <a href=\"https://www.linkedin.com/in/hunterford/\">Hunter Ford</a>, and <a href=\"https://www.linkedin.com/in/thiagopnts/\">Thiago Pontes</a> for storage lifecycle management; to <a href=\"https://www.linkedin.com/in/ameya-vasani-8904304/\">Ameya Vasani</a> for e2e test framework; <a href=\"https://www.linkedin.com/in/thomas-symborski-b4216728/\">Thomas Symborski</a> for orchestrator integration; to <a href=\"https://www.linkedin.com/in/jschek/\">James Schek</a> for Open Connect integration; to <a href=\"https://www.linkedin.com/in/kzwang/\">Kevin Wang</a> for platform priority rate limit; to <a href=\"https://www.linkedin.com/in/di-li-09663968/\">Di Li</a>, <a href=\"mailto:nhubbard@netflix.com\">Nathan Hubbard</a> for origin scalability testing.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=41f1b0ad5371\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371\">Netflix Live Origin</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-12-15T17:38:16.000Z",
    "url": "https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "AV1 — Now Powering 30% of Netflix Streaming",
    "partialText": "<h3><strong>AV1 — Now Powering 30% of Netflix Streaming</strong></h3><p><a href=\"https://www.linkedin.com/in/liwei-guo/\">Liwei Guo</a>, <a href=\"https://www.linkedin.com/in/henryzhili/\">Zhi Li</a>, <a href=\"https://www.linkedin.com/in/sheldon-radford/\">Sheldon Radford</a>, <a href=\"https://www.linkedin.com/in/jeffrwatts/\">Jeff Watts</a></p><p>Streaming video has become an integral part of our daily lives. At Netflix, our top priority is delivering the best possible entertainment experience to our members, regardless of their devices or network conditions. One of the key technologies enabling this is <a href=\"https://aomedia.org/specifications/av1/\">AV1</a>, a modern, open video codec that is rapidly transforming both how we stream content and how users experience it. Today, AV1 powers approximately 30% of all Netflix viewing, marking a major milestone in our efforts to bring more efficient and higher-quality streaming to our members.</p><p>In this post, we’ll revisit Netflix’s AV1 journey to date, highlight emerging use cases, and share adoption trends across the device ecosystem. Having witnessed AV1’s significant impact，and with <a href=\"https://aomedia.org/press%20releases/AOMedia-Announces-Year-End-Launch-of-Next-Generation-Video-Codec-AV2-on-10th-Anniversary/\">AV2 on the horizon</a>, we’re more excited than ever about how open codecs will continue to revolutionize streaming for everyone.</p><h3>AV1: A Modern, Open Codec</h3><p>Since entering the streaming business in 2007, Netflix has primarily relied on H.264/AVC as its streaming format. However, we quickly recognized that a modern, open codec would benefit not only Netflix, but the entire multimedia industry. In 2015, together with a group of like-minded industry leaders, Netflix co-founded the <a href=\"https://aomedia.org/\">Alliance for Open Media (AOMedia)</a> to develop and promote next generation, open source media technologies. The AV1 codec became the first major project of this collaboration, with ambitious goals: to deliver significant improvements in compression efficiency over state-of-the-art codecs, and to introduce rich features that enable new use cases. After three years of collaborative development, AV1 was officially released in 2018.</p><h3>Netflix’s AV1 Journey: From Android to TVs and Beyond</h3><h4><strong>Piloting on Android Mobile</strong></h4><p>When we first set out to bring AV1 streaming to Netflix members, Android was the ideal starting point. Android’s flexibility allowed us to quickly integrate a software AV1 decoder using the efficient <a href=\"https://code.videolan.org/videolan/dav1d\">dav1d</a> library, which was already optimized for ARM chipsets in mobile devices.</p><p>AV1’s superior compression efficiency was especially valuable for mobile users, many of whom are mindful of their data usage and network conditions. By adopting AV1, we were able to deliver noticeably better video quality at lower bitrates. For members relying on cellular data, this meant crisper images with fewer compression artifacts, even when bandwidth was limited. <a href=\"https://netflixtechblog.com/netflix-now-streaming-av1-on-android-d5264a515202\">Launching AV1 support on Android</a> in 2020 marked a significant step forward for Netflix on mobile, making high-quality streaming more accessible and enjoyable for members everywhere.</p><h4><strong>Front-and-Center for Netflix VOD Streaming</strong></h4><p>The success of our AV1 launch on Android proved its value for Netflix streaming, motivating us to expand support to smart TVs and other large-screen devices, where most of our members watch their favorite shows.</p><p>Smart TVs depend on hardware decoders for efficient high-quality playback. We worked closely with device manufacturers and SoC vendors to certify these devices, ensuring they are both conformant and performant. This collaborative effort enabled our AV1 streaming to TV devices in <a href=\"https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320\">late 2021</a>. Shortly thereafter, we expanded AV1 streaming to web browsers (in 2022) and continued to broaden device support. In 2023, this included Apple devices with the introduction of AV1 hardware support in the new M3 and A17 Pro chips.</p><p>As more devices began shipping with AV1 hardware support, a rapidly growing share of our members could enjoy the benefits of this advanced codec. Combined with our investment in adding AV1 streams across the entire catalog, AV1 viewing share has been consistently increasing in recent years. Today, AV1 accounts for approximately 30% of all Netflix streaming, making it our second most-used codec — and it’s on track to become number one very soon. The payoff has been substantial.</p><ul><li><strong>Elevating Streaming Experience Across the Board</strong>: Large-screen TVs and other devices demand higher bitrates to deliver stunning 4K, high frame rate (HFR) experiences. AV1’s superior compression efficiency has allowed us to provide these experiences using less data, making high-quality streaming more accessible and reliable. On average, AV1 streaming sessions achieve VMAF scores¹ that are 4.3 points higher than AVC and 0.9 points higher than HEVC sessions. At the same time, AV1 sessions use one-third less bandwidth than both AVC and HEVC, resulting in 45% fewer buffering interruptions. Moreover, Netflix’s diverse content catalog benefits universally from AV1, with improvements across all content types.</li><li><strong>Driving Network Efficiency Worldwide</strong>: Netflix streams are delivered through our own content delivery network (<a href=\"https://openconnect.netflix.com/en/?utm_referrer=https://www.google.com/\">Open Connect</a>), in partnership with local ISPs around the globe. With more than 300 million members, Netflix streaming constitutes a non-trivial portion of global internet traffic. Because AV1 is a more efficient codec, its streams are smaller in size (while providing even better visual quality). By shifting a substantial share of our streaming to AV1, we reduce overall internet bandwidth consumption, and lessen system and network load for both Netflix and our partners.</li></ul><h4>Unlocking Advanced Experiences</h4><p>In addition to its superior compression efficiency, AV1 was designed to support a rich set of features. Once we established a robust framework for the continuous expansion of AV1 streaming, we quickly shifted our focus towards exploring AV1’s unique features to unlock even more advanced and immersive experiences for our members.</p><p><strong>High-Dynamic-Range(HDR)<br></strong>HDR brings enhanced detail, vivid colors, and greater clarity to images. As a premium streaming service, Netflix has been a pioneer in adopting HDR, offering HDR streaming since 2016. In March 2025, we launched <a href=\"https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b\">AV1 HDR streaming</a>. We chose HDR10+ as the HDR format for its use of dynamic metadata, which enabled us to adapt the tone mapping per device in a scene-dependent manner.</p><p>As anticipated, the combination of AV1 and HDR10+ allows us to deliver images with greater detail, more vibrant colors, and an overall heightened sense of immersion for our members. At the moment, 85% of our HDR catalog (from the perspective of view-hours) has AV1-HDR10+ coverage, and this number is expected to reach 100% in the next couple of months.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/759/1*Ubhj9prgqb0zuTHt6oOx0g.png\" /><figcaption><strong><em>Photographs of devices displaying the same (cropped) frame with HDR10 metadata (left) and HDR10+ metadata (right). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one.</em></strong></figcaption></figure><p><strong>Cinematic Film Grain<br></strong>Film grain is a hallmark of the cinematic experience, widely used in the movie industry to enhance a film’s depth, texture, and realism. However, because film grain is inherently random, faithfully representing it in digital video requires a significant amount of data. This presents a unique challenge for streaming: restricting the bitrate can result in grain that appears unnatural or distorted, while increasing the bitrate to accurately preserve cinematic grain almost inevitably leads to elevated rebuffering. The AV1 specification incorporates a unique solution called Film Grain Synthesis (FGS). Instead of encoding grain as part of every frame, the grain is stripped out before encoding and then resynthesized at the decoder using parameters sent in the bitstream, delivering a realistic cinematic film grain experience without the usual data costs.</p><p>This approach represents a significant shift from traditional compression and streaming techniques. Our team invested substantial effort in fine-tuning the media processing pipeline, ensuring FGS delivers robust performance at scale. In July 2025, we successfully <a href=\"https://netflixtechblog.com/av1-scale-film-grain-synthesis-the-awakening-ee09cfdff40b\">productized AV1 FGS</a>, and the results were astonishing: AV1 with FGS could deliver videos with cinematic film grain at a bitrate well within the capabilities of typical household internet connections. For non-FGS AV1 encodings, even at much higher bitrate, they may not be able to achieve comparable quality.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1016/1*9fB5xuoFpbpN8ZQIzTHDtg.png\" /><figcaption><strong><em>The same (cropped) frame from source (left), regular AV1 stream encoded at 8274kbps (middle) and AV1 FGS stream encoded at 2804 kbps (right). The AV1 FGS stream reduces the bitrate by 66% while delivering clearly better quality.</em></strong></figcaption></figure><h4><strong>Beyond VOD Streaming</strong></h4><p>So far, our AV1 journey has been mainly on VOD, but we see significant opportunities for AV1 beyond traditional VOD streaming. On a mission to entertain the world, Netflix has constantly explored and established other ways to bring joy to our members, and we believe AV1 could contribute to the success of these new products.</p><p><strong>Live Streaming<br></strong>Debuting in 2023, live streaming has experienced <a href=\"https://help.netflix.com/en/node/129840\">rapid growth</a> at Netflix, becoming a key part of our streaming offerings in just two short years. We are actively evaluating the use of AV1 in live streaming, as we believe it could help further scale Netflix’s live programming:</p><ul><li><strong>Hyper-scale concurrent viewership: </strong>Live streaming at Netflix means delivering content to <a href=\"https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news\">tens of millions</a> of viewers simultaneously. AV1’s superior compression efficiency could significantly reduce the required bandwidth, enabling us to deliver high-quality live experiences to large audiences without compromising video quality.</li><li><strong>Customizable graphics overlay</strong>: for live sport events such as football, tennis and boxing, graphics overlays have become an integral part of the member experience — from embedding game statistics to delivering sponsorships. AV1 offers an opportunity to make the graphics highly customizable: layered coding is supported in AV1’s main profile, allowing encoding the main content in the base layer, and graphics in the enhancement layer, and easily swapping out one version of the enhancement layer with another. We envision that the use of AV1’s layered coding can greatly simplify the live streaming workflow and reduce delivery costs.</li></ul><p><strong>Cloud Gaming<br></strong>Cloud gaming is a new Netflix offering that is currently in the <a href=\"https://help.netflix.com/en/node/132197\">beta phase</a> and is available to members in select countries. The game engines run on cloud servers, while the rendered graphics are streamed directly to members’ devices. By removing barriers and transforming every Netflix-enabled device into a game console, Cloud gaming aims to deliver a seamless, “play anywhere” experience for our members. For a glimpse of this in action, <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7382077927875825664/\">watch as Co-CEO Greg Peters and CTO Elizabeth Stone play a round of Boggle Party — powered entirely by Netflix’s cloud gaming platform</a>!</p><p>Unlike traditional video streaming, cloud gaming requires that every player action is reflected instantly on the screen to ensure a responsive and immersive experience. This makes delivering high-quality video frames with extremely low latency, despite fluctuating network conditions, one of the biggest challenges in cloud gaming.</p><p>Our team is actively working on productizing AV1 for cloud gaming. Given AV1’s high compression efficiency, we can reduce frame sizes, helping video frames get through even when network conditions become challenging. This positions AV1 as a promising technology for enabling a high-quality, low-latency gaming experience across a wide range of devices.</p><h3>A Device Ecosystem United for AV1</h3><p>Netflix is a streaming company, and we have worked diligently to create highly efficient and standards-conformant AV1 streams for our catalog. However, an equally, if not more, important factor in AV1’s success is the widespread support from device manufacturers. Throughout our AV1 journey, we have been impressed by the unprecedented pace at which the device ecosystem has embraced AV1.</p><p>Just six months after the AV1 specification was finalized, the open-source AV1 decoder library sponsored by AOM, dav1d, was released. Small, performant, and highly resource-efficient, dav1d bridged the gap for early adopters like Netflix while hardware solutions were still in development. Continuous improvements to its performance and compatibility have made dav1d the preferred choice for a wide range of platforms and practical applications. Today, it serves as <a href=\"https://aomedia.org/av1-adoption-showcase/google-story/\">Android’s default software decoder</a>. Additionally, it plays a key role in web browsers — for Netflix, it powers approximately 40% of our browser playback. This broad adoption has significantly expanded access to high-quality AV1 streaming, even in the absence of dedicated hardware decoders.</p><p>Netflix maintains a close working relationship with device manufacturers and SoC vendors, and we have witnessed first-hand their enthusiasm for adopting AV1. To ensure optimal streaming performance, Netflix has a rigorous certification process to verify proper support for our streaming formats on devices. AV1 was added to this certification process in 2019, and since then, we have seen a steady increase in the number of devices with full AV1 decoding capabilities. Over the past five years (2021–2025), 88% of large-screen devices, including TVs, set-top boxes, and streaming sticks, submitted for Netflix certification have supported AV1, with the vast majority offering full 4K@60fps capability. Notably, since 2023, almost all devices we have received for certification are AV1-capable.</p><p>We have also been impressed by the robustness of AV1 implementations across these devices. As mentioned earlier, FGS is an innovative tool that departs from traditional codec architectures and was not included in our initial full-scale AV1 streaming rollout. When we launched FGS this July, we worked closely with our partners to ensure broad device compatibility. We are pleased with the successful progress made, and AV1 with FGS is now supported across a significant and growing number of in-field devices.</p><h3>Looking Ahead: AV1 Today, AV2 Tomorrow</h3><p>As we reflect on our AV1 journey, it’s clear that the codec has already transformed the streaming experience for hundreds of millions of Netflix members worldwide. Thanks to industry-wide collaboration and rapid device adoption, AV1 is delivering higher quality, greater efficiency, and new cinematic features to more screens than ever before.</p><p>Looking ahead, we are excited about the forthcoming release of AV2, announced by the Alliance for Open Media for the end of 2025. <a href=\"https://www.youtube.com/watch?v=RUMwMe_2Dqo\">AV2 is poised to set a new benchmark for compression efficiency and streaming capabilities, building on the solid foundation laid by AV1</a>. At Netflix, we remain committed to adopting the best open technologies to delight our members around the globe. While AV2 represents the future of streaming, AV1 is very much the present — serving as the backbone of our platform and powering exceptional entertainment experiences across a vast and ever-expanding ecosystem of devices.</p><h3>Acknowledgement</h3><p>The success of AV1 at Netflix is the result of the dedication, expertise, and collaboration of many teams across the company — including Encoding, Clients, Device Certification, Partner Engineering, Data Science &amp; Engineering, Infra, Platform, etc.</p><p>We would also like to thank <a href=\"https://www.linkedin.com/in/artemdanylenko/\">Artem Danylenko</a>, <a href=\"https://www.linkedin.com/in/aditya-mavlankar-7139791/\">Aditya Mavlankar</a>, <a href=\"https://www.linkedin.com/in/anne-aaron/\">Anne Aaron</a>, <a href=\"https://www.linkedin.com/in/cyril-concolato-567a522/\">Cyril Concolato</a>, <a href=\"https://www.linkedin.com/in/allanzp/\">Allan Zhou</a> and <a href=\"https://www.linkedin.com/in/anush-moorthy-b8451142/\">Anush Moorthy</a> for their valuable comments and feedback on earlier drafts of this post.</p><h3>Footnotes</h3><ol><li>These numbers represent a snapshot of data from November 13, 2025. Actual values may vary slightly from day to day and across different regions, depending on the mix of content, devices, and internet connectivity.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xuLP8glDDcj-DBYO8djmNA.png\" /></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=02f592242d80\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80\">AV1 — Now Powering 30% of Netflix Streaming</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-12-04T20:09:30.000Z",
    "url": "https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "Supercharging the ML and AI Development Experience at Netflix",
    "partialText": "<h3>Supercharging the ML and AI Development Experience at Netflix with Metaflow</h3><p><a href=\"https://www.linkedin.com/in/shashanksrikanth/\"><em>Shashank Srikanth</em></a>, <a href=\"https://www.linkedin.com/in/romain-cledat-4a211a5/\"><em>Romain Cledat</em></a></p><p><a href=\"https://docs.metaflow.org\">Metaflow</a> — a framework we started and <a href=\"https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9\">open-sourced</a> in 2019 — now powers <a href=\"https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d\">a wide range of ML and AI systems across Netflix</a> and at <a href=\"https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md\">many other companies</a>. It is well loved by users for helping them take their ML/AI workflows from <a href=\"https://docs.metaflow.org/introduction/what-is-metaflow#how-does-metaflow-support-prototyping-and-production-use-cases\">prototype to production</a>, allowing them to focus on building cutting-edge systems that bring joy and entertainment to audiences worldwide.</p><p>Metaflow allows users to:</p><ol><li><strong>Iterate and ship quickly </strong>by minimizing friction</li><li><strong>Operate systems reliably</strong> in production with minimal overhead, at Netflix scale.</li></ol><p>Metaflow works with many battle-hardened tooling to address the second point — among them <a href=\"https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041\">Maestro</a>, our newly open-sourced workflow orchestrator that powers nearly every ML and AI system at Netflix and serves as a backbone for Metaflow itself.</p><p>In this post, we focus on the first point and introduce a new Metaflow functionality, <strong>Spin</strong>, that helps users <strong>accelerate their iterative development process</strong>. By the end, you’ll have a solid understanding of Spin’s capabilities and learn how to try it out yourself with <strong>Metaflow 2.19</strong>.</p><h3>Iterative development in ML and AI workflows</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0I8DAvXCQEN1RpTTC0ZyaQ.jpeg\" /><figcaption>Developing a Metaflow flow with cards in VSCode</figcaption></figure><p>To understand our approach to improving the ML and AI development experience, it helps to consider how these workflows differ from traditional software engineering.</p><p>ML and AI development revolves not just around code but also around data and models, which are large, mutable, and computationally expensive to process. Iteration cycles can involve long-running data transformations, model training, and stochastic processes that yield slightly different results from run to run. These characteristics make fast, stateful iteration a critical part of productive development.</p><p>This is where notebooks — such as Jupyter, <a href=\"https://observablehq.com/documentation/notebooks/\">Observable</a>, or <a href=\"https://marimo.io/\">Marimo</a> — shine. Their ability to preserve state in memory allows developers to load a dataset once and iteratively explore, transform, and visualize it without reloading or recomputing from scratch. This persistent, interactive environment turns what would otherwise be a slow, rigid loop into a fluid, exploratory workflow — perfectly suited to the needs of ML and AI practitioners.</p><p>Because ML and AI development is computationally intensive, stochastic, and data- and model-centric, tools that optimize iteration speed must treat state management as a first-class design concern. Any system aiming to improve the development experience in this domain must therefore enable quick, incremental experimentation without losing continuity between iterations.</p><h3>New: rapid, iterative development with spin</h3><p>At first glance, Metaflow code looks like a workflow — similar to <a href=\"https://airflow.apache.org/\">Airflow</a> — but there’s another way to look at it: each Metaflow @step serves as <a href=\"https://docs.metaflow.org/metaflow/basics#what-should-be-a-step\">a checkpoint boundary</a>. At the end of every step, Metaflow automatically persists all instance variables as <em>artifacts</em>, allowing the execution to <a href=\"https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command\">resume</a> seamlessly from that point onward. The below animation shows this behavior in action:</p><figure><img alt=\"An animated GIF showing how resume can be used in Metaflow. The GIF shows how using `flow.py resume join` makes Metaflow clone previously executed steps and resumes the computation from the `join` step and continues executing till the end of the flow.\" src=\"https://cdn-images-1.medium.com/max/1024/1*AEDpnt-YULYV4mcyrwLk7g.gif\" /><figcaption>Using resume in Metaflow</figcaption></figure><p>In a sense, we can consider a @step similar to a notebook cell: it is the smallest unit of execution that updates state upon completion. It does have a few differences that address the issues with notebook cells:</p><ul><li><strong>The execution order is explicit and deterministic: </strong>no surprises due to out-of-order cell execution;</li><li><strong>The state is not hidden: </strong>state is explicitly stored as self. variables as shared state, which can be <a href=\"https://docs.metaflow.org/metaflow/client\">discovered and inspected</a>;</li><li><strong>The state is versioned and persisted</strong> making results more reproducible.</li></ul><p>While <strong>Metaflow</strong>’s resume feature can approximate the incremental and iterative development approach of notebooks, it restarts execution from the selected step onward, introducing more latency between iterations. In contrast, a <strong>notebook</strong> allows near-instant feedback by letting users tweak and rerun individual cells while seamlessly reusing data from earlier cells held in memory.</p><p>The new spin command in Metaflow 2.19 addresses this gap. Similar to executing a single notebook cell, it quickly executes a single Metaflow @step — with all the state carried over from the parent step. As a result, users can develop and debug Metaflow steps as easily as a cell in a notebook.</p><p>The effect becomes clear when considering the three complementary execution modes — run, resume, and spin — side by side, mapping them to the corresponding notebook behavior:</p><figure><img alt=\"Diagram showing the various modes of execution in Metaflow: Run, Resume and Spin\" src=\"https://cdn-images-1.medium.com/max/1024/1*DgRIxOu-7keiFoHia9JMrg.png\" /><figcaption>Run, Resume and Spin “modes”</figcaption></figure><p>Another major difference isn’t just what gets executed, but what gets recorded. Both run and resume create a full, versioned run with complete metadata and artifacts, while spin skips tracking altogether. It’s built for fast, throw-away iterations during development.</p><p>The one-minute clip below illustrates a typical iterative development workflow that alternates between run and spin. In this example, we are building a flow that reads a dataset from a Parquet file and trains a separate model for each product category, focusing on computer-related categories.</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3RNMM-lthm0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3RNMM-lthm0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3RNMM-lthm0%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href\">https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href</a></iframe><p>As shown in the video, we start by creating a flow from scratch and running a minimal version of it to persist test artifacts — in this case, a Parquet dataset. From there, we can use spin to iterate on one step at a time, incrementally building out the flow, for example, by adding the parallel training steps demonstrated in the clip.</p><p>Once the flow has been iterated on locally, it can be seamlessly deployed to production orchestrators like Maestro or <a href=\"https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows\">Argo</a>, and <a href=\"https://docs.metaflow.org/scaling/remote-tasks/requesting-resources\">scaled up</a> on compute platforms such as AWS Batch, Titus, Kubernetes and more. Thus, the experience is as smooth as developing in a notebook, but the outcome is a production-ready, scalable workflow, implemented as an idiomatic Python project!</p><h3>Spin up smooth development in VSCode/Cursor</h3><p>Instead of typing run and spin manually in the terminal, we can bind them to keyboard shortcuts. For example, <a href=\"https://github.com/outerbounds/metaflow-dev-vscode\">the simple metaflow-dev VS Code extension</a> (works with Cursor as well) maps Ctrl+Opt+R to run and Ctrl+Opt+S to spin. Just hack away, hit Ctrl+Opt+S, and the extension will save your file and spin the step you are currently editing.</p><p>One area where spin truly shines is in creating mini-dashboards and reports with <a href=\"https://docs.metaflow.org/metaflow/visualizing-results\">Metaflow Cards</a>. Visualization is another strong point of notebooks but the combination of spin and cards makes Metaflow a very compelling alternative for developing real-time and post-execution visualizations. Developing cards is inherently iterative and visual (much like building web pages) where you want to tweak code and see the results instantly. This workflow is readily available with the combination of VSCode/Cursor, which includes a built-in web-view, <a href=\"https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#using-local-card-viewer\">the local card viewer</a>, and spin.</p><p>To see the trio of tools — along with the VS Code extension — in action, in this short clip we add observability to the train step that we built in the earlier example:</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FhoRO5eePjqo%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DhoRO5eePjqo&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FhoRO5eePjqo%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href\">https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href</a></iframe><p>A major benefit of Metaflow Cards is that we don’t need to deploy any extra services, data streams, and databases for observability. Just develop visual outputs as above, deploy the flow, and wehave a complete system in production with reporting and visualizations included.</p><h3>Spin to the next level: injecting inputs, inspecting outputs</h3><p>Spin does more than just run code — it also lets us take full control of a spun @step’s inputs and outputs, enabling a range of advanced patterns.</p><p>In contrast to notebooks, we can spin any arbitrary @step in a flow using state from any past run, making it easy to test functions with different inputs. For example, if we have multiple models produced by separate runs, we could spin an inference step, supplying a different model run each time.</p><p>We can also override artifact values or inject arbitrary Python objects — similar to a notebook cell — for spin. Simply specify a Python module with an ARTIFACTS dictionary:</p><pre>ARTIFACTS = {<br>  &quot;model&quot;: &quot;kmeans&quot;,<br>  &quot;k&quot;: 15<br>}</pre><p>and point spin at the module:</p><pre>spin train --artifacts-module artifacts.py</pre><p>By default spin doesn’t persist artifacts, but we can easily change this by adding --persist. Even in this case, artifacts are not persisted in the usual Metaflow datastore but to a directory-specific location which you can easily clean up after testing. We can access the results with <a href=\"https://docs.metaflow.org/metaflow/client\">the Client API</a> as usual — just specify the directory you want to inspect with inspect_spin:</p><pre>from metaflow import inspect_spin<br><br>inspect_spin(&quot;.&quot;)<br>Flow(&quot;TrainingFlow&quot;).latest_run[&quot;train&quot;].task[&quot;model&quot;].data</pre><p>Being able to inspect and modify a step’s inputs and outputs on the fly unlocks a powerful use case:<strong> unit testing individual steps</strong>. We can use spin programmatically through <a href=\"https://docs.metaflow.org/metaflow/managing-flows/runner\">the Runner API</a> and assert the results:</p><pre>from metaflow import Runner<br><br>with Runner(&quot;flow.py&quot;).spin(&quot;train&quot;, persist=True) as spin:<br>  assert spin.task[&quot;model&quot;].data == &quot;kmeans&quot;</pre><h3>Making AI agents spin</h3><p>In addition to speeding up development for humans, spin turns out to be surprisingly handy for coding agents too. There are two major advantages to teaching AI how to spin:</p><ol><li><strong>It accelerates the development loop</strong>. Agents don’t naturally understand what’s slow, or why speed matters, so they need to be nudged to favor faster tools over slower ones.</li><li><strong>It helps surface errors faster </strong>and contextualizes them to a specific piece of code, increasing the chance that the agent is able to fix errors by itself.</li></ol><p>Metaflow users are already <a href=\"https://claude.com/product/claude-code\">using</a> Claude Code; spin makes this even easier. In the example below, we added the following section in a CLAUDE.md file:</p><pre>## Developing Metaflow code<br>Follow this incremental development workflow that ensures quick iterations<br>and correct results. You must create a flow incrementally, step by step<br>following this process:<br>1. Create a flow skeleton with empty `@step`s.<br>2. Add a data loading step.<br>3. `run` the flow.<br>4. Populate the next step and use `spin` to test it with the correct inputs.<br>5. `run` the flow to record outputs from the new step.<br>5. Iterate on (4–5) until all steps have been implemented and work correctly.<br>6. `run` the whole flow to ensure final correctness.<br><br>To test a flow, run the flow as follows<br>```<br>python flow.py - environment=pypi run<br>```<br><br>Do this once before running `spin`.<br>As you are building the flow, you `spin` to test steps quickly.<br>For instance<br>```<br>python flow.py - environment=pypi spin train<br>```</pre><p>Just based on these quick instructions, the agent is able to use spin effectively. Take a look at the following inspirational example that one-shots Claude to create a flow, along the lines of our earlier examples, which trains a classifier to predict product categories:</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FkcCkLXernR0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DkcCkLXernR0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkcCkLXernR0%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/706e83950e73a05ca41b4fb463702690/href\">https://medium.com/media/706e83950e73a05ca41b4fb463702690/href</a></iframe><p>In the video, we can see Claude using spin around the 45-second mark to test a preprocess step. The step initially fails due to a classic data science pitfall: during testing, Claude samples only a small subset of data, causing some classes to be underrepresented. The first spin surfaces the issue, which Claude then fixes by switching to stratified sampling — and finally does another spin to confirm the fix, before proceeding to complete the task.</p><h3>The inner loop of end-to-end ML/AI</h3><p>To circle back to where we started, our motivation for adding spin — and for creating Metaflow in the first place — is to accelerate development cycles so we can deliver more joy to our subscribers, faster. Ultimately, we believe there’s no single magic feature that makes this possible. It takes all parts of an ML/AI platform working together coherently — spin included.</p><p>From this perspective, it’s useful to place spin in the context of other Metaflow features. It’s designed for the innermost loop of model and business-logic development, with the added benefit of supporting unit testing during deployment, as shown in the overall blueprint of the Metaflow toolchain below.</p><figure><img alt=\"Metaflow tool-chain.\" src=\"https://cdn-images-1.medium.com/max/1024/1*9cd6SHFrW7A4iWMZHYkVkw.png\" /><figcaption>Metaflow tool-chain</figcaption></figure><p>In this diagram, the solid blue boxes represent different Metaflow commands, while the blue text denotes decorators and other features. In particular, note the <em>Shared Functionality</em> box — another key focus area for us over the past year — which includes <a href=\"https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6\">configuration management</a> and <a href=\"https://docs.metaflow.org/metaflow/composing-flows/introduction\">custom decorators</a>. These capabilities let domain-specific teams and platform providers tailor Metaflow to their own use cases. Following our ethos of composability, all of these features integrate seamlessly with spin as well.</p><p>Another key design philosophy of Metaflow is to let projects start small and simple, adding complexity only when it becomes necessary. So don’t be overwhelmed by the diagram above. To get started, install Metaflow easily with</p><pre>pip install metaflow</pre><p>and take your first baby @steps for a spin! Check out the <a href=\"https://docs.metaflow.org/metaflow/authoring-flows/introduction\">docs</a> and for questions, support, and feedback, join the friendly <a href=\"http://chat.metaflow.org\">Metaflow Community Slack</a>.</p><h3>Acknowledgments</h3><p>We would like to thank our partners at <a href=\"https://outerbounds.com\">Outerbounds</a>, and particularly <a href=\"https://www.linkedin.com/in/villetuulos/\">Ville Tuulos</a>, <a href=\"https://www.linkedin.com/in/savingoyal/\">Savin Goyal</a>, and <a href=\"https://www.linkedin.com/in/madhur-tandon/\">Madhur Tandon</a>, for their collaboration on this feature, from initial ideation to review, testing and documentation. We would also like to acknowledge the rest of the Model Development and Management team (<a href=\"https://www.linkedin.com/in/maria-alder/\">Maria Alder</a>, <a href=\"https://www.linkedin.com/in/david-j-berg/\">David J. Berg</a>, <a href=\"https://www.linkedin.com/in/shaojingli/\">Shaojing Li</a>, <a href=\"https://www.linkedin.com/in/rui-lin-483a83111/\">Rui Lin</a>, <a href=\"https://www.linkedin.com/in/nissanpow/\">Nissan Pow</a>, <a href=\"https://www.linkedin.com/in/chaoying-wang/\">Chaoying Wang</a>, <a href=\"https://www.linkedin.com/in/reginalw/\">Regina Wang</a>, <a href=\"https://www.linkedin.com/in/shuishiyang/\">Seth Yang</a>, <a href=\"https://www.linkedin.com/in/zitingyu/\">Darin Yu</a>) for their input and comments.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b2d5b95c63eb\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb\">Supercharging the ML and AI Development Experience at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-11-04T20:33:44.000Z",
    "url": "https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning",
    "partialText": "<p>Author: <a href=\"https://keertanavc.github.io/\">Keertana Chidambaram</a>, <a href=\"https://www.linkedin.com/in/qiuling-xu-a445b815a\">Qiuling Xu</a>, <a href=\"https://www.linkedin.com/in/markhsiao/\">Ko-Jen Hsiao</a>, <a href=\"https://www.linkedin.com/in/moumitab/\">Moumita Bhattacharya</a></p><p>(*The work was done when Keertana interned at Netflix.)</p><h3>Introduction</h3><p>This blog focuses on post-training generative recommender systems. Generative recommenders (GRs) represent a new paradigm in the field of recommendation systems (e.g. <a href=\"https://github.com/meta-recsys/generative-recommenders\">HSTU</a>, <a href=\"https://arxiv.org/abs/2502.18965\">OneRec</a>). These models draw inspiration from recent advancements in transformer architectures used for language and vision tasks. They approach the recommendation problem, including both ranking and retrieval, as a sequential transduction task. This perspective enables generative training, where the model learns by imitating the next event in a sequence of user activities, thereby effectively modeling user behavior over time.</p><p>However, a key challenge with simply replicating observed user patterns is that it may not always lead to the best possible recommendations. User interactions are influenced by a variety of factors — such as trends, or external suggestions — and the system’s view of these interactions is inherently limited. For example, if a user tries a popular show but later indicates it wasn’t a good fit, a model that only imitates this behavior might continue to recommend similar content, missing the chance to enhance the user’s experience.</p><p>This highlights the importance of incorporating user preferences and feedback, rather than solely relying on observed behavior, to improve recommendation quality. In the context of recommendation systems, we benefit from a wealth of user feedback, which includes explicit signals such as ratings and reviews, as well as implicit signals like watch time, click-through rates, and overall engagement. This abundance of feedback serves as a valuable resource for improving model performance.</p><p>Given the recent success of reinforcement learning techniques in post-training large language models, such as DPO and GRPO, this study investigates whether similar methods can be applied to generative recommenders. Ultimately, our goal is to identify both the opportunities and challenges in using these techniques to enhance the quality and relevance of recommendations.</p><p>Unlike language models, post-training generative recommenders presents unique challenges. One of the most significant is the difficulty of obtaining counterfactual feedback in recommendation scenarios. The recommendation feedback is generated on-policy — that is, it reflects users’ real-time interactions with the system as they naturally use it. Since a typical user sequence can span weeks or even years of activity, it is impractical to ask users to review or provide feedback on hypothetical, counterfactual experiences. As a result, the absence of counterfactual data makes it challenging to apply post-training methods such as PPO or DPO, which require feedback from counterfactual user sequences.</p><p>Furthermore, post-training methods typically rely on a reward model — either implicit or explicit — to guide optimization. The quality of reward models heavily influences the effectiveness of post-training. In the context of recommendation systems, however, reward signals tend to be much noisier. For instance, if we use watch time as an implicit reward, it may not always accurately reflect user satisfaction: a viewer might stop watching a favorite show simply due to time constraints, while finishing a lengthy show doesn’t necessarily indicate genuine enjoyment.</p><p>To address these post-training challenges, we introduce a novel algorithm called Advantage-Weighted Supervised Fine-tuning (A-SFT). Our analysis first demonstrates that reward models in recommendation systems often exhibit higher uncertainty due to the issues discussed above. Rather than relying solely on these uncertain reward models, A-SFT combines supervised fine-tuning with the advantage function to more effectively guide post-training optimization. This approach proves especially effective when the reward model has high variance but still provides valuable directional signals. We benchmark A-SFT against four other representative methods, and our results show that A-SFT achieves better alignment between the pre-trained generative recommendation model and the reward model.</p><p>In Figure 1, we conceptualize the pros and cons of different post-training paradigms. For example, Online Reinforcement Learning is most useful when the reward model has a good generalization ability, and behavior cloning is suitable when no reward models are available. Using these algorithms under fitting use cases is the key to a successful post-training. For example, over-exploitation of noisy reward models will hurt task performance, as guidance from the reward models can be simply noise. Conversely, not leveraging a good reward model leaves out potential improvements. We find A-SFT fits the sweet point between offline reinforcement learning and behavior cloning, where it benefits from the directional signals in those noisy estimations and is less dependent on the reward accuracy.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*N9QNNLspEJJCxQjtNl9HZw.png\" /></figure><p>Figure 1: The landscape of RL algorithms based on the reward models’ accuracy</p><h3>Challenges in Post-training for Recommendation</h3><p>Reinforcement Learning from Human Feedback (RLHF) is the most popular framework for post-training large language models. In this framework, human annotators evaluate and rank different outputs generated by a model. This feedback is then used to train a reward model that predicts how well a model output aligns with human preferences. This reward model then serves as a proxy for human judgment during reinforcement learning, guiding the model to generate outputs that are more likely to be preferred by humans.</p><p>While traditional RLHF methods like PPO or DPO are effective for aligning LLMs, there are several challenges in applying them directly to large-scale recommendation systems:</p><ol><li>Lack of Counter-factual Observations</li></ol><p>As in typical RLHF settings, collecting real-time feedback from a diverse user base across a wide range of items is both costly and impractical. The data in recommendation are generated by the real-time user interests. Any third-party annotators or even the user themselves lack the practical means to evaluate an alternative reality. For example, it is impractical to ask the Netflix users to evaluate hundreds of unseen movies. Consequently, we lack a live environment in which to perform reinforcement learning.</p><p>2. Noisy Reward Models</p><p>In addition to the limited counter-factual data, the recommendation task itself has a higher randomness by its nature. The recommendation data has less structure than language data. Users choose to watch some shows not because there is a grammar rule that nouns need to follow by the verbs. In fact, the users’ choices usually exhibit a level of permutation invariance, where swapping the order of events in the user history still makes a valid activity sequence. This randomness in the behaviors makes learning a good reward model extremely difficult. Often the reward models we learnt still have a large margin of errors.</p><p>Here is an ablation study we did on the reward model performance with O(Millions) users and O(Billions) of tokens. The reward model uses an open-sourced HSTU architecture in the convenience of reproducing this study. We adopt the standard RLHF approach of training a reward model using offline, human-collected feedback. We start by creating a proxy reward, scored on a scale from 1 to 5 in the convenience of understanding. This reward model is co-trained as a shallow reward head on top of the generative recommender. It predicts the reward for the most recently selected title based on a user’s interaction history. To evaluate its effectiveness, we compare the model’s performance against two simple baselines: (1) predicting the next reward as the average reward the user has given in their past interactions, and (2) predicting it as the average reward that all users have assigned to that particular title in previous interactions.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tmYdEXqVjSkne__-k6gSig.png\" /></figure><p>Table 1: Reward model performance metrics</p><p>We observe that the model’s predictions do not significantly outperform the simple baselines. This result is intuitive, as a user’s historical interactions typically cover only a small subset of titles, making it difficult to accurately predict their responses to the vast number of unexplored titles in the catalogue. We expect this to be a potential issue for any large recommendation systems where the ratio between explored and unexplored titles is very small.</p><p>3. Lack of Logged Policy</p><p>In recommendation systems, the policy that generated the logged data is typically unknown and cannot be directly estimated. Offline reinforcement learning methods often rely on Inverse Propensity Scoring (IPS) to debias such data by reweighting interactions according to the logging policy’s action probabilities. However, estimating the logging policy accurately is challenging and prone to error, which can introduce additional biases, and IPS itself is known to suffer from high variance. Consequently, offline RL approaches that depend on IPS are ill-suited for our setting.</p><h3>Advantage Weighted Supervised Fine Tuning</h3><p>Given the three challenges outlined above, we propose a new algorithm Advantage-Weighted SFT (A-SFT). It leverages a combination of supervised fine-tuning and advantage reweighting from reinforcement learning. The key observation is as follows. Despite the reward estimation for each individual event having a high uncertainty, we find the estimations of rewards contain directional signals between high-reward and low-reward events. These signals could help better align the model during post-training.</p><p>A central factor in this study is the generalization ability of the reward model. Better generalization enables more accurate predictions of user preferences for unseen titles, thereby making exploration more effective. For reward models with moderate to high generalization power, both online RL methods such as PPO and offline RL methods such as CQL can perform effectively. However, in our setting, reward model generalization is worse than the language counterparts’, which makes these algorithms less appropriate. In addition, the use of techniques like inverse propensity scoring (IPS) introduces a heightened risk of high-variance estimates, prompting us to exclude algorithms such as off-policy REINFORCE.</p><p>Our proposed method A-SFT does not rely on IPS. With no need of prior knowledge of the logging policy, it can be generally applied to cases where observation of the environments are limited or biased. This is particularly useful to the recommendation setting due to the user feedback loop and distribution shifts with time. Without knowing the logging policy, A-SFT still provides means to control the policy deviation between the current policy and logging policy by tuning the parameter. This design provides essential means to control the learnt bias from uncertain reward models. We show that A-SFT outperforms baseline behavior cloning by directly optimizing observed rewards.</p><p>The advantage-weighted SFT algorithm is as follows:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eP-_FyLRs6vrGnwyIp_j_A.png\" /></figure><p>For the results presented in this blog post, we treat the recommendation problem as a contextual bandit, i.e. given a history of user interactions as the context, can we recommend a high reward next title recommendation for the user?</p><h3>Benchmarks</h3><p>We compared representative algorithms including PPO, IPO, DPO, CQL and SFT as the baselines:</p><ol><li><strong>Reward weighted Behavior Cloning</strong>: This benchmark algorithm modifies supervised fine-tuning (SFT) by weighting the loss with the raw rewards of the chosen item instead of weighing the loss with advantage as in the proposed algorithm.</li><li><strong>Rejection Sampling Direct Preference Optimization / Identity Preference Optimization (RS DPO/IPO)</strong>: this is a variant of DPO/IPO where, for each user history x, ​we generate contrasting response pairs by training an ensemble of reward models to estimate confidence intervals for the reward of multiple potential responses y. If the lower bound of the reward confidence interval for one response​ is less than the upper bound for another response, then this pair is used to train DPO/IPO.</li><li><strong>Conservative Q-Learning (CQL)</strong>: This is a standard offline algorithm that learns a conservative Q function, penalizing overestimation of Q-values, particularly in regions of the state-action space with little or no reward data.</li><li><strong>Proximal Policy Optimization (PPO)</strong>: This is a standard RLHF (Reinforcement Learning from Human Feedback) algorithm that uses reward models as an online environment. PPO learns an advantage function and optimizes the policy to maximize expected reward while maintaining proximity to the initial policy.</li></ol><p>We sampled a separate test set of O(Millions) users. This test set is collected on a future date after the training.</p><h3>Offline Evaluation Results</h3><p>We evaluate our algorithm on a dataset of high-reward user trajectories. For sake of simplicity, we consider a trajectory to have a high reward if the accumulated reward is higher than the median of the population. We present the following metrics for the held out test dataset:</p><ol><li><strong>NDCG@k</strong>: This measures the ranking quality of the recommended items up to position k. It accounts for the position of relevant items in the recommendation list, assigning higher scores when relevant items appear higher in the ranking. The gain is discounted logarithmically at lower ranks, and the result is normalized by the ideal ranking (i.e., the best possible ordering of items).</li><li><strong>HR@k</strong>: This measures the proportion of test cases in which the ground-truth chosen item y appears in the top k recommendations. It is a binary metric per test case (hit or miss) and is averaged over all test cases.</li><li><strong>MRR</strong>: MRR evaluates the ranking quality by measuring the reciprocal of the rank at which the chosen item appears in the recommendation list. The metric is averaged across all test cases.</li><li><strong>Reward Model as A Judge</strong>: We use the reward model to evaluate the policy for future user events. We propose to use an ensemble of reward models for the evaluation to increase confidence. The result is based on the discounted reward generated for a few steps. The standard deviation is less than 4%.</li></ol><p>We measure the percentage improvement in each metric compared to the baseline, Reward Weighted Behavior Cloning(BC). We notice that advantage weighted SFT shows the largest improvement in metrics, outweighing BC as well as reward model dependent algorithms like CQL, PPO, DPO and IPO.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Z8wcCOETlobx8T_oVuDRiA.png\" /></figure><p>Our experiments show that advantage weighted SFT is a simple but promising approach for post-training generative recommenders as it deals with the issue of poor reward model generalizations and lack of IPS. More specifically, we find PPO, IPO and DPO achieve a good reward score, but also causes the overfitting from the reward model. Conservative Q-Learning achieves more robust improvements but does not fully capture the potential signals in the reward modeling. A-SFT achieved both better recommendation metrics and reward scores.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61a538d717a9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9\">Post-Training Generative Recommenders with Advantage-Weighted Supervised Finetuning</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-10-25T22:01:00.000Z",
    "url": "https://netflixtechblog.com/post-training-generative-recommenders-with-advantage-weighted-supervised-finetuning-61a538d717a9?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "Behind the Streams: Real-Time Recommendations for Live Events Part 3",
    "partialText": "<p>By: <a href=\"https://www.linkedin.com/in/krisrange/\">Kris Range</a>, <a href=\"https://www.linkedin.com/in/gulatiankush/\">Ankush Gulati</a>, <a href=\"https://www.linkedin.com/in/jimpisaacs/\">Jim Isaacs</a>, <a href=\"https://www.linkedin.com/in/jennifer-s-0019a516/\">Jennifer Shin</a>, <a href=\"https://www.linkedin.com/in/jeremy-kelly-526a30180/\">Jeremy Kelly</a>, <a href=\"https://www.linkedin.com/in/jason-t-26850b26/\">Jason Tu</a></p><p><em>This is part 3 in a series called “Behind the Streams”. Check out </em><a href=\"https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40\"><em>part 1</em></a><em> and </em><a href=\"https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967\"><em>part 2</em></a><em> to learn more.</em></p><p>Picture this: It’s seconds before the biggest fight night in Netflix history. Sixty-five million fans are waiting, devices in hand, hearts pounding. The countdown hits zero. What does it take to get everyone to the action on time, every time? At Netflix, we’re used to on-demand viewing where everyone chooses their own moment. But with live events, millions are eager to join in at once. Our job: make sure our members never miss a beat.</p><p>When Live events break streaming records <a href=\"https://about.netflix.com/en/news/60-million-households-tuned-in-live-for-jake-paul-vs-mike-tyson\">¹</a> <a href=\"https://about.netflix.com/en/news/netflix-nfl-christmas-gameday-reaches-65-million-us-viewers\">²</a> <a href=\"https://about.netflix.com/en/news/over-41-million-global-viewers-on-netflix-watch-terence-crawford-defeat\">³</a>, our infrastructure faces the ultimate stress test. Here’s how we engineered a discovery experience for a global audience excited to see a knockout.</p><h3>Why are Live Events Different?</h3><p>Unlike Video on Demand (VOD), members want to catch live events as they happen. There’s something uniquely exciting about being part of the moment. That means we only have a brief window to recommend a Live event at just the right time. Too early, excitement fades; too late, the moment is missed. Every second counts.</p><p>To capture that excitement, we enhanced our recommendation delivery systems to serve real-time suggestions, providing members richer and more compelling signals to hit play in the moment when it matters most. The challenge? Sending dynamic, timely updates concurrently to over a hundred million devices worldwide without creating a <a href=\"https://en.wikipedia.org/wiki/Thundering_herd_problem\">thundering herd effect</a> that would overwhelm our cloud services. Simply scaling up linearly isn’t efficient and reliable. For popular events, it could also divert resources from other critical services. We needed a smarter and more scalable solution than just adding more resources.</p><h3>Orchestrating the moment: Real-time Recommendations</h3><p>With millions of devices online and live event schedules that can shift in real time, the challenge was to keep everyone perfectly in sync. We set out to solve this by building a system that doesn’t just react, but adapts by dynamically updating recommendations as the event unfolds. We identified the need to balance three constraints:</p><ul><li><strong>Time</strong>: the duration required to coordinate an update.</li><li><strong>Request throughput</strong><em>: </em>the capacity of our cloud services to handle requests.</li><li><strong>Compute cardinality</strong>: the variety of requests necessary to serve a unique update.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-z6A8FriBAbJW5BcwMrZTA.png\" /><figcaption>Visualizing constraints for real-time updates</figcaption></figure><p>We solved this constraint optimization problem by splitting the real-time recommendations into two phases: <strong>prefetching</strong> and <strong>real-time broadcasting</strong>. First, we prefetch the necessary data ahead of time, distributing the load over a longer period to avoid traffic spikes. When the Live event starts or ends, we broadcast a low cardinality message to all connected devices, prompting them to use the prefetched data locally. The timing of the broadcast also adapts when event times shift to preserve accuracy with the production of the Live event. By combining these two phases, we’re able to keep our members’ devices in sync and solve the thundering herd problem. To maximize device reach, especially for those with unstable networks, we use “at least once” broadcasts to ensure every device gets the latest updates and can catch up on any previously missed broadcasts as soon as they’re back online.</p><p>The first phase optimizes <strong>request throughput </strong>and <strong>compute cardinality</strong> by prefetching materialized recommendations, displayed title metadata, and artwork for a Live event. As members naturally browse their devices before the event, this data is prepopulated and stored locally in device cache, awaiting the notification trigger to serve the recommendations instantaneously. By distributing these requests naturally over time ahead of the event, we can eliminate any related traffic spikes and avoid the need for large-scale, real-time system scaling.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QmB99SosLqs-JEo0gp1wwg.png\" /><figcaption>A phased approach, smoothing traffic requests over time with a real-time low-cardinality broadcast</figcaption></figure><p>The second phase optimizes<strong> request throughput </strong>and<strong> time </strong>to update<strong> </strong>devices by broadcasting a low-cardinality, real-time message to all connected devices at critical moments in a Live event’s lifecycle. Each broadcast payload includes a <strong>state key</strong> and a <strong>timestamp</strong>. The state key indicates the current stage of the Live event, allowing devices to use their pre-fetched data to update cached responses locally without additional server requests. The timestamp ensures that if a device misses a broadcast due to network issues, it can catch up by replaying missed updates upon reconnecting. This mechanism guarantees devices receive updates at least once, significantly increasing delivery reliability even on unstable networks.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*h6CIrfYnpR24NS5hgvxEGA.png\" /><figcaption>A phased approach optimizes each constraint to ensure we can deliver for the big moment!</figcaption></figure><blockquote>Moment in Numbers: During peak load, we have successfully delivered updates at multiple stages of our events to over 100 million devices in under a minute.</blockquote><h3>Under the Hood: How It Works</h3><p>With the big picture in mind, let’s examine how these pieces interact in practice.</p><p>In the diagram below, the Message Producer microservice centralizes all of the business logic. It continuously monitors live events for setup and timing changes. When it detects an update, it schedules broadcasts to be sent at precisely the right moment. The Message Producer also standardizes communication by providing a concise GraphQL schema for both device queries and broadcast payloads.</p><p>Rather than sending broadcasts directly to devices via WebSocket, the Message Producer hands them off to the Message Router. The Message Router is part of a robust two-tier pub/sub architecture built on proven technologies like <a href=\"https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658\">Pushy</a> (our WebSocket proxy), Apache Kafka, and <a href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\">Netflix’s KV key-value store</a>. The Message Router tracks subscriptions at the Pushy node granularity, while Pushy nodes map the subscriptions to individual connections, creating a low-latency fanout that minimizes compute and bandwidth requirements.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Kc1l_Wnc4i08xFA8JnRLCA.png\" /></figure><p>Devices interface with our GraphQL <a href=\"https://netflix.github.io/dgs/\">Domain Graph Service (DGS)</a>. These schemas offer multiple query interfaces for prefetching, allowing devices to tailor their requests to the specific experience being presented. Each response adheres to a consistent API that resolves to a map of stage keys, enabling fast lookups and keeping business logic off the device. Our broadcast schema specifies WebSocket connection parameters, the current event stage, and the timestamp of the last broadcast message. When a device receives a broadcast, it injects the payload directly into its cache, triggering an immediate update and re-render of the interface.</p><h3>Balancing the Moment: Throughput Management</h3><p>In addition to building the new technology to support real-time recommendations, we also evaluated our existing systems for potential traffic hotspots. Using high-watermark traffic projections for live events, we generated synthetic traffic to simulate game-day scenarios and observed how our online services handled these bursts. Through this process, several common patterns emerged:</p><p><strong>Breaking the Cache Synchrony</strong></p><p>Our game-day simulations revealed that while our approach mitigated the immediate thundering herd risks driven by member traffic during the events, live events introduced unexpected mini thundering herds in our systems hours before and after the actual events. The surge of members joining just in time for these events led to concentrated cache expirations and recomputations, which created traffic spikes well outside the event window that we did not anticipate. This was not a problem for VOD content because the member traffic patterns are a lot smoother. We found that fixed TTLs caused cache expirations and refresh-traffic spikes to happen all at once. To address this, we added jitter to server and client cache expirations to spread out refreshes and smooth out traffic spikes.</p><p><strong>Adaptive Traffic Prioritization</strong></p><p>While our services already leverage traffic prioritization and partitioning based on factors such as request type and device type, live events introduced a distinct challenge. These events generated brief traffic bursts that were intensely spiky and placed significant strain on our systems. Through simulations, we recognized the need for an additional event-driven layer of traffic management.</p><p>To tackle this, we improved our traffic sharding strategies by using event-based signals. This enabled us to route live event traffic to dedicated clusters with more aggressive scaling policies. We also added a dynamic traffic prioritization ruleset that activates whenever we see high requests per second (RPS) to ensure our systems can handle the surge smoothly. During these peaks, we aggressively deprioritize non-critical server-driven updates so that our systems can devote resources to the most time-sensitive computations. This approach ensures smooth performance and reliability when demand is at its highest.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/729/1*PYhFbFxK5PbOEAnYtTfD4Q.jpeg\" /><figcaption>Snapshot of non-critical traffic volume decline (in %) for a member-facing service during a live event — achieved via aggressive de-prioritization</figcaption></figure><h3>Looking Ahead</h3><p>When we set out to build a seamlessly scalable scheduled viewing experience, our goal was to create a dynamic and richer member experience for live content. Popular live events like the Crawford v. Canelo fight and the NFL Christmas games truly put our systems to the test. Along the way, we also uncovered valuable learnings that continue to shape our work. Our attempts to deprioritize traffic to other non-critical services caused unexpected call patterns and spikes in traffic elsewhere. Similarly, in hindsight, we also learned that the high traffic volume from popular events caused excessive non-essential logging and was putting unnecessary pressure on our ingestion pipelines.</p><p>None of this work would have been possible without our stunning colleagues at Netflix who collaborated across multiple functions to architect, build, and test these approaches, ensuring members can easily access events at the right moment: UI Engineering, Cloud Gateway, Data Science &amp; Engineering, Search and Discovery, Evidence Engineering, Member Experience Foundations, Content Promotion and Distribution, Operations and Reliability, Device Playback, Experience and Design and Product Management.</p><p>As Netflix’s content offering expands to include new formats like live titles, free-to-air linear content, and games, we’re excited to build on what we’ve accomplished and look ahead to even more possibilities. Our roadmap includes extending the capabilities we developed for scheduled live viewing to these emerging formats. We’re also focused on enhancing our engineering tooling for greater visibility into operations, message delivery, and error handling to help us continue to deliver the best possible experience for our members.</p><h3>Join Us for What’s Next</h3><p>We’re just scratching the surface of what’s possible as we bring new live experiences to members around the world. If you are looking to solve interesting technical challenges in a <a href=\"https://jobs.netflix.com/culture\">unique culture</a>, then <a href=\"https://jobs.netflix.com/\">apply</a> for a role that captures your curiosity.</p><p><em>Look out for future blog posts in our “Behind the Streams” series, where we’ll explore the systems that ensure viewers can watch live streams once they manage to find and play them.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e027cb313f8f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f\">Behind the Streams: Real-Time Recommendations for Live Events Part 3</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-10-21T00:53:29.000Z",
    "url": "https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…",
    "partialText": "<h3>How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data Streams at Internet Scale</h3><p>Authors: <a href=\"https://www.linkedin.com/in/ataruc/\">Adrian Taruc</a> and <a href=\"https://www.linkedin.com/in/jamesdalydalton/\">James Dalton</a></p><p><em>This is the first entry of a multi-part blog series describing how we built a Real-Time Distributed Graph (RDG). In Part 1, we will discuss the motivation for creating the RDG and the architecture of the data processing pipeline that populates it.</em></p><h3>Introduction</h3><p>The Netflix product experience historically consisted of a single core offering: streaming video on demand. Our members logged into the app, browsed, and watched titles such as Stranger Things, Squid Game, and Bridgerton. Although this is still the core of our product, our business has changed significantly over the last few years. For example, we introduced ad-supported plans, live programming events (e.g., <a href=\"https://www.netflix.com/tudum/articles/jake-paul-vs-mike-tyson-live-release-date-news\">Jake Paul vs. Mike Tyson</a> and <a href=\"https://www.netflix.com/tudum/articles/nfl-games-on-netflix\">NFL Christmas Day Games</a>), and <a href=\"https://about.netflix.com/en/news/let-the-games-begin-a-new-way-to-experience-entertainment-on-mobile\">mobile games</a> as part of a Netflix subscription. This evolution of our business has created a new class of problems where we have to analyze member interactions with the app across different business verticals. Let’s walk through a simple example scenario:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*TPFlIvYqGC3L2x1A-KqkyQ.png\" /></figure><ol><li>Imagine a Netflix member logging into the app on their smartphone and beginning to watch an episode of Stranger Things.</li><li>Eventually, they decide to watch on a bigger screen, so they log into the app on a smart TV in their home and continue watching the same episode.</li><li>Finally, after completing the episode, they log into the app on their tablet and play the game “Stranger Things: 1984”.</li></ol><p>We want to know that these three activities belong to the same member, despite occurring at different times and across various devices. In a traditional data warehouse, these events would land in at least two different tables and may be processed at different cadences. But in a graph system, they become connected almost instantly. Ultimately, analyzing member interactions in the app across domains empowers Netflix to create more personalized and engaging experiences.</p><p>In the early days of our business expansion, discovering these relationships and contextual insights was extremely difficult. Netflix is famous for adopting a microservices architecture — hundreds of microservices developed and maintained by hundreds of individual teams. Some notable benefits of microservices are:</p><ol><li><strong>Service Decomposition</strong>: The overall platform is separated into smaller services, each responsible for a specific business capability. This modularity allows for independent service development, deployment, and scaling.</li><li><strong>Data Isolation</strong>: Each service manages its own data, reducing interdependencies. This allows teams to choose the most suitable data schemas and storage technologies for their services.</li></ol><p><strong>However, these benefits also led to drawbacks for our data science and engineering partners.</strong> In practice, the separation of business concerns and service development ultimately resulted in a separation of data. Manually stitching data together from our data warehouse and siloed databases was an onerous task for our partners. Our data engineering team recognized we needed a solution to process and store our enormous swath of interconnected data while enabling fast querying to discover insights. Although we could have structured the data in various ways, we ultimately settled on a graph representation. We believe a graph offers key advantages, specifically:</p><ul><li><strong>Relationship-Centric Queries:</strong> Graphs enable fast “hops” across multiple nodes and edges without expensive joins or manual denormalization that would be required in table-based data models.</li><li><strong>Flexibility as Relationships Grow:</strong> As new connections and entities emerge, graphs can quickly adapt without significant schema changes or re-architecture.</li><li><strong>Pattern and Anomaly Detection:</strong> Our stakeholders’ use cases often require identifying hidden relationships, cycles, or groupings in the data — capabilities much more naturally expressed and efficiently executed using graph traversals than siloed point lookups.</li></ul><p>This is why we set out to build a Real-Time Distributed Graph, or “RDG” for short.</p><h3>Ingestion and Processing</h3><p>Three main layers in the system power the RDG:</p><ol><li><strong>Ingestion and Processing</strong> — receive events from disparate upstream data sources and use them to generate graph nodes and edges.</li><li><strong>Storage</strong> — write nodes and edges to persistent data stores.</li><li><strong>Serving</strong> — expose ways for internal clients to query graph nodes and edges.</li></ol><p><strong>The rest of this post will focus on the first layer, while subsequent posts in this blog series will cover the other layers.</strong> The diagram below depicts a high-level overview of the ingestion and processing pipeline:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Jy0eVxvB-AzFNijNfRb9ZA.png\" /></figure><p>Building and updating the RDG in real-time requires continuously processing vast volumes of incoming data. Batch processing systems and traditional data warehouses cannot offer the low latency needed to maintain an up-to-date graph that supports real-time applications. We opted for a stream processing architecture, enabling us to update the graph’s data as events happen, thus minimizing delay and ensuring the system reflects the latest member actions with the Netflix app.</p><h3>Kafka as the Ingestion Backbone</h3><p>Member actions in the Netflix app are published to our API Gateway, which then writes them as records to <a href=\"https://kafka.apache.org/\">Apache Kafka</a> topics. Kafka is the mechanism through which internal data applications can consume these events. It provides durable, replayable streams that downstream processors, such as <a href=\"https://flink.apache.org/\">Apache Flink</a> jobs, can consume in real-time.</p><p>Our team’s applications consume several different Kafka topics, each generating up to roughly <strong>1 million messages per second</strong>. Topic records are encoded in the <a href=\"https://avro.apache.org/\">Apache Avro</a> format, and Avro schemas are persisted in an internal centralized schema registry. In order to strike a balance between maintaining data availability and managing the financial expenses of storage infrastructure, we tailor retention policies for each topic according to its throughput and record size. We also persist topic records to <a href=\"https://iceberg.apache.org/\">Apache Iceberg</a> data warehouse tables, which allows us to backfill data in scenarios where older data is no longer available in the Kafka topics.</p><h3>Processing Data with Apache Flink</h3><p>The event records in the Kafka streams are ingested by Flink jobs. We chose Flink because of its strong capabilities around near-real-time event processing. There is also robust internal platform support for Flink within Netflix, which allows jobs to integrate with Kafka and various storage backends seamlessly. At a high level, the anatomy of an RDG Flink job looks like this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0G-yrGzB_ZbgaRlPbextTQ.png\" /></figure><p>For the sake of simplicity, the diagram above depicts a basic flow in which a member logs into their Netflix account and begins watching an episode of Stranger Things. Reading the diagram from left to right:</p><ul><li>The actions of logging into the app and watching the Stranger Things episode are ultimately written as events to Kafka topics.</li><li>The Flink job consumes event records from the upstream Kafka topics.</li><li>Next, we have a series of Flink processor functions that:</li></ul><ol><li>Apply filtering and projections to remove noise based on the individual fields that are present — or in some cases, not present — in the events.</li><li>Enrich events with additional metadata, which are stored and accessed by the processor functions via side inputs.</li><li>Transform events into graph primitives — nodes representing entities (e.g., member accounts and show/movie titles), and edges representing relationships or interactions between them. In this example, the diagram only shows a few nodes and an edge to keep things simple. However, in reality, we create and update up to a few dozen different nodes and edges, depending on the member actions that occurred within the Netflix app.</li><li>Buffer, detect, and deduplicate overlapping updates that occur to the same nodes and edges within a small, configurable time window. This step reduces the data throughput we publish downstream. It is implemented using stateful process functions and timers.</li><li>Publish nodes and edges records to <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh</a>, an abstraction layer that connects data applications and storage systems. We write a total (nodes + edges) of <strong>more than 5 million records per second</strong> to Data Mesh, which handles persisting the records to various data stores that other internal services can query.</li></ol><h3>From One Job to Many: Scaling Flink the Hard Way</h3><p>Initially, we tried having just one Flink job that consumed all the Kafka source topics. However, this quickly became a big operational headache since different topics can have different data volumes and throughputs at different times during the day. Consequently, tuning the monolithic Flink job became extremely difficult — we struggled to find CPU, memory, job parallelism, and checkpointing interval configurations that ensured job stability.</p><p>Instead, we pivoted to having a 1:1 mapping from the Kafka source topic to the consuming Flink job. Although this led to additional operational overhead due to more jobs to develop and deploy, each job has been much simpler to maintain, analyze, and tune.</p><p>Similarly, each node and edge type is written to a separate Kafka topic. This means we have significantly more Kafka topics to manage. However, we decided the tradeoff of having bespoke tuning and scaling per topic was worth it. We also designed the graph data model to be as generic and flexible as possible, so adding new types of nodes and edges would be an infrequent operation.</p><h3>Acknowledgements</h3><p>We would be remiss if we didn’t give a special shout-out to our stunning colleagues who work on the internal Netflix data platform. Building the RDG was a multi-year effort that required us to design novel solutions, and the investments and foundations from our platform teams were critical to its successful creation. You make the lives of Netflix data engineers much easier, and the RDG would not exist without your diligent collaboration!</p><p>—</p><p>Thanks for reading the first season of the RDG blog series. Check out <a href=\"https://netflixtechblog.medium.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-2-building-a-scalable-storage-layer-ff4a8dbd3d1f\">Part 2</a>, where we go over the storage layer containing the graph’s various nodes and edges.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=80113e124acc\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc\">How and Why Netflix Built a Real-Time Distributed Graph: Part 1 — Ingesting and Processing Data…</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-10-17T18:42:37.000Z",
    "url": "https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc?source=rss----2615bd06b42e---4"
  },
  {
    "publisherId": "netflix",
    "publisherName": "Netflix Tech Blog",
    "specTitle": "스케일링",
    "categories": [
      "backend"
    ],
    "specUrl": "https://medium.com/feed/netflix-techblog",
    "title": "100X Faster: How We Supercharged Netflix Maestro’s Workflow Engine",
    "partialText": "<p>By <a href=\"https://www.linkedin.com/in/jheua/\">Jun He</a>, <a href=\"https://www.linkedin.com/in/yingyi-zhang-a0a164111/\">Yingyi Zhang</a>, <a href=\"https://www.linkedin.com/in/spearsem/\">Ely Spears</a></p><h3>TL;DR</h3><p>We recently upgraded the Maestro engine to go beyond scalability and improved its performance by <strong>100X</strong>! The overall overhead is reduced from seconds to milliseconds. We have updated the Maestro open source project with this improvement! Please visit the <a href=\"https://github.com/Netflix/maestro\">Maestro GitHub repository</a> to get started. If you find it useful, please <a href=\"https://github.com/Netflix/maestro\">give us a star</a>.</p><h3>Introduction</h3><p>In our previous <a href=\"https://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78\">blog post</a>, we introduced Maestro as a horizontally scalable workflow orchestrator designed to manage large-scale Data/ML workflows at Netflix. Over the past two and a half years, Maestro has achieved its design goal and successfully supported massive workflows with hundreds of thousands of jobs, managing millions of executions daily. As the adoption of Maestro increases at Netflix, new use cases have emerged, driven by Netflix’s evolving business needs, such as Live, Ads, and Games. To meet these needs, some of the workflows are now scheduled on a sub-hourly basis. Additionally, Maestro is increasingly being used for low-latency use cases, such as ad hoc queries, beyond traditional daily or hourly scheduled ETL data pipeline use cases.</p><p>While Maestro excels in orchestrating various heterogeneous workflows and managing user end-to-end development experiences, users have experienced noticeable speedbumps (i.e. ten seconds overhead) from the Maestro engine during workflow executions and development, affecting overall efficiency and productivity. Although being fully scalable to support Netflix-scale use cases, the processing overhead from Maestro internal engine state transitions and lifecycle activities have become a bottleneck, particularly during development cycles. Users have expressed the need for a high performance workflow engine to support iterative development use cases.</p><p>To visualize our end users’ needs for the workflow orchestrator, we create a 5-layer structure graph shown below. Before the change, Maestro reached level 4 but faced challenges to satisfy the user’s needs in level 5. With the new engine design, Maestro is able to power the users to work with their highest capacity and spark joy for end users during their development over the Maestro.</p><figure><img alt=\"Figure 1. A 5-layer structure showing needs for the workflow orchestrator\" src=\"https://cdn-images-1.medium.com/max/562/1*q871tK1C7Y8VSAXVOmu4Ig.png\" /><figcaption>Figure 1. A 5-layer structure showing needs for the workflow orchestrator.</figcaption></figure><p>In this blog post, we will share our new engine details, explain our design trade-off decisions, and share learnings from this redesign work.</p><h3>Architectural Evolution of Maestro</h3><h4>Before the change</h4><p>To understand the improvements, we will first revisit the original architecture of Maestro to understand why the overhead is high. The system was divided into three main layers, as illustrated in the diagram below. In the sections that follow we will explain each layer and the role it played in our performance optimization.</p><figure><img alt=\"Figure 2. The architecture diagram before the evolution.\" src=\"https://cdn-images-1.medium.com/max/1024/1*198QCvklU8o6aUrDdaBGRA.png\" /><figcaption>Figure 2. The architecture diagram before the evolution.</figcaption></figure><p><strong>Maestro API and Step Runtime Layer</strong></p><p>This layer offers seamless integrations with other Netflix services (e.g., compute engines like Spark and Trino). Using Maestro, thousands of practitioners build production workflows using a paved path to access platform services . They can focus primarily on their business logic while relying on Maestro to manage the lifecycle of jobs and workflows plus the integration with data platform services and required integrations such as for authentication, monitoring and alerting. This layer functioned efficiently without introducing significant overhead.</p><p><strong>Maestro Engine Layer</strong></p><p>The Maestro engine serves several crucial functions:</p><ul><li>Managing the lifecycle of workflows, their steps and maintaining their state machines</li><li>Supporting all user actions (e.g., start, restart, stop, pause) on workflow and step entities</li><li>Translating complex Maestro workflow graphs into parallel flows, where each flow is an array of sequentially chained flow tasks, translating every step into a flow task, and then executing transformed flows using the internal flow engine</li><li>Acting as a middle layer to maintain isolation between the Maestro step runtime layer and the underlying flow engine layer</li><li>Implementing required data access patterns and writing Maestro data into the database</li></ul><p>In terms of speed, this layer had acceptable overhead but faced edge cases (e.g. a step might be concurrently executed by two workers at the same time, causing race conditions) due to lacking a strong guarantee from the internal flow engine and the external distributed job queue.</p><p><strong>Maestro Internal Flow Engine Layer</strong></p><p>The Maestro internal flow engine performed <strong>2</strong> primary functions:</p><ul><li>Calling task’s execution functions at a given interval.</li><li>Starting the next tasks in an array of sequential task flows (not a graph), if applicable.</li></ul><p>This foundational layer was based on Netflix OSS Conductor 2.x (<a href=\"https://github.com/Netflix/conductor/releases/tag/v3.0.0\">deprecated since Apr 2021</a>), which requires a dedicated set of separate database tables and distributed job queues.</p><p>The existing implementation of this layer introduces an impactful overhead (e.g. a few seconds to tens of seconds overall delays). The lack of strong guarantees (e.g. exactly once publishing) from this layer leads to race conditions which cause stuck jobs or lost executions.</p><h4>Options to consider</h4><p>We have evaluated three options to address those existing issues:</p><ul><li>Option 1: Implement an internal flow engine optimized for Maestro specific use cases</li><li>Option 2: Upgrade Conductor library to 4.0, which addresses the overheads and offers other improvements and enhancements compared with Conductor 2.X.</li><li>Option 3: Use Temporal as the internal flow engine</li></ul><p>One aspect that influenced our assessment of option two is that Conductor 2 provided a final callback capability in the state machine that was contributed specifically for Maestro’s use case to ensure database synchronization between the Conductor and Maestro engine states. It would require porting this functionality to Conductor 4 though it had been dropped given no other Conductor use cases besides Maestro relied on this. By rewriting the flow engine it would allow removal of several complex internal databases and database synchronization requirements which was attractive for simplifying operational reliability. Given Maestro did not need the full set of state engine features offered by Conductor, this motivated us to consider a flow engine rewrite as a higher priority.</p><p>The decision for Temporal was more straightforward. Temporal is optimized towards facilitating inter-process orchestration and would involve calling an external service to interact with the Temporal flow engine. Given Maestro is operating greater than a million tasks per day, many of which are long running, we felt it was an unnecessary source of risk to couple the DAG engine execution with an external service call. If our requirements went beyond lightweight state transition management we might reconsider because Temporal is a very robust control plane orchestration system, but for our needs it introduced complexity and potential reliability weak spots when there was no direct need for the advanced feature set that it offered.</p><p>After considering Option 2 and Option 3, we developed more conviction that Maestro’s architecture could be greatly simplified by not using a full DAG evaluation engine and having to maintain the state machine for two systems (Maestro and Conductor/Temporal). Therefore, we have decided to go with Option 1.</p><h4>After the change</h4><p>To address these issues, we completely rewrote the Maestro internal flow engine layer to satisfy Maestro’s specific needs and optimize its performance. This new flow engine is lightweight with minimal dependencies, focusing on excelling in the two primary functions mentioned <a href=\"#4bd7\">above</a>. We also replaced existing distributed job queues with internal ones to provide a strong guarantee.</p><p>The new engine is <strong>highly performant, efficient, scalable, and fault-tolerant</strong>. It is the foundation for all upper components of Maestro and provides the following guarantees to avoid race conditions:</p><ul><li>A single step should only be executed by a single worker at any given time</li><li>Step state should never be rolled back</li><li>Steps should always eventually run to a terminal state</li><li>The internal flow state should be eventually consistent with the Maestro workflow state</li><li>External API and user actions should not cause race conditions on the workflow execution</li></ul><p>Here is the new architecture diagram after the change, which is much simpler with less dependencies:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZGutO7STwBr81NQ2qAWjYQ.png\" /><figcaption>Figure 3. The architecture diagram after the evolution.</figcaption></figure><h3>New Flow Engine Optimization</h3><p>The new flow engine significantly boosts speed by maintaining state in memory. It ensures consistency by using Maestro engine’s database as the source of truth for workflow and step states. During bootstrapping, the flow engine rebuilds its in-memory state from the database, improving performance and simplifying the overall architecture. This is in contrast to the previous design in which multiple databases had to be reconciled against one another (Conductor’s tables and Maestro’s tables) or else suffer race conditions and rare orphaned job status.</p><p>The flow engine operates on in-memory flow states, resembling a <a href=\"https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html#write-through\">write through caching pattern</a>. Updates to workflow or step state in the database also update the in-memory flow state. If in-memory state is lost, the flow engine rebuilds it from the database, ensuring eventual consistency and resolving race conditions.</p><p>This design delivers lower latency and higher throughput, avoids inconsistencies from dual persistence, simplifies the architecture, and keeps the in‑memory view eventually consistent with the database.</p><h4>Maintaining Scalability While Gaining Speed</h4><p>With the new engine, we significantly boost performance by collocating flows and their tasks on the same node throughout their lifecycle. Therefore, states of a flow and its tasks will stay in a single node’s memory without persisting to the database. This stickiness and locality bring great performance benefits but inevitably impact scalability since tasks are no longer reassigned to a new worker of the whole cluster in each polling cycle.</p><p>To maintain horizontal scalability, we introduced a flow group concept to partition running flows into groups. In this way, each Maestro flow engine instance only needs to maintain ownership of groups rather than individual flows, reducing maintenance costs (e.g., heartbeat) and simplifying reconciliation by allowing each Maestro node to load flows for a group in batches. Each Maestro node claims ownership of a group of flows through a flow group actor and manages their entire lifecycle via child flow actors. If ownership is lost due to node failure or long JVM GC, another node can claim the group to resume flow executions by reconciling internal state from Maestro database. The following diagram illustrates the ownership maintenance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nvUSa5zxZOWi4G8EIh9bQA.png\" /><figcaption>Figure 4. Ownership maintenance sequence diagram.</figcaption></figure><h4>Flow Partitioning</h4><p>To efficiently distribute traffic, Maestro assigns a consistent group ID to flows/workflows by a simple stable ID assignment method, as shown in the diagram’s Partitioning Function box. We chose this simpler partitioning strategy over advanced ones, e.g. consistent hashing, primarily due to execution and reconciliation costs and consistency challenges in a distributed system.</p><p>Since Maestro decomposes workflows into hierarchical internal flows (e.g., foreach), parent flows need to interact with child flows across different groups. To enable this, the maximal group number from the parent, denoted as N’ in the diagram, is passed down to all child flows. This allows child flows, such as subworkflows or foreach iterations, to recompute their own group IDs and also ensures that a parent flow can always determine the group ID of its child flows using only their workflow identifiers.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PdP7OEWMFDpbNTZZNuKe7Q.png\" /><figcaption>Figure 5. Flow group partitioning mechanism diagram.</figcaption></figure><p>After a flow’s group ID is determined, the flow operator routes the flow request to the appropriate node. Each node owns a specific range of group IDs. For example, in the diagram, Node 1 owns groups 0, 1, and 2, while Node 3 owns groups 6, 7, and 8. The groups then contain the individual flows (e.g., Flow A, Flow B).</p><p>In this design, the group size is configurable and nodes can also have different group size configurations. The following diagram shows a flow group partitioning example while the maximal group number is changed during the engine execution without impacting any existing workflows.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cHk1MpAAcjfKC0GvRL57gA.png\" /><figcaption>Figure 6. A flow group partitioning example.</figcaption></figure><p>In short, Maestro flow engine shares the group info across the parent and child workflows to provide a flexible and stable partitioning mechanism to distribute work across the cluster.</p><h4>Queue Optimization</h4><p>We replaced both external distributed job queues in the existing system with internal ones, preserving the same fault‑tolerance and recovery guarantees while reducing latency and boosting throughput.</p><p>For the internal flow engine, the queue is a simple in‑memory Java blocking queue. It requires no persistence and can be rebuilt from Maestro state during reconciliation.</p><p>For the Maestro engine, we implemented a database‑backed in‑memory queue that provides <strong>exactly‑once publishing and at‑least‑once delivery guarantees</strong>, addressing multiple edge cases that previously required manual state correction.</p><p>This design is similar to the<a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/transactional-outbox.html\"> transactional outbox pattern</a>. In the same transaction that updates Maestro tables, a row is inserted into the `maestro_queue` table. Upon transaction commit, the job is immediately pushed to a queue worker on the same node, eliminating polling latency. After successful processing, the worker deletes the row from the database. A periodic sweeper re-enqueues any rows whose timeout has expired, ensuring another worker picks them up if a worker stalls or a node fails.</p><p>This design handles failures cleanly. If the transaction fails, both data and message roll back atomically, no partial publishing. If a worker or node fails after commit, the timeout mechanism ensures the job is retried elsewhere. On restart, a node rebuilds its in‑memory queue from the queue table, providing at-least-once delivery guarantee.</p><p>To enhance scalability and avoid contention across event types, each event type is assigned a `queue_id`. Job messages are then partitioned by `queue_id`, optimizing performance and maintaining system efficiency under high load.</p><h3>From Stateless Worker Model to Stateful Actor Model</h3><p>Maestro previously used a shared-nothing stateless worker model with a polling mechanism. When a task started, its identifier was enqueued to a distributed task queue. A worker from the flow engine would pick the task identifier from the queue, load the complete states of the whole workflow (including the flow itself and every task), execute the task interface method once, write the updated task data back to the database, and put the task back in the queue with a polling delay. The worker would then forget this task and start polling the next one.</p><p>That architecture was simple and horizontally scalable (excluding database scalability considerations), but it had drawbacks. The process introduced considerable overhead due to polling intervals and state loading. The time spent in one polling cycle on distributed queues, loading complete states, and other DB queries was significant.</p><p>As Maestro engine decomposes complex workflow graphs into multiple flows, actions might involve multiple flows spanning multiple polling cycles, adding up to significant overhead (around ten seconds in the worst cases). Also, this design didn’t offer strong execution guarantees mainly because the distributed job queue could only provide at-least-once guarantees. Tasks might be dequeued and dispatched to multiple workers, workers might reset states in certain race conditions, or load stale states of other tasks and make incorrect decisions. For example, after a long garbage-collection pause or network hiccup, two workers can pick up the same task: one sets the task status as completed and then unblocks the downstream steps to move forward. However, the other worker, working off stale state, resets the task status back to running, leaving the whole workflow in a conflicting state.</p><p>In the new design, we developed a stateful actor model, keeping internal states in memory. All tasks of a workflow are collocated in the same Maestro node, providing the best performance as states are in the same JVM.</p><h4>Actor-Based Model</h4><p>The new flow engine fits well into an actor model. We also deliberately designed it to allow sharing certain local states (read-only) between parent, child, and sibling actors. This optimization gains performance benefits without losing thread safety due to Maestro’s use cases. We used Java 21’s virtual thread support to implement it with minimal dependencies.</p><p>The new actor-based flow engine is fully message/event-driven and can take actions immediately when events are received, eliminating polling interval delays. To maintain compatibility with the existing polling-based logic, we developed a wakeup mechanism. This model requires flow actors and their child task actors to be collocated in the same JVM for communication over the in-memory queue. Since the Maestro engine already decomposes large-scale workflow instances into many small flows, each flow has a limited number of tasks that fit well into memory.</p><p>Below is a high-level overview of the Maestro execution flow based on the actor model.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*acx3i0wXvowwOm0t0ufOHA.png\" /><figcaption>Figure 7. The high level overview of the Maestro execution.</figcaption></figure><ul><li>When a workflow starts or during reconciliation, the flow engine inserts (if not existing) or loads the Maestro workflow and step instance from the database, transforming it into the internal flow and task state. This state remains in JVM memory until evicted (e.g., when the workflow instance reaches a terminal state).</li><li>A virtual thread is created for each entity (workflow instance or step attempt) as an actor to handle all updates or actions for this entity, ensuring thread safety and eliminating distributed locks and potential race conditions.</li><li>Each virtual thread actor contains an in-memory state, a thread-safe blocking queue, and a state machine to update states, ensuring thread safety and high efficiency.</li><li>Actors are organized hierarchically, with flow actors managing all their task actors. Flow actors and their task actors are kept in the same JVM for locality benefits, with the ability to relocate flow instances to other nodes if needed.</li><li>An event can wake up a virtual thread by pushing a message to the actor’s job queue, enabling Maestro to move toward an event-driven approach alongside the current polling-based approach.</li><li>A reconciliation process transforms the Maestro data model into the internal flow data.</li></ul><h4>Virtual Thread Based Implementation</h4><p>We chose Java virtual threads to implement various actors (e.g. group actors and flow actors), which simplified the actor model implementation. With a smaller amount of code, we developed a fully functional and highly performant event-driven distributed flow engine. Virtual threads fit very well in use cases like state machine transitions within actors. They are lightweight enough to be created in a large number without Out-Of-Memory risks.</p><p>However, virtual threads can potentially deadlock. They’re not suitable for executing user-provided logic or complex step runtime logic that might depend on external libraries or services outside our control. To address this, we separate flow engine execution from task execution logic by adding a separate worker thread pool (not virtual threads) to run actual step runtime business logic like launching containers or making external API calls. Flow/task actors can <a href=\"https://github.com/Netflix/maestro/blob/main/maestro-flow/src/main/java/com/netflix/maestro/flow/engine/ExecutionContext.java#L96-L100\">wait indefinitely for the future of the thread poll executor to complete</a> but don’t perform actual execution, allowing us to benefit from virtual threads while avoiding deadlock issues.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ulNZD0dkW4Tj50OaQ0e8aw.png\" /><figcaption>Figure 8. Virtual thread and worker thread separation.</figcaption></figure><h4>Providing Strong Execution Guarantees</h4><p>To provide strong execution guarantees, we implemented a generation ID-based solution to ensure that a single flow or task is executed by only one actor at any time, with states that never roll back and eventually reach a terminal state.</p><p>When a node claims a new group or a group with an expired heartbeat, it updates the database table row and increments the group generation ID. During node bootstrap, the group actor updates all its owned flows’ generation IDs while rebuilding internal flow states. When creating a new flow, the group actor verifies that the database generation ID matches its in-memory generation ID, otherwise rejecting the creation and reporting a retryable error to the caller. Please check <a href=\"https://github.com/Netflix/maestro/blob/main/maestro-flow/src/main/java/com/netflix/maestro/flow/dao/MaestroFlowDao.java\">the source code</a> for the implementation details.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UexwsCGLLE-lrzV3HVzCEg.png\" /><figcaption>Figure 9. An example sequence diagram showing how generation id provides a strong guarantee.</figcaption></figure><p>Additionally, the new flow engine supports both event-driven execution and polling-based periodic reconciliation. Event-driven support allows us to extend polling intervals for state reconciliation at a very low cost, while polling-based reconciliation relaxes event delivery requirements to at-most-once.</p><h3>Testing, Validation and Rollout</h3><p>Migrating hundreds of thousands of Netflix data processing jobs to a new workflow engine required meticulous planning and execution to avoid data corruption, unexpected traffic patterns, and edge cases that could hinder performance gains. We adopted a principled approach to ensure a smooth transition:</p><ol><li><strong>Realistic Testing:</strong> Our testing mirrored real-world use cases as closely as possible.</li><li><strong>Balanced Approach:</strong> We balanced the need for rapid delivery with comprehensive testing.</li><li><strong>Minimal User Disruption:</strong> The goal was for users to be unaware of the underlying changes.</li><li><strong>Clear Communication:</strong> For cases requiring user involvement, clear communication was provided.</li></ol><h4>Maestro Test Framework</h4><p>To achieve our testing goals, we developed an adaptable testing framework for Maestro. This framework addresses the limitations of static unit and integration tests by providing a more dynamic and comprehensive approach, mimicking organic production traffic. It complements existing tests to instill confidence when rolling out major changes, such as new DAG engines.</p><p>The framework is designed to sample real user workflows, disconnecting business logic from external side effects like data reads or writes. This allows us to run workflow graphs of various shapes and sizes, reflecting the diverse use cases across Netflix. While system integrations are handled through deployment pipeline integration tests, the ability to exercise a wide variety of workflow topologies (e.g., parallel executions, for-each jobs, conditional branching and parameter passing between jobs) was crucial for ensuring the new flow engine’s correctness and performance.</p><p>The prototype workflow for the test framework focuses on auto-testing parameters, involving two main steps:</p><p><strong>1. Caching Production Workflows:</strong></p><ul><li>Successful production instances are queried from a historical Maestro feed table over a specified period.</li><li>Run parameters, initiator, and instance IDs are extracted and organized into an instance data map.</li><li>YAML definitions and subworkflow IDs are pulled from S3 storage.</li><li>Both workflow definitions and instance data are cached on S3 for subsequent steps.</li></ul><p><strong>2. Pushing, Running, and Monitoring Workflows:</strong></p><ul><li>Cached workflow definitions and instance data are loaded.</li><li>Notebook-based jobs are replaced with custom notebooks, and certain job types (e.g., vanilla container runtime jobs, templated data movement jobs) and signal triggers are converted to a special no-op job type or skipped.</li><li>Abstract job types like Write-Audit-Publish are expressed as a single step template but are translated to multiple reified nodes of the DAG when executed. These are auto-translated into several custom notebook job types to replace the generated nodes.</li><li>Workflows and subworkflows are pushed, with only non-subworkflows being run using original production instance information.</li><li>1. In the parent workflow, each sub-workflow is replaced with a special no-op placeholder so that the overall topology is preserved but without executing any side-effects of child workflows and avoid cases using dynamic runtime parameter logic.</li><li>2. Each sub-workflow is then separately treated like a top-level parent workflow not initiated from its parent, to exercise the actual workflow steps of the sub-workflow.</li><li>The custom notebook internally compares all passed parameters for each job.</li><li>Workflow instances are monitored until termination (success or failure).</li><li>An email detailing failed workflow instances is generated.</li></ul><p>Future phases of the test framework aim to expand support for native steps, more templates, Titus and Metaflow workflows, and include more robust signal testing. Further integration with the ecosystem, including dedicated Genie clusters for no-op jobs and DGS for our internal workflow UI feature verification, is also being explored.</p><h4>Rollout Plan</h4><p>Our rollout strategy prioritized minimal user disruption. We determined that an entire workflow, from its root instance, must reside in either the old or new flow engine, preventing mixed operations that could lead to complex failure modes and manual data reconciliation.</p><p>To facilitate this, we established a parallel infrastructure for the new workflow engine and leveraged our orchestrator gateway API to hide any routing or redirection logic from users. This approach provided excellent isolation for managing the migration. Initially, specific workflows could explicitly opt in via a system flag, allowing us to observe their execution and gain confidence. By scaling up traffic to the parallel infrastructure in direct proportion to what was scaled down from the original infrastructure, the dual infrastructure cost increase was negligible.</p><p>Once confident, we transitioned to a percentage-based cutover. In the event of a sustained failure in the new engine, our team could roll back a workflow by removing it from the new engine’s database and restarting it in the original stack. However, one consequence of rollback was that failed workflows had to restart from the beginning, recomputing previously successful steps, to ensure all artifacts were generated from a consistent flow engine.</p><p>Leveraging Maestro’s 10-day workflow timeout, we migrated users without disruption. Existing executions would either complete or time out. Upon restarting (due to failure/timeout) or triggering a new instance (due to success), the workflow would be picked up by the new engine. This effectively allowed us to gradually “drain” traffic from the old engine to the new one with no user involvement.</p><p>While the plan generally proceeded as expected with limited edge cases, we did encounter a few challenges:</p><ul><li><strong>Stuck Workflows:</strong> Around 50 workflows with defunct or incorrect ownership information entered a stuck state. In some cases, a backlog of queued instances behind a stuck instance created a race condition in which a new instance would be started immediately when an old instance was terminated, perpetually keeping the workflow on the old engine. For these, we proactively contacted users to negotiate manual stop-and-restart times, forcing them onto the new engine.</li><li><strong>Configuration Discrepancies:</strong> A significant lesson learned was the importance of meticulous record-keeping and management of parallel infrastructure components. We discovered alerts, system flags, and feature flags configured for one stack but not the other. This led to a failure in a partner team’s system that dynamically rolled out a Python migration by analyzing workflow configurations. The absence of a required feature flag in the new engine stack caused the process to be silently skipped, resulting in incorrect Python version configurations for about 40 workflows. Although quickly remediated, this caused user inconvenience as affected workflows needed to be restarted and verified for no lingering data corruption issues. This issue also highlighted limitations in the testing framework since runtime configuration based on external API calls to the configuration service were not exercised in simulated workflow executions.</li></ul><p>Despite these challenges, the migration was a success. We migrated over 60,000 active workflows generating over a million data processing tasks daily with almost no user involvement. By observing the flow engine’s lifecycle management latency, we validated a reduction in step launch overhead from around 5 seconds to 50 milliseconds. Workflow start overhead (incurred once per each workflow execution) also improved from 200 milliseconds to 50 milliseconds. Aggregating this over a million daily step executions translates to saving approximately 57 days of flow engine overhead per day, leading to a snappier user experience, more timely workflow status for data practitioners and greater overall task throughput for the same infrastructure scale.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/826/1*63TFS2uVWjXxlZGpHhTpoA.jpeg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/874/1*yxfbCcs1Y2h4dpxJ5hZkDA.jpeg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*6Sq7ozdrxqeHI56SvejJkw.png\" /></figure><p>We additionally realized significant benefits internally with reduced maintenance effort due to the new flow engine’s simplified set of database components. We were able to delete nearly 40TB of obsolete tables related to the previous stateless flow engine and saw a 90% reduction in internal database query traffic which had previously been a significant source of system alerts for the team.</p><h3>Conclusion</h3><p>The architectural evolution of Maestro represents a significant leap in performance, reducing overhead from seconds to milliseconds. This redesign with a stateful actor model not only enhances speed by 100X but also maintains scalability and reliability, ensuring Maestro continues to meet the diverse needs of Netflix’s data and ML workflows.</p><p>Key takeaways from this evolution include:</p><ul><li><strong>Performance matters:</strong> Even in a system designed for scale, the speed of individual operations significantly impacts user experience and productivity.</li><li><strong>Simplicity wins:</strong> Reducing dependencies and simplifying architecture not only improved performance but also enhanced reliability and maintainability.</li><li><strong>Strong guarantees are essential:</strong> Providing strong execution guarantees eliminates race conditions and edge cases that previously required manual intervention.</li><li><strong>Locality optimizations pay off:</strong> Collocating related flows and tasks in the same JVM dramatically reduces overhead from the Maestro engine.</li><li><strong>Modern language features help:</strong> Java 21’s virtual threads enabled an elegant actor-based implementation with minimal code complexity and dependencies.</li></ul><p>We’re excited to share these improvements with the open-source community and look forward to seeing how Maestro continues to evolve. The performance gains we’ve achieved open new possibilities for low-latency workflow orchestration use cases while continuing to support the massive scale that Netflix and other organizations require.</p><p>Visit the <a href=\"https://github.com/Netflix/maestro\">Maestro GitHub repository</a> to explore these improvements. If you have any questions, thoughts, or comments about Maestro, please feel free to create a <a href=\"https://github.com/Netflix/maestro/issues\">GitHub issue</a> in the Maestro repository. We are eager to hear from you. If you are passionate about solving large scale orchestration problems, please <a href=\"https://explore.jobs.netflix.net/careers?query=Data%20Platform&amp;Teams=Engineering&amp;domain=netflix.com&amp;sort_by=relevance\">join us</a>.</p><h3>Acknowledgements</h3><p>Special thanks to Big Data Orchestration team members for general contributions to Maestro and diligent review, discussion and incident response required to make this project successful: Davis Shepherd, Natallia Dzenisenka, Praneeth Yenugutala, Brittany Truong, Jonathan Indig, Deepak Ramalingam, Binbing Hou, Zhuoran Dong, Victor Dusa, and Gabriel Ikpaetuk — and and internal partners Yun Li and Romain Cledat.</p><p>Thank you to Anoop Panicker and Aravindan Ramkumar from our partner organization that leads Conductor development in Netflix. They helped us understand issues in Conductor 2.X that initially motivated the rearchitecture and helped provide context on later versions of Conductor that defined some of the core trade-offs for the decision to implement a custom DAG engine in Maestro.</p><p>We’d also like to thank our partners on the Data Security &amp; Infrastructure and Engineering Support teams who helped identify and rapidly fix the configuration discrepancy error encountered during production rollout: Amer Hesson, Ye Ji, Sungmin Lee, Brandon Quan, Anmol Khurana, and Manav Garekar.</p><p>A special thanks also goes out to partners from the Data Experience team including Jeff Bothe, Justin Wei, and Andrew Seier. The flow engine speed improvement was actually so dramatic that it broke some integrations with our internal workflow UI that reported state transition durations. Our partners helped us catch and fix UI regressions before they shipped to avoid impact to users.</p><p>We also thank Prashanth Ramdas, Anjali Norwood, Eva Tse, Charles Zhao, Sumukh Shivaprakash, Joey Lynch, Harikrishna Menon, Marcelo Mayworm, Charles Smith and other leaders for their constructive feedback and guidance on the Maestro project.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=028e9637f041\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041\">100X Faster: How We Supercharged Netflix Maestro’s Workflow Engine</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "date": "2025-09-29T16:10:40.000Z",
    "url": "https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041?source=rss----2615bd06b42e---4"
  }
]